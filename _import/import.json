{"db":[{"meta":{"exported_on":1587247477091,"version":"3.12.1"},"data":{"posts":[{"id":"7a05b039-c6bc-41d5-9867-7e1deed48020","title":"2012 Lab Management Standard Environment: Configuring UI Tests Agent Identity Problem","slug":"2012-lab-management-standard-environment-configuring-ui-tests-agent-identity-problem","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-pD5HLC_xO4Y/T_7JwU6rnDI/AAAAAAAAAas/33oOZUb7460/s1600-h/image%25255B3%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-ggttrGGYagY/T_7JyOfi9BI/AAAAAAAAAa0/-jYoOIkmSac/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"405\\\" height=\\\"266\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-HpXy8l2aE9k/T_7JzVqxk-I/AAAAAAAAAa8/QQ3eBOwgTyE/s1600-h/image%25255B7%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-1XfyWDeKwvc/T_7J014xXpI/AAAAAAAAAbA/1pBJyNGZIpo/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"545\\\" height=\\\"184\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Verify that the machines are accessible using the user name and password provided. The credentials provided for configuring the test agent to run as process are not valid. Provide valid credentials.</font>\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"I am a huge fan of Lab Management. Being able to manage test rigs centrally (and, if you’re using Hyper-V, self-provisioning and self-servicing) is a huge productivity bonus.\"]]],[1,\"p\",[[0,[],0,\"I wanted to test out a Standard Environment (what used to be called Physical Environment in TFS 2010). I am using Brian Keller’s RC VM and another VM (WebTest) for my test machine, both of which are workgrouped (and not connected to any domain).\"]]],[1,\"p\",[[0,[],0,\"When configuring the environment, I came across a rather strange error. One of the wizard screens when you create a Standard Environment allows you to configure the environment for UI testing (meaning that the test agent in your rig will run in interactive mode). The dialog requests that you specify the username and password for the test agent. I assumed that this would require an account on the test machine, so I specified a local account (i.e. a WebTest account).\"]]],[10,0],[1,\"p\",[[0,[],0,\"Unfortunately, that failed verification with the following error:\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"That didn’t make much sense to me, since I was providing credentials that I was absolutely positive worked on the test machine.\"]]],[1,\"p\",[[0,[],0,\"After trying various things and searching online, I realized that you need the account for the Test Agent to be a shadow account of the one on TFS (i.e. same name and password on both machines) when you’re on a workgroup. If both machines are on a domain, you just need to make sure the domain user for the Test Agent is an admin on the test machine. Oh, and if you’re going to use the build-deploy-test workflow, make sure that identity also has read-access onto your drops folder!\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1342130280000,"status":"published","published_by":1},{"id":"766de1fc-9caa-4086-aa6f-440f6b95c40f","title":"A Day of DevOps, Release Management, Software Quality and Agile Project Requirements Management","slug":"a-day-of-devops-release-management-software-quality-and-agile-project-requirements-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2014/09/01/a-day-of-devops-release-management-software-quality-and-agile-project-requirements-management-cape-town-and-johannesburg.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Unfortunately there is no TechEd Africa this year – Microsoft have opted to go for smaller, more focused sessions in multiple cities (or at least that’s what I gather). I think it’s a shame, since the TechEd Africa event was always fantastic – and who doesn’t like getting out the office for a couple of days?\"]]],[1,\"p\",[[0,[],0,\"Anyway, the good news is that Microsoft are hosting “A Day of DevOps, Release Management, Software Quality and Agile Project Requirements Management” in Cape Town (Wed 10th at Crystal Towers Hotel) and in Johannesburg (on Mon 15th at the Microsoft offices on William Nicol). I’ll be presenting at both events, so make sure you get along.\"]]],[1,\"p\",[[0,[],0,\"The event has 2 themes – DevOps in the morning and Agile Project Management in the afternoon. You can attend one or the other or both events.\"]]],[1,\"p\",[[0,[],0,\"I’m particularly excited about the DevOps session – here’s some of the content that I’ll be covering:\"]]],[3,\"ul\",[[[0,[],0,\"Release management and automation, release pipelines and approvals to accelerate deployment to operations, including using DSC, Chef and Puppet\"]],[[0,[],0,\"Treating Configuration as Code\"]],[[0,[],0,\"Application Insights\"]],[[0,[],0,\"Cloud Based Load Testing\"]],[[0,[],0,\"Production Debugging and Monitoring\"]],[[0,[],0,\"Leveraging Azure for DevOps and Dev/Test Environments\"]],[[0,[],0,\"System Centre and TFS Integration\"]]]],[1,\"p\",[[0,[],0,\"For more details, go to the Microsoft SA Developer blog post \"],[0,[0],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Hope to see you there!\"]]]]}","published_at":1409755402000,"status":"published","published_by":1},{"id":"0782b46c-8d2c-44cf-bcf1-d8b127aefd6d","title":"A/B Testing with Azure Linux Web Apps for Containers","slug":"ab-testing-with-azure-linux-web-apps-for-containers","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"public void Configure(IApplicationBuilder app, IHostingEnvironment env)\\n{\\n\\tvar aiKey = Environment.GetEnvironmentVariable(\\\"AIKey\\\");\\n\\tif (aiKey != null)\\n\\t{\\n\\t\\tTelemetryConfiguration.Active.InstrumentationKey = aiKey;\\n\\t}\\n\\t...\",\"language\":\"csharp; highlight:[3,4,5,6,7]\"}],[\"code\",{\"code\":\"FROM microsoft/aspnetcore:2.0\\nARG source\\nENV AIKey=\\\"11111111-2222-3333-4444-555555555555\\\"\\n\\nWORKDIR /app\\nEXPOSE 80\\nCOPY ${source:-obj/Docker/publish} .\\nENTRYPOINT [\\\"dotnet\\\", \\\"DockerWebApp.dll\\\"]\",\"language\":\"plain; hightlight:[3]\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8a0b3fdd-a4a8-4a6f-ae23-5649417b4c83.png\\\" target=\\\"_blank\\\"><img width=\\\"347\\\" height=\\\"263\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2a103cde-cf8f-4d39-8565-1df1c6e187f3.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3e4272e8-7adf-41e2-a380-4788789ff1f9.png\\\" target=\\\"_blank\\\"><img width=\\\"426\\\" height=\\\"219\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/95525cce-9e64-4c6e-a5d9-fde55dac1c61.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5a5c5588-fe1e-40c7-a1d0-6672737bfde1.png\\\" target=\\\"_blank\\\"><img width=\\\"415\\\" height=\\\"300\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ef7b3671-201b-48c8-995b-80028b8d9046.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/18b715fa-27e6-4c4a-b89f-9c5a017e1d2e.png\\\" target=\\\"_blank\\\"><img width=\\\"406\\\" height=\\\"384\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b02e400d-4af8-42c8-b947-a83bc5794d31.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0dbd78d7-a08c-40c9-94ba-305ca4066e9c.png\\\" target=\\\"_blank\\\"><img width=\\\"420\\\" height=\\\"307\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f5eeee80-61a1-4fcf-97d4-277861558d27.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/73f4e603-a868-412f-a26c-b7a3ce51edbc.png\\\" target=\\\"_blank\\\"><img width=\\\"421\\\" height=\\\"261\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4e176ba2-5e46-4121-86f7-242e8b355a0f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3de86305-5a1b-4ff6-a8ce-ec567947e717.png\\\" target=\\\"_blank\\\"><img width=\\\"478\\\" height=\\\"275\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/64c0c13f-9a27-4d89-b38f-193846afaec7.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8ced880e-bc29-4770-b14a-c1b50d8042da.png\\\" target=\\\"_blank\\\"><img width=\\\"492\\\" height=\\\"248\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/439783a7-6a46-468f-8b8b-d8621fbdd9df.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/50fe6bda-a6b5-4e3c-b479-e37aaf333dd6.png\\\" target=\\\"_blank\\\"><img width=\\\"380\\\" height=\\\"136\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9b448ff9-539c-4bf6-a007-b7edc19603d9.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/858b81dc-c719-4832-a009-b63d3ca20416.png\\\" target=\\\"_blank\\\"><img width=\\\"357\\\" height=\\\"105\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d0aecb91-19fb-4b39-8d0d-9995c8a5cd99.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/13c0be08-5355-4e4c-a067-80ab2efe56a8.png\\\" target=\\\"_blank\\\"><img width=\\\"387\\\" height=\\\"226\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/779f16ca-87d4-45ab-90c3-84e1358fefbf.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/66997232-c4f5-45a1-802f-5d63604e11ea.png\\\" target=\\\"_blank\\\"><img width=\\\"334\\\" height=\\\"364\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1d9772ab-a2e9-43ac-bddf-e18a8eff7feb.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0a5d48e2-ad22-46db-b6d6-7b4a3f598fc6.png\\\" target=\\\"_blank\\\"><img width=\\\"366\\\" height=\\\"239\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/09684f33-830e-418c-a3dc-8a95a6c1f6b5.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/26f4df71-8743-4b1d-bd13-7261413060bd.png\\\" target=\\\"_blank\\\"><img width=\\\"376\\\" height=\\\"249\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/820c349d-32e7-4851-9710-e322b02d09a2.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7dd477d9-ed1e-4442-a000-a5ac22bc8b1d.png\\\" target=\\\"_blank\\\"><img width=\\\"375\\\" height=\\\"259\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8235b58b-1c46-4172-baff-fc5c5bf5fcc5.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c8d5c9a3-897d-44d9-be70-3b6b650b0a01.png\\\" target=\\\"_blank\\\"><img width=\\\"376\\\" height=\\\"254\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/069ece53-f0ba-4583-bf4b-98127939a94c.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8df49df4-2893-42bf-ab8c-8eff10894150.png\\\" target=\\\"_blank\\\"><img width=\\\"387\\\" height=\\\"151\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ecd70eaa-33ea-471f-9c6e-f0c8069127b8.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/74b01145-f53a-4e6d-847e-aa34b2964ae1.png\\\" target=\\\"_blank\\\"><img width=\\\"416\\\" height=\\\"97\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4da52247-525f-440d-aa32-0d02c03acbfa.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b0d66643-af3e-4af4-ad29-1bf0bd5d4801.png\\\" target=\\\"_blank\\\"><img width=\\\"386\\\" height=\\\"101\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8c54816a-54f5-40d6-a6f3-6f2c8e5e7185.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d67d3002-626b-43ff-91e9-42988cc3d89a.png\\\" target=\\\"_blank\\\"><img width=\\\"353\\\" height=\\\"193\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8c8df481-3052-486f-9cc8-fe2511ad4676.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a9abe499-050b-4bf5-b201-16151428d026.png\\\" target=\\\"_blank\\\"><img width=\\\"344\\\" height=\\\"246\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0dbd4a08-043c-4bfa-8963-495ca3c8749c.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f0398045-8773-403b-a629-a948fbd5122d.png\\\" target=\\\"_blank\\\"><img width=\\\"362\\\" height=\\\"195\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/49873b1f-da8f-473a-bdd6-57d4cbc46ee8.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://kubernetes.io/\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/swarm/\"]],[\"a\",[\"href\",\"https://dcos.io/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/container-instances/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/app-service/containers/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/webapplinux-abtesting\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/application-insights\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/container-registry/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/webapplinux-abtesting/blob/master/DockerWebApp.ARM/azuredeploy.json\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/RouteTraffic\"]],[\"a\",[\"href\",\"https://bit.ly/cacbuildtasks\"]],[\"a\",[\"href\",\"https://launchdarkly.com/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I love containers. I've said before that I think they're the future. Just as hardly anyone installs on tin any more since we're so comfortable with Virtualization, I think that in a few years time hardly anyone will deploy VMs - we'll all be on containers. However, container orchestration is still a challenge. Do you choose \"],[0,[0],1,\"Kubernetes\"],[0,[],0,\" or \"],[0,[1],1,\"Swarm\"],[0,[],0,\" or \"],[0,[2],1,\"DCOS\"],[0,[],0,\"? (For my money I think Kubernetes is the way to go). But that means managing a cluster of nodes (VMs). What if you just want to deploy a single container in a useful manner?\"]]],[1,\"p\",[[0,[],0,\"You can do that now using \"],[0,[3],1,\"Azure Container Instances\"],[0,[],0,\" (ACI). You can also host a container in an \"],[0,[4],1,\"Azure Web App for Containers\"],[0,[],0,\". The Web App for Containers is what I'll use for this post - mostly because I already know how to do A/B testing with Azure Web Apps, so once the container is running then you get all the same paradigms as you would for \\\"conventional\\\" web apps - like slots, app settings in the portal etc.\"]]],[1,\"p\",[[0,[],0,\"In this post I'll cover publishing a .NET Core container to Azure Web Apps using VSTS with an ARM template. However, since the hosting technology is \\\"container\\\" you can host whatever application you want - could be Java/TomCat or node.js or python or whatever. The A/B Testing principles will still apply.\"]]],[1,\"p\",[[0,[],0,\"You can grab the code for this demo from this \"],[0,[5],1,\"Github repo\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Overview of the Moving Parts\"]]],[1,\"p\",[[0,[],0,\"There are a couple of moving parts for this demo:\"]]],[3,\"ul\",[[[0,[],0,\"The source code. This is just a File->New Project .NET Core 2.0 web app. I've added a couple lines of code and \"],[0,[6],1,\"Application Insights\"],[0,[],0,\" for monitoring - but other than that there's really nothing there. The focus of this post is about how to A/B test, not how to make an app!\"]],[[0,[],0,\"Application Insights - this is how you can monitor the web app to make sure\"]],[[0,[],0,\"An \"],[0,[7],1,\"Azure Container Registry\"],[0,[],0,\" (ACR). This is a private container repository. You can use whatever repo you want, including DockerHub.\"]],[[0,[],0,\"A VSTS Build. I'll show you how to set up a build in VSTS to build the container image and publish it to the ACR.\"]],[[0,[],0,\"An ARM template. This is the definition of the resources necessary for running the container in Azure Web App for Containers. It includes a staging slot and Application Insights.\"]],[[0,[],0,\"A VSTS Release. I'll show you how to create a release that will spin up the web app and deploy the container. Then we'll set up Traffic Manager to (invisibly) divert a percentage of traffic from the prod slot to the staging slot - this is the basis for A/B testing and the culmination of the all the other steps.\"]]]],[1,\"h2\",[[0,[],0,\"The Important Source Bits\"]]],[1,\"p\",[[0,[],0,\"Let's take a look at some of the important code files. Firstly, the Startup.cs file:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3: I read the environment variable \\\"AIKey\\\" to get the Application Insights key\"]],[[0,[],0,\"Lines 4 - 7: If there is a key, then I set the key for the Application Insights config\"]]]],[1,\"p\",[[0,[],0,\"The point here is that for configuration, I want to get values from the environment. This would include database connection strings etc. Getting them from the environment lets me specify them in the Web App appSettings so that I don't have to know the values at build time - only at release time.\"]]],[1,\"p\",[[0,[],0,\"Let's look at the Dockerfile:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3 was the only thing I added to make the AIKey configurable as an environment variable\"]]]],[1,\"p\",[[0,[],0,\"Finally, let's look at the ARM template that defines the resources. The file is too large to paste here and it's Json so it's not easy to read. You can have a look at the \"],[0,[8],1,\"file\"],[0,[],0,\" yourself. The key points are:\"]]],[3,\"ul\",[[[0,[],0,\"a HostingPlan with kind = \\\"linux\\\" and properties.reserved = true. This creates a Linux app plan.\"]],[[0,[],0,\"a Site with properties.siteConfig.appSettings that specify the DOCKER_REGISTRY_SERVER_URL, DOCKER_REGISTRY_SERVER_USERNAME, DOCKER_REGISTRY_SERVER_PASSWORD (using the listCredentials function) and AIKey. We do not specify the DOCKER_CUSTOM_IMAGE_NAME for reasons that will be explained later. Any other environment variables (like connection strings) could be specified here.\"]],[[0,[],0,\"a staging slot (called blue) that has the same settings as the production slot\"]],[[0,[],0,\"a slotconfignames resource that locks the DOCKER_CUSTOM_IMAGE_NAME so that the value is slot-sticky\"]],[[0,[],0,\"an Application Insights resource - the key for this resource is referenced by the appSettings section for the site and the slot\"]]]],[1,\"h2\",[[0,[],0,\"Building (and Publishing) Your Container\"]]],[1,\"p\",[[0,[],0,\"At this point we can look at the build. The build needs to compile, test and publish the .NET Core app and build a container. It then needs to publish the container to the ACR (or whatever registry you created).\"]]],[1,\"p\",[[0,[],0,\"Create a new build using the ASP.NET Core Web App template and then edit the steps to look as follows:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Make sure you point the Source settings to the repo (you can create a new one and import mine from the \"],[0,[5],1,\"Github URL\"],[0,[],0,\" if you want to try this yourself). I then changed the queue to Hosted Linux Preview and changed the name to something appropriate.\"]]],[1,\"p\",[[0,[],0,\"On the Options page I set the build number format to 1.0$(rev:.r) which gives a 1.0.0, 1.0.1, 1.0.2 etc. format for my build number.\"]]],[1,\"p\",[[0,[],0,\"Click on the Build task and set the arguments to --configuration $(BuildConfiguration) /p:Version=$(Build.BuildNumber). This versions the assemblies to match the build number.\"]]],[1,\"p\",[[0,[],0,\"Click on the Publish task and set Publish Web Apps, change the arguments to --configuration $(BuildConfiguration) --output $(Build.ArtifactStagingDirectory) /p:Version=$(Build.BuildNumber) and unselect Zip files. This puts the output files into the artifact staging directory and doesn't zip them. It also publishes with a version number matching the build number:\"]]],[10,3],[1,\"p\",[[0,[],0,\"I deleted the Test task since I don't have tests in this simple project - but you can of course add testing in before publishing.\"]]],[1,\"p\",[[0,[],0,\"I then deleted the Publish build artifact tasks since this build won't be publishing an artifact - it will be pushing a container image to my ACR.\"]]],[1,\"p\",[[0,[],0,\"In order to build the docker container image, I first need to put the Dockerfile at the correct location relative to the output. So I add a Copy Files task in and configure it to copy the Dockerfile to the artifact staging directory:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now I add a 2 Docker tasks: the has a Build an image Action. I set the ACR by selecting the settings from the dropdowns. I set the path to the Dockerfile and specify \\\"DockerWebApp\\\" as the source build argument (the Publish task will have places the compiled site and content into this folder in the artifact staging directory). I set Qualify the Image name to correctly tag the container with the ACR prefix and I include the Latest tag in the build (so the current build is always the Latest).\"]]],[10,5],[1,\"p\",[[0,[],0,\"The 2nd Docker task has Action set to Publish an Image. I set the ACR like the Docker Build task. I also change the image name to $(Build.Repository.Name):$(Build.BuildNumber) instead of $(Build.Repository.Name):$(Build.BuildId) and I set the Latest tag.\"]]],[10,6],[1,\"p\",[[0,[],0,\"Now I can run the build. Lots of green! I can also see the image in my ACR in the Azure portal:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Woot! We now have a container image that we can host somewhere.\"]]],[1,\"h2\",[[0,[],0,\"Releasing the Container\"]]],[1,\"p\",[[0,[],0,\"Now that we have a container and an infrastructure template, we can define a release. Here's what the release looks like:\"]]],[10,8],[1,\"p\",[[0,[],0,\"There are 2 incoming artifacts: the build and the Git repo. The build doesn't actually have any artifacts itself - I just set the build as the trigger mechanism. I specify a \\\"master\\\" artifact filter so that only master builds trigger this release. The Git repo is referenced for the deployment scripts (in this case just the ARM template). I started with an empty template and then changed the Release number format to $(Build.BuildNumber)-$(rev:r) in the Options page.\"]]],[1,\"p\",[[0,[],0,\"There are 3 environments: Azure blue, Azure prod and blue failed. These are all \\\"production\\\" environments - you could have Dev and Staging environments prior to these environments. However, I want to A/B test in production, so I'm just showing the \\\"production environment\\\" here.\"]]],[1,\"p\",[[0,[],0,\"Let's look at the Azure blue environment:\"]]],[10,9],[1,\"p\",[[0,[],0,\"There are 3 tasks: Azure Resource Group Deployment (to deploy the ARM template), an Azure CLI command to deploy the correct container, and a Traffic Manager Route Traffic task. The Azure Resource Group Deployment task specifies the path to the ARM template and parameters files as well as the Azure endpoint for the subscription I want to deploy to. I specify a variable called $(RGName) for the resource group name and then override the parameters for the template using $(SiteName) for the name of the web app in Azure, $(ImageName) for the name of the container image, $(ACR) for the name of my ACR and $(ACRResourceGroup) for the name of the resource group that contains my ACR. Once this task has run, I will have the following resources in the resource group:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Let's take a quick look at the app settings for the site:\"]]],[10,11],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"At this point the site (and slot) are provisioned, but they still won't have a container running. For that, we need to specify which container (and tag) to deploy. The reason I can't do this in the ARM template is because I want to update the staging slot and leave the prod slot on whatever container tag it is on currently. Let's imagine I specified \\\"latest\\\" for the prod slot - then when we run the template, the prod slot will update, which I don't want. Let's say we specify latest for the blue slot - then the blue slot will update - but what version do we specify in the template for the prod slot? We don't know that ahead of time. So to work around these issues, I don't specify the container tag in the template - I use an Azure CLI command to update it after the template has deployed (or updated) all the other infrastructure and settings.\"]]],[1,\"p\",[[0,[],0,\"To deploy a container, we add an Azure CLI task and specify a simple inline script:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Here I select version 1.* of the task (version 0.* doesn't let you specify an inline script). I set the script to sleep for 30 seconds - if I don't do this, then the site is still updating or something and the operation succeeds, but doesn't work - it doesn't actually update the image tag. I suspect that the update is async and so if you don't wait for it to complete, then issuing another tag change succeeds but is ignored. It's not pretty, but that's the only workaround I've found. After pausing for a bit, we invoke the \\\"az webapp config container set\\\" command, specifying the site name, slot name, resource group name and image name. (If you're running this phase on a Linux agent, use $1, $2 etc. for the args - if you're running this on a Windows agent then specify the args using %1, %2 etc. in the script). Then I pass the arguments in - using $(SiteName), blue, $(RGName) and $(ImageName):$(Build.BuildNumber) for the respective arguments.\"]]],[1,\"p\",[[0,[],0,\"If you now navigate to the site in the Azure portal and click on the Docker Container tab on the slot, you'll see the container settings:\"]]],[10,13],[1,\"p\",[[0,[],0,\"You can see that the image name and version are specified.\"]]],[1,\"p\",[[0,[],0,\"The final task in this environment (a \"],[0,[9],1,\"Route Traffic task\"],[0,[],0,\" from my \"],[0,[10],1,\"Build and Release extension pack\"],[0,[],0,\") adds a traffic manager rule - we divert 20% of traffic to the blue slot (unobtrusively to the client):\"]]],[10,14],[1,\"p\",[[0,[],0,\"There's a snag to this method: the first time you deploy, there's nothing (yet) in the prod slot. This is just a first time condition - so after you run this environment for the very first time, navigate to the Azure portal and click on the site. Then swap the blue and prod slots. Now the prod slot is running the container and the blue slot is empty. Repeat the deployment and now both slots have the latest version of the container.\"]]],[1,\"p\",[[0,[],0,\"Let's go back to the release and look at the Azure prod environment. This has a pre-approval set so that someone has to approve the deployment to this environment. The Azure blue has a post-deployment approver so that someone can sign off on the release - approved if the experiment works, rejected if it's not. The Azure prod environment triggers when the Azure blue environment is successful - all it has to do is swap the slots and reset the traffic router to reroute 100% of traffic to the prod slot:\"]]],[10,15],[10,16],[1,\"p\",[[0,[],0,\"So what do we do if the experiment fails? We can reject the Azure blue environment (so that the Azure prod environment doesn't run). We then manually run the blue fail environment - this just resets the traffic to route 100% back to prod. It does not swap the slots:\"]]],[10,17],[1,\"p\",[[0,[],0,\"Don't forget to specify values for the variables we used:\"]]],[10,18],[1,\"h2\",[[0,[],0,\"Running a Test\"]]],[1,\"p\",[[0,[],0,\"So imagine I have container version 1.0.43 in both slots. Now I make a change and commit and push - this triggers a build (I enabled CI) and we get version 1.0.45 (1.0.44 had an intermittent build failure). This triggers the release (I enabled CD) and now the blue slot has version 1.0.45 and 20% of traffic from the prod slot is going to the blue slot.\"]]],[10,19],[1,\"p\",[[0,[],0,\"Let's navigate to the blue and prod slots and see them side-by-side:\"]]],[10,20],[1,\"p\",[[0,[],0,\"The traffic routing is \\\"sticky\\\" to the user - so if a user navigates to the prod slot and gets diverted to the blue slot, then all requests to the site from the user go to the blue slot. Try opening some incognito windows and hitting the prod site - you'll get the blue content 20% of the time! You can also force the routing using a query parameter - just tack \\\"?x-ms-routing-name=blue\\\" onto the end of any request and you'll end up on the blue slot:\"]]],[10,21],[1,\"p\",[[0,[],0,\"Now you wait for users to generate traffic (or in my case, I totally fake client traffic - BWAHAHA!). So how do we know if the experiment is successful? We use Application Insights.\"]]],[1,\"h2\",[[0,[],0,\"Application Insights Analytics\"]]],[1,\"p\",[[0,[],0,\"Let's click on the Azure portal and go to the App Insights resource for our web app. We can see some telemetry, showing traffic to the site:\"]]],[10,22],[1,\"p\",[[0,[],0,\"But how do we know which requests went to the blue slot and how many went to the prod slot? It turns out that's actually pretty simple. Click the Analytics button to launch App Insights Analytics. We enter a simple query:\"]]],[10,23],[1,\"p\",[[0,[],0,\"We can clearly see that there is more usage of the 1.0.45 contact page. Yes, this metric is bogus - the point is to show you that you can slice by \\\"Application_Version\\\" and so you actually have metrics to determine if your new version (1.0.45) is better or worse that 1.0.43. Maybe it's more traffic to a page. Maybe it's more sales. Maybe it's less exceptions or better response time - all of these metrics can be sliced by Application_Version.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Deploying containers to Azure Web Apps for Containers is a great experience. Once you have the container running, you can use any Web App paradigm - such as Traffic Routing or appSettings - so it's easy if you've ever done any Web App deployments before.\"]]],[1,\"p\",[[0,[],0,\"There are a couple of key practices that are critical to A/B testing: telemetry and unobtrusive routing.\"]]],[1,\"p\",[[0,[],0,\"Telemetry is absolutely critical to A/B testing: if you can't decide if A is better (or worse) than B, then there's no point deploying both versions. Application Insights is a great tool for telemetry in general - but especially with A/B testing, since you can put some data and science behind your hypotheses. You don't have to use AppInsights - but you do have to have some monitoring tool or framework in order to even contemplate A/B testing.\"]]],[1,\"p\",[[0,[],0,\"The other key is how you release the A and B sites or apps. Having traffic manager seamlessly divert customer traffic is an excellent way to do this since the customer is none the wiser about which version they are seeing - so they don't have to change their URLs or anything obscure. You could also use \"],[0,[11],1,\"LaunchDarkly\"],[0,[],0,\" or some other feature flag mechanism - as long as your users don't have to change their usual way of accessing your app. This will give you \\\"real\\\" data. If users have to go to a beta site, they could change their behavior subconsciously. Maybe that isn't a big deal - but at least prefer \\\"seamless\\\" routing between A and B sites before you explicitly tell users to navigate to a new site altogether.\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1507974198000,"status":"published","published_by":1},{"id":"3d89a6d3-fdd0-47fa-8851-87e58d94b4e8","title":"Adding Custom Team Field to MS Project Mappings","slug":"adding-custom-team-field-to-ms-project-mappings","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"%programfiles%\\\\Common Files\\\\microsoft shared\\\\Team Foundation Server\\\\11.0\",\"language\":\"\"}],[\"code\",{\"code\":\"TFSFieldMapping download /collection:CollectionURL /teamproject:ProjectName /mappingfile:MappingFile\",\"language\":\"\"}],[\"code\",{\"code\":\"<mapping workitemtrakingfieldreferencename=\\\"”Custom.Team”\\\" projectfield=\\\"”pjTaskText27”\\\" projectname=\\\"”Team”\\\"></mapping>\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-HI8vfnFKN1U/Ud5eMMY-deI/AAAAAAAAA9Q/eLSwtugJHZk/s1600-h/image%25255B2%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-YO8b1UWFwBM/Ud5eMpZLwcI/AAAAAAAAA9Y/oTbq4HWqSz4/image_thumb.png?imgmax=800\\\" width=\\\"164\\\" height=\\\"244\\\"></a>\"}],[\"code\",{\"code\":\"TFSFieldMapping upload /collection:CollectionURL /teamproject:ProjectName /mappingfile:MappingFile\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-CJ4lX0LAfSs/Ud5eNKngoXI/AAAAAAAAA9g/AwkmimP_8eM/s1600-h/image%25255B6%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-OUcWBx6mOAc/Ud5eNqdzJpI/AAAAAAAAA9o/u0_OHkw4Uh8/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"135\\\" height=\\\"227\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-qFD3Fx38rQg/Ud5eOEar87I/AAAAAAAAA9w/H1tTlKehc3E/s1600-h/image%25255B9%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-Y_ROdhDDhKY/Ud5eOk5PYZI/AAAAAAAAA94/O6acWtbUUsc/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"196\\\" height=\\\"119\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://nakedalm.com/team-foundation-server-2012-teams-without-areas/\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/vstudio/dd286701.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/ms404684(v=vs.100).aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Note: This post is NOT about TFS and Project Server integration – this is for adding, editing and deleting work items using MS Project.\"]]],[1,\"p\",[[0,[],0,\"I was working with a customer who are using a custom field (called “Team”) instead of Area Path to denote “ownership” of work items to teams. You can find out how to do this on Martin Hinshelwood’s \"],[0,[0],1,\"excellent post\"],[0,[],0,\". So I did the customization (the team is working off MSF Agile, and not the Scrum template, but the steps are the same). Everything was looking great.\"]]],[1,\"p\",[[0,[],0,\"Then their project managers (who work a lot in MS Project) started \"],[0,[1],1,\"adding work using the MS Project\"],[0,[],0,\". However, once they added in the User Stories, the items did not appear on the Backlogs. I realized that this is because the Backlogs are filtered to Team – any work item that does not have the Team field set (when using a custom team field) won’t show for the team.\"]]],[1,\"p\",[[0,[],0,\"The solution: customize the Team Project “\"],[0,[2],1,\"TFSFieldMapping\"],[0,[],0,\"” – this is the file that controls how Work Item fields map to MS Project fields.\"]]],[1,\"h2\",[[0,[],0,\"Customize TFSFieldMapping\"]]],[1,\"p\",[[0,[],0,\"First, you have to download the mapping file. Open up a VS command prompt and navigate to this folder:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"(This is a different folder than previous versions of TFS for some strange reason).\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"Run the following command:\"]]],[1,\"p\",[[1,[],0,2]]],[10,1],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"where collectionURL is the URL to your collection, ProjectName is the name of your Team Project and MappingFile is the file you want to create.\"]]],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"Now edit the file in your favourite editor and add the following line:\"]]],[1,\"p\",[[1,[],0,5]]],[10,2],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"Of course the reference name is the reference name of your custom team field. The ProjectName attribute is a friendly name for the field – this will actually be the column name in Project. The project field is an unused field in the project plan (to see these, open Project and add a new column to the plan – the fields that display here are the fields that you need to use for the mapping – see the below picture).\"]]],[1,\"p\",[[1,[],0,7]]],[10,3],[1,\"p\",[[1,[],0,8]]],[1,\"p\",[[0,[],0,\"Now you can upload the mapping file again:\"]]],[1,\"p\",[[1,[],0,9]]],[10,4],[1,\"p\",[[1,[],0,10],[1,[],0,11]]],[1,\"p\",[[0,[],0,\"Now when you open up a project plan, you’ll be able to add the “Team” field:\"]]],[1,\"p\",[[1,[],0,12]]],[10,5],[1,\"p\",[[1,[],0,13]]],[1,\"p\",[[0,[],0,\"The column will even get its values from your global list:\"]]],[1,\"p\",[[1,[],0,14]]],[10,6],[1,\"p\",[[1,[],0,15]]],[1,\"p\",[[0,[],0,\"Now when you add work using MS Project, you can select the Team and the work will appear on the Backlogs accordingly. Of course this technique will work for any other fields too.\"]]],[1,\"p\",[[1,[],0,16]]],[1,\"p\",[[0,[],0,\"Happy Mapping!\"]]]]}","published_at":1373559960000,"status":"published","published_by":1},{"id":"b8336a26-79cb-40b1-87f5-6f1ad62bf89f","title":"AppInsights Analytics in the Real World","slug":"appinsights-analytics-in-the-real-world","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">Microsoft.ApplicationInsights.Extensibility.TelemetryConfiguration.Active.InstrumentationKey = WebConfigurationManager.AppSettings[\\\"AIKey\\\"];</font>\"}],[\"code\",{\"code\":\"private static string aiKey;\\npublic static string AIKey\\n{\\n    get\\n    {\\n        if (string.IsNullOrEmpty(aiKey))\\n        {\\n            aiKey = WebConfigurationManager.AppSettings.Get(\\\"AIKey\\\");\\n        }\\n        return aiKey;\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"&lt;script type=\\\"text/javascript\\\"&gt;\\n    var appInsights=window.appInsights||function(config){\\n        function s(config){t[config]=function(){var i=arguments;t.queue.push(function(){t[config].apply(t,i)})}}var t={config:config},r=document,f=window,e=\\\"script\\\",o=r.createElement(e),i,u;for(o.src=config.url||\\\"//az416426.vo.msecnd.net/scripts/a/ai.0.js\\\",r.getElementsByTagName(e)[0].parentNode.appendChild(o),t.cookie=r.cookie,t.queue=[],i=[\\\"Event\\\",\\\"Exception\\\",\\\"Metric\\\",\\\"PageView\\\",\\\"Trace\\\"];i.length;)s(\\\"track\\\"+i.pop());return config.disableExceptionTracking||(i=\\\"onerror\\\",s(\\\"_\\\"+i),u=f[i],f[i]=function(config,r,f,e,o){var s=u&amp;&amp;u(config,r,f,e,o);return s!==!0&amp;&amp;t[\\\"_\\\"+i](config,r,f,e,o),s}),t\\n    }({\\n        instrumentationKey: \\\"@Easton.Web.Helpers.Utils.AIKey\\\"\\n    });\\n\\n    window.appInsights=appInsights;\\n    appInsights.trackPageView();\\n&lt;/script&gt;\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"public class AppInsightsTelemetryInitializer : ITelemetryInitializer\\n{\\n    string appVersion = GetApplicationVersion();\\n    string siteType = GetSiteType();\\n\\n    private static string GetSiteType()\\n    {\\n        return WebConfigurationManager.AppSettings[\\\"SiteType\\\"];\\n    }\\n\\n    private static string GetApplicationVersion()\\n    {\\n        return typeof(AppInsightsTelemetryInitializer).Assembly.GetName().Version.ToString();\\n    }\\n\\n    public void Initialize(ITelemetry telemetry)\\n    {\\n        telemetry.Context.Component.Version = appVersion;\\n        telemetry.Context.Properties[\\\"siteType\\\"] = siteType;\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"&lt;TelemetryInitializers&gt;\\n  ...\\n  &lt;Add Type=\\\"Easton.Web.AppInsights.AppInsightsTelemetryInitializer, Easton.Web\\\"/&gt;\\n&lt;/TelemetryInitializers&gt;\\n\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"public abstract class BaseLogger : ILogger\\n{\\n    public virtual void TraceError(string message)\\n    {\\n        Trace.TraceError(message);\\n    }\\n\\n    public virtual void TraceError(string message, params object[] args)\\n    {\\n        Trace.TraceError(message, args);\\n    }\\n\\n    public virtual void TraceException(Exception ex)\\n    {\\n        Trace.TraceError(ex.ToString());\\n    }\\n\\n    // ... TraceInformation and TraceWarning methods same as above\\n\\n    public virtual void TraceCustomEvent(string eventName, IDictionary&lt;string, string&gt; properties = null, IDictionary&lt;string, double&gt; metrics = null)\\n    {\\n        var propertiesStr = \\\"\\\";\\n        if (properties != null)\\n        {\\n            foreach (var key in properties.Keys)\\n            {\\n                propertiesStr += string.Format(\\\"{0}{1}{2}\\\", key, properties[key], Environment.NewLine);\\n            }\\n        }\\n\\n\\n        var metricsStr = \\\"\\\";\\n        if (metrics != null)\\n        {\\n            foreach (var key in metrics.Keys)\\n            {\\n                metricsStr += string.Format(\\\"{0}{1}{2}\\\", key, metrics[key], Environment.NewLine);\\n            }\\n        }\\n\\n        Trace.TraceInformation(\\\"Custom Event: {0}{1}{2}{1}{3}\\\", eventName, Environment.NewLine, propertiesStr, metricsStr);\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"Get[\\\"/videos\\\"] = p =&gt;\\n{\\n    try\\n    {\\n        logger.TraceInformation(\\\"[/Videos] Returning {0} videos\\\", videoManager.Videos.Count);\\n        return new JsonResponse(videoManager.Videos, new EastonJsonNetSerializer());\\n    }\\n    catch (Exception ex)\\n    {\\n        logger.TraceException(ex);\\n        throw ex;\\n    }\\n};\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">logger.TraceCustomEvent(\\\"ViewVideo\\\", new Dictionary&lt;string, string&gt;() { { \\\"TenantId\\\", tenantId }, { \\\"User\\\", userId }, { \\\"VideoId\\\", videoId } });</font>\"}],[\"code\",{\"code\":\"public override void TraceCustomEvent(string eventName, IDictionary&lt;string, string&gt; properties = null, IDictionary&lt;string, double&gt; metrics = null)\\n{\\n    AppInsights.TrackEvent(eventName, properties, metrics);\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/d3f3fe7f-3b73-4a84-b1c3-3a550dd6ce89.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/66d6768d-b2ac-4bb2-b9fa-a41d3b533a7a.png\\\" width=\\\"321\\\" height=\\\"314\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/2d1a099e-9795-44cb-bfe5-e72e46f02568.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8f4b66db-f304-4d46-b596-01ae7a2853fd.png\\\" width=\\\"461\\\" height=\\\"208\\\"></a>\"}],[\"code\",{\"code\":\"requests\\n| where timestamp &gt;= ago(7d)\\n| summarize percentiles(duration,50,95,99) by bin (timestamp, 1h)\\n| render timechart\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/034d4bad-1c0a-4d92-b224-94de1c340742.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/9eb64962-7829-4af4-9833-fbb68a72c20e.png\\\" width=\\\"371\\\" height=\\\"178\\\"></a>\"}],[\"code\",{\"code\":\"requests\\n| where timestamp &gt; ago(2d)\\n| where success == \\\"False\\\"\\n| join kind=leftouter (\\n    exceptions\\n    | where timestamp &gt; ago(2d)\\n) on operation_Id\\n| summarize exceptionCount=count() by operation_Name\\n| order by exceptionCount asc\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a3b253b5-eeb3-4987-b6b6-c547e42ccb8a.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/73638438-1a40-4d31-a632-6fb776258470.png\\\" width=\\\"411\\\" height=\\\"145\\\"></a>\"}],[\"code\",{\"code\":\"customEvents\\n| where timestamp &gt; ago(7d)\\n| where name == \\\"ValidateToken\\\"\\n| extend user = tostring(customDimensions.User), tenantId = tostring(customDimensions.TenantId)\\n| summarize logins = dcount(user) by tenantId, bin(timestamp, 1d)\\n| order by logins asc\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/270acbcd-6b70-4605-bfb9-1ec2cdcb12d8.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/f8037c71-324a-4e4e-900e-53ce34503802.png\\\" width=\\\"439\\\" height=\\\"160\\\"></a>\"}],[\"code\",{\"code\":\"customEvents\\n| where timestamp &gt;= ago(7d)\\n| extend user = tostring(customDimensions.User), videoId = tostring(customDimensions.VideoId), tenantId = tostring(customDimensions.TenantId)\\n| summarize UserCount = dcount(user) by tenantId, bin (timestamp, 1h)\\n| render barchart\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/9fd825be-e6a5-4d68-8630-a365617dfd51.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4b41a4df-ed56-4f67-9a8a-4823deebeca9.png\\\" width=\\\"414\\\" height=\\\"206\\\"></a>\"}],[\"code\",{\"code\":\"/*\\nThe exported Power Query Formula Language (M Language ) can be used with Power Query in Excel \\nand Power BI Desktop. \\nFor Power BI Desktop follow the instructions below: \\n 1) Download Power BI Desktop from https://powerbi.microsoft.com/en-us/desktop/ \\n 2) In Power BI Desktop select: 'Get Data' -&gt; 'Blank Query'-&gt;'Advanced Query Editor' \\n 3) Paste the M Language script into the Advanced Query Editor and select 'Done' \\n*/\\n\\n\\nlet\\nSource = Json.Document(Web.Contents(\\\"https://management.azure.com/subscriptions/someguid/resourcegroups/rg/providers/microsoft.insights/components/app-insights-instance/api/query?api-version=2014-12-01-preview\\\", \\n[Query=[#\\\"csl\\\"=\\\"customEvents| where timestamp &gt;= ago(7d)| extend user = tostring(customDimensions.User), videoId = tostring(customDimensions.VideoId), tenantId = tostring(customDimensions.TenantId)| summarize UserCount = dcount(user) by tenantId, bin (timestamp, 1h)| render barchart\\\"]])),\\nSourceTable = Record.ToTable(Source), \\nSourceTableExpanded = Table.ExpandListColumn(SourceTable, \\\"Value\\\"), \\nSourceTableExpandedValues = Table.ExpandRecordColumn(SourceTableExpanded, \\\"Value\\\", {\\\"TableName\\\", \\\"Columns\\\", \\\"Rows\\\"}, {\\\"TableName\\\", \\\"Columns\\\", \\\"Rows\\\"}), \\nRowsList = SourceTableExpandedValues{0}[Rows], \\nColumnsList = SourceTableExpandedValues{0}[Columns],\\nColumnsTable = Table.FromList(ColumnsList, Splitter.SplitByNothing(), null, null, ExtraValues.Error), \\nColumnNamesTable = Table.ExpandRecordColumn(ColumnsTable, \\\"Column1\\\", {\\\"ColumnName\\\"}, {\\\"ColumnName\\\"}), \\nColumnsNamesList = Table.ToList(ColumnNamesTable, Combiner.CombineTextByDelimiter(\\\",\\\")), \\nTable = Table.FromRows(RowsList, ColumnsNamesList), \\nColumnNameAndTypeTable = Table.ExpandRecordColumn(ColumnsTable, \\\"Column1\\\", {\\\"ColumnName\\\", \\\"DataType\\\"}, {\\\"ColumnName\\\", \\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType1 = Table.ReplaceValue(ColumnNameAndTypeTable,\\\"Double\\\",Double.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType2 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType1,\\\"Int64\\\",Int64.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType3 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType2,\\\"Int32\\\",Int32.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType4 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType3,\\\"Int16\\\",Int16.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType5 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType4,\\\"UInt64\\\",Number.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType6 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType5,\\\"UInt32\\\",Number.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType7 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType6,\\\"UInt16\\\",Number.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType8 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType7,\\\"Byte\\\",Byte.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType9 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType8,\\\"Single\\\",Single.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType10 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType9,\\\"Decimal\\\",Decimal.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType11 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType10,\\\"TimeSpan\\\",Duration.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType12 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType11,\\\"DateTime\\\",DateTimeZone.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}),\\nColumnNameAndTypeTableReplacedType13 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType12,\\\"String\\\",Text.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}),\\nColumnNameAndTypeTableReplacedType14 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType13,\\\"Boolean\\\",Logical.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType15 = Table.ReplaceValue(ColumnNameAndTypeTableReplacedType14,\\\"SByte\\\",Logical.Type,Replacer.ReplaceValue,{\\\"DataType\\\"}), \\nColumnNameAndTypeTableReplacedType16 = Table.SelectRows(ColumnNameAndTypeTableReplacedType15, each [DataType] is type), \\nColumnNameAndTypeList = Table.ToRows(ColumnNameAndTypeTableReplacedType16), \\nTypedTable = Table.TransformColumnTypes(Table, ColumnNameAndTypeList) \\nin\\nTypedTable\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/67b87404-e41f-4252-bb67-6352b93c52ff.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a5cead1f-f37a-45ce-93b8-2050e98b8320.png\\\" width=\\\"344\\\" height=\\\"211\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/3cbbe27e-17b2-4319-8f8b-48b97a47ed86.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/c2c86a37-e7aa-463c-b181-8046b3bdcade.png\\\" width=\\\"276\\\" height=\\\"198\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/f11921cc-7a26-458e-bc54-d31423f31290.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/bae5a9dc-1b0b-41f2-98ba-4c02f445a6a7.png\\\" width=\\\"277\\\" height=\\\"227\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/382d58fd-fe06-4b54-a850-6ff1c9fed2b0.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/90252b21-893a-49e8-be63-1650268cfef5.png\\\" width=\\\"420\\\" height=\\\"245\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/0863c82c-8865-4c55-8d02-6f94f3c4a260.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/e5c4c392-1968-4be5-9e0f-826eea025567.png\\\" width=\\\"239\\\" height=\\\"321\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/27f766e0-950a-4a76-8afd-a01b1441cf4d.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/330c3205-1117-46ab-ba18-c46c35d6b40f.png\\\" width=\\\"576\\\" height=\\\"326\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/7c95e26f-c8a1-4e1f-ac6b-f921085bd68f.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/72d138c7-7cc7-4320-89ae-b760df0c6c76.png\\\" width=\\\"375\\\" height=\\\"213\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/application-insights/\"]],[\"a\",[\"href\",\"https://blogs.msdn.microsoft.com/bharry/2016/03/28/introducing-application-analytics/\"]],[\"a\",[\"href\",\"http://nwcadence.com/\"]],[\"a\",[\"href\",\"https://library.nwcadence.com/\"]],[\"a\",[\"href\",\"http://stackoverflow.com/questions/8142768/c-sharp-static-instance-members-for-each-inherited-class\"]],[\"a\",[\"href\",\"http://nancyfx.org/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/app-analytics/\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Ever since \"],[0,[0],1,\"Application Insights\"],[0,[],0,\" (AppInsights) was released, I’ve loved it. Getting tons of analytics about site usage, performance and diagnostics – pretty much for free – makes adding Application Performance Monitoring (APM) to you application a no-brainer. If you aren’t using AppInsights, then you really should be.\"]]],[1,\"p\",[[0,[],0,\"APM is the black sheep of DevOps – most teams are concentrating on getting continuous integration and deployment and release management, which are critical pillars of DevOps. But few teams are taking DevOps beyond deployment into APM, which is also fundamental to successful DevOps. AppInsights is arguably the easiest, least-friction method of quickly and easily getting real APM into your applications. However, getting insights from your AppInsights data has not been all that easy up until now.\"]]],[1,\"h3\",[[0,[],0,\"Application Insights Analytics\"]]],[1,\"p\",[[0,[],0,\"A few days ago Brian Harry wrote a blog post called \"],[0,[1],1,\"Introducing Application Insights Analytics\"],[0,[],0,\". Internally, MS was using a tool called Kusto to do log analytics for many systems – including Visual Studio Team Services (VSTS) itself. (Perhaps Kusto is a reference perhaps to the naval explorer Jacques Cousteau – as in, Kusto lets you explore the oceans of data?) MS then productized their WPF Kusto app into web-based Application Insights Analytics. App Insights Analytics adds phenomenal querying and visualizations onto AppInsights telemetry, allowing you to really dig into the data AppInsights logs. Later on I’ll show you some really simple queries that we use to analyze our usage data.\"]]],[1,\"p\",[[0,[],0,\"Brian goes into detail about how fast the Application Insights Analytics engine is – and he should know since they process terrabytes worth of telemetry. Our telemetry is nowhere near that large, so performance of the query language isn’t that big a deal for us. What is a big deal is the analytics and visualizations that the engine makes possible.\"]]],[1,\"p\",[[0,[],0,\"In this post I want to show you how to get AppInsights into a real world application. \"],[0,[2],1,\"Northwest Cadence\"],[0,[],0,\" has a \"],[0,[3],1,\"Knowledge Library\"],[0,[],0,\" application and in order to generate tracing diagnostics and usage telemetry, we added AppInsights. We learned some lessons about AppInsights on the way, and here are some of our lessons-learned.\"]]],[1,\"h3\",[[0,[],0,\"Configuring AppInsights\"]]],[1,\"p\",[[0,[],0,\"We have 4 sites that we deploy the same code to – there are 2 production sites, Azure Library and Knowledge Library, and each has a dev environment too. By default the AppInsights key is configured in ApplicationInsights.config. We wanted to have a separate AppInsights instance for each site, so we created 4 in Azure. Now we had the problem of where to set the key so that each site logs to the correct AppInsights instance.\"]]],[1,\"p\",[[0,[],0,\"Server-side telemetry is easy to configure. Add an app setting called “AIKey” in the web.config. In a startup method somewhere, you make a call to the Active TelemetryConfig:\"]]],[10,0],[1,\"p\",[[0,[],0,\"This call then sets the AIKey for all serve-side telemetry globally. But what about client side?\"]]],[1,\"p\",[[0,[],0,\"For that we added a static getter to a class like this:\"]]],[10,1],[1,\"p\",[[0,[],0,\"In the master.cshtml file, we added the client-side script for AppInsights and made a small modification to get the key injected in instead of hard-coded:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can see how we’re using Razor syntax to get the AIKey static property value for the instrumentationKey value.\"]]],[1,\"p\",[[0,[],0,\"The next thing we wanted was to set the application version (assembly version) and site type (either KL for “Knowledge Library” or Azure for “Azure Library”). Perhaps this is a bit overkill since we have 4 separate AppInsights instances anyway, but if we decide to consolidate at some stage we can do so and preserve partitioning in the data.\"]]],[1,\"p\",[[0,[],0,\"Setting telemetry properties for every log entry is a little harder – there used to be an IConfigurationInitializer interface, but it seems it was deprecated. So we implemented an ITelmetryInitializer instance:\"]]],[10,3],[1,\"p\",[[0,[],0,\"In order to tell AppInsights to use the initializer, you need to add an entry to the ApplicationInsights.config file:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now the version and siteType properties are added to every server-side log. Of course we could add additional “global” properties using the same code if we needed more.\"]]],[1,\"h3\",[[0,[],0,\"Tracing\"]]],[1,\"p\",[[0,[],0,\"Last week we had an issue with our site – there’s a signup process in which we generate an access code and customers then enter the access code and enable integration with their Azure Active Directory so that their users can authenticate against their AAD when logging into our site. Customers started reporting that the access code “wasn’t found”. The bug turned out to be the fact that a static variable on a base class is shared across all child instances too – so our Azure Table data access classes were pointing to the incorrect tables (We fixed the issue using a \"],[0,[4],1,\"curiously recurring generic base class\"],[0,[],0,\" – a study for another day) but the issue had us stumped for a while.\"]]],[1,\"p\",[[0,[],0,\"Initially I thought, “I can debug this issue quickly – I have AppInsights on the site so I can see what’s going on.” Turns out that there wasn’t any exception for the issue – the data access searched for an entity and couldn’t find it, so it reported the “access code not found” error that our customers were seeing. I didn’t have AppInsights tracing enabled – so I immediately set about adding it.\"]]],[1,\"p\",[[0,[],0,\"First, you install the Microsoft.ApplicationInsights.TraceListener package from NuGet. Then you can pepper your code with trace calls to System.Diagnostics.Trace – each one is sent to AppInsights by the TraceListener.\"]]],[1,\"p\",[[0,[],0,\"We decided to create an ILogger interface and a base class that just did a call to System.Diagnostics.Trace. Here’s a snippet:\"]]],[10,5],[1,\"p\",[[0,[],0,\"The TraceInformation and TraceError methods are pretty straightforward – the TraceCustomEvent was necessary to enable custom telemetry. Using the logger to add tracing and exception logging is easy. We inject an instance of our AppInsightsLogger (more on this later) and then we can use it to log. Here’s an example of our GET videos method (we use \"],[0,[5],1,\"NancyFx\"],[0,[],0,\" which is why this is an indexer method):\"]]],[10,6],[1,\"h3\",[[0,[],0,\"Custom Telemetry\"]]],[1,\"p\",[[0,[],0,\"Out of the box you get a ton of great logging in AppInsights – page views (including browser type, region, language and performance) and server side requests, exceptions and performance. However, we wanted to start doing some custom analytics on usage. Our application is multi-tenant, so we wanted to track the tenantId as well as the user. We want to track each time a user views a video so we can see which users (across which tenants) are accessing which videos. Here’s the call we make to log that a user has accessed a video:\"]]],[10,7],[1,\"p\",[[0,[],0,\"The method in the AppInsightsLogger is as follows:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Pretty simple.\"]]],[1,\"h3\",[[0,[],0,\"Analytics Queries\"]]],[1,\"p\",[[0,[],0,\"Now that we’re getting some telemetry, including requests and custom events, we can start to query. Logging on to the Azure Portal I navigate to the AppInsights instance and click on the Analytics button in the toolbar:\"]]],[10,9],[1,\"p\",[[0,[],0,\"That will open the AppInsights Analytics page. Here I can start querying my telemetry. There are several “tables” that you can query – requests, traces, exceptions and so on. If I want to see the performance percentiles of my requests in 1 hour bins for the last 7 days, I can use this query which calculates the percentiles and then renders to a time chart:\"]]],[10,10],[10,11],[1,\"p\",[[0,[],0,\"The query syntax is fairly “natural” though I did have to look at these \"],[0,[6],1,\"help docs\"],[0,[],0,\" to get to grips with the language.\"]]],[1,\"p\",[[0,[],0,\"Sweet!\"]]],[1,\"p\",[[0,[],0,\"You can even join the tables. Here’s an example from Brian Harry’s post that correlates exceptions and requests:\"]]],[10,12],[10,13],[1,\"p\",[[0,[],0,\"Note that I did have some trouble with the order by direction – it could be a bug (this is still in preview) or maybe I just don’t understand the ordering will enough.\"]]],[1,\"p\",[[0,[],0,\"Here are a couple of queries against our custom telemetry:\"]]],[10,14],[10,15],[1,\"p\",[[0,[],0,\"Again, the ordering direction seems odd to me.\"]]],[1,\"p\",[[0,[],0,\"I love the way that the customDimensions (which is just a json snippet) is directly addressable. Here’s what the json looks like for our custom events:\"]]],[10,16],[1,\"p\",[[0,[],0,\"You can see how the “siteType” property is there because of our ITelemetryInitializer.\"]]],[1,\"h3\",[[0,[],0,\"Visualizations\"]]],[1,\"p\",[[0,[],0,\"After writing a couple queries, we can then add a visualization by adding a render clause. You’ve already seen the “render timechart“ above – but there’s also piechart, barchart and table. Here’s a query that renders a stacked bar chart showing user views (per tenant) in hourly bins:\"]]],[10,17],[10,18],[1,\"p\",[[0,[],0,\"This is just scratching the surface, but I hope you get a feel for what this tool can bring out of your telemetry.\"]]],[1,\"h3\",[[0,[],0,\"Exporting Data to PowerBI\"]]],[1,\"p\",[[0,[],0,\"The next step is to make a dashboard out of the queries that we’ve created. You can export to Excel, but for a more dynamic experience, you can also export to PowerBI. I was a little surprised that when I clicked “Export to PowerBI” I got a text file. Here’s the same bar chart query exported to PowerBI:\"]]],[10,19],[1,\"p\",[[0,[],0,\"Ah, so I’ll need PowerBI desktop. No problem. Download it, open it and follow the helpful instructions in the comments at the top of the file:\"]]],[10,20],[1,\"p\",[[0,[],0,\"Now I can create visualizations, add custom columns – do whatever I would normally do in PowerBI.\"]]],[1,\"p\",[[0,[],0,\"One thing I did want to do was fix up the nasty “tenantId”. This is a guid which is the Partition Key for an Azure Table that we use to store our tenants. So I just added a new Query to the report to fetch the tenant data from the table. Then I was able to create a relationship (i.e. foreign key) that let me use the tenant name rather than the nasty guid in my reports:\"]]],[10,21],[1,\"p\",[[0,[],0,\"Here’s what the relationship looks like for the “Users Per Tenant Per Hour Query”:\"]]],[10,22],[1,\"p\",[[1,[],0,0],[0,[],0,\"Once I had the tables in, I could create reports. Here’s a performance report:\"]]],[10,23],[1,\"p\",[[0,[],0,\"One tip – when you add the “timestamp” property, PowerBI defaults to a date hierarchy (Year, Quarter, Month, Day). To use the timestamp itself, you can just click on the field in the axis box and select “timestamp” from the values:\"]]],[10,24],[1,\"p\",[[0,[],0,\"Here’s one of our usage reports:\"]]],[10,25],[1,\"p\",[[0,[],0,\"And of course, once I’ve written the report, I can just upload it to PowerBI to share with the team:\"]]],[10,26],[1,\"p\",[[0,[],0,\"Look ma – it’s the same report!\"]]],[1,\"h3\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"If you’re not doing APM, then you need to get into AppInsights. If you’re already using AppInsigths, then it’s time to move beyond \"],[0,[7],1,\"logging\"],[0,[],0,\" telemetry to actually \"],[0,[7],1,\"analyzing\"],[0,[],0,\" telemetry and gaining insights from your applications using AppInights Analytics.\"]]],[1,\"p\",[[0,[],0,\"Happy analyzing!\"]]]]}","published_at":1459450655000,"status":"published","published_by":1},{"id":"afd683da-1062-474e-87a6-aa871dc3a1b6","title":"Application Insights Telemetry for WAWS or Customer-Hosted Sites Without MMA","slug":"application-insights-telemetry-for-waws-or-customer-hosted-sites-without-mma","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-XVbm6nZskWo/U1AOIPHhUII/AAAAAAAABSc/wy57Wm7KnOM/s1600-h/image%25255B6%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-aIBjS7Ia0yg/U1AOJL2dO2I/AAAAAAAABSk/ACMq-q7q8X4/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"154\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-w4UWdReOkZk/U1AOKe9R2vI/AAAAAAAABSs/bzZ3E93vDGc/s1600-h/image%25255B9%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-MR1ORIZk2rE/U1AOLGNlXTI/AAAAAAAABS0/lvGC3oYZS84/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"228\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-GwSIoIRx7GY/U1AOMG6FcKI/AAAAAAAABS8/7h-Kx6t_j6o/s1600-h/image%25255B42%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-fheSpW7Jre8/U1AONCgIVpI/AAAAAAAABTE/MXxX3fIjgWY/image_thumb%25255B16%25255D.png?imgmax=800\\\" width=\\\"310\\\" height=\\\"102\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-OVHgGzNylOI/U1AOPXtkBHI/AAAAAAAABTM/tcvj2EdrEhI/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-fchvYpOj7ko/U1AOQyAMFMI/AAAAAAAABTU/hofZBkiwmCk/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"310\\\" height=\\\"214\\\"></a>\"}],[\"code\",{\"code\":\"var appInsightsKey = ConfigurationManager.AppSettings[\\\"AppInsightsID\\\"];\\nif (string.IsNullOrEmpty(appInsightsKey))\\n{\\n    ServerAnalytics.Enabled = false;\\n}\\nelse\\n{\\n    ServerAnalytics.Start(appInsightsKey);\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"<add key=\\\"AppInsightsID\\\" value=\\\"guid\\\">\\n</add>\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-Mdt2hsY-bZI/U1AOSArCA2I/AAAAAAAABTc/BD3qjbJj628/s1600-h/image%25255B12%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-dyGP963H2sI/U1AOTNEMIqI/AAAAAAAABTk/aFZVwGBRSu8/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"211\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-qyEtFamopx8/U1AOUdjW5UI/AAAAAAAABTs/ZFVzW0zl66E/s1600-h/image%25255B15%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-6Zns-J-36kY/U1AOVp-rX-I/AAAAAAAABT0/kb2mZVuf9Cs/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"224\\\" height=\\\"244\\\"></a>\"}],[\"code\",{\"code\":\"<script type=\\\"text/javascript\\\">\\n    var appInsightsKey = \\\"@System.Configuration.ConfigurationManager.AppSettings[\\\"AppInsightsID\\\"]\\\";\\n    if (appInsightsKey !== null || appInsightsKey.length > 0) {\\n        window.appInsights = { queue: [], applicationInsightsId: null, accountId: null, appUserId: null, configUrl: null, start: function (n) { function u(n, t) { n[t] = function () { var i = arguments; n.queue.push(function () { n[t].apply(n, i) }) } } function f(n) { var t = document.createElement(\\\"script\\\"); return t.type = \\\"text/javascript\\\", t.src = n, t.async = !0, t } function r() { i.appendChild(f(\\\"//az416426.vo.msecnd.net/scripts/ai.0.js\\\")) } var i, t; this.applicationInsightsId = n; u(this, \\\"logEvent\\\"); u(this, \\\"logPageView\\\"); i = document.getElementsByTagName(\\\"script\\\")[0].parentNode; this.configUrl === null ? r() : (t = f(this.configUrl), t.onload = r, t.onerror = r, i.appendChild(t)); this.start = function () { } } };\\n        appInsights.start(appInsightsKey);\\n        appInsights.logPageView();\\n    }\\n</script>\",\"language\":\"js;\"}],[\"code\",{\"code\":\"var aiEvent = ServerAnalytics.CurrentRequest.StartTimedEvent(\\\"DummySite/Index\\\");\\n\\ntry\\n{\\n    // do stuff\\n    aiEvent.End();\\n}\\ncatch(Exception ex)\\n{\\n    // log stuff\\n    aiEvent.Cancel();\\n    throw;\\n}\\n\\nreturn View();\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public ActionResult Contact()\\n{\\n    ServerAnalytics.CurrentRequest.LogEvent(\\\"DummySite/Contact\\\");\\n    ViewBag.Message = \\\"Your contact page.\\\";\\n\\n    return View();\\n}\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public ActionResult About()\\n{\\n    var even = DateTime.Now.Second % 2 == 0 ? \\\"even\\\" : \\\"odd\\\";\\n    var props = new Dictionary<string, object=\\\"\\\">() { { \\\"mod\\\", even } };\\n\\n    ServerAnalytics.CurrentRequest.LogEvent(\\\"DummySite/About\\\", props);\\n\\n    ViewBag.Message = \\\"Your application description page.\\\";\\n\\n    return View();\\n}</string,>\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/--ElBYbnTezE/U1AOWwwdJcI/AAAAAAAABT8/7LhPcolip1A/s1600-h/image%25255B19%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-3rPRTtsgn0U/U1AOX3U__iI/AAAAAAAABUE/hUIZgM-Comw/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"337\\\" height=\\\"285\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-wHk0rCmjqHs/U1AOYuM5rbI/AAAAAAAABUM/ir1GBgw7TPg/s1600-h/image%25255B22%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-Fej5MvT6GvU/U1AOZVLlFQI/AAAAAAAABUU/6pxG6ZPuU34/image_thumb%25255B8%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"159\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-Sai19kQWT1U/U1AOafkyxYI/AAAAAAAABUc/rldKOij0GHo/s1600-h/image%25255B25%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-v0ZpNg4z0EA/U1AObMADMBI/AAAAAAAABUk/N_yJE2bnA7s/image_thumb%25255B9%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"54\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-qW1F_7vzKCQ/U1AOcFjxRsI/AAAAAAAABUs/ctnqln1btj8/s1600-h/image%25255B28%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-1uDtR4lTWgY/U1AOdT-jOlI/AAAAAAAABU0/KGI4stGfIjo/image_thumb%25255B10%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"236\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-uWAa1p7sGYo/U1AOeOU6oSI/AAAAAAAABU8/JEYDJlvHH2I/s1600-h/image%25255B31%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-FTnQire5GtM/U1AOfb2FeLI/AAAAAAAABVE/aM3Z2JYjI-8/image_thumb%25255B11%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"163\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-iRFu0K1qULU/U1AOgJi5gKI/AAAAAAAABVM/a-dkyp7JggI/s1600-h/image%25255B35%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-2itUe45dl10/U1AOhMBh1wI/AAAAAAAABVU/69Bv9byGEdE/image_thumb%25255B13%25255D.png?imgmax=800\\\" width=\\\"322\\\" height=\\\"327\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-Ek3V2JJnHvg/U1AOiXayJBI/AAAAAAAABVc/109phYYQuzA/s1600-h/image%25255B38%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-iOCFtrpmw9g/U1AOjfSVvwI/AAAAAAAABVg/ksDaNXF_2AA/image_thumb%25255B14%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"129\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd264915.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dn481095.aspx\"]],[\"a\",[\"href\",\"http://go.microsoft.com/fwlink/?LinkID=328054\"]],[\"a\",[\"href\",\"http://go.microsoft.com/fwlink/?LinkID=390446\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"There are a couple of tools that change the development landscape significantly for .Net developers. One such tool is \"],[0,[0],1,\"IntelliTrace\"],[0,[],0,\" – this revolutionizes how you debug production issues. Another game-changing tool is \"],[0,[1],1,\"Application Insights\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"There are a few broad categories for Application Insights telemetry – APM (Application Performance Monitoring), Availability, Usage and Diagnostics. In order to leverage APM and Diagnostics, you need to install \"],[0,[2],1,\"Microsoft Monitoring Agent\"],[0,[],0,\" (MMA) on your IIS server. This isn’t a problem if you’re using an on-premises IIS or using your own VM in the cloud. However, if you’ve got a website that’s running on Windows Azure Websites (WAWS) or your site is going to be hosted on someone else’s tin or cloud, then you can’t install MMA. However, not all is lost – you can still get Availability and Usage telemetry for these scenarios.\"]]],[1,\"h2\",[[0,[],0,\"Add Application Insights to Your Web App\"]]],[1,\"p\",[[0,[],0,\"Normally this is as easy as installing the \"],[0,[3],1,\"Application Insights VS Extensions\"],[0,[],0,\", right-clicking your Web Application and selecting “Add Application Insights Telemetry”. This adds an ApplicationInsights.config file to your project that allows you to configure Application Insights. This works great in conjunction with MMA. Of course, the scenario we’re talking about is one where there is no MMA.\"]]],[1,\"p\",[[0,[],0,\"This scenario is supported, but there’s not a lot of guidance on how to do it. In cases where I’m running my site on WAWS or I’ve created a website that my customer is going to be hosting on their infrastructure (be that on their own premises, their own public cloud or even on Azure) and there’s no MMA, you can still get some really good telemetry. I also had the goal of minimizing admin and configuration footprint as much as possible, since I don’t want to have to maintain lots of config files on other people’s infrastructure. Happily, you can get a lot of good telemetry with a single application setting. Let’s see how to do it.\"]]],[1,\"h2\",[[0,[],0,\"Create an Application in Application Insights\"]]],[1,\"p\",[[0,[],0,\"For the WAWS (or customer-hosted website) scenarios, you need to create an application in Application Insights. This will generate a unique (guid) key for the application. This is the only piece of config you really need.\"]]],[1,\"p\",[[0,[],0,\"The first thing you need to do is “Choose Your Adventure” in Application Insights. There isn’t an adventure path for the scenario we’re trying, so we’ll hijack the Web Service adventure. Log into your VSO account and go to Application Insights. Click on “Overview” and then “Add Application”. Follow the prompts along the following path:\"]]],[10,0],[1,\"p\",[[0,[],0,\"AppInsights will give us a list of instructions at this point. Since we’re going off-track on this adventure, we’ll ignore most of the instructions – we really just need the Application ID.\"]]],[1,\"p\",[[0,[],0,\"Type a name for your application into “Step 2” and click “Create”. Then copy the guid from Step 4 (this is the application ID we need to tie AppInsights events to this application in AppInsights):\"]]],[10,1],[1,\"p\",[[0,[],0,\"Note: You can get the application ID at any time by clicking on the gear icon in the top right of AppInsights. Then click the “Keys & Downloads” tab, select your application in the drop-down on the top left and scroll down to the “Web Services SDK” section. The guid there is the one you need for your application:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Follow the instructions in Step 3 – you can follow them here if you like. Select your web application in Solution Explorer, right-click and select “Manage NuGet packages”. Then type in “application insights” (including the quotes) and install the Application Insights SDK package.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Once that’s done, you can add the following code to your Global.asax.cs file:\"]]],[10,4],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Now add a key to your application settings in your Web.config:\"]]],[10,5],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"The value for the key is the guid you copied earlier when you created the Application in AppInsights. If you deploy this to another customer, create a new Application in AppInsights and update the key in the config file. That’s it.\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"h2\",[[0,[],0,\"Setup Usage Telemetry from the Browser\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"To get usage telemetry from the browser (like Language, OS, Browser Type, Geographic Region etc.) you need to add some javascript to the pages you want to track. Back in Application Insights, you can click on the “Usage” tab. Make sure your new application is selected on the top left. Then click “Web site” for the type of application you want usage stats from. Click on “Click here to show instructions” at the bottom.\"]]],[1,\"p\",[[1,[],0,4]]],[10,6],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"If your site is on WAWS, you’ll want to create a ping test when you’ve published your application – if it’s already in Azure, then go ahead and enter the site url for the ping-test. This is a synthetic monitor that can give you a bit of basic availability information about your site. Of course when it’s on your customer’s infrastructure there may not even be a publically accessible url.\"]]],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"What’s important for now though is that you’ll see the usage insights snippet in step 3. You can copy that code if you like – though I made a slight modification to this code to make the application ID a parameter, rather than a hard-coded value.\"]]],[1,\"p\",[[1,[],0,7]]],[10,7],[1,\"p\",[[1,[],0,8]]],[1,\"p\",[[0,[],0,\"I’ve got an MVC application, so I’ve added the usage snippet javascript (mixed with a little Razor) into the _Layout.cshtml page:\"]]],[10,8],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"You can see how I added a C# call (using Razor) to read the applicationID from the web.config – it’s the same call that the code in Global.asax.cs makes.\"]]],[1,\"p\",[[1,[],0,10]]],[1,\"h2\",[[0,[],0,\"Custom Events\"]]],[1,\"p\",[[1,[],0,11]]],[1,\"p\",[[0,[],0,\"So that will “buy” me some usage telemetry as well as page hits – all for free (almost). However, I wanted to add some specific events and even do some timing on events so that I can monitor performance. To do that I had to add some code into my methods.\"]]],[1,\"p\",[[1,[],0,12]]],[1,\"p\",[[0,[],0,\"You can log an event (just give it a name – the name allows a hierarchy separated by “/”). You can log a timed event – call StartTimedEvent() with the same naming convention. Later, you call event.End() or event.Cancel() to stop timing. Furthermore, you can log these events with metrics (and/or properties) using a Dictionary. Here are some snippets of stuff you can do:\"]]],[1,\"p\",[[1,[],0,13]]],[1,\"p\",[[0,[],0,\"Create a Timed Event for a Controller method:\"]]],[10,9],[1,\"p\",[[1,[],0,14]]],[1,\"p\",[[0,[],0,\"Here’s just logging an event:\"]]],[10,10],[1,\"p\",[[1,[],0,15]]],[1,\"p\",[[0,[],0,\"Here’s an event with some properties:\"]]],[10,11],[1,\"p\",[[1,[],0,16]]],[1,\"p\",[[0,[],0,\"Of course you can add whatever custom events you need – the beauty of the API is that once you’ve called the Start method (in the Global.asax.cs) you don’t need to worry about IDs or config at all.\"]]],[1,\"p\",[[1,[],0,17]]],[1,\"p\",[[0,[],0,\"Now just publish and start monitoring! (Don’t forget to set your ping-test in the Availability tab in AppInsights if your site is publically available and you haven’t done it before).\"]]],[1,\"p\",[[1,[],0,18]]],[1,\"h2\",[[0,[],0,\"Viewing the Data in Application Insights\"]]],[1,\"p\",[[1,[],0,19]]],[1,\"p\",[[0,[],0,\"When you debug your application locally, you can check you events in the “Diagnostics->Streaming Data” page on AppInsights.\"]]],[1,\"p\",[[1,[],0,20]]],[10,12],[1,\"p\",[[1,[],0,21]]],[1,\"p\",[[0,[],0,\"Once you’ve published your application, you’ll see events start flooding in.\"]]],[1,\"p\",[[1,[],0,22]]],[1,\"p\",[[0,[],0,\"In the Usage->Features->Events page, you’ll see your events, organized into the hierarchy you specified with the / notation:\"]]],[1,\"p\",[[1,[],0,23]]],[10,13],[1,\"p\",[[0,[],0,\"When I first saw this page, I couldn’t figure out where the timing for the timed events was. You can click on the little arrow (left of the pin icon for each event) or click the “DETAILS” link on the top left of the graph.\"],[1,[],0,24]]],[10,14],[1,\"p\",[[1,[],0,25]]],[1,\"p\",[[0,[],0,\"Clicking on the “dummysite/index” event arrow takes me to the details for that event, where I can analyze the timings:\"]]],[1,\"p\",[[1,[],0,26]]],[10,15],[1,\"p\",[[1,[],0,27]]],[1,\"p\",[[0,[],0,\"Clicking on the “dummysite/about” event, I get to see the properties (in this case, “mod” is the property and the values are “even” or “odd” – you can filter the graph by a particular value):\"]]],[1,\"p\",[[1,[],0,28]]],[10,16],[1,\"p\",[[0,[],0,\"The Usage->User page shows user/session specific telemetry – mine aren’t all that interesting since I only logged on with 1 user.\"],[1,[],0,29]]],[10,17],[1,\"p\",[[1,[],0,30]]],[1,\"p\",[[0,[],0,\"Clicking on Usage->Environment gives me some telemetry around browsers, OSs, locations and so on.\"]]],[1,\"p\",[[1,[],0,31]]],[10,18],[1,\"p\",[[1,[],0,32]]],[1,\"p\",[[0,[],0,\"All in all, it’s a LOT of data for only a couple of minor edits to code. You can also add *some* of this data to Dashboards, so you can make custom Dashboards for monitoring your sites.\"]]],[1,\"p\",[[1,[],0,33]]],[1,\"p\",[[0,[],0,\"Best of all, if I give this application to many different customers, all I need to do is supply them with a new AppInsightsID for their web.config file and I’ll instantly get telemetry. Very cool!\"]]],[1,\"p\",[[1,[],0,34]]],[1,\"p\",[[0,[],0,\"Happy monitoring!\"]]]]}","published_at":1397787900000,"status":"published","published_by":1},{"id":"6906f0b6-6bba-40be-bf4f-7b4532d52870","title":"Auditing VSTS Client Access IPs","slug":"auditing-vsts-client-access-ips","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/00fe0946-f583-4439-8c81-73f320aa5dd1.png\\\" target=\\\"_blank\\\"><img width=\\\"296\\\" height=\\\"184\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/44cd5992-8b35-4e70-abdc-93866be34952.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9e0404a1-2f29-4152-82c0-d06137b0dfff.png\\\" target=\\\"_blank\\\"><img width=\\\"317\\\" height=\\\"131\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/84d10ee7-8496-40c5-9a7f-a19f4db05107.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/cac7d178-31c6-4339-8456-501e15cb9f4a.png\\\" target=\\\"_blank\\\"><img width=\\\"297\\\" height=\\\"35\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d073731e-f3d1-417e-8be5-bd2408a325f3.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/accounts/manage-conditional-access?view=vsts\"]],[\"u\"],[\"a\",[\"href\",\"https://<your account>.visualstudio.com/_apis/Utilization/UsageSummary\"]],[\"a\",[\"href\",\"https://gist.github.com/colindembovsky/e4f5d67ab807914cad5af1e5752e00de\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Visual Studio Team Services (VSTS) is a cloud platform. That means it's publicly accessible from anywhere - at least, by default. However, Enterprises that are moving from TFS to VSTS may want to ensure that VSTS is only accessed from a corporate network or some white-list of IPs.\"]]],[1,\"p\",[[0,[],0,\"To enable conditional access to VSTS, you'll have to have an Azure Active Directory (AAD) backed VSTS account. The conditional access is configured on AAD, not on VSTS itself, since that's where the authentication is performed. For instructions on how to do this, read this \"],[0,[0],1,\"MSDN article\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"Audit Access\"]]],[1,\"p\",[[0,[],0,\"However, this may be a bit heavy-handed. Perhaps you just want to audit access that isn't from a white-list of IPs, instead of blocking access totally. If you're an administrator, you may have come across the Usage page in VSTS. To get there, navigate to the landing page for your VSTS account, click the gear icon and select Usage:\"]]],[10,0],[1,\"p\",[[0,[],0,\"This page will show you all \"],[0,[1],1,\"your\"],[0,[],0,\" access to VSTS. To see the IPs, you have to add the \\\"IP Address\\\" column in the column options:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Nice, but what about other users? To get that you have to use the VSTS REST API.\"]]],[1,\"h2\",[[0,[],0,\"Dumping an Exception Report with PowerShell\"]]],[1,\"p\",[[0,[],0,\"There is an (undocumented) endpoint for accessing user access. It's \"],[0,[2],1,\"https://<your account>.visualstudio.com/_apis/Utilization/UsageSummary\"],[0,[],0,\" with a whole string of query parameters. And since it's a REST call, you'll need to be authenticated so you'll have to supply a header with a base-64 encoded Personal Access Token (PAT).\"]]],[1,\"p\",[[0,[],0,\"Using PowerShell, you can make a call to the endpoint and then filter results where the IP is not in the white-list of IPs. Fortunately for you, I've made a gist for you, which you can access \"],[0,[3],1,\"here\"],[0,[],0,\". When you call the script, just pass in your account name, your PAT, the start and end date and the white-list of IPs. Any access from an IP not in this list is dumped to a CSV.\"]]],[10,2],[1,\"h3\",[[0,[],0,\"Limitations\"]]],[1,\"p\",[[0,[],0,\"There are some limitations to the API:\"]]],[3,\"ol\",[[[0,[],0,\"You'll need to be a Project Collection or Account admin to make this call (since there's no documentation, I'm guessing here).\"]],[[0,[],0,\"You can only go back 28 days, so if you need this as an official exception report you'll have to schedule the run.\"]]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"VSTS knows the client IP for any access. Using the API, you can dump a list of access events that are not from a white-list of IPs.\"]]],[1,\"p\",[[0,[],0,\"Happy auditing!\"]]]]}","published_at":1527712061000,"status":"published","published_by":1},{"id":"2dd3eb55-32b7-482e-8f05-0091807c8040","title":"Aurelia – Debugging from within Visual Studio","slug":"aurelia--debugging-from-within-visual-studio","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"import auf = require(\\\"aurelia-framework\\\");\\nimport aur = require(\\\"aurelia-router\\\");\\n\\nexport class App {\\n    static inject = [aur.Router];\\n\\n    constructor(private router: aur.Router) {\\n        console.log(\\\"in constructor\\\");\\n        this.router.configure((config: aur.IRouterConfig) =&gt; {\\n            config.title = \\\"Aurelia VS/TS\\\";\\n            config.map([\\n                { route: [\\\"\\\", \\\"welcome\\\"], moduleId: \\\"./views/welcome\\\", nav: true, title: \\\"Welcome to VS/TS\\\" },\\n                { route: \\\"flickr\\\", moduleId: \\\"./views/flickr\\\", nav: true },\\n                { route: \\\"child-router\\\", moduleId: \\\"./views/child-router\\\", nav: true, title: \\\"Child Router\\\" }\\n            ]);\\n        });\\n    }\\n}\\n\",\"language\":\"js; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bde471f7-6b43-4667-beb3-e268b058e445.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0325ab1e-3f02-409e-b9b2-049ae6853314.png\\\" width=\\\"280\\\" height=\\\"315\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4c2d13a9-a1d7-4b06-b070-15b1ab78a389.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ff0a1081-0847-4f56-8918-de4dd90e43cc.png\\\" width=\\\"242\\\" height=\\\"368\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3265a898-218c-47f5-85a9-549b39c535d1.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6780660b-6a5c-42e8-95c5-3d9b89e0a879.png\\\" width=\\\"239\\\" height=\\\"352\\\"></a>\"}],[\"code\",{\"code\":\"auf.LogManager.addAppender(new aul.ConsoleAppender());\\nauf.LogManager.setLevel(auf.LogManager.levels.debug);\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"import auf = require(\\\"aurelia-framework\\\");\\nimport aur = require(\\\"aurelia-router\\\");\\n\\nexport class App {\\n    private logger: auf.Logger = auf.LogManager.getLogger(\\\"App\\\");\\n\\n    static inject = [aur.Router];\\n\\n    constructor(private router: aur.Router) {\\n        this.logger.info(\\\"Constructing app\\\");\\n\\n        this.router.configure((config: aur.IRouterConfig) =&gt; {\\n            this.logger.debug(\\\"Configuring router\\\");\\n            config.title = \\\"Aurelia VS/TS\\\";\\n            config.map([\\n                { route: [\\\"\\\", \\\"welcome\\\"], moduleId: \\\"./views/welcome\\\", nav: true, title: \\\"Welcome to VS/TS\\\" },\\n                { route: \\\"flickr\\\", moduleId: \\\"./views/flickr\\\", nav: true },\\n                { route: \\\"child-router\\\", moduleId: \\\"./views/child-router\\\", nav: true, title: \\\"Child Router\\\" }\\n            ]);\\n        });\\n    }\\n}\\n\",\"language\":\"js; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bac13628-73c8-4136-a409-5369b3fef335.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/54c286ee-544e-41f8-bdf4-8df04426652d.png\\\" width=\\\"229\\\" height=\\\"312\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8e75e9b7-74fc-4f8f-ab73-2dcc956e5013.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1b8a2aac-c262-4a71-8665-5e63e70ae9e1.png\\\" width=\\\"362\\\" height=\\\"225\\\"></a>\"}],[\"code\",{\"code\":\"&lt;body aurelia-app&gt;\\n    &lt;div class=\\\"splash\\\"&gt;\\n        &lt;div class=\\\"message\\\"&gt;Welcome to Aurelia&lt;/div&gt;\\n        &lt;i class=\\\"fa fa-spinner fa-spin\\\"&gt;&lt;/i&gt;\\n    &lt;/div&gt;\\n    &lt;script src=\\\"jspm_packages/system.js\\\"&gt;&lt;/script&gt;\\n    &lt;script src=\\\"config.js\\\"&gt;&lt;/script&gt;\\n\\n    &lt;!-- jquery layout scripts --&gt;\\n    &lt;script src=\\\"Content/scripts/jquery-1.8.0.min.js\\\"&gt;&lt;/script&gt;\\n    &lt;script src=\\\"Content/scripts/jquery-ui-1.8.23.min.js\\\"&gt;&lt;/script&gt;\\n    &lt;script src=\\\"Content/scripts/jquery.layout.min.js\\\"&gt;&lt;/script&gt;\\n\\n    &lt;script&gt;\\n    //System.baseUrl = 'dist';\\n    System.import('aurelia-bootstrapper');\\n    &lt;/script&gt;\\n&lt;/body&gt;\\n\",\"language\":\"xml; highlight\"}],[\"code\",{\"code\":\"&lt;body aurelia-main&gt;\\n    &lt;div class=\\\"splash\\\"&gt;\\n        &lt;div class=\\\"message\\\"&gt;Welcome to Aurelia AppInsights Demo&lt;/div&gt;\\n        &lt;i class=\\\"fa fa-spinner fa-spin\\\"&gt;&lt;/i&gt;\\n    &lt;/div&gt;\\n\\n    &lt;script src=\\\"Content/scripts/core-js/client/core.js\\\"&gt;&lt;/script&gt;\\n    &lt;script src=\\\"Content/scripts/requirejs/require.js\\\"&gt;&lt;/script&gt;\\n    &lt;script&gt;\\n        var baseUrl = window.location.origin\\n        console.debug(\\\"baseUrl: \\\" + baseUrl);\\n        require.config({\\n            baseUrl: baseUrl + \\\"/dist\\\",\\n            paths: {\\n                aurelia: baseUrl + \\\"/Content/scripts/aurelia\\\",\\n                webcomponentsjs: baseUrl + \\\"/Content/scripts/webcomponentsjs\\\",\\n                dist: baseUrl + \\\"/dist\\\",\\n                views: baseUrl + \\\"/dist/views\\\",\\n                resources: baseUrl + \\\"/dist/resources\\\",\\n            }\\n        });\\n\\n        require(['aurelia/aurelia-bundle-latest']);\\n    &lt;/script&gt;\\n&lt;/body&gt;\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7ae8ed76-6aeb-4bd1-8e97-fd9cbf4fc0e8.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6c81a550-05d3-44ab-887c-3e531eebd888.png\\\" width=\\\"448\\\" height=\\\"198\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://aurelia.io/\"]],[\"a\",[\"href\",\"http://vswebessentials.com/\"]],[\"a\",[\"href\",\"http://www.typescriptlang.org\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Console\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/aurelia-appInsights\"]],[\"a\",[\"href\",\"http://jspm.io/\"]],[\"a\",[\"href\",\"https://gitter.im/Aurelia/Discuss\"]],[\"a\",[\"href\",\"https://github.com/cmichaelgraham/aurelia-typescript\"]],[\"a\",[\"href\",\"https://github.com/cmichaelgraham/aurelia-typescript/tree/master/aurelia-require-bundle\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/aurelia-appInsights/blob/master/Aurelia-AppInsights/build/tasks/build.js\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In my last couple of posts I’ve spoken about the amazing Javascript framework, \"],[0,[0],1,\"Aurelia\"],[0,[],0,\", that I’ve been coding in. Visual Studio is my IDE of choice – not only because I’m used to it but because it’s just a brilliant editor – even for Javascript, Html and other web technologies. If you’re using VS for web development, make sure that you install \"],[0,[1],1,\"Web Essentials\"],[0,[],0,\" – as the name implies, it’s essential!\"]]],[1,\"h2\",[[0,[],0,\"Debugging\"]]],[1,\"p\",[[0,[],0,\"One of the best things about doing web development in VS – especially if you have a lot of Javascript – is the ability to debug from within VS. You set breakpoints in your script, run your site in IE, and presto! you’re debugging. You can see call-stack, autos, set watches – it’s really great. Unfortunately, until recently I haven’t been able to debug Aurelia projects in VS. We’ll get to why that is shortly – but I want to take a small tangent to talk about console logging in Aurelia. It’s been the lifesaver I’ve needed while I work out why debugging Aurelia wasn’t working.\"]]],[1,\"h3\",[[0,[],0,\"Console\"]]],[1,\"p\",[[0,[],0,\"Few developers actually make use of the browser console while developing – which is a shame, since the console is really powerful. The easiest way to see it in action is to open an Aurelia project, locate app.ts (yes, I’m using \"],[0,[2],1,\"TypeScript\"],[0,[],0,\" for my Aurelia development) and add a “console.debug(“hi there!”) to the code:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Line 8 is where I add the call to console.log. Here it is in IE’s console when I run the solution:\"]]],[10,1],[1,\"p\",[[0,[],0,\"(To access the console in Chrome or in IE, press F12 to bring up “developer tools” – then just open the console tab). Here’s the same view in Chrome:\"]]],[10,2],[1,\"p\",[[0,[],0,\"There are a couple of logging methods: log(), info(), warn(), error() and debug(). You can also group entries together and do \"],[0,[3],1,\"host of other useful debugging tricks\"],[0,[],0,\", like timing or logging stack traces.\"]]],[1,\"h3\",[[0,[],0,\"Logging an Object\"]]],[1,\"p\",[[0,[],0,\"Beside simply logging a string message you can also log an object. I found this really useful to inspect objects I was working with – usually VS lets you inspect objects, but since I couldn’t access the object in VS, I did it in the console. Let’s change the “console.log” line to “console.log(“In constructor: %O”, this);” The “%O” argument tells the console to log a hyperlink to the object that you can then use to inspect it. Here is the same console output, this time with “%O” (Note: you have to have the console open for this link to actually expand – otherwise you’ll just see a log entry, but won’t be able to inspect the object properties):\"]]],[10,3],[1,\"p\",[[0,[],0,\"You can now expand the nodes in the object tree to see the properties and methods of the logged object.\"]]],[1,\"h2\",[[0,[],0,\"Aurelia Log Appenders\"]]],[1,\"p\",[[0,[],0,\"If you’re doing a lot of debugging, then you may end up with dozens of view-models. Aurelia provides a LogManager class – and you can add any LogAppender implementation you want to create custom log collectors. (I do this for \"],[0,[4],1,\"Application Insights\"],[0,[],0,\" so that you can have Aurelia traces sent up to App Insights). Aurelia also provides an out-of-the-box ConsoleLogAppender. Here’s how you can add it (and set the logging level) – I do this in main.ts just before I bootstrap Aurelia:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now we can change the app.ts file to create a logger specifically for the class – anything logged to this will be prepended by the class name:\"]]],[10,5],[1,\"p\",[[0,[],0,\"On line 5 I set up a logger for the class – which I then use in lines 10 and 13. Here’s the console output:\"]]],[10,6],[1,\"p\",[[0,[],0,\"You can see how the “info” and the “debug” are colored differently (and info has a little info icon in the left gutter) and both entries are prepended with “[App]” – this makes wading through the logs a little bit easier. Also, when I want to switch the log level, I just set it down to LogManager.levels.error and no more info or debug messages will appear in the console – no need to remove them from the code.\"]]],[1,\"h2\",[[0,[],0,\"Why Can’t VS Debug Aurelia?\"]]],[1,\"p\",[[0,[],0,\"Back to our original problem: debugging Aurelia in Visual Studio. Here’s what happens when you set a breakpoint using the skeleton app:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Visual Studio says that “No symbols have been loaded for this document”. What gives?\"]]],[1,\"p\",[[0,[],0,\"The reason is that Visual Studio cannot debug modules loaded using system.js. Let’s look at how Aurelia is bootstrapped in index.html:\"]]],[10,8],[1,\"p\",[[0,[],0,\"You can see that system.js is being used to load Aurelia and all its modules – it will also be the loader for your view-models. I’ve pinged the VS team about this – but haven’t been able to get an answer from anyone as to why this is the case.\"]]],[1,\"h2\",[[0,[],0,\"Switching the Loader to RequireJS\"]]],[1,\"p\",[[0,[],0,\"Aurelia (out of the box) uses \"],[0,[5],1,\"jspm\"],[0,[],0,\" to load its packages – and it’s a great tool. Unfortunately, for anyone who wants to debug with VS you’ll have to find another module loader. Fortunately Aurelia allows you to swap out your loader! I got in touch with Mike Graham via the \"],[0,[6],1,\"Aurelia gitter discussion page\"],[0,[],0,\" – and he was kind enough to point me in the right direction – thanks Mike!\"]]],[1,\"p\",[[0,[],0,\"Following some \"],[0,[7],1,\"examples by Mike Graham\"],[0,[],0,\", I was able to switch from system.js to requirejs. The switch is fairly straight-forward – here they are:\"]]],[3,\"ol\",[[[0,[],0,\"Create a bundled require-compatible version of aurelia using \"],[0,[8],1,\"Mike’s script\"],[0,[],0,\" and add it to the solution as a static script file. Updating the file means re-running the script and replacing the aurelia-bundle. Unfortunately this is not as clean an upgrade path as jspm, where you’d just run “jspm update” to update the jspm packages automatically.\"]],[[0,[],0,\"Change the index.html page to load require.js and then configure it.\"]],[[0,[],0,\"Make a call to load the Aurelia run-time using requirejs.\"]],[[0,[],0,\"Fix relative paths to views in router configurations – though this may not be required for everyone, depending on how you’re referencing your modules when you set up your routes.\"]]]],[1,\"p\",[[0,[],0,\"Here’s an update index page that uses requirejs:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Now instead of loading system.js, you need to load core.js and require.js. Then I have a script (this could be placed into its own file) which configures requirejs (lines 9-24). I set the baseUrl for requirejs as well as some paths. You’ll have to play with these until requirejs can successfully locate all or your dependencies and view-models. Line 23 then loads the Aurelia runtime bundle via requirejs – this then calls your main or app class, depending on how you configure the <body> tag (either as aurelia-main or aurelia-app).\"]]],[1,\"p\",[[0,[],0,\"Now that you’re loading Aurelia using requirejs, you can set breakpoints in your ts file (assuming that you’re generating symbols through VS or \"],[0,[9],1,\"through Gulp\"],[0,[],0,\"/Grunt):\"]]],[10,10],[1,\"p\",[[0,[],0,\"Voila – you can now debug Aurelia using VS!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"When you’re doing Aurelia development using Visual Studio, you’re going to have to decide between the ease of package update (using jspm) or debugging ability (using requirejs). Using requirejs requires (ahem) a bit more effort since you need to bundle Aurelia manually, and I found getting the requirejs paths correct proved a fiddly challenge too. However, the ability to set breakpoints in your code in VS and debug is, in my opinion, worth the effort. I figure you’re probably not going to be updating the Aurelia framework that often (once it stabilizes after release) but you’ll be debugging plenty. Also, don’t forget to use the console and log appenders! Every tool in your arsenal makes you a better developer.\"]]],[1,\"p\",[[0,[],0,\"Happy debugging!\"]]],[1,\"p\",[[0,[],0,\"P.S. If you know how to debug modules that are loaded using system.js from VS, please let the rest of us know!\"]]]]}","published_at":1427224686000,"status":"published","published_by":1},{"id":"59a5b680-7651-440d-8286-901593717cc1","title":"Aurelia, Azure and VSTS","slug":"aurelia-azure-and-vsts","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"{\\n  \\\"sdk\\\": {\\n    \\\"version\\\": \\\"1.0.4\\\"\\n  }\\n}\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/61a4dc74-b9ef-466e-839c-e8bef965e68e.png\\\" target=\\\"_blank\\\"><img width=\\\"302\\\" height=\\\"218\\\" title=\\\"image\\\" style=\\\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8ba06ff0-8452-4001-976f-c6de8fda1880.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"mkdir frontend\\ncd frontend\\ndotnet new webapi\\ncd ..\\nmkdir API\\ncd API\\ndotnet new webapi\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1e7d032a-075e-4e00-a32d-7ee0b27dfcdc.png\\\" target=\\\"_blank\\\"><img width=\\\"371\\\" height=\\\"221\\\" title=\\\"image\\\" style=\\\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a47cfea4-6910-4eed-9de0-2f1d957b7db0.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/637a8409-48f7-484c-b183-aa6a0a39f59a.png\\\"><img width=\\\"299\\\" height=\\\"105\\\" title=\\\"image\\\" style=\\\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8714a2ae-aeeb-4750-a9cb-3569af41e412.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">au new --here</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">--here</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/886c77ba-42fe-4bc1-b1e4-c2006b7ee226.png\\\"><img width=\\\"354\\\" height=\\\"126\\\" title=\\\"image\\\" style=\\\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/143629bb-f9f7-4c0e-ba77-251c16e29728.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"{\\n  \\\"extends\\\": [\\\"tslint:latest\\\"],\\n  \\\"rules\\\": {\\n    \\n  }\\n}\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5e8d659a-4185-48d6-8b46-96fa9fdbc862.png\\\"><img width=\\\"309\\\" height=\\\"285\\\" title=\\\"image\\\" style=\\\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a91465e3-2b40-4ba1-97c6-6180273b61f4.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"{\\n  \\\"extends\\\": [\\\"tslint:latest\\\"],\\n  \\\"rules\\\": {\\n    \\\"ordered-imports\\\": [\\n      false\\n    ],\\n    \\\"quotemark\\\": [\\n      false\\n    ]\\n  }\\n}\\n\",\"language\":\"javascript; highlight\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">au install aurelia-fetch-client whatwg-fetch</font>\"}],[\"code\",{\"code\":\"import { autoinject } from 'aurelia-framework';\\nimport { HttpClient } from 'aurelia-fetch-client';\\n\\nconst baseUrl = \\\"http://localhost:1360/api\\\";\\n\\n@autoinject\\nexport class ApiWrapper {\\n    public message = 'Hello World!';\\n    public values: string[];\\n\\n    constructor(public client: HttpClient) {\\n\\t\\tclient.configure(config =&gt; {\\n\\t\\t\\tconfig\\n\\t\\t\\t\\t.withBaseUrl(baseUrl)\\n\\t\\t\\t\\t.withDefaults({\\n\\t\\t\\t\\t\\theaders: {\\n\\t\\t\\t\\t\\t\\tAccept: 'application/json',\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t});\\n\\t\\t});\\n\\t}\\n}\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"import { autoinject } from 'aurelia-framework';\\nimport { ApiWrapper } from './api';\\n\\n@autoinject\\nexport class App {\\n  public message = 'Hello World!';\\n  public values: string[];\\n\\n  constructor(public api: ApiWrapper) {\\n    this.initValues();\\n  }\\n\\n  private async initValues() {\\n    try {\\n      this.values = await this.api.client.fetch(\\\"/values\\\")\\n        .then((res) =&gt; res.json());\\n    } catch (ex) {\\n      console.error(ex);\\n    }\\n  }\\n}\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"<template>\\n  <h1>${message}</h1>\\n  <h3>Values</h3>\\n  <ul>\\n    <li repeat.for=\\\"val of values\\\">${val}</li>\\n  </ul>\\n</template>\\n\",\"language\":\"html;\"}],[\"code\",{\"code\":\"public void ConfigureServices(IServiceCollection services)\\n{\\n  // Add framework services.\\n  services.AddMvc();\\n\\tservices.AddCors();\\n}\\n\\n// This method gets called by the runtime. Use this method to configure the HTTP request pipeline.\\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)\\n{\\n  loggerFactory.AddConsole(Configuration.GetSection(\\\"Logging\\\"));\\n  loggerFactory.AddDebug();\\n\\n  app.UseCors(p =&gt; p.AllowAnyOrigin().AllowAnyMethod());\\n  app.UseMvc();\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2d4e4be1-dd11-422d-990c-b33fe7969174.png\\\" target=\\\"_blank\\\"><img width=\\\"270\\\" height=\\\"165\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5e26fa9e-fbff-4216-9280-37081283ae84.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"export default {\\n  apiBaseUrl: \\\"http://localhost:64705/api\\\",\\n  debug: true,\\n  testing: true,\\n};\\n\",\"language\":\"javascript; highlight\"}],[\"code\",{\"code\":\"import { autoinject } from 'aurelia-framework';\\nimport { HttpClient } from 'aurelia-fetch-client';\\nimport environment from './environment';\\n\\n@autoinject\\nexport class ApiWrapper {\\n    public message = 'Hello World!';\\n    public values: string[];\\n\\n    constructor(public client: HttpClient) {\\n        client.configure(config =&gt; {\\n            config\\n                .withBaseUrl(environment.apiBaseUrl)\\n                .withDefaults({\\n                    headers: {\\n                        Accept: 'application/json',\\n                    },\\n                });\\n        });\\n    }\\n}\\n\",\"language\":\"javascript; highlight\"}],[\"code\",{\"code\":\"{\\n      \\\"api\\\": {\\n            \\\"baseUri\\\": \\\"http://localhost:12487/api\\\"\\n      }\\n}\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"import { autoinject } from 'aurelia-framework';\\nimport { HttpClient } from 'aurelia-fetch-client';\\nimport { AureliaConfiguration } from 'aurelia-configuration';\\n\\n@autoinject\\nexport class ApiWrapper {\\n    public message = 'Hello World!';\\n    public values: string[];\\n\\n    constructor(public client: HttpClient, private aureliaConfig: AureliaConfiguration) {\\n        client.configure(config =&gt; {\\n            config\\n                .withBaseUrl(aureliaConfig.get(\\\"api.baseUri\\\"))\\n                .withDefaults({\\n                    headers: {\\n                        Accept: 'application/json',\\n                    },\\n                });\\n        });\\n    }\\n}\\n\",\"language\":\"javascript; highlight\"}],[\"code\",{\"code\":\"import {Aurelia} from 'aurelia-framework';\\nimport environment from './environment';\\n\\nexport function configure(aurelia: Aurelia) {\\n  aurelia.use\\n    .standardConfiguration()\\n    .feature('resources')\\n    .plugin('aurelia-configuration');\\n  ...\\n\",\"language\":\"javascript; highlight=[8];\"}],[\"code\",{\"code\":\"{\\n  \\\"name\\\": \\\"frontend\\\",\\n  \\\"type\\\": \\\"project:application\\\",\\n  \\\"platform\\\": {\\n    ...\\n  },\\n  ...\\n  \\\"build\\\": {\\n    \\\"targets\\\": [\\n     ...\\n    ],\\n    \\\"loader\\\": {\\n      ...\\n    },\\n    \\\"options\\\": {\\n      ...\\n    },\\n    \\\"bundles\\\": [\\n      ...\\n    ],\\n    \\\"copyFiles\\\": {\\n      \\\"src/config/*.json\\\": \\\"wwwroot/config\\\"\\n    }\\n  }\\n}\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"import {App} from '../../src/app';\\n\\ndescribe('the app', () =&gt; {\\n  it('says hello', () =&gt; {\\n    expect(new App().message).toBe('Hello World!');\\n  });\\n});\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"import { HttpClient } from 'aurelia-fetch-client';\\n\\nexport class HttpClientMock extends HttpClient {\\n}\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"let aureliaConfig = new AureliaConfiguration();\\naureliaConfig.set(\\\"api.baseUri\\\", \\\"http://test\\\");\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"import {App} from '../../src/app';\\nimport {ApiWrapper} from '../../src/api';\\nimport {HttpClientMock} from './utils/mock-fetch';\\nimport {AureliaConfiguration} from 'aurelia-configuration';\\n\\ndescribe('the app', () =&gt; {\\n  it('says hello', async done =&gt; {\\n    // arrange\\n    let aureliaConfig = new AureliaConfiguration();\\n    aureliaConfig.set(\\\"api.baseUri\\\", \\\"http://test\\\");\\n\\n    const client = new HttpClientMock();\\n    client.setup({\\n      data: [\\\"testValue1\\\", \\\"testValue2\\\", \\\"testValue3\\\"],\\n      headers: {\\n        'Content-Type': \\\"application/json\\\",\\n      },\\n      url: \\\"/values\\\",\\n    });\\n    const api = new ApiWrapper(client, aureliaConfig);\\n\\n    // act\\n    let sut: App;\\n    try {\\n      sut = new App(api);\\n    } catch (e) {\\n      console.error(e);\\n    }\\n\\n    // assert\\n    setTimeout(() =&gt; {\\n      expect(sut.message).toBe('Hello World!');\\n      expect(sut.values.length).toBe(3);\\n      expect(sut.values).toContain(\\\"testValue1\\\");\\n      expect(sut.values).toContain(\\\"testValue2\\\");\\n      expect(sut.values).toContain(\\\"testValue3\\\");\\n      done();\\n    }, 10);\\n  });\\n});\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"import { HttpClient } from 'aurelia-fetch-client';\\nexport class HttpClientMock extends HttpClient {\\n}\\n\\nexport interface IMethodConfig {\\n    url: string;\\n    method?: string;\\n    status?: number;\\n    statusText?: string;\\n    headers?: {};\\n    data?: {};\\n};\\n\\nexport class HttpClientMock extends HttpClient {\\n    private config: IMethodConfig[] = [];\\n\\n    public setup(config: IMethodConfig) {\\n        this.config.push(config);\\n    }\\n\\n    public async fetch(input: Request | string, init?: RequestInit) {\\n        let url: string;\\n        if (typeof input === \\\"string\\\") {\\n            url = input;\\n        } else {\\n            url = input.url;\\n        }\\n\\n        // find the matching setup method\\n        let methodConfig: IMethodConfig;\\n        methodConfig = this.config.find(c =&gt; c.url === url);\\n        if (!methodConfig) {\\n            console.error(`---MockFetch: No such method setup: ${url}`);\\n            return Promise.reject(new Response(null,\\n                {\\n                    status: 404,\\n                    statusText: `---MockFetch: No such method setup: ${url}`,\\n                }));\\n        }\\n\\n        // set up headers\\n        let responseInit: ResponseInit = {\\n            headers: methodConfig.headers || {},\\n            status: methodConfig.status || 200,\\n            statusText: methodConfig.statusText || \\\"\\\",\\n        };\\n\\n        // get a unified request object\\n        let request: Request;\\n        if (Request.prototype.isPrototypeOf(input)) {\\n            request = (<request> input);\\n        } else {\\n            request = new Request(input, responseInit || {});\\n        }\\n\\n        // create a response object\\n        let response: Response;\\n        const data = JSON.stringify(methodConfig.data);\\n        response = new Response(data, responseInit);\\n\\n        // resolve or reject accordingly\\n        return response.status &gt;= 200 &amp;&amp; response.status &lt; 300 ?\\n            Promise.resolve(response) : Promise.reject(response);\\n    }\\n}\\n</request>\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"'use strict';\\nconst path = require('path');\\nconst project = require('./aurelia_project/aurelia.json');\\nconst tsconfig = require('./tsconfig.json');\\n\\nlet testSrc = [\\n  { pattern: project.unitTestRunner.source, included: false },\\n  'test/aurelia-karma.js'\\n];\\n\\nlet output = project.platform.output;\\nlet appSrc = project.build.bundles.map(x =&gt; path.join(output, x.name));\\nlet entryIndex = appSrc.indexOf(path.join(output, project.build.loader.configTarget));\\nlet entryBundle = appSrc.splice(entryIndex, 1)[0];\\nlet files = [entryBundle].concat(testSrc).concat(appSrc);\\n\\nmodule.exports = function(config) {\\n  config.set({\\n    basePath: '',\\n    frameworks: [project.testFramework.id],\\n    files: files,\\n    exclude: [],\\n    preprocessors: {\\n      [project.unitTestRunner.source]: [project.transpiler.id],\\n      'wwwroot/scripts/app-bundle.js': ['coverage']\\n    },\\n    typescriptPreprocessor: {\\n      typescript: require('typescript'),\\n      options: tsconfig.compilerOptions\\n    },\\n    reporters: ['progress', 'tfs', 'coverage', 'karma-remap-istanbul'],\\n    port: 9876,\\n    colors: true,\\n    logLevel: config.LOG_INFO,\\n    autoWatch: true,\\n    browsers: ['PhantomJS'],\\n    singleRun: false,\\n    // client.args must be a array of string.\\n    // Leave 'aurelia-root', project.paths.root in this order so we can find\\n    // the root of the aurelia project.\\n    client: {\\n      args: ['aurelia-root', project.paths.root]\\n    },\\n\\n    phantomjsLauncher: {\\n      // Have phantomjs exit if a ResourceError is encountered (useful if karma exits without killing phantom)\\n      exitOnResourceError: true\\n    },\\n\\n    coverageReporter: {\\n      dir: 'reports',\\n      reporters: [\\n        { type: 'json', subdir: 'coverage', file: 'coverage-final.json' },\\n      ]\\n    },\\n\\n    remapIstanbulReporter: {\\n      src: 'reports/coverage/coverage-final.json',\\n      reports: {\\n        cobertura: 'reports/coverage/cobertura.xml',\\n        html: 'reports/coverage/html'\\n      }\\n    }\\n  });\\n};\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"// hack to fix the relative paths in the generated mapped html report\\nlet fixPaths = done =&gt; {\\n  let repRoot = path.join(__dirname, '../../reports/');\\n  let repPaths = [\\n    path.join(repRoot, 'src/**/*.html'),\\n    path.join(repRoot, 'src/*.html'),\\n  ];\\n  return gulp.src(repPaths, { base: repRoot })\\n        .pipe(replace(/(..\\\\/..\\\\/..\\\\/)(\\\\w)/gi, '../coverage/html/$2'))\\n        .pipe(gulp.dest(path.join(repRoot)));\\n};\\n\\nlet unit;\\n\\nif (CLIOptions.hasFlag('watch')) {\\n  unit = gulp.series(\\n    build,\\n    gulp.parallel(\\n      watch(build, onChange),\\n      karma,\\n      fixPaths\\n    )\\n  );\\n} else {\\n  unit = gulp.series(\\n    build,\\n    karma,\\n    fixPaths\\n  );\\n}\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/72b236bf-fd45-45b3-9070-3287303247e6.png\\\" target=\\\"_blank\\\"><img width=\\\"305\\\" height=\\\"306\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0573ca73-9e3b-4e2e-98f0-448f5d9c50ae.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9b01b4c6-bca4-4f1e-8fb9-2d4498b76231.png\\\" target=\\\"_blank\\\"><img width=\\\"333\\\" height=\\\"179\\\" title=\\\"SNAGHTML323918f\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"SNAGHTML323918f\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ac8a03f6-4c0b-489f-a757-e7fa56b35e3c.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/54921ef4-6167-46e1-9de3-4277bda6fa3e.png\\\" target=\\\"_blank\\\"><img width=\\\"319\\\" height=\\\"163\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ae799414-0963-480f-9fbb-5e64273d45d4.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1fe95c7d-f588-4063-abda-479c87794755.png\\\" target=\\\"_blank\\\"><img width=\\\"277\\\" height=\\\"104\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e91eff17-6284-4452-b096-c421006379b4.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e1ee97d8-a7a6-4ecc-bb7c-4a16cf65dc6e.png\\\" target=\\\"_blank\\\"><img width=\\\"330\\\" height=\\\"109\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0e80ad3c-4056-4ed3-87f7-ec8247095fa8.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/09a2c2e9-0ad1-459a-8832-963872835b86.png\\\" target=\\\"_blank\\\"><img width=\\\"338\\\" height=\\\"111\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/17a16497-36e7-4011-86d1-f1ea5718de6d.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://aurelia.io/\"]],[\"a\",[\"href\",\"https://d3js.org/\"]],[\"a\",[\"href\",\"https://github.com/aurelia/cli\"]],[\"a\",[\"href\",\"https://code.visualstudio.com/\"]],[\"a\",[\"href\",\"http://blog.aurelia.io/2016/10/11/introducing-the-aurelia-vs-code-plugin/\"]],[\"a\",[\"href\",\"https://www.visualstudio.com/downloads/\"]],[\"a\",[\"href\",\"https://www.typescriptlang.org/\"]],[\"a\",[\"href\",\"https://karma-runner.github.io/1.0/index.html\"]],[\"a\",[\"href\",\"http://phantomjs.org/\"]],[\"a\",[\"href\",\"https://www.microsoft.com/net/core#windowsvs2017\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/app-service/\"]],[\"a\",[\"href\",\"https://www.visualstudio.com/team-services/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/AzureAureliaDemo\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=emmanuelbeziat.vscode-great-icons\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=eg2.tslint\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=AureliaEffect.aurelia\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I am a huge fan of \"],[0,[0],1,\"Aurelia\"],[0,[],0,\" – and that was even when I was working with it in the beta days. I recently had to do some development to display \"],[0,[1],1,\"d3\"],[0,[],0,\" graphs, and needed a simple SPA app. Of course I decided to use Aurelia. During development, I was again blown away by how well thought out Aurelia is – and using some new (to me) tooling, the experience was super. In this post I’ll walk through the tools that I used as well as the build/release pipeline that I set up to host the site in Azure.\"]]],[1,\"h2\",[[0,[],0,\"Tools\"]]],[1,\"p\",[[0,[],0,\"Here are the tools that I used:\"]]],[3,\"ol\",[[[0,[2],1,\"aurelia-cli\"],[0,[],0,\" to create the project, scaffold and install components, build and run locally\"]],[[0,[3],1,\"VS Code\"],[0,[],0,\" for frontend editing, with a great \"],[0,[4],1,\"Aurelia extension\"],[0,[],0,\"\"]],[[0,[5],1,\"Visual Studio 2017\"],[0,[],0,\" for coding/running the API\"]],[[0,[6],1,\"TypeScript\"],[0,[],0,\" for the Aurelia code\"]],[[0,[7],1,\"Karma\"],[0,[],0,\" (with \"],[0,[8],1,\"phantomJS\"],[0,[],0,\") and Istanbul for frontend testing and coverage\"]],[[0,[],0,\".\"],[0,[9],1,\"NET Core\"],[0,[],0,\" for the Aurelia host as well as for an API\"]],[[0,[10],1,\"Azure App Services\"],[0,[],0,\" to host the web app\"]],[[0,[11],1,\"VSTS\"],[0,[],0,\" for Git source control, build and release\"]]]],[1,\"h2\",[[0,[],0,\"The Demo App and the Challenges\"]]],[1,\"p\",[[0,[],0,\"To walk through the development process, I’m going to create a stupid-simple app. This isn’t a coding walkthrough per se – I want to focus on how to use the tooling to support your development process. However, I’ll demonstrate the challenges as well as the solutions, hopefully showing you how quickly you can get going and do what you do best – code!\"]]],[1,\"p\",[[0,[],0,\"The demo app will be an Aurelia app with just a REST call to an API. While it is a simple app, I’ll walk through a number of important development concepts:\"]]],[3,\"ol\",[[[0,[],0,\"Creating a new project\"]],[[0,[],0,\"Configuring VS Code\"]],[[0,[],0,\"Installing components\"]],[[0,[],0,\"Building, bundling and running the app locally\"]],[[0,[],0,\"Handling different configs for different environments\"]],[[0,[],0,\"Automated build, test and deployment of the app\"]]]],[1,\"h3\",[[0,[],0,\"Creating the DotNet Projects\"]]],[1,\"p\",[[0,[],0,\"There are some prerequisites to getting started, so I installed all of these:\"]]],[3,\"ul\",[[[0,[],0,\"nodejs\"]],[[0,[],0,\"npm\"]],[[0,[],0,\"dotnet core\"]],[[0,[],0,\"aurelia-cli\"]],[[0,[],0,\"VS Code\"]],[[0,[],0,\"VS 2017\"]]]],[1,\"p\",[[0,[],0,\"Once I had the prereqs installed, I created a new empty folder (actually I cloned an empty Git repo – if you don’t clone a repo, remember to git init). Since I wanted to peg the dotnet version, I created a new file called global.json:\"]]],[10,0],[1,\"p\",[[0,[],0,\"I also created a .gitignore (helpful tip: if you open the folder in Visual Studio and use Team Explorer->Settings->Repository Settings, you can create a default .gitignore and .gitattributes file).\"]]],[1,\"p\",[[0,[],0,\"Then I created a new dotnet webapi project to “host” the Aurelia app in a folder called frontend and another dotnet project to be the API in a folder called API:\"]]],[10,1],[1,\"p\",[[0,[],0,\"The commands are:\"]]],[10,2],[1,\"p\",[[0,[],0,\"I then opened the API project in Visual Studio. Pressing save prompted me to create a solution file, which I did in the API folder. I also created an empty readme.txt file in the wwwroot folder (I’ll explain why when we get to the build) and changed the Launch URL in the project properties to “api/values”:\"]]],[10,3],[1,\"p\",[[0,[],0,\"When I press F5 to debug, I see this:\"]]],[10,4],[1,\"h3\",[[0,[],0,\"Creating the Aurelia Project\"]]],[1,\"p\",[[0,[],0,\"I was now ready to create the Aurelia skeleton. The last time I used Aurelia, there was no such thing as the \"],[0,[2],1,\"aurelia-cli\"],[0,[],0,\" – so it was a little bumpy getting started. I found using the cli and the project structure it creates for building/bundling made development smooth as butter. So I cd’d back to the frontend folder and ran the aurelia-cli command to create the Aurelia project:\"]]],[10,5],[1,\"p\",[[0,[],0,\". The “\"]]],[10,6],[1,\"p\",[[0,[],0,\"” is important because it tells the aurelia-cli to create the project in this directory without creating another subdirectory. A wizard then walked me through some choices: here are my responses:\"]]],[3,\"ul\",[[[0,[],0,\"Target platform: .NET Core\"]],[[0,[],0,\"Transpiler: TypeScript\"]],[[0,[],0,\"Template: With minimum minification\"]],[[0,[],0,\"CSS Processor: Less\"]],[[0,[],0,\"Unit testing: Yes\"]],[[0,[],0,\"Install dependencies: Yes\"]]]],[1,\"p\",[[0,[],0,\"That created the Aurelia project for me and installed all of the nodejs packages that Aurelia requires. Once the install completed, I was able to run by typing “au run”:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Whoop! The skeleton is up, so it’s time to commit!\"]]],[1,\"p\",[[0,[],0,\"You can find the repo I used for this post \"],[0,[12],1,\"here\"],[0,[],0,\". There are various branches – start is the the start of the project up until now – in other words, the absolute bare skeleton of the project.\"]]],[1,\"h3\",[[0,[],0,\"Configuring VS Code\"]]],[1,\"p\",[[0,[],0,\"Now that I have a project structure, I can start coding. I’ve already got Visual Studio for the API project, which I could use for the frontend editing, but I really like doing nodejs development in VS Code. So I open up the frontend folder in VS Code.\"]]],[1,\"p\",[[0,[],0,\"I’ve also installed some VS Code extensions:\"]]],[3,\"ol\",[[[0,[13],1,\"VSCode Great Icons\"],[0,[],0,\" – makes the icons in the file explorer purdy (don’t forget to configure your preferences after you install the extension!)\"]],[[0,[14],1,\"TSLint\"],[0,[],0,\" – lints my TypeScript as I code\"]],[[0,[15],1,\"aurelia\"],[0,[],0,\" – palette commands and html intellisense\"]]]],[1,\"h4\",[[0,[],0,\"Configuring TSLint\"]]],[1,\"p\",[[0,[],0,\"There is already an empty tslint.json file in the root of the frontend project. Once you’ve installed the VS Code TSLint extension, you’ll see lint warnings in the status bar: though you have to first configure which rules you want to run. I usually start by extending the tslint:latest rules. Edit the tslint.json file to look like this:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Now you’ll see some warnings and green squigglies in the code:\"]]],[10,9],[1,\"p\",[[0,[],0,\"I don’t care about the type of quotation marks (single or double) and I don’t care about alphabetically ordering my imports, so I override those rules:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Of course you can put whatever ruleset you want into this file – but making a coding standard for your team that’s enforced by a tool rather than in a wiki or word doc is a great practice! A helpful tip is that if you edit the json file in VS Code you get intellisense for the rules – and you can see the name of the rule in the warnings window.\"]]],[1,\"h3\",[[0,[],0,\"Installing Components\"]]],[1,\"p\",[[0,[],0,\"Now we can use the aurelia cli (au) to install components. For example, I want to do some REST calls, so I want to install the fetch-client:\"]]],[10,11],[1,\"p\",[[0,[],0,\"This not only adds the package, but amends the aurelia.json manifest file (in the aurelia_project folder) so that the aurelia-fetch-client is bundled when the app is “compiled”. I also recommend installing whatwg-fetch which is a fetch polyfill. Let’s create a new class which is a wrapper for the fetch client:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Note that (for now) we’re hard-coding the baseUrl. We’ll address config shortly.\"]]],[1,\"p\",[[0,[],0,\"We can now import in the ApiWrapper (via injection) and call the values method:\"]]],[10,13],[1,\"p\",[[0,[],0,\"Here’s the updated html for the app.html page:\"]]],[10,14],[1,\"p\",[[0,[],0,\"Nothing too fancy – but shown here to be complete. I’m not going to make a full app here, since that’s not the goal of this post.\"]]],[1,\"p\",[[0,[],0,\"Finally, we need to enable CORS on the Web API (since it does not allow CORS by default). Add the Microsoft.AspNet.Cors package to the API project and then add the services.AddCors() and app.UseCors() lines (see this snippet):\"]]],[10,15],[1,\"p\",[[0,[],0,\"Now we can get this when we run the project (using “au run”):\"]]],[10,16],[1,\"p\",[[0,[],0,\"If you’re following along in the \"],[0,[12],1,\"repo\"],[0,[],0,\" code, the changes are on the “Step1-AddFetch” branch.\"]]],[1,\"h3\",[[0,[],0,\"Running Locally\"]]],[1,\"p\",[[0,[],0,\"Running locally is trivial. I end up with Visual Studio open and pressing F5 to run the backend API project – the frontend project is just as trivial. In VSCode, with the frontend folder open, just hit ctrl-shift-p to bring up the command palette and then type/select “au run --watch” to launch the frontend “build”. This transpiles the TypeScript to JavaScript, compiles Less (or SASS) to css, bundles and minifies all your html, compiled css and JavaScript into a single app-bundle.js in wwwroot\\\\scripts. It also minifies and bundles Aurelia and its dependencies into vendor-bundle.js, using the settings from the aurelia.json file. It’s a lot of work, but Aurelia takes care of it all for you – just run “au run” to do all that stuff and launch a server. If you add the --watch parameter, the process watches your source files (html, Less, TypeScript) and automatically recompiles everything and refreshes the browser automagically using browsersync. It’s as smooth as butter!\"]]],[1,\"h2\",[[0,[],0,\"Config Management\"]]],[1,\"h3\",[[0,[],0,\"Attempt 1 – Using environment.ts\"]]],[1,\"p\",[[0,[],0,\"Let’s fix up the hard-coded base URL for the api class. Aurelia does have the concept of “environments” – you can see that by looking in the src\\\\environment.ts file. You would be tempted to change the values of that file, but you’ll see that if you do, the contents get overwritten each time Aurelia compiles. Instead, open up the aurelia-project\\\\environments folder, where you’ll see three environment files – dev, stage and prod.ts. To change environment, just enter “au run --env dev” to get the dev environment or “au run --env prod” to get the prod environment. (Unfortunately you can’t change the environment using VSCode command palette, so you have to run the run command from a console or from the VSCode terminal).\"]]],[1,\"p\",[[0,[],0,\"Let’s edit the environments to put the api base URL there instead of hard-coding it:\"]]],[10,17],[1,\"p\",[[0,[],0,\"Of course we add the apiBaseUrl property to the stage and prod files too!\"]]],[1,\"p\",[[0,[],0,\"With that change, we can simply import the environment and use the value of the property in the api.ts file:\"]]],[10,18],[1,\"p\",[[0,[],0,\"The important changes are on line 2 (reading in the environment settings) and line 13 (using the value). Now we can run for different environments. If you’re following along in the \"],[0,[12],1,\"repo\"],[0,[],0,\" code, the changes are on the “Step2-EnvTsConfig” branch.\"]]],[1,\"h3\",[[0,[],0,\"Attempt 2 – Using a Json File\"]]],[1,\"p\",[[0,[],0,\"There’s a problem with the above approach though – if we have secrets (like access tokens or keys) then we don’t want them checked into source control. Also, when we get to build/release, we want the same build to go to multiple environments – using environment.ts means we have to build once for each environment and then select the correct package for the corresponding environment – it’s nasty. Rather, we want to be able to configure the environment settings during a release. This puts secret information in the release tool instead of source control, which is much better, and allows a single build to be deployed to any number of environments.\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, it’s not quite so simple (at first glance). The environment.ts file is bundled into app-bundle.js, so there’s no way to inject values at deploy time, unless you want to monkey with the bundle itself. It would be much better to take a leaf out of the .NET CORE playbook and set up a Json config file. Fortunately, there’s an Aurelia plugin that allows you to do just that! Conveniently, it’s called aurelia-configuration.\"]]],[1,\"p\",[[0,[],0,\"Run “au install aurelia-configuration” to install the module.\"]]],[1,\"p\",[[0,[],0,\"Now (by convention) the config module looks for a file called “config\\\\config.json”. So in the src folder, add a new folder called config and add a new file into the config folder called config.json:\"]]],[10,19],[1,\"p\",[[0,[],0,\"We can then inject the AureliaConfiguration class into our classes and call the get() method to retrieve a variable value. Let’s change the api.ts file again:\"]]],[10,20],[1,\"p\",[[0,[],0,\"Line 3 has us importing the type, line 10 has the constructor arg for the autoinjection and we get the value on line 13.\"]]],[1,\"p\",[[0,[],0,\"We also have to tell Aurelia to use the config plugin. Open main.ts and add the plugin code (line 8 below):\"]]],[10,21],[1,\"p\",[[0,[],0,\"There’s one more piece to this puzzle: the config.json file doesn’t get handled anywhere, so running the program won’t work. We need to tell the Aurelia bundler that it needs to add in the config.json file and publish it to the wwwroot folder. To do that, we can add in a copyFiles target onto the aurelia.json settings file:\"]]],[10,22],[1,\"p\",[[0,[],0,\"At the bottom of the file, just after the build.bundles settings, we add the copyFiles target. The config.json file is now copied to the wwwroot/config folder when we build, ready to be read at run time! If you’re following along in the \"],[0,[12],1,\"repo\"],[0,[],0,\" code, the changes are on the “Step3-JsonConfig” branch.\"]]],[1,\"h2\",[[0,[],0,\"Testing\"]]],[1,\"h3\",[[0,[],0,\"Authoring the Tests\"]]],[1,\"p\",[[0,[],0,\"Of course the API project would require tests – but doing .NET testing is fairly simple and there’s a ton of guidance on how to do that. I was more interested in testing the frontend (Aurelia) code with coverage results.\"]]],[1,\"p\",[[0,[],0,\"When I created the frontend project, Aurelia created a test stub project. If you open the test folder, there’s a simple test spec in unit\\\\app.spec.ts:\"]]],[10,23],[1,\"p\",[[0,[],0,\"We’ve changed the App class, so this code won’t compile correctly. Now we need to pass an ApiWrapper to the App constructor. And if we want to construct an ApiWrapper, we need an AureliaConfiguration instance as well as an HttpClient instance. We’re going to want to mock the API calls that the frontend makes, so let’s stub out a mock implementation of HttpClient. I add a new class in src\\\\test\\\\unit\\\\utils\\\\mock-fetch.ts:\"]]],[10,24],[1,\"p\",[[0,[],0,\"We’ll flesh this class out shortly. For now, it’s enough to get an instance of HttpClient for the ApiWrapper constructor. What about the AureliaConfiguration instance? Fortunately, we can create (and even configure) one really easily:\"]]],[10,25],[1,\"p\",[[0,[],0,\"We add the “api.BaseUri” key since that’s the value that the ApiWrapper reads from the configuration object. We can now flesh out the remainder of our test:\"]]],[10,26],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 13-19: configure the mock fetch response (we’ll see the rest of the mock HttpClient class shortly)\"]],[[0,[],0,\"Line 20: instantiate a new ApiWrapper\"]],[[0,[],0,\"Lines 23-28: call the App constructor\"]],[[0,[],0,\"Lines 31-38: we wrap the asserts in a timeout since the App constructor calls an async method (perhaps there’s a better way to do this?)\"]]]],[1,\"p\",[[0,[],0,\"Let’s finish off the test code by looking at the mock-fetch class:\"]]],[10,27],[1,\"p\",[[0,[],0,\"I won’t go through the whole class, but essentially you configure a mapping of routes to responses so that when the mock object is called it can return predictable data.\"]]],[1,\"p\",[[0,[],0,\"With those changes in place, we can run the tests using “au test”. This launches Chrome and runs the test. The Aurelia project did the heavy lifting to configure paths for the test runner (Karma) so that the tests “just work”.\"]]],[1,\"h3\",[[0,[],0,\"Going Headless and Adding Reports and Coverage\"]]],[1,\"p\",[[0,[],0,\"Now that we can run the tests in Chrome with results splashed to the console, we should consider how these tests would run in a build. Firstly, we want to produce a report file of some sort so that the build can save the results. We also want to add coverage. Finally, we want to run headless so that we can run this on an agent that doesn’t need access to a desktop to launch a browser!\"]]],[1,\"p\",[[0,[],0,\"We’ll need to add some development-time node packages to accomplish these changes:\"]]],[1,\"p\",[[0,[],0,\"yarn add karma-phantomjs-launcher karma-coverage karma-tfs gulp-replace --dev\"]]],[1,\"p\",[[0,[],0,\"With those package in place, we can change the karma.conf.js file to use phantomjs (a headless browser) instead of Chrome. We’re also going to add in the test result reporter, coverage reporter and a coverage remapper. The coverage will report coverage on the JavaScript files, but we would ideally want coverage on the TypeScript files – that’s what the coverage remapper will do for us.\"]]],[1,\"p\",[[0,[],0,\"Here’s the new karma.conf.js:\"]]],[10,28],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 25: add a preprocessor to instrument the code that we’re going to execute\"]],[[0,[],0,\"Line 31: we add reporters to produce results files (tfs), coverage and remapping\"]],[[0,[],0,\"Lines 45-48: we configure a catch-all to close phantomjs if something fails\"]],[[0,[],0,\"Lines 50-55: we configure the coverage to output a Json coverage file\"]],[[0,[],0,\"Lines 57-63: we configure the remapper so that we get TypeScript coverage results\"]]]],[1,\"p\",[[0,[],0,\"One gotcha I had that I couldn’t find a work-around for: the html files that are generated showing which lines of code were hit is generated with incorrect relative paths and the src folder (with detailed coverage) generated outside the html report folder. Eventually, I decided that a simple replace and file move was all I needed, so I modified the test.ts task in the aurelia-project\\\\tasks folder:\"]]],[10,29],[1,\"p\",[[0,[],0,\"I add new tasks called “updateIndex” and “copySrc” that fix up the paths for me. Perhaps there’s a config setting for the remapper that will render this obsolete, but this was the best I could come up with.\"]]],[1,\"p\",[[0,[],0,\"Now when you run “au test” you get a result file and coverage results for the TypeScript code all in the html folder with the correct paths. If you’re following along in the \"],[0,[12],1,\"repo\"],[0,[],0,\" code, these changes are on the master branch (this is the final state of the demo code).\"]]],[1,\"h2\",[[0,[],0,\"Automated Build and Test\"]]],[1,\"p\",[[0,[],0,\"We now have all the pieces in place to do a build. The build is fairly straightforward once you work out how to invoke the Arelia cli. Starting with a .NET Core Web App template, here is the definition I ended up with:\"]]],[10,30],[1,\"p\",[[0,[],0,\"Here are the task settings:\"]]],[3,\"ol\",[[[0,[],0,\".NET Core Restore – use defaults\"]],[[0,[],0,\".NET Core Build\"]],[[0,[],0,\"Change “Arguments” to --configuration $(BuildConfiguration) --version-suffix $(Build.BuildNumber)\"]],[[0,[],0,\"The extra bit added is the version-suffix arg which produces binaries with the same version as the build number\"]],[[0,[],0,\"npm install\"]],[[0,[],0,\"Change “working folder” to frontend (this is the directory of the Aurelia project).\\\\node_modules\\\\aurelia-cli\\\\bin\\\\aurelia-cli.js test\"]],[[0,[],0,\"Run command\"]],[[0,[],0,\"Set “Tool” to node\"]],[[0,[],0,\"Set “Arguments” to .\\\\node_modules\\\\aurelia-cli\\\\bin\\\\aurelia-cli.js test\"]],[[0,[],0,\"Expand “Advanced” and set “Working folder” to frontend\"]],[[0,[],0,\"This runs the tests and produces the test results and coverage results files\"]],[[0,[],0,\"Run command\"]],[[0,[],0,\"Set “Tool” to node\"]],[[0,[],0,\"Set “Arguments” to .\\\\node_modules\\\\aurelia-cli\\\\bin\\\\aurelia-cli.js build --env prod\"]],[[0,[],0,\"Expand “Advanced” and set “Working folder” to frontend\"]],[[0,[],0,\"This does transpilation, minification and bundling so that we’re ready to deploy\"]],[[0,[],0,\"Publish Test Results\"]],[[0,[],0,\"Set “Test Result Format” to VSTest\"]],[[0,[],0,\"Set “Test results files” to frontend/testresults/TEST*.xml\"]],[[0,[],0,\"Set “Test run title” to Aurelia\"]],[[0,[],0,\"Publish code coverage Results\"]],[[0,[],0,\"Set “Code Coverage Tool” to Cobertura\"]],[[0,[],0,\"Set “Summary File” to $(Build.SourcesDirectory)/frontend/reports/coverage/cobertura.xml\"]],[[0,[],0,\"Set “Report Directory” to $(System.DefaultWorkingDirectory)/frontend/reports/coverage/html\"]],[[0,[],0,\".NET Core Publish\"]],[[0,[],0,\"Make sure “Publish Web Projects” is checked – this is why I added a dummy readme file into the wwwroot folder of the API app, otherwise it’s not published as a web project\"]],[[0,[],0,\"Set “Arguments” to --configuration $(BuildConfiguration) --output $(build.artifactstagingdirectory) --version-suffix $(Build.BuildNumber)\"]],[[0,[],0,\"Make sure “Zip Published Projects” is checked\"]],[[0,[],0,\"On the Options Tab\"]],[[0,[],0,\"Set the build number format to 1.0.0$(rev:.r) to give the build number a 1.0.0.x format\"]],[[0,[],0,\"Set the default agent queue to Hosted VS2017 (or you can select a private build agent with VS 2017 installed)\"]]]],[1,\"p\",[[0,[],0,\"Now when I run the build, I get test and coverage results in the summary:\"]]],[10,31],[1,\"p\",[[0,[],0,\"The coverage files are there if you click the Code Coverage results tab, but there’s a problem with the css.\"]]],[10,32],[1,\"p\",[[0,[],0,\"The <link> elements are stripped out of the html pages when the iFrame for the coverage results shows – I’m working with the product team to find a workaround for this. If you download the results from the Summary page and unzip them, you get the correct rendering.\"]]],[1,\"p\",[[0,[],0,\"I can also see both web projects ready for deployment in the Artifacts tab:\"]]],[10,33],[1,\"p\",[[0,[],0,\"We’re ready for a release!\"]]],[1,\"h2\",[[0,[],0,\"The Release Definition\"]]],[1,\"p\",[[0,[],0,\"I won’t put the whole release to Azure here – the key point to remember is the configuration. We’ve done the work to move the configuration into the config.json file for this very reason.\"]]],[1,\"p\",[[0,[],0,\"Once you’ve set up an Azure endpoint, you can add in an “Azure App Services Deploy” task. Select the subscription and app service and then change the “Package or folder” from “$(System.DefaultWorkingDirectory)/**/*.zip” to “$(System.DefaultWorkingDirectory)/drop/frontend.zip” (or API.zip) to deploy the corresponding site. To handle the configuration, you simply add “wwwroot/config/config.json” to the “JSON variable substitution”.\"]]],[10,34],[1,\"p\",[[0,[],0,\"Now we can define an environment variable for the substitution. Just add one with the full “JSON path” for the variable. In our case, we want “api.baseUri” to be the name and then put in whatever the corresponding environment value is:\"]]],[10,35],[1,\"p\",[[0,[],0,\"We can repeat this for other variables if we need more.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I really love the Aurelia framework – and with the solid Aurelia cli, development is a really good experience. Add to that simple build and release management to Azure using VSTS, and you can get a complete site skeleton with full CI/CD in half a day. And that means you’re delivering better software, faster – always a good thing!\"]]],[1,\"p\",[[0,[],0,\"Happy Aurelia-ing!\"]]]]}","published_at":1497053122000,"status":"published","published_by":1},{"id":"af9e1bcd-cac4-40f0-84e7-b9013e2882cf","title":"Aurelia, Karma and More VS Debugging Goodness","slug":"aurelia-karma-and-more-vs-debugging-goodness","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"// Karma configuration\\nmodule.exports = function (config) {\\n    config.set({\\n        basePath: \\\"\\\",\\n\\n        frameworks: [\\\"jasmine\\\", \\\"requirejs\\\", \\\"sinon\\\"],\\n\\n        // list of files / patterns to load in the browser\\n        files: [\\n            // test specific files\\n            \\\"test-main.js\\\",\\n            \\\"node_modules/jasmine-sinon/lib/jasmine-sinon.js\\\",\\n\\n            // source files\\n            { pattern: \\\"dist/**/*.js\\\", included: false },\\n\\n            // test files\\n            { pattern: 'test/unit/**/*.js', included: false },\\n\\n            // framework and lib files\\n            { pattern: \\\"Content/scripts/**/*.js\\\", included: false },\\n        ],\\n\\n        // list of files to exclude\\n        exclude: [\\n        ],\\n\\n        // available reporters: https://npmjs.org/browse/keyword/karma-reporter\\n        reporters: [\\\"progress\\\"],\\n\\n        // web server port\\n        port: 9876,\\n\\n        // enable / disable colors in the output (reporters and logs)\\n        colors: true,\\n\\n        // possible values: config.LOG_DISABLE || config.LOG_ERROR || config.LOG_WARN || config.LOG_INFO || config.LOG_DEBUG\\n        logLevel: config.LOG_DEBUG,\\n\\n        // enable / disable watching file and executing tests whenever any file changes\\n        autoWatch: true,\\n\\n        // available browser launchers: https://npmjs.org/browse/keyword/karma-launcher\\n        browsers: [\\\"Chrome\\\"],\\n\\n        // Continuous Integration mode\\n        // if true, Karma captures browsers, runs the tests and exits\\n        singleRun: true\\n    });\\n};\",\"language\":\"js; highlight\"}],[\"code\",{\"code\":\"var allTestFiles = [];\\nvar allSourceFiles = [];\\n\\nvar TEST_REGEXP = /(spec|test)\\\\.js$/i;\\nvar SRC_REGEXP = /dist\\\\/[a-zA-Z]+\\\\/[a-zA-Z]+.js$/im;\\n\\nvar normalizePathToSpecFiles = function (path) {\\n    return path.replace(/^\\\\/base\\\\//, '').replace(/\\\\.js$/, '');\\n};\\n\\nvar normalizePathToSourceFiles = function (path) {\\n    return path.replace(/^\\\\/base\\\\/dist\\\\//, '').replace(/\\\\.js$/, '');\\n};\\n\\nvar loadSourceModulesAndStartTest = function () {\\n    require([\\\"aurelia/aurelia-bundle\\\"], function () {\\n        require(allSourceFiles, function () {\\n            require(allTestFiles, function () {\\n                window.__karma__.start();\\n            });\\n        });\\n    });\\n};\\n\\nObject.keys(window.__karma__.files).forEach(function (file) {\\n    if (TEST_REGEXP.test(file)) {\\n        allTestFiles.push(normalizePathToSpecFiles(file));\\n    } else if (SRC_REGEXP.test(file)) {\\n        allSourceFiles.push(normalizePathToSourceFiles(file));\\n    }\\n});\\n\\nrequire.config({\\n    // Karma serves files under /base, which is the basePath from your config file\\n    baseUrl: \\\"/\\\",\\n\\n    paths: {\\n        test: \\\"/base/test\\\",\\n        dist: \\\"/base/dist\\\",\\n        views: \\\"/base/dist/views\\\",\\n        resources: \\\"/base/dist/resources\\\",\\n        aurelia: \\\"/base/Content/scripts/aurelia\\\",\\n    },\\n\\n    // dynamically load all test files\\n    deps: [\\\"aurelia/aurelia-bundle\\\"],\\n\\n    // we have to kickoff jasmine, as it is asynchronous\\n    callback: loadSourceModulesAndStartTest\\n});\\n\",\"language\":\"js; highlight\"}],[\"code\",{\"code\":\"var gulp = require('gulp');\\nvar karma = require(\\\"karma\\\").server;\\n\\ngulp.task(\\\"unit-test\\\", [\\\"build-system\\\"], function () {\\n    return karma.start({\\n        configFile: __dirname + \\\"/../../karma.conf.js\\\",\\n        singleRun: true\\n    });\\n});\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/989fe25d-d1fb-4e2c-836e-b09eeb23b607.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c1a1b3d7-1335-4a1a-839f-d95243937e2c.png\\\" width=\\\"549\\\" height=\\\"136\\\"></a>\"}],[\"code\",{\"code\":\"&lt;!DOCTYPE html&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n    &lt;meta charset=\\\"utf-8\\\"&gt;\\n    &lt;title&gt;Jasmine Spec Runner v2.2.0&lt;/title&gt;\\n\\n    &lt;link rel=\\\"stylesheet\\\" href=\\\"../../node_modules/jasmine-core/lib/jasmine-core/jasmine.css\\\"&gt;\\n    &lt;script src=\\\"/Content/scripts/jquery-2.1.3.js\\\"&gt;&lt;/script&gt;\\n    \\n    &lt;!-- source files here... --&gt;\\n    &lt;script src=\\\"/Content/scripts/core-js/client/core.js\\\"&gt;&lt;/script&gt;\\n    &lt;script src=\\\"/Content/scripts/requirejs/require.js\\\"&gt;&lt;/script&gt;\\n    \\n    &lt;script&gt;\\n        var baseUrl = window.location.origin;\\n        require.config({\\n            baseUrl: baseUrl + \\\"/src\\\",\\n            paths: {\\n                jasmine: baseUrl + \\\"/node_modules/jasmine-core/lib/jasmine-core/jasmine\\\",\\n                \\\"jasmine-html\\\": baseUrl + \\\"/node_modules/jasmine-core/lib/jasmine-core/jasmine-html\\\",\\n                \\\"jasmine-boot\\\": baseUrl + \\\"/node_modules/jasmine-core/lib/jasmine-core/boot\\\",\\n                \\\"sinon\\\": baseUrl + \\\"/node_modules/sinon/pkg/sinon\\\",\\n                \\\"jasmine-sinon\\\": baseUrl + \\\"/node_modules/jasmine-sinon/lib/jasmine-sinon\\\",\\n                aurelia: baseUrl + \\\"/Content/scripts/aurelia\\\",\\n                webcomponentsjs: baseUrl + \\\"/Content/scripts/webcomponentsjs\\\",\\n                dist: baseUrl + \\\"/dist\\\",\\n                views: baseUrl + \\\"/dist/views\\\",\\n                resources: baseUrl + \\\"/dist/resources\\\",\\n                test: \\\"/test\\\"\\n            },\\n            shim: {\\n                \\\"jasmine-html\\\": {\\n                    deps: [\\\"jasmine\\\"],\\n                },\\n                \\\"jasmine-boot\\\": {\\n                    deps: [\\\"jasmine\\\", \\\"jasmine-html\\\"]\\n                }\\n            }\\n        });\\n\\n        // load Aurelia and jasmine...\\n        require([\\\"aurelia/aurelia-bundle\\\"], function() {\\n            // ... then jasmine...\\n            require([\\\"jasmine-boot\\\"], function () {\\n                // .. then jasmine plugins...\\n                require([\\\"sinon\\\", \\\"jasmine-sinon\\\"], function () {\\n                    // build a list of specs\\n                    var specs = [];\\n                    specs.push(\\\"test/unit/aurelia-appInsights.spec\\\");\\n\\n                    // ... then load the specs\\n                    require(specs, function () {\\n                        // finally we can run jasmine\\n                        window.onload();\\n                    });\\n                });\\n            });\\n        });\\n    &lt;/script&gt;\\n&lt;/head&gt;\\n\\n&lt;body&gt;\\n&lt;/body&gt;\\n&lt;/html&gt;\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/51bf3f4c-64f2-4f74-9dd3-7bf7c0825afa.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3c97596e-1d60-4f91-b7e0-ff1df45debda.png\\\" width=\\\"354\\\" height=\\\"170\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/49f3907d-044c-4be6-9b49-8260465651d8.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6f30f6e2-d174-4283-9029-1b8945ebfd43.png\\\" width=\\\"380\\\" height=\\\"206\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">TypeError: 'undefined' is not a function (evaluating 'Array.prototype.forEach.call.bind(Array.prototype.forEach)')</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">ReferenceError: Can't find variable: Map</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">\\\"node_modules/harmony-collections/harmony-collections.min.js\\\",</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">ReferenceError: Can't find variable: Promise</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">\\\"node_modules/promise-polyfill/Promise.min.js\\\",</font>\"}],[\"code\",{\"code\":\"reporters: [\\\"progress\\\", \\\"coverage\\\"],\\n\\ncoverageReporter: {\\n    dir: \\\"test/coverage/\\\",\\n    reporters: [\\n        { type: 'lcov', subdir: 'report-lcov' },\\n        { type: 'text-summary', subdir: '.', file: 'coverage-summary.txt' },\\n        { type: 'text' },\\n    ]\\n},\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"preprocessors: {\\n    \\\"dist/**/*.js\\\": [\\\"coverage\\\"]\\n},\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3ae2cabb-522c-4223-99e7-49c7ebe4852d.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4975c7b8-4961-4d9d-997f-68f2bcc6e6e6.png\\\" width=\\\"367\\\" height=\\\"96\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c6445ebf-6d48-4466-9226-a930de914efd.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a99f2622-c202-4a67-aac3-559fac6b0d1a.png\\\" width=\\\"357\\\" height=\\\"273\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a37d8fdf-bdfe-4899-8f4b-d7374328e5bb.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/05970f61-df8b-4506-8973-cdc631cdc600.png\\\" width=\\\"359\\\" height=\\\"261\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/db9e4f06-1ac0-40b8-bff4-15ab3edd2d34.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5e16a06b-31b9-499c-9c76-be792410de29.png\\\" width=\\\"325\\\" height=\\\"247\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/aurelia--debugging-from-within-visual-studio\"]],[\"a\",[\"href\",\"http://aurelia.io/\"]],[\"a\",[\"href\",\"http://requirejs.org/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-absolutely-need-to-unit-test\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/aurelia-appInsights\"]],[\"em\"],[\"a\",[\"href\",\"http://jasmine.github.io/\"]],[\"a\",[\"href\",\"http://en.wikipedia.org/wiki/Behavior-driven_development\"]],[\"a\",[\"href\",\"http://karma-runner.github.io/0.12/index.html\"]],[\"a\",[\"href\",\"https://www.npmjs.com/package/karma-cli\"]],[\"a\",[\"href\",\"http://karma-runner.github.io/0.12/intro/configuration.html\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708\"]],[\"a\",[\"href\",\"http://localhost\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/aurelia-appInsights/blob/master/RunGulpTests.ps1\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/4cd59e4a-82e8-4b4e-8302-d102fc81b090\"]],[\"a\",[\"href\",\"http://en.wikipedia.org/wiki/Mostly_Harmless\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In my \"],[0,[0],1,\"previous post\"],[0,[],0,\" I walked through how to change \"],[0,[1],1,\"Aurelia\"],[0,[],0,\" to load modules via \"],[0,[2],1,\"Require.js\"],[0,[],0,\" so that you can set breakpoints and debug from VS when you run your Aurelia project. In this post I want to share some tips about unit testing your Aurelia view-models.\"]]],[1,\"h2\",[[0,[],0,\"Unit Testing Javascript\"]]],[1,\"p\",[[0,[],0,\"If you aren’t yet convinced of the value of unit testing, please read my \"],[0,[3],1,\"post about why you absolutely should be\"],[0,[],0,\". Unfortunately, unit testing Javascript in Visual Studio (and during automated builds) is a little harder to do than running unit tests on managed code. This post will show you some of the techniques I use to unit test Javascript in my Aurelia project – though of course you don’t need to be using Aurelia to make use of these techniques. If you want to see the code I’m using for this post, check out \"],[0,[4],1,\"this repo\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"But I’ve already got tests!\"]]],[1,\"p\",[[0,[],0,\"This post isn’t going to go too much into \"],[0,[5],1,\"how to\"],[0,[],0,\" unit test – there are hundreds of posts about how to test. I’m going to assume that you already have some unit tests. I’ll discuss the following topics in this post:\"]]],[3,\"ul\",[[[0,[],0,\"Basic Karma/Jasmine overview\"]],[[0,[],0,\"Configuring Karma and RequireJS\"]],[[0,[],0,\"Running Karma from Gulp\"]],[[0,[],0,\"Using a SpecRunner.html page to enable debugging unit tests\"]],[[0,[],0,\"Fudges to enable PhantomJS\"]],[[0,[],0,\"Code Coverage\"]],[[0,[],0,\"Running tests in your builds (using TeamBuild)\"]],[[0,[],0,\"Karma VS Test adapter\"]]]],[1,\"h3\",[[0,[],0,\"Karma and Jasmine\"]]],[1,\"p\",[[0,[],0,\"There are many JavaScript testing frameworks out there. I like \"],[0,[6],1,\"Jasmine\"],[0,[],0,\" as a (\"],[0,[7],1,\"BDD\"],[0,[],0,\") testing framework, and I like \"],[0,[8],1,\"Karma\"],[0,[],0,\" (which used to be called Testacular) as a test runner. One of the things I like about Karma is that you can run your tests in several browsers – it also has numerous “reporters” that let you track the tests, and even add code coverage. Aurelia itself uses Karma for its testing.\"]]],[1,\"h3\",[[0,[],0,\"Configuring Karma and RequireJS\"]]],[1,\"p\",[[0,[],0,\"To configure karma, you have to set up a karma config file – by convention it’s usually called karma.conf.js. If you use \"],[0,[9],1,\"karma-cli\"],[0,[],0,\", you can run “\"],[0,[10],1,\"karma init\"],[0,[],0,\"” to get karma to lead you through a series of questions to help you set up a karma config file for the first time. I wanted to use requirejs, mostly because using requirejs means I can set breakpoints in Visual Studio and debug. So I made sure to answer “yes” for that question. Unfortunately, that opens a can of worms!\"]]],[1,\"p\",[[0,[],0,\"The reason for the “can of worms” is that karma tries to serve all the files necessary for the test run – but if they are AMD modules, then you can’t “serve” them – they need to be loaded by requirejs. In order to do that, we have to fudge the karma startup a little. We specify the files that should be served in the karma.conf.js file, being careful to “exclude” the files. This flag tells karma to serve the file when it is requested, but not to execute it (think of it as treating the file as static text rather than a JavaScript file to execute). Then, we create a “test-main.js” file to configure requirejs, load the modules and then launch karma.\"]]],[1,\"p\",[[0,[],0,\"Here’s the karma.conf.js file:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 6: We tell karma what frameworks to use when running the tests – jasmine (the test framework), requirejs (for loading modules) and sinon (for mocking). These are installed using “npm install [karma-jasmine|karma-requirejs|karma-sinon] respectively\"]],[[0,[],0,\"Lines 11/12: We load files that the tests will need – test-main to configure the modules for the test and the sinon file to load the sinon libs. Since these files are not “excluded”, karma executes them on load.\"]],[[0,[],0,\"Line 15: We serve all the source files we are testing, using the “exclude” to tell karma to serve them but not execute them (so it only serves them when requested – requirejs will load them)\"]],[[0,[],0,\"Line 18: We serve all the test (spec) files to run (again, not executing them)\"]],[[0,[],0,\"Line 21: We serve libraries (including the Aurelia framework)\"]]]],[1,\"p\",[[0,[],0,\"Here’s the test-main.js file:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 4, 5: We set up regex patterns to match test (spec) files as well as source files\"]],[[0,[],0,\"Line 8, 12: We normalize the path to test or source files. This is necessary since the paths that requirejs use are a different to the base path that karma sets up.\"]],[[0,[],0,\"Lines 16-18: We load the modules we need in order of dependency – starting with Aurelia (frameworks), then the sources, and then the test files\"]],[[0,[],0,\"Line 19: We need to start the karma engine ourselves, since we’re hijacking the default start to load everything via requirejs\"]],[[0,[],0,\"Line 25: We hook into the karma function that loads files to normalize the file paths\"]],[[0,[],0,\"Line 37: We set up paths for requirejs\"]],[[0,[],0,\"Line 46: We tell requirejs that the most “basic” dependency is the Aurelia framework\"]],[[0,[],0,\"Line 49: We tell karma to execute our custom launch function once the “base” dependency is loaded\"]]]],[1,\"p\",[[0,[],0,\"To be honest, figuring out the final path and normalize settings was a lot of trial and error. I turned karma logging onto debug, and then just played around until karma was serving all the files and requirejs was happy with path resolution. You’ll have to play around with these paths yourself for your project structure.\"]]],[1,\"h4\",[[0,[],0,\"Running Karma Test from the CLI\"]]],[1,\"p\",[[0,[],0,\"Now we can run the karma tests: simply type “karma start” and karma will fire up and run the tests: you should see the Chrome window popping up (assuming you’re using the Chrome karma launcher) and a message telling you that the tests were run successfully.\"]]],[1,\"h3\",[[0,[],0,\"Running Karma from Gulp\"]]],[1,\"p\",[[0,[],0,\"Now that we have the tests running from the karma CLI, we can easily run them from within Gulp. We are using Gulp to transpile TypeScript to Javascript, compile LESS files to CSS and do minification and any other “production-izing” we need – so running tests in Gulp makes sense. Also, this way we make sure we’re using the latest sources we have instead of old stale code that’s been lying around (especially if you forget to run the gulp build tasks!). Here are the essential bits of the “unit-test” target in Gulp:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"We import Gulp and Karma server. I didn’t install gulp-karma – rather, I just rely on “pure” karma.\"]],[[0,[],0,\"We create a task called “unit-test” that fist calls “build-system” before invoking karma\"]],[[0,[],0,\"The build-system task transpiles TypeScript to JavaScript – we make sure that we generate un-minified files and source maps in this task (so that later on we can set breakpoints and debug)\"]],[[0,[],0,\"We tell karma where to find the karma config file (so you need to specify the path relative to __dirName, which is the current directory where the Gulp script is\"]],[[0,[],0,\"We tell karma to perform a single run, rather then keeping the browsers open and running the tests every time we change a file\"]]]],[1,\"p\",[[0,[],0,\"We can now run “gulp unit-test” from the command line, or we can execute the gulp “unit-test” task from the Visual Studio Task Runner Explorer (which is native to VS 2015 and can be installed into VS 2013 \"],[0,[11],1,\"via an extension\"],[0,[],0,\"):\"]]],[10,3],[1,\"h3\",[[0,[],0,\"Debugging Tests\"]]],[1,\"p\",[[0,[],0,\"Now that we can run the tests from Gulp, we may want to debug while testing. In order to do that, we’ll need to make sure the tests can run in IE (since VS will break on code running in IE). The karma launcher creates its own dynamic page to launch the tests, so we’re going to need to code an html page ourselves if we want to be able to debug tests. I create a “SpecRunner.html” page in my unit-test folder:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 15-39: We configure requirejs for the tests\"]],[[0,[],0,\"Lines 15: the base url is the window location – when debugging from VS this is usually \"],[0,[12],1,\"http://localhost\"],[0,[],0,\" followed by some port\"]],[[0,[],0,\"Lines 19-29: the paths requirejs needs to resolve all the modules we want to load, as well as some jasmine-specific libs\"]],[[0,[],0,\"Lines 31-38: we need to shim a couple of jasmine libs to let requirejs know about their dependencies\"]],[[0,[],0,\"Lines 42, 44, 46: We load the dependencies in order so that requirejs loads them in the correct order\"]],[[0,[],0,\"Line 49: We create an array of all our test files\"]],[[0,[],0,\"Lines 52, 54: After loading the test specs, we trigger the onload() method which start the karma tests\"]]]],[1,\"p\",[[0,[],0,\"Again you see that we hijack the usual Jasmine startup so that we can get requirejs to load all the sources, libs and tests before launching the test runner. Now we set the SpecRunner.html page to be the startup page for the project, and hit F5:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Now that we can finally run the tests from VS in IE, we can set a breakpoint, hit F5 and we can debug!\"]]],[10,6],[1,\"h3\",[[0,[],0,\"PhantomJS – mostly harmless*, er, headless\"]]],[1,\"p\",[[0,[],0,\"While debugging in IE or launching Chrome from karma is great, there are situations where we may want to run our tests without the need for an actual browser (like on the build server). Fortunately there is a tool that allows you to run “headless” tests – PhantomJS. And even better – there’s a PhantomJS launcher for karma! Let’s add it in:\"]]],[1,\"p\",[[0,[],0,\"Run “npm install karma-phantomjs-launcher --save-dev” to install the PhantomJS launcher for karma. Then change the launcher config in the karma.conf.js file from [“Chrome”] to [“PhantomJS2”] and run karma. Unfortunately, this won’t work: you’ll likely see an error like this:\"]]],[10,7],[1,\"p\",[[0,[],0,\"This sounds like a native JavaScript problem – perhaps since Aurelia uses ES6 (and even ES7) we need a more modern launcher. Let’s try install PhantomJS2 (the PhantomJS launcher that uses an experimental Phantom 2, a more modern version of PhantomJS). That seems to get us a little further:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Hmm. Map is again, an \"],[0,[13],1,\"ES6 structure\"],[0,[],0,\". Fortunately there is a library with the ES5 polyfills for some newer ES6 structures like Map: harmony-collections. We run “npm install harmony-collections --save-dev” to install the harmony-collections package, and then reference it in the karma.conf.js file (on line 13):\"]]],[10,9],[1,\"p\",[[0,[],0,\"We get a bit further, but there is still something missing:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Again a little bit of searching leads to another node package: so we run “npm install promise-polyfill --save-dev” and again reference the file (just after the harmony-collections reference):\"]]],[10,11],[1,\"p\",[[0,[],0,\"Success! We can now run our tests headless.\"]]],[1,\"p\",[[0,[],0,\"In another system I was coding, I ran into a further problem with the “find” method on arrays. Fortunately, we can polyfill the find method too! I didn’t find a package for that – I simply added the polyfill from \"],[0,[14],1,\"here\"],[0,[],0,\" into one of my spec files.\"]]],[1,\"h3\",[[0,[],0,\"Code Coverage\"]]],[1,\"p\",[[0,[],0,\"So we can now run test from karma, from Gulp, from Task explorer, from VS using F5 and we can run them headless using PhantomJS2. If we add in a coverage reporter, we can even get some code coverage analysis: run “npm install karma-coverage --save-dev”. That will install a new reporter, so we need to add it in to the reporters section of karma.conf.js:\"]]],[10,12],[1,\"p\",[[0,[],0,\"We add the reporter in (just after “progress”). We also configure what sort of coverage information formats we want and which directory the output should go to. Since the coverage requires our code to be instrumented, we need to add in a preprocessor (just above reporters):\"]]],[10,13],[1,\"p\",[[0,[],0,\"This tells the coverage engine to instrument all the js files in the dist folder. Any other files we want to calculate coverage from, we’ll need to add in the glob pattern.\"]]],[10,14],[1,\"p\",[[0,[],0,\"The output in the image above is from the “text” output. For more detailed coverage reports, we browse to test/coverage/report-lcov/lcov-report/index. We can then click through the folders and then down to the files, where we’ll be able to see exactly which lines our test covered (or missed):\"]]],[10,15],[1,\"p\",[[0,[],0,\"This will help us discover more test opportunities.\"]]],[1,\"h3\",[[0,[],0,\"Running Test in TeamBuilds\"]]],[1,\"p\",[[0,[],0,\"With all the basics in place, we can easily include the unit tests into our builds. If you’re using TFS 2013 builds, you can just add a PowerShell script into your repo and then add that script as a pre- or post-test script. Inside the PowerShell you simply invoke “gulp unit-test” to run the unit tests via Gulp. I wanted to be a bit fancier, so I also added code to inspect the output from the test run and the coverage to add them into the build summary:\"]]],[10,16],[1,\"p\",[[0,[],0,\"The full PowerShell script is \"],[0,[15],1,\"here\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"Seeing Tests in Visual Studio\"]]],[1,\"p\",[[0,[],0,\"Finally, just in case we don’t have enough ways of running the tests, we can install the \"],[0,[16],1,\"Visual Studio Karma Test Adapter\"],[0,[],0,\". This great adapter picks up the tests we’ve configured in karma and displays them in the test explorer window, where we can run them:\"]]],[10,17],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Unit testing your front-end view-model logic is essential if you’re going to deploy quality code. Enabling a good experience for unit testing requires a little bit of thought and some work – but once you’ve got the basics in place, you’ll be good to go. Ensuring quality as you code means you’ll have better quality down the road – and that means more time for new features and less time fixing bugs. Using Gulp and Karma enables continuous testing, and augmenting these with the techniques I’ve outlines you can also debug tests, run tests several ways and even integrate the tests (and coverage) into your builds.\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]],[1,\"p\",[[0,[],0,\"* \"],[0,[17],1,\"Mostly Harmless\"],[0,[],0,\" – from the Hitchhikers Guide to the Galaxy by Douglas Adams\"]]]]}","published_at":1428364607000,"status":"published","published_by":1},{"id":"bc966292-6e4f-4da0-9bca-3803edadbd19","title":"Aurelia: Object Binding Without Dirty Checking","slug":"aurelia-object-binding-without-dirty-checking","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"&lt;template&gt;\\n  &lt;section&gt;\\n    &lt;h2&gt;${heading}&lt;/h2&gt;\\n\\n    &lt;form role=\\\"form\\\" submit.delegate=\\\"welcome()\\\"&gt;\\n      &lt;div class=\\\"form-group\\\"&gt;\\n        &lt;label for=\\\"fn\\\"&gt;First Name&lt;/label&gt;\\n        &lt;input type=\\\"text\\\" value.bind=\\\"firstName\\\" class=\\\"form-control\\\" id=\\\"fn\\\" placeholder=\\\"first name\\\"&gt;\\n      &lt;/div&gt;\\n      &lt;div class=\\\"form-group\\\"&gt;\\n        &lt;label for=\\\"ln\\\"&gt;Password&lt;/label&gt;\\n        &lt;input type=\\\"text\\\" value.bind=\\\"lastName\\\" class=\\\"form-control\\\" id=\\\"ln\\\" placeholder=\\\"last name\\\"&gt;\\n      &lt;/div&gt;\\n      &lt;div class=\\\"form-group\\\"&gt;\\n        &lt;label&gt;Full Name&lt;/label&gt;\\n        &lt;p class=\\\"help-block\\\"&gt;${fullName | upper}&lt;/p&gt;\\n      &lt;/div&gt;\\n      &lt;button type=\\\"submit\\\" class=\\\"btn btn-default\\\"&gt;Submit&lt;/button&gt;\\n    &lt;/form&gt;\\n  &lt;/section&gt;\\n&lt;/template&gt;\\n\",\"language\":\"xml; highlight\"}],[\"code\",{\"code\":\"export class Home {\\n    public heading: string;\\n    public firstName: string;\\n    public lastName: string;\\n\\n    constructor() {\\n        this.heading = \\\"Welcome to Aurelia!\\\";\\n        this.firstName = \\\"John\\\";\\n        this.lastName = \\\"Doe\\\";\\n    }\\n\\n    get fullName() {\\n        return this.firstName + \\\" \\\" + this.lastName;\\n    }\\n\\n    welcome() {\\n        alert(\\\"Welcome, \\\" + this.fullName + \\\"!\\\");\\n    }\\n}\\n\\nexport class UpperValueConverter {\\n    toView(value) {\\n        return value &amp;&amp; value.toUpperCase();\\n    }\\n}\\n\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"get fullName() {\\n    console.debug(\\\"Getting fullName\\\");\\n    return this.firstName + \\\" \\\" + this.lastName;\\n}\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8acee728-be52-4af3-b44d-1d1a26a9fdd3.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ac40cfab-c6ba-443d-a71b-c3f352680c44.png\\\" width=\\\"408\\\" height=\\\"314\\\"></a>\"}],[\"code\",{\"code\":\"declare module \\\"aurelia-binding\\\" {\\n    function declarePropertyDependencies(moduleType: any, propName: string, deps: any[]): void;\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"import aub = require(\\\"aurelia-binding\\\");\\n\\nexport class Home {\\n    public heading: string;\\n    public firstName: string;\\n    public lastName: string;\\n\\n    constructor() {\\n        this.heading = \\\"Welcome to Aurelia!\\\";\\n        this.firstName = \\\"John\\\";\\n        this.lastName = \\\"Doe\\\";\\n    }\\n\\n    get fullName() {\\n        console.debug(\\\"Getting fullName\\\");\\n        return this.firstName + \\\" \\\" + this.lastName;\\n    }\\n\\n    welcome() {\\n        alert(\\\"Welcome, \\\" + this.fullName + \\\"!\\\");\\n    }\\n}\\n\\naub.declarePropertyDependencies(Home, \\\"fullName\\\", [\\\"firstName\\\", \\\"lastName\\\"]);\\n\\nexport class UpperValueConverter {\\n    toView(value) {\\n        return value &amp;&amp; value.toUpperCase();\\n    }\\n}\\n\\n\",\"language\":\"js; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/79850200-d5b2-41d3-961e-16eee681f751.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/64c1c8eb-ed5d-42b2-b9ca-98da6ea7c8f2.png\\\" width=\\\"281\\\" height=\\\"324\\\"></a>\"}],[\"code\",{\"code\":\"import aur = require(\\\"aurelia-router\\\");\\n\\nexport class Redirect implements aur.INavigationCommand {\\n    public url: string;\\n    public shouldContinueProcessing: boolean;\\n\\n    /**\\n      * Application redirect (works with approuter instead of current child router)\\n      *\\n      * @url the url to navigate to (ex: \\\"#/home\\\")\\n      */\\n    constructor(url) {\\n        this.url = url;\\n        this.shouldContinueProcessing = false;\\n    }\\n\\n    navigate(appRouter) {\\n        appRouter.navigate(this.url, { trigger: true, replace: true });\\n    }\\n}\\n\\nclass AppState {\\n    public isAuthenticated: boolean;\\n    public userName: string;\\n\\n    /**\\n      * Simple application state\\n      *\\n      */\\n    constructor() {\\n        this.isAuthenticated = false;\\n    }\\n\\n    login(username: string, password: string): boolean {\\n        if (username == \\\"Admin\\\" &amp;&amp; password == \\\"xxx\\\") {\\n            this.isAuthenticated = true;\\n            this.userName = \\\"Admin\\\";\\n            return true;\\n        }\\n        this.logout();\\n        return false;\\n    }\\n\\n    logout() {\\n        this.isAuthenticated = false;\\n        this.userName = \\\"\\\";\\n    }\\n}\\n\\nvar appState = new AppState();\\nexport var state = appState;\\n\\n\",\"language\":\"js; highlight\"}],[\"code\",{\"code\":\"import auf = require(\\\"aurelia-framework\\\");\\nimport aps = require(\\\"scripts/app-state\\\");\\n\\nexport class NavBar {\\n    static metadata = auf.Behavior.withProperty(\\\"router\\\");\\n\\n    get userName() {\\n        console.debug(\\\"Getting userName\\\");\\n        return aps.state.userName;\\n    }\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"&lt;template&gt;\\n  &lt;nav class=\\\"navbar navbar-default navbar-fixed-top\\\" role=\\\"navigation\\\"&gt;\\n    &lt;div class=\\\"navbar-header\\\"&gt;\\n      &lt;button type=\\\"button\\\" class=\\\"navbar-toggle\\\" data-toggle=\\\"collapse\\\" data-target=\\\"#bs-example-navbar-collapse-1\\\"&gt;\\n        &lt;span class=\\\"sr-only\\\"&gt;Toggle Navigation&lt;/span&gt;\\n        &lt;span class=\\\"icon-bar\\\"&gt;&lt;/span&gt;\\n        &lt;span class=\\\"icon-bar\\\"&gt;&lt;/span&gt;\\n        &lt;span class=\\\"icon-bar\\\"&gt;&lt;/span&gt;\\n      &lt;/button&gt;\\n      &lt;a class=\\\"navbar-brand\\\" href=\\\"#\\\"&gt;\\n        &lt;i class=\\\"fa fa-home\\\"&gt;&lt;/i&gt;\\n        &lt;span&gt;${router.title}&lt;/span&gt;\\n      &lt;/a&gt;\\n    &lt;/div&gt;\\n\\n    &lt;div class=\\\"collapse navbar-collapse\\\" id=\\\"bs-example-navbar-collapse-1\\\"&gt;\\n      &lt;ul class=\\\"nav navbar-nav\\\"&gt;\\n        &lt;li repeat.for=\\\"row of router.navigation\\\" class=\\\"${row.isActive ? 'active' : ''}\\\"&gt;\\n          &lt;a href.bind=\\\"row.href\\\"&gt;${row.title}&lt;/a&gt;\\n        &lt;/li&gt;\\n      &lt;/ul&gt;\\n\\n      &lt;ul class=\\\"nav navbar-nav navbar-right\\\"&gt;\\n        &lt;li&gt;&lt;a href=\\\"#\\\"&gt;${userName}&lt;/a&gt;&lt;/li&gt;\\n        &lt;li class=\\\"loader\\\" if.bind=\\\"router.isNavigating\\\"&gt;\\n          &lt;i class=\\\"fa fa-spinner fa-spin fa-2x\\\"&gt;&lt;/i&gt;\\n        &lt;/li&gt;\\n      &lt;/ul&gt;\\n    &lt;/div&gt;\\n  &lt;/nav&gt;\\n&lt;/template&gt;\\n\",\"language\":\"xml; highlight\"}],[\"code\",{\"code\":\"import auf = require(\\\"aurelia-framework\\\");\\nimport aub = require(\\\"aurelia-binding\\\");\\nimport aps = require(\\\"scripts/app-state\\\");\\n\\nexport class NavBar {\\n    static metadata = auf.Behavior.withProperty(\\\"router\\\");\\n\\n    get userName() {\\n        return aps.state.userName;\\n    }\\n}\\n\\naub.declarePropertyDependencies(NavBar, \\\"userName\\\", [aps.state.userName]);\\n\",\"language\":\"js; highlight\"}],[\"code\",{\"code\":\"import auf = require(\\\"aurelia-framework\\\");\\n\\nexport class MultiObserver {\\n    static inject = [auf.ObserverLocator];\\n\\n    constructor(private observerLocator: auf.ObserverLocator) {\\n    }\\n\\n    /**\\n     * Set up dependencies on an arbitrary object.\\n     * \\n     * @param properties the properties to observe\\n     * @param callback the callback to fire when one of the properties changes\\n     * \\n     * Example:\\n     * export class App {\\n     *      static inject() { return [MultiObserver]; }\\n     *      constructor(multiObserver) {\\n     *        var session = {\\n     *          fullName: 'John Doe',\\n     *          User: {\\n     *            firstName: 'John',\\n     *            lastName: 'Doe'\\n     *          }\\n     *        };\\n     *        this.session = session;\\n     *\\n     *        var disposeFunction = multiObserver.observe(\\n     *          [[session.User, 'firstName'], [session.User, 'lastName']],\\n     *          () =&gt; session.fullName = session.User.firstName + ' ' + session.User.lastName);\\n     *      }\\n     *    }\\n     */\\n    observe(properties, callback) {\\n        var subscriptions = [], i = properties.length, object, propertyName;\\n        while (i--) {\\n            object = properties[i][0];\\n            propertyName = properties[i][1];\\n            subscriptions.push(this.observerLocator.getObserver(object, propertyName).subscribe(callback));\\n        }\\n\\n        // return dispose function\\n        return () =&gt; {\\n            while (subscriptions.length) {\\n                subscriptions.pop()();\\n            }\\n        }\\n    }\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"interface IObserver {\\n    subscribe(callback: Function): void;\\n}\\n\\nclass ObserverLocator {\\n    getObserver(object: any, propertyName: string): IObserver;\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"import auf = require(\\\"aurelia-framework\\\");\\nimport aub = require(\\\"aurelia-binding\\\");\\nimport aps = require(\\\"scripts/app-state\\\");\\nimport muo = require(\\\"views/utils/multi-observer\\\");\\n\\nexport class NavBar {\\n    static metadata = auf.Behavior.withProperty(\\\"router\\\");\\n    static inject = [muo.MultiObserver];\\n\\n    dispose: () =&gt; void;\\n    userName: string;\\n\\n    constructor(multiObserver: muo.MultiObserver) {\\n        // set up a dependency on the session router object\\n        this.dispose = multiObserver.observe([[aps.state, \\\"userName\\\"]],() =&gt; {\\n            console.debug(\\\"Setting new value for userName\\\");\\n            this.userName = aps.state.userName;\\n        });\\n    }\\n\\n    deactivate() {\\n        this.dispose();\\n    }\\n}\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a26b14ee-c3c9-4c15-8178-b751902c6100.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8e3cd17e-5009-4696-a620-22534f09ba62.png\\\" width=\\\"499\\\" height=\\\"355\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://aurelia.io/\"]],[\"a\",[\"href\",\"https://twitter.com/eisenbergeffect\"]],[\"a\",[\"href\",\"http://www.typescriptlang.org/\"]],[\"a\",[\"href\",\"https://github.com/lukehoban/es6features\"]],[\"a\",[\"href\",\"https://babeljs.io/\"]],[\"a\",[\"href\",\"https://www.npmjs.com/package/gulp-typescript\"]],[\"a\",[\"href\",\"https://github.com/cmichaelgraham/aurelia-typescript\"]],[\"a\",[\"href\",\"http://knockoutjs.com/\"]],[\"a\",[\"href\",\"https://github.com/aurelia/binding/issues/37\"]],[\"a\",[\"href\",\"https://github.com/jdanyow\"]],[\"a\",[\"href\",\"https://github.com/jdanyow/aurelia-computed\"]],[\"a\",[\"href\",\"https://gitter.im/Aurelia/Discuss\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Over the past few weeks I have been developing a Web UI using \"],[0,[0],1,\"Aurelia\"],[0,[],0,\" by \"],[0,[1],1,\"Rob Eisenberg\"],[0,[],0,\". It’s really well thought out – though it’s got a steep learning curve at the moment since the documentation is still very sparse. Of course it hasn’t officially released yet, so that’s understandable!\"]]],[1,\"h2\",[[0,[],0,\"TypeScript\"]]],[1,\"p\",[[0,[],0,\"I love \"],[0,[2],1,\"TypeScript\"],[0,[],0,\" – if it wasn’t for TypeScript, I would really hate Javascript development! Aurelia is written in \"],[0,[3],1,\"ES6\"],[0,[],0,\" and ES7 which is \"],[0,[4],1,\"transpiled to ES5\"],[0,[],0,\". You can easily write Aurelia apps in TypeScript – you can \"],[0,[5],1,\"transpile in Gulp\"],[0,[],0,\" if you want to – otherwise Visual Studio will transpile to Javascript for you anyway. Since I use TypeScript, I also use \"],[0,[6],1,\"Mike Graham’s TypeScript Aurelia sample repos\"],[0,[],0,\". He has some great samples there if you’re just getting started with Aurelia/TypeScript. Code for this post comes from the “aurelia-vs-ts” solution in that repo.\"]]],[1,\"h2\",[[0,[],0,\"Binding in Aurelia\"]]],[1,\"p\",[[0,[],0,\"Aurelia has many powerful features out the box – and most of its components are pluggable too – so you can switch out components as and when you need to. Aurelia allows you to separate the view (html) from the view-model (a Javascript class). When you load a view, Aurelia binds the properties of the view-model with the components in the view. This works beautifully for primitives – Aurelia knows how to create a binding between an HTML element (or property) and the object property. Let’s look at home.html and home.ts to see how this works:\"]]],[10,0],[1,\"p\",[[0,[],0,\"This is the view (html) for the home page (views\\\\home.html). You bind to variables in the view-model using the ${var} syntax (lines 3 and 16). You can also bind attributes directly – like value.bind=”firstName” in line 8 binds the value of the input box to the “firstName” property. Line 16 uses a value converter to convert the value of the bound parameter to uppercase. Line 5 binds a function to the submit action. I don’t want to get into all the Aurelia binding capabilities here – that’s for another discussion.\"]]],[1,\"p\",[[0,[],0,\"Here’s the view-model (views\\\\home.ts):\"]]],[10,1],[1,\"p\",[[0,[],0,\"The code is very succinct – and easy to test. Notice the absence of any “binding plumbing”. So how does the html know to update when values in the view-model change? (If you’ve ever used \"],[0,[7],1,\"Knockout\"],[0,[],0,\" you’ll be wondering where the observables are!)\"]]],[1,\"h2\",[[0,[],0,\"Dirty Binding\"]]],[1,\"p\",[[0,[],0,\"The bindings for heading, firstName and lastName are primitive bindings – in other words, when Aurelia binds the html to the property, it creates an observer on the property so that when the property is changed, a notification of the change is triggered. It’s all done under the covers for you so you can just assume that any primitive on any model will trigger change notifications to anything bound to them.\"]]],[1,\"p\",[[0,[],0,\"However, if you’re not using a primitive, then Aurelia has to fall-back on “dirty binding”. Essentially it sets up a polling on the object (\"],[0,[8],1,\"every 120ms\"],[0,[],0,\"). You’ll see this if you put a console.debug into the getter method:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Here’s what the console looks like when you browse (the console just keeps logging forever and ever):\"]]],[10,3],[1,\"p\",[[0,[],0,\"Unfortunately there simply isn’t an easy way around this problem.\"]]],[1,\"h2\",[[0,[],0,\"Declaring Dependencies\"]]],[1,\"p\",[[0,[9],1,\"Jeremy Danyow\"],[0,[],0,\" did however leverage the pluggability of Aurelia and wrote a plugin for observing computed properties without dirty checking called \"],[0,[10],1,\"aurelia-computed\"],[0,[],0,\". This is now incorporated  into Aurelia and is plugged in by default.\"]]],[1,\"p\",[[0,[],0,\"This plugin allows you to specify dependencies explicitly – thereby circumventing the need to dirty check. Here are the changes we need to make:\"]]],[3,\"ol\",[[[0,[],0,\"Add a definition for the declarePropertyDependencies() method in Aurelia.d.ts (only necessary for TypeScript)\"]],[[0,[],0,\"Add an import to get the aurelia-binding libs\"]],[[0,[],0,\"Register the dependency\"]]]],[1,\"p\",[[0,[],0,\"Add these lines to the bottom of the aurelia.d.ts file (in the typings\\\\aurelia folder):\"]]],[10,4],[1,\"p\",[[0,[],0,\"This just lets Visual Studio know about the function for compilation purposes.\"]]],[1,\"p\",[[0,[],0,\"Now change home.ts to look as follows:\"]]],[10,5],[1,\"p\",[[0,[],0,\"The highlighted lines are the lines I added in. Line 24 is the important line – this explicitly registers a dependency on the “fullName” property of the Home class – on “firstName” and “lastName”. Now any time either firstName or lastName changes, the value of “fullName” is recalculated. Bye-bye polling!\"]]],[1,\"p\",[[0,[],0,\"Here’s the console output now:\"]]],[10,6],[1,\"p\",[[0,[],0,\"We can see that the fullName getter is called 4 times. This is a lot better than polling the value every 120ms. (I’m not sure why it’s called 4 times – probably to do with how the binding is initially set up. Both firstName and lastName change when the page loads and they are instantiated to “John” and “Doe” so I would expect to see a couple firings of the getter function at least).\"]]],[1,\"h2\",[[0,[],0,\"Binding to an Object\"]]],[1,\"p\",[[0,[],0,\"So we’re ok to bind to primitives – but we get stuck again when we want to bind to objects. Let’s take a look at app-state.ts (in the scripts folder):\"]]],[10,7],[1,\"p\",[[0,[],0,\"The AppState is a static global object that tracks the state of the application. This is a good place to track logged in user, for example. I’ve added in the highlighted lines so that we can expose AppState.userName. Let’s open nav-bar.ts (in views\\\\controls) and add a getter so that the nav-bar can display the logged in user’s name:\"]]],[10,8],[1,\"p\",[[0,[],0,\"We can now bind to userName in the nav-bar.html view:\"]]],[10,9],[1,\"p\",[[0,[],0,\"I’ve added line 24. Of course we’ll see polling if we run the solution as is. So we can just declare the dependency, right? Let’s try it:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Seems to compile and run – but the value of userName is never updated!\"]]],[1,\"p\",[[0,[],0,\"It turns out that we can only declare dependencies to the same object (and only to primitives) using declarePropertyDependencies. Seems like we’re stuck.\"]]],[1,\"h2\",[[0,[],0,\"The Multi-Observer\"]]],[1,\"p\",[[0,[],0,\"I posed this question on the \"],[0,[11],1,\"gitter discussion page for Aurelia\"],[0,[],0,\". The guys working on Aurelia (and the community) are very active there – I’ve been able to ask Rob Eisenberg himself questions! Jeremy Danyow is also active on there (as is Mike Graham) so getting help is usually quick. Jeremy quickly verified that declarePropertyDependencies cannot register dependencies on other objects. However, he promptly whacked out the “Multi-Observer”. Here’s the TypeScript for the class:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Add this file to a new folder called “utils” under “views”. To get this to compile, you have to add this definition to the aurelia.d.ts file (inside the aurelia-framework module declaration):\"]]],[10,12],[1,\"p\",[[0,[],0,\"Now we can use the multi-observer to register a callback when any property on any object changes. Let’s do this in the nav-bar.ts file:\"]]],[10,13],[1,\"p\",[[0,[],0,\"We register the function to execute when the value of the property on the object changes – we can execute whatever code we want in this callback.\"]]],[1,\"p\",[[0,[],0,\"Here’s the console after logging in:\"]]],[10,14],[1,\"p\",[[0,[],0,\"There’s no polling – the view-model is bound to the userName primitive on the view-model. But whenever the value of userName on the global state object changes, we get to update the value. We’ve successfully avoided the dirty checking!\"]]],[1,\"p\",[[0,[],0,\"One last note: we register the dependency callback into a function object called “dispose”. We can then simply call this function when we want to unregister the callback (to free up resources). I’ve put the call in the deactivate() method, which is the method Aurelia calls on the view-model when navigating away from it. In this case it’s not really necessary, since the nav-bar is “global” and we won’t navigate away from it. But if you use the multi-observer in a view-model that is going to be unloaded (or navigated away from), be sure to put the dispose function somewhere sensible.\"]]],[1,\"p\",[[0,[],0,\"A big thank you to Jeremy Danyow for his help!\"]]],[1,\"p\",[[0,[],0,\"Happy binding!\"]]]]}","published_at":1427146313000,"status":"published","published_by":1},{"id":"4588bc07-bdbf-4a27-bbe8-3a81d3ae4400","title":"Automated Builds–Why They’re Absolutely Essential (Part 1)","slug":"automated-buildswhy-theyre-absolutely-essential-(part-1)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"em\"],[\"u\"],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/automated-buildswhy-theyre-absolutely_18.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"A couple of weeks ago I was doing a road-show where I did demos of TFS 2012 and it’s capabilities. I do a 4 hour demo that shows an end-to-end scenario, showing capabilities such as requirements management and elicitation, work management, developer tools, quality tools, testing tools, automated builds, lab management and reporting all using TFS. I visited 9 different companies, and most of them asked, “Why should we do builds?” This is something I want to address – you need to be doing builds, and you need to understand why they are so key to successful and effective software development. Builds are no longer an optional extra!\"]]],[1,\"h2\",[[0,[],0,\"General Reasons to do Automated Builds\"]]],[1,\"p\",[[0,[],0,\"Before I dive into doing automated builds using TFS Team Build, there are two general principles that apply to doing automated builds that I’d like to unpack: \"],[0,[0],1,\"Consistency\"],[0,[],0,\" and \"],[0,[0],1,\"Quality\"],[0,[],0,\". These principles transcend the choice of build engine – be it TFS Build (or TeamBuild), Hudson, Cruise Control, Jenkins or any other engine.\"]]],[1,\"p\",[[0,[1],1,\"1. Consistency\"],[0,[],0,\"\"]]],[1,\"p\",[[0,[],0,\"How do you deploy your code into test (or for that matter, into production)? Most teams start off using the “Publish” menu option from within Visual Studio, either publishing directly to test (or even production) environments, or publishing locally and copying to the target servers. Let me be brutally honest – this is simply immature. This “deploy from Visual Studio” is really something only amateur developers and hobbyists should be using. Why? Because there is so much risk attached to doing things this way.\"]]],[1,\"p\",[[0,[],0,\"Some teams argue that they only do this to “test” environments and start getting “serious” only when deploying to production. I argue that you should treat even your test environments as if they were production targets, so that you get the deployment process right there before promoting to production. If you can do it right in pre-production environments, chances are you’ll be able to do it right in production.\"]]],[1,\"p\",[[0,[],0,\"Let’s look at some of the risks of “Publish from VS” deployments:\"]]],[3,\"ol\",[[[0,[],0,\"You could be deploying anything\"]],[[0,[],0,\"How do you know that you’ve only got the code that’s in Source Control? Perhaps you opened your solution and fiddled around a bit, just to try an idea. Now you’ve published untested code into production.\"]],[[0,[],0,\"You could have dependencies on anything\"]],[[0,[],0,\"Let’s imagine you’ve got a dependency on some 3rd party library – Enterprise Library or Entity Framework or MVC or any other library. Your target server has version X installed, but you’re a dev, so you install version Y (you want to be on the latest and greatest, right?). Now you’ve compiled and deployed against the wrong version.\"]],[[0,[],0,\"Your “guy/gal who deploys” gets hit by a bus\"]],[[0,[],0,\"Most times, a senior team member is doing the deployment. Now all your deployment know-how is invested in one person – what if that person is out sick or his/her machine crashes? Now you can’t deploy.\"]],[[0,[],0,\"Even if you spread the deployment pain knowledge around, how long is the deploy-er going to spend doing this deployment? In the best case scenario, a few minutes. Generally though, this takes a lot longer. Do you really want a senior team member indisposed for hours and hours every month to do deployment?\"]],[[0,[],0,\"You can’t “do it again” (in most cases)\"]],[[0,[],0,\"What if your deploy-er is having a rough day and forgets to copy this folder or forgets to add that bit of new config to the existing configuration files? Manual deployments are not repeatable, so they’re error prone.\"]]]],[1,\"p\",[[0,[],0,\"So what does this have to do with automated builds? Automated builds address each of the above risks. Fewer risks translates into better quality and higher productivity.\"]]],[3,\"ol\",[[[0,[],0,\"You’re deploying “known” code\"]],[[0,[],0,\"Since the build engine is going to check out the latest version of source control, you know exactly what code is going to be compiled. No wondering if there are any “unintentional experiments that I forgot to delete”.\"]],[[0,[],0,\"You control dependencies on the build server\"]],[[0,[],0,\"You’re going to configure your build server – and that means it will have only what you install (assuming you lock it down). No rogue libraries or experimental versions – just exactly what you need to get into production.\"]],[[0,[],0,\"Anyone (who has permission) can trigger the build\"]],[[0,[],0,\"Since you’re setting up an automated build process, you’ll be able to trigger it with a single-click. No need for you to designate a deploy-er who has a whole lot of knowledge about what to compile. Once your build is set up, you can “just do it” again and again.\"]],[[0,[],0,\"Also, since it’s automated, you can “fire and forget”. Even if the build takes half an hour, your team lead is free to continue coding or whatever you really pay him/her to do (as opposed to watching compilation and copying files all over the place).\"]],[[0,[],0,\"Builds are repeatable\"]],[[0,[],0,\"Most build engines can “re-build” – do an exact build again. Since the process is automated, you can rest assured that no step is going to be forgotten by mistake.\"]]]],[1,\"p\",[[0,[1],1,\"2. Quality\"]]],[1,\"p\",[[0,[],0,\"Quality is something that’s hard to measure. Let’s consider an example. If a user expects the system to save a record to the database when they click the “Save” button, and it works, then quality is high, right? Not necessarily. Perhaps the “Save” operation only works when the information is “clean” – and breaks if the data is invalid (perhaps it should warn the user that there is invalid data?).\"]]],[1,\"p\",[[0,[],0,\"Automated builds go some way to providing a \"],[0,[0],1,\"measure of quality\"],[0,[],0,\". How do you know that the code you are publishing from VS is good code? What measures do you have to even assess this? I argue that if you don’t have automated builds (with unit tests – something we’ll again discuss in a later post) then you’ve got no objective measure.\"]]],[1,\"p\",[[0,[],0,\"Let’s consider some of the advantages that automated builds bring in terms of quality:\"]]],[3,\"ol\",[[[0,[],0,\"Packaging your binaries\"]],[[0,[],0,\"Let’s say you want to test your code in Staging before you deploy to Production. If you publish from VS, then you’ll be doing the publish twice – once for Staging, and then once for Production. Automated builds give you the advantage of producing one package (be that a WebDeploy package or installer or whatever) that you can deploy multiple times, knowing that you’re deploying exactly the same thing every time.\"]],[[0,[],0,\"Quality Measurement – i.e. test statistics\"]],[[0,[],0,\"I’ll discuss unit testing in another post – but assume that you have unit tests (and you’re analysing code coverage). If you don’t have a build of some sort, how do you know that the tests all passed? You’d have to take the word of your deploy-er. With automated builds, you can look at the build reports to see test pass/fail rates as well as coverage. If there are failing tests or coverage is too low, you block deployment. Also, if your build engine is putting these measurements into some sort of database you’ll be able to track quality trends over time.\"]],[[0,[],0,\"Removing user error\"]],[[0,[],0,\"A good automated build process is exactly that – \"],[0,[0],1,\"automated\"],[0,[],0,\". That means that the process can’t “forget” to link to some library or to run this or that test. This means better quality.\"]],[[0,[],0,\"Definition of Done\"]],[[0,[],0,\"Just because a developer says it’s done, it usually means that “it’s sort-of-nicely-coded-and-works-on-my-machine”. An automated build will run unit tests (the first of a few quality gates that should be part of your process). Then you should be deploying this build out to a pre-production environment. Testers (or at least “Power” users) should then manually test the build. Only once they sign off should the build be deployed out to Production. Since an automated build has produced the package, you know that the same package (that’s now passed automated and manual tests) is going out to Production.\"]],[[0,[],0,\"All of this means you get a standard, repeatable and consistent process that can become part of your “definition of done”. If it doesn’t pass unit tests, block it. If coverage is too low, block it. If it doesn’t pass manual testing, block it. Only once the build has passed these gates can it go to Production.\"]]]],[1,\"h2\",[[0,[],0,\"Consider the Cost\"]]],[1,\"p\",[[0,[],0,\"So let’s now ask which process would let you sleep easier the night of your rollout to Production? The one where a developer claims the unit tests are passing with sufficient coverage and does a “Publish” from Visual Studio, or the one where you’ve got an automated build report showing test success and coverage, approval from testers that the build passed manual tests, and a script that’s proven itself over and over for deployment?\"]]],[1,\"p\",[[0,[],0,\"I’d like to end with this consideration: the “earlier” in the process you find a bug, the cheaper it is to fix. Consider finding a bug while you’re coding. You fix it then-and-there: cost to company is a couple minutes of your time, so probably a few cents. On the other end of the spectrum, consider cost to company of a bug in Production: in terms of pure development, there’s the time of the person who finds the bug – then the time of the call-centre that they call, sometimes the ops team, 2nd line support, then some developer who spends a few hours trying to reproduce the issue and eventually fix it. Then it has to go through testing etc. etc. – and that’s just in terms of “direct” costs. Bugs in Production could cause other costs such as financial costs for legal errors or even reputational cost for your company.\"]]],[1,\"p\",[[0,[],0,\"The bottom line is this: investing some time now to automate builds (and add unit tests) will save you lots of time and money in production and operational issues. It’s a fact.\"]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[2],1,\"next post\"],[0,[],0,\", I’ll talk about the Team Build automation engine.\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1371583380000,"status":"published","published_by":1},{"id":"f1d113dc-e056-4376-878f-18ce3306043a","title":"Automated Builds–Why They’re Absolutely Essential (Part 2)","slug":"automated-buildswhy-theyre-absolutely-essential-(part-2)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-2oc8o8alSrU/UcA1XfmLm6I/AAAAAAAAAzc/0QZcvAe3qWE/s1600-h/image3.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-MxXvEL4K9R0/UcA1Y78AXRI/AAAAAAAAAzk/j4KEDZDvPR8/image_thumb1.png?imgmax=800\\\" width=\\\"344\\\" height=\\\"90\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-EagqFV4mlsg/UcA1aBcY59I/AAAAAAAAAzs/hS1sApAgDb8/s1600-h/image7.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-3w9Tl5usdIE/UcA1bukCT7I/AAAAAAAAAz0/1UzlgBY0bpI/image_thumb3.png?imgmax=800\\\" width=\\\"389\\\" height=\\\"205\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-7NdOwTOYSc8/UcA1czOlurI/AAAAAAAAAz8/cvTmMsm9ep4/s1600-h/image11.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-YxyFJbZUhBc/UcA1eZQoUzI/AAAAAAAAA0E/y7acUNEYVm0/image_thumb5.png?imgmax=800\\\" width=\\\"345\\\" height=\\\"255\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-2r-pW6LPNlw/UcA1ftMHdEI/AAAAAAAAA0M/2ZIbnP61Yew/s1600-h/image29.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-dsDEEt4ywig/UcA1g6t0lHI/AAAAAAAAA0U/eHl355SF6rk/image_thumb13.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"126\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-arwaZV04ERg/UcA1iCcRySI/AAAAAAAAA0c/tB3sAfBnpzQ/s1600-h/image41.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-UyPGgc2HZ8Q/UcA1jrJWcVI/AAAAAAAAA0k/FlXo3t5rsn8/image_thumb21.png?imgmax=800\\\" width=\\\"286\\\" height=\\\"257\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-2aCt6IFSzKo/UcA1kgPq6YI/AAAAAAAAA0s/_x5Oc8HSLjM/s1600-h/image57.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-wXwwboWzDNI/UcA1l1ear8I/AAAAAAAAA00/LNIYw-SP6W8/image_thumb28.png?imgmax=800\\\" width=\\\"451\\\" height=\\\"58\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-s9krhHh0A-8/UcA1nHtcUXI/AAAAAAAAA08/y1XQVtAfLQw/s1600-h/image15.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-NBV8fE80xbI/UcA1oWGA1JI/AAAAAAAAA1E/gtZq9XM_3_M/image_thumb7.png?imgmax=800\\\" width=\\\"370\\\" height=\\\"129\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-1Yy9zPHhcb4/UcA1pVGDShI/AAAAAAAAA1M/giN9IebOrl8/s1600-h/image19.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-cOZSAbPjctg/UcA1qx8H23I/AAAAAAAAA1U/5XoerYOGFcY/image_thumb9.png?imgmax=800\\\" width=\\\"391\\\" height=\\\"115\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-iVtU6BpsH1E/UcA1r_bVRHI/AAAAAAAAA1c/jMwwoF_aaq8/s1600-h/image23.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-Xg7EeJeNDwk/UcA1tQ-MV8I/AAAAAAAAA1k/WW26xDUtrxw/image_thumb11.png?imgmax=800\\\" width=\\\"313\\\" height=\\\"278\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-3TeK4EJjZwM/UcA1uRVOaGI/AAAAAAAAA1s/fARXyePvFJU/s1600-h/image49.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-wQtsQYdrYzg/UcA1v71NKZI/AAAAAAAAA10/fyH0m5N80f4/image_thumb25.png?imgmax=800\\\" width=\\\"341\\\" height=\\\"142\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-3d6HeFG1gJQ/UcA1wwvaNiI/AAAAAAAAA18/HW7mPi8HoGM/s1600-h/image53.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-kwJih5m7A4Q/UcA1yeeEJJI/AAAAAAAAA2E/1tx7O4Ec5yA/image_thumb27.png?imgmax=800\\\" width=\\\"316\\\" height=\\\"199\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-qx4dvkwOkEM/UcA1zmG7WfI/AAAAAAAAA2M/IKkDFNdXWpA/s1600-h/image61.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-iuDMg57R-x4/UcA11FTASnI/AAAAAAAAA2U/DOFhA1XMWsY/image_thumb30.png?imgmax=800\\\" width=\\\"368\\\" height=\\\"250\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-1ANeCujc_XM/UcA12di30eI/AAAAAAAAA2c/mZx71Eh_6ZU/s1600-h/image45.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-8aAVqdcSzaM/UcA14F-XlVI/AAAAAAAAA2k/7miAOAr8kFo/image_thumb23.png?imgmax=800\\\" width=\\\"330\\\" height=\\\"345\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-uCJFEdBErbg/UcA15SJFzJI/AAAAAAAAA2s/amvvVd-dKD8/s1600-h/image69.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-5DbIckW9V_8/UcA16j52yyI/AAAAAAAAA20/XLg49Gi1q54/image_thumb34.png?imgmax=800\\\" width=\\\"307\\\" height=\\\"224\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-THC-HX_tcPE/UcA18PhxXbI/AAAAAAAAA28/1-dxuORn4UE/s1600-h/image65.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-z5wz_2ixz7s/UcA19ux4vTI/AAAAAAAAA3E/-DimWuSXjK4/image_thumb32.png?imgmax=800\\\" width=\\\"370\\\" height=\\\"240\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-G6v6adV-BtA/UcA1-2ozIaI/AAAAAAAAA3M/qGxQ_aX178g/s1600-h/image73.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-_Muf5qZsNu4/UcA2AVGNhvI/AAAAAAAAA3U/orhSKt2KLS8/image_thumb36.png?imgmax=800\\\" width=\\\"437\\\" height=\\\"219\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-enJCQu5OZTY/UcA2BTuQdNI/AAAAAAAAA3c/PG3xiGujBIM/s1600-h/image81.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-tkkIAE3ewJY/UcA2C_JxrKI/AAAAAAAAA3k/O_Imn6bUmoM/image_thumb39.png?imgmax=800\\\" width=\\\"366\\\" height=\\\"242\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-QTtdFTJm50g/UcA2D1AiKuI/AAAAAAAAA3s/bGP72TMzk5E/s1600-h/image85.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-yLmWSqjzG_o/UcA2FSH3FVI/AAAAAAAAA30/ZZTc4j9jYyw/image_thumb41.png?imgmax=800\\\" width=\\\"177\\\" height=\\\"337\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/automated-buildswhy-theyre-absolutely.html\"]],[\"em\"],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/02/custom-build-task-include-merged.html\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"In my \"],[0,[0],1,\"previous post\"],[0,[],0,\" I wrote about why you should be doing automated builds in general terms. In this post I’ll show you how TFS’s automated build engine gives you \"],[0,[1],1,\"consistency\"],[0,[],0,\" and \"],[0,[1],1,\"quality\"],[0,[],0,\" in your build processes. There are other build engines, but if you’re using TFS for source control (and/or test management and/or lab management and/or work item tracking) then Team Build makes the most sense as a build engine since it ties so many other parts of the ALM spectrum together.\"]]],[1,\"p\",[[0,[],0,\"TFS Team Build uses Workflow Foundation as the engine underneath the build. When you create a Team Project you get a few workflow XAML files out-the-box. For this post I’ll primarily discuss features of the DefaultTemplate11.1.xaml (the default build template).\"]]],[1,\"h2\",[[0,[],0,\"Environment\"]]],[1,\"p\",[[0,[],0,\"When you configure a Build Agent, you install it on a build server. Ideally this is some Virtual Machine that is “clean” – the only things installed on the machine are the things that you need to compile (and test) your code. No rogue libraries or experimental settings – just a clean, controlled environment.\"]]],[1,\"p\",[[0,[],0,\"Installing the build agent is a snap – mount the TFS install media and install TFS. Then run the Build Configuration wizard and connect to a build controller (which can reside on the build machine if you want).\"]]],[1,\"h2\",[[0,[],0,\"Labelling Sources During a Build\"]]],[1,\"p\",[[0,[],0,\"The build agent checks out the latest version of source control when it starts the build. As it does so, by default it labels the code that it checks out with the build name. This means that you can get the exact point-in-time code that the build used to compile, test and package. To see the labels, open the Source Control Explorer, find the folder that your build workspace is configured to download (in the Sources section of the build workflow), right-click and select “View History”. Then click on the Labels tab.\"]]],[10,0],[1,\"p\",[[0,[1],1,\"In the above picture you can see the build reports on the left, and the labels for the root folder of the build workspace on the right.\"]]],[1,\"p\",[[0,[],0,\"If you don’t want the build to label the sources on each build, then go to the Process tab of your build definition, expand the Advanced parameters and set “Label Sources” to false.\"]]],[1,\"h2\",[[0,[],0,\"Perform Code Analysis During a Build\"]]],[1,\"p\",[[0,[],0,\"Most of us developers know we should be doing code analysis, but few teams that I work with actually do it. Most of the time it comes down to the fact that it’s hard to monitor. However, if you include code analysis as part of your build, you’ll easily be able to track Code Analysis over time.\"]]],[1,\"p\",[[0,[],0,\"If you have particular projects that you care about and don’t want to run Code Analysis on all projects, then you can configure that in the Build. The default build template sets “Perform Code Analysis” to “AsConfigured” which means if a project is configured to do code analysis on build, then it does so.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Of course you can set the Code Analysis to “Never” or “Always” too.\"]]],[1,\"p\",[[0,[],0,\"And as easy as that you now have Code Analysis as part of your build process:\"]]],[10,2],[1,\"h2\",[[0,[],0,\"Layer Validation\"]]],[1,\"p\",[[0,[],0,\"If you’ve got Visual Studio Ultimate, you’ll be able to draw Layer Diagrams. These diagrams allow you to visualize (and validate) layering within your architecture. Team Build can validate layering when building – all you have to do is right click your modelling project (that contains your layer diagrams) in the Solution Explorer, select Properties and set the “Validate Architecture” property to true.\"]]],[10,3],[1,\"p\",[[0,[],0,\"As long as this project is part of the solution(s) being built, you’ll get layer validation as part of your build.\"]]],[10,4],[1,\"p\",[[0,[1],1,\"Oh dear – someone broke our layering!\"]]],[1,\"h2\",[[0,[],0,\"Symbols\"]]],[1,\"p\",[[0,[],0,\"You should never deploy pdb files (symbol files) to production environments. But there are times when you’ll need the symbols files – for example remote debugging, for IntelliTrace or for Manual Test Coverage (see below). Team Build effortlessly publishes your symbols to a network share and indexes them for you, so you never have to think about them or hunt for them again.\"]]],[10,5],[1,\"p\",[[0,[1],1,\"Configuring Symbols and indexing on a build – the build creates a folder structure, so just supply the root folder and the build takes care of the rest.\"]]],[1,\"h2\",[[0,[],0,\"Unit Testing and Code Coverage\"]]],[1,\"p\",[[0,[],0,\"Having a automated build without unit tests is like brushing your teeth without toothpaste. Once you’ve got a build in place, add unit tests and code coverage. This will increase the quality and consistency of your releases exponentially.\"]]],[1,\"p\",[[0,[],0,\"So let’s assume you have unit tests. You can easily configure Team Build to run the tests and perform code coverage. Set the automated test settings (you can have multiple) appropriately. By default the discovery filter is **\\\\*test*.dll (which is any dll with the word “test” in it in any subdirectory). Click on “Edit” to enable Code Coverage and you’re done. I’ve even configured a build engine to run QUnit js files to test JavaScript in my web projects! Of course you can add category filters too if you want to filter which tests the build should be running.\"]]],[10,6],[1,\"p\",[[0,[],0,\"Besides being a metric for each individual build, the pass/fail rates and coverage percentages go into the TFS Warehouse so that you can report off them and trend them.\"]]],[1,\"p\",[[0,[],0,\"If you’re not using the MSTest framework and you’re using nUnit or XUnit or some other framework and you have a corresponding Test Adapter in VS for running your unit tests within VS, then make sure you install the same Test Adapter on your build machine to enable it to run those tests during the build.\"]]],[10,7],[1,\"p\",[[0,[1],1,\"This build output shows a failed test.\"]]],[10,8],[1,\"p\",[[0,[1],1,\"That’s better – a 100% pass rate. Looks like the coverage is a little on the low side though…\"]]],[1,\"h2\",[[0,[],0,\"Code Coverage for Manual Tests\"]]],[1,\"p\",[[0,[],0,\"At present this only works for Web Applications running in IIS. Get your testers to run test cases out of Microsoft Test Manager (MTM) against your test servers, and then enable the “Code Coverage” diagnostic adapter. You’ll have to tell it where to find the symbols files (which you hopefully configured on your build anyway) and you’re good to go.\"]]],[10,9],[10,10],[1,\"p\",[[0,[1],1,\"Setting the Code Coverage diagnostic adapter (and the path to the symbols) in the Test Settings section of a Lab Environment.\"]]],[1,\"p\",[[0,[],0,\"The great thing about Team Build is that the manual code coverage is fed back onto the build report as testers execute their manual tests. Each time a manual test run is completed, the build report is updated.\"]]],[10,11],[1,\"p\",[[0,[1],1,\"This build report has been updated to show an additional test run (the manual test run) and the coverage has been merged into the total coverage (so it’s showing total unit test plus manual test coverage).\"]]],[1,\"h2\",[[0,[],0,\"Build Reports – or what the heck is in this build?\"]]],[1,\"p\",[[0,[],0,\"Get your developers into the habit of associating checkins with work items. By default, the build lists all associated checkins between “good builds”. (The last good build is the last build that was successful – no compiler errors or test failures). If those checkins are associated with work items, the work items get associated with the builds too. That means that you can look at the build report and quickly answer the question, “What work is included in this build?”. This works for “direct” associations, such as when a developer checks in code against a Bug, but also “indirect” – when a developer checks in against a Task, the Tasks parent Product Backlog Item (or User Story) is also associated with the build.\"]]],[10,12],[1,\"p\",[[0,[1],1,\"Here we can see that Bug 82 was fixed in this build. We also see that Task 84 of PBI 83 is in this build.\"]]],[1,\"p\",[[0,[],0,\"Unfortunately this won’t work out-the-box for \"],[0,[1],1,\"merges\"],[0,[],0,\". If you queue a build that has only merges as changesets, the only changesets you’ll see will be the merges themselves. Never fear though – I created a custom build task that pulls in the merged changesets and work items into the build report. You can get it \"],[0,[2],1,\"here\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Found In and Integrated In – Tracking Bugs Effectively\"]]],[1,\"p\",[[0,[],0,\"If you’ve got a build, and your testers specify that build number as the build they are testing, then any bug logged during testing has it’s “Found In” field automatically set. When you fix the bug and check it in, the Integrated In field is set so you know which build the bug was fixed in.\"]]],[10,13],[1,\"p\",[[0,[1],1,\"The System Tab of the default Bug work item: the “Found In” field gets populated automatically when logging a bug from MTM (where the build under test is specified) and the “Integrated In” field gets populated automatically when you resolve the bug during checkin.\"]]],[1,\"h2\",[[0,[],0,\"Test Impact Analysis\"]]],[1,\"p\",[[0,[],0,\"Let’s say you have 100 manual tests. You run them all successfully. The developer then changes some code. Which tests should you run again? Ideally, all of them – but you may not have time to run all of them. So Team Build narrows down the list by doing \"],[0,[1],1,\"test impact analysis\"],[0,[],0,\". When you enable this diagnostic adapter in MTM, TFS builds a map of test vs code – it tracks which code is hit for each test the tester is executing. Then on the next build, for each \"],[0,[3],1,\"passed\"],[0,[],0,\" test case, TFS does a lookup to see if any tests hit the code that changed since the last build. Each test that is “potentially impacted” is flagged during the build so that you can test is again to make sure the changes didn’t break the code. Of course TFS assumed you’ll re-run failed tests, so this only works against passed test cases.\"]]],[10,14],[1,\"p\",[[0,[1],1,\"Two tests were impacted by changes to the code – clicking on the “code changes” link opens details about what methods changed.\"]]],[1,\"h2\",[[0,[],0,\"Build-Deploy-Test Workflow\"]]],[1,\"p\",[[0,[],0,\"I won’t go into any details on this workflow, but you get a LabDefault.xaml template out-the-box when you create a Team Project. This build doesn’t compile code – it takes the output of another build (TFS or even another engine), allows you to specify scripts that automate deployment to a Lab Environment (that you’ve set up using MTM’s Lab Manager) and even run automated tests, which could include manual test cases that you’ve converted to Coded UI Tests.\"]]],[1,\"h2\",[[0,[],0,\"Metrics\"]]],[1,\"p\",[[0,[],0,\"I’ve mentioned that the build data go into the TFS warehouse – so you can see test results, coverage and code churn over time. Then you can slice-and-dice and create dashboards and reports.\"]]],[10,15],[1,\"p\",[[0,[1],1,\"The out-of-the-box Build Summary report.\"]]],[10,16],[1,\"p\",[[0,[1],1,\"The out-of-the-box Build Success Over Time report.\"]]],[10,17],[1,\"p\",[[0,[1],1,\"Some of the build-specific measures available when you pivot against the TFS Cube from Excel.\"]]],[1,\"h2\",[[0,[],0,\"Summary\"]]],[1,\"p\",[[0,[],0,\"There are other build engines that you can use (such as Hudson or TeamCity or Jenkins). Where they cannot compete with TFS is in the rich integration you get into work items, source control, lab management, testing and reporting. And you get most of it for free – out-the-box. In short, if you want to take a quantum leap in consistency and quality, you need to get building! The small investment up-front will be well worth it in the long run. And you’ll be able to sleep at night…\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1371583620000,"status":"published","published_by":1},{"id":"6fcbdace-8044-4d76-a52b-6125f4c0d015","title":"Automating Office Tasks in a TeamBuild","slug":"automating-office-tasks-in-a-teambuild","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font size=\\\"4\\\" face=\\\"Cordia New\\\">$word = new-object -ComObject \\\"word.application\\\"<br>&nbsp;&nbsp;&nbsp; </font>\"}],[\"html\",{\"html\":\"<font size=\\\"4\\\" face=\\\"Cordia New\\\"><br>&nbsp;&nbsp;&nbsp; $missing = [System.Reflection.Missing]::Value<br>&nbsp;&nbsp; <font color=\\\"#008000\\\"> # open read only<br></font>&nbsp;&nbsp;&nbsp; <font color=\\\"#ff0000\\\">$doc = $word.documents.open($source, $missing, $true)<br></font>&nbsp;&nbsp;&nbsp; if ($doc -eq $null) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Throw \\\"Could not open $source\\\"<br>&nbsp;&nbsp;&nbsp; }</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/Tbqod75P4sI/AAAAAAAAAQc/L1zVevQRBbY/s1600-h/image%5B3%5D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TbqofT2NXuI/AAAAAAAAAQg/lhXIymMmNaU/image_thumb%5B1%5D.png?imgmax=800\\\" width=\\\"329\\\" height=\\\"255\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://pdfsharp.com/PDFsharp/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I was working on a TeamBuild that doesn’t compile code – this build checks out a number of MS Word documents from source control, converts them to PDF and then uses \"],[0,[0],1,\"PDFSharp\"],[0,[],0,\" to merge them all into one PDF document. I started with the DefaultTemplate.xaml and ripped out the compile / test tasks. I then created a Powershell script that would do the heavy lifting. So all the build does is check out the doc files in the workspace, invoke the powershell script and then copy the final file to the drop folder. Seems pretty simple, right?\"]]],[1,\"p\",[[0,[],0,\"Well, in theory it was. I modified the workflow and created the Powershell script. I tested the Powershell script “manually” by calling it from a command prompt. I then got the workflow to call the script, expecting goodness. However, that’s where Simple ended and Frustration started – the script wouldn’t work when invoked from the workflow.\"]]],[1,\"p\",[[0,[],0,\"I tried to do several things to figure out the problem. Here’s the snippet of the code that was not working (the line in red):\"]]],[10,0],[10,1],[1,\"p\",[[0,[],0,\"Whenever the script was invoked from within the build, the open() method to open the Word doc would return null and the script would Throw. For some reason, it couldn’t open the file. At first I thought it was an “interactive / non-interactive” problem, so I set the Visible property on the $word ComObject to false. No luck. I checked permissions. I checked the readonly attribute on the Word doc. Nothing helped.\"]]],[1,\"h2\",[[0,[],0,\"The Solution – the SystemProfile Desktop directory\"]]],[1,\"p\",[[0,[],0,\"I eventually stumbled onto some websites talking about a similar problem when doing Office automation from an ASP.NET site. They did all sorts of things with the identities and impersonation and so on, but that wouldn’t apply here since there’s no ASP.NET site.\"]]],[1,\"p\",[[0,[],0,\"There was one other piece of advice that turned out to solve the problem: the Desktop folder for the system profile. I was incredulous at first, but then decided what the heck and tried it.\"]]],[1,\"p\",[[0,[],0,\"All I had to do was create a folder called “Desktop” in the systemProfile folder (which is different for 64 bit and 32 bit machines). Once that folder existed, there was no more problem and the build worked like a charm. So I added in a sequence to check that the folder exists and to create it if it doesn’t. Here’s the activity:\"]]],[10,2],[1,\"p\",[[0,[],0,\"I added an If with the condition set to:\"]]],[1,\"p\",[[0,[],0,\"Directory.Exists(“C:\\\\Windows\\\\SysWOW64\\\\config”)\"]]],[1,\"p\",[[0,[],0,\"In the “Then”, I assign “C:\\\\Windows\\\\SysWOW64\\\\config\\\\systemProfile\\\\Desktop” to a variable called profileFolder, and in the Else I assign the profileFolder the value “C:\\\\Windows\\\\System32\\\\config\\\\systemProfile\\\\Desktop”.\"]]],[1,\"p\",[[0,[],0,\"Next I added a “CreateDirectory” activity and pass in profileFolder as the directory to create.\"]]],[1,\"p\",[[0,[],0,\"I placed this sequence before any activity that performs Office automation and voila, the build works.\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1304110860000,"status":"published","published_by":1},{"id":"230f6f5d-1161-4cdb-9844-47a4014af9e4","title":"Azure DevOps Build and Test Reports using OData and REST in PowerBI","slug":"azure-devops-build-and-test-reports-using-odata-and-rest-in-powerbi","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fc5728e0-b6e5-4000-b62e-8839a2fdedb5.png\\\" target=\\\"_blank\\\"><img width=\\\"604\\\" height=\\\"494\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/460e593f-294e-4d44-99a7-6e98319cba8b.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/Builds?\\n   $apply=filter(CompletedDate ge 2019-09-01Z \\\"\\n   &amp;$select=* \\n   &amp;$expand=Project($select=ProjectName),BuildPipeline($select=BuildPipelineName),Branch($select=RepositoryId,BranchName)\\n\",\"language\":\"plain;\"}],[\"code\",{\"code\":\"let\\n    Source = OData.Feed (\\\"https://analytics.dev.azure.com/\\\" &amp; #\\\"AzureDevOpsOrg\\\" &amp; \\\"/_odata/v3.0-preview/Builds?\\\"\\n        &amp; \\\"$apply=filter(CompletedDate ge \\\" &amp; Date.ToText(Date.From(Date.AddDays(DateTime.LocalNow(), -14)), \\\"yyyy-MM-dd\\\") &amp; \\\"Z )\\\"\\n        &amp; \\\"&amp;$select=* \\\"\\n        &amp; \\\"&amp;$expand=Project($select=ProjectName),BuildPipeline($select=BuildPipelineName),Branch($select=RepositoryId,BranchName)\\\"\\n     ,null, [Implementation=\\\"2.0\\\",OmitValues = ODataOmitValues.Nulls,ODataVersion = 4]),\\n    #\\\"Changed Type\\\" = Table.TransformColumnTypes(Source,{{\\\"BuildSK\\\", type text}, {\\\"BuildId\\\", type text}, {\\\"BuildDefinitionId\\\", type text}, {\\\"BuildPipelineId\\\", type text}, {\\\"BuildPipelineSK\\\", type text}, {\\\"BranchSK\\\", type text}, {\\\"BuildNumberRevision\\\", type text}}),\\n    #\\\"Expanded BuildPipeline\\\" = Table.ExpandRecordColumn(#\\\"Changed Type\\\", \\\"BuildPipeline\\\", {\\\"BuildPipelineName\\\"}, {\\\"BuildPipelineName\\\"}),\\n    #\\\"Expanded Branch\\\" = Table.ExpandRecordColumn(#\\\"Expanded BuildPipeline\\\", \\\"Branch\\\", {\\\"RepositoryId\\\", \\\"BranchName\\\"}, {\\\"RepositoryId\\\", \\\"BranchName\\\"}),\\n    #\\\"Renamed Columns\\\" = Table.RenameColumns(#\\\"Expanded Branch\\\",{{\\\"PartiallySucceededCount\\\", \\\"PartiallySucceeded\\\"}, {\\\"SucceededCount\\\", \\\"Succeeded\\\"}, {\\\"FailedCount\\\", \\\"Failed\\\"}, {\\\"CanceledCount\\\", \\\"Canceled\\\"}}),\\n    #\\\"Expanded Project\\\" = Table.ExpandRecordColumn(#\\\"Renamed Columns\\\", \\\"Project\\\", {\\\"ProjectName\\\"}, {\\\"ProjectName\\\"})\\nin\\n    #\\\"Expanded Project\\\"\\n\",\"language\":\"plain;\"}],[\"code\",{\"code\":\"(project as text) =&gt;\\nlet\\n    Source = Json.Document(Web.Contents(\\\"https://dev.azure.com/\\\" &amp; #\\\"AzureDevOpsOrg\\\" &amp; \\\"/\\\" &amp; project &amp; \\\"/_apis/git/repositories?api-version=5.1\\\"))\\nin\\n    Source\\n\",\"language\":\"plain;\"}],[\"code\",{\"code\":\"let\\n    Source = OData.Feed (\\\"https://analytics.dev.azure.com/\\\" &amp; #\\\"AzureDevOpsOrg\\\" &amp; \\\"/_odata/v3.0-preview/Projects?\\\"\\n        &amp;\\\"&amp;$select=ProjectSK, ProjectName \\\"\\n    ,null, [Implementation=\\\"2.0\\\",OmitValues = ODataOmitValues.Nulls,ODataVersion = 4])\\nin\\n    Source\\n\",\"language\":\"plain;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/431f5012-0301-48e7-b4f2-cda45ee38ca7.png\\\" target=\\\"_blank\\\"><img width=\\\"365\\\" height=\\\"317\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d1946ae0-71da-45d5-9c8f-b8035b05f835.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/399078e0-171e-4fd1-a15e-29694500ce96.png\\\" target=\\\"_blank\\\"><img width=\\\"535\\\" height=\\\"261\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f1c24f59-153a-4310-9708-f452dc77d1e2.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b636e87e-e41a-493a-a033-4114ecf030d8.png\\\" target=\\\"_blank\\\"><img width=\\\"512\\\" height=\\\"298\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/811a8e54-f2c8-4eb3-89dd-e75e0f0c3c6f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/dd6da92a-83c5-4129-ad15-c673a29a0649.png\\\" target=\\\"_blank\\\"><img width=\\\"429\\\" height=\\\"201\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/17876bb9-34a9-4162-9c62-3276d45d33cc.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">#\\\"Filter nulls\\\" = Table.SelectRows(#\\\"Expanded value\\\", each [value] &lt;&gt; null)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/11b4a832-1227-4756-b423-bb3f77a770c4.png\\\" target=\\\"_blank\\\"><img width=\\\"427\\\" height=\\\"291\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/56078f03-1d02-4e27-9c05-841004a5b5c3.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"let\\n   Source = OData.Feed (\\\"https://analytics.dev.azure.com/\\\" &amp; #\\\"AzureDevOpsOrg\\\" &amp; \\\"/_odata/v3.0-preview/Projects?\\\"\\n        &amp;\\\"&amp;$select=ProjectSK, ProjectName \\\"\\n    ,null, [Implementation=\\\"2.0\\\",OmitValues = ODataOmitValues.Nulls,ODataVersion = 4]),\\n    #\\\"Invoked Custom Function\\\" = Table.AddColumn(Source, \\\"Repos\\\", each GetGitRepos([ProjectName])),\\n    #\\\"Expanded Repos\\\" = Table.ExpandRecordColumn(#\\\"Invoked Custom Function\\\", \\\"Repos\\\", {\\\"value\\\"}, {\\\"Repos.value\\\"}),\\n    #\\\"Expanded Repos.value\\\" = Table.ExpandListColumn(#\\\"Expanded Repos\\\", \\\"Repos.value\\\"),\\n    #\\\"Filter nulls\\\" = Table.SelectRows(#\\\"Expanded Repos.value\\\", each [Repos.value] &lt;&gt; null),\\n    #\\\"Expanded Repos.value2\\\" = Table.ExpandRecordColumn(#\\\"Filter nulls\\\", \\\"Repos.value\\\", {\\\"id\\\", \\\"name\\\", \\\"defaultBranch\\\", \\\"size\\\", \\\"webUrl\\\"}, {\\\"Repos.value.id\\\", \\\"Repos.value.name\\\", \\\"Repos.value.defaultBranch\\\", \\\"Repos.value.size\\\", \\\"Repos.value.webUrl\\\"}),\\n    #\\\"Renamed Columns1\\\" = Table.RenameColumns(#\\\"Expanded Repos.value2\\\",{{\\\"Repos.value.id\\\", \\\"RepositoryId\\\"}, {\\\"Repos.value.name\\\", \\\"Name\\\"}, {\\\"Repos.value.defaultBranch\\\", \\\"DefaultBranch\\\"}, {\\\"Repos.value.size\\\", \\\"Size\\\"}, {\\\"Repos.value.webUrl\\\", \\\"WebURL\\\"}})\\nin\\n    #\\\"Renamed Columns1\\\"\\n\",\"language\":\"plain;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b93b844e-0f5b-4f78-bb03-20e7aba2396d.png\\\" target=\\\"_blank\\\"><img width=\\\"342\\\" height=\\\"274\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1818879a-64cf-4fb8-8ef1-d341e48bbfef.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c500998f-581e-494a-a96d-e1dc1ea054c9.png\\\" target=\\\"_blank\\\"><img width=\\\"376\\\" height=\\\"213\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/06bbeb42-e245-4e2a-b7c5-455bd9da738f.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/report/extend-analytics/?view=azure-devops\"]],[\"a\",[\"href\",\"https://wouterdekort.com/2019/09/12/measuring-your-way-around-azure-devops/\"]],[\"a\",[\"href\",\"https://twitter.com/wouterdekort\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/azdo-build-test-pbi\"]],[\"a\",[\"href\",\"https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/$metadata\"]],[\"a\",[\"href\",\"https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/Builds\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have been playing with the Azure DevOps \"],[0,[0],1,\"OData service\"],[0,[],0,\" recently to start creating some reports. Most of my fiddling has been with the Work Item and Work Item Board Snaphot entities, but I recently read a great post focused more on \"],[0,[1],1,\"Build metrics\"],[0,[],0,\" by my friend and fellow ALM MVP, \"],[0,[2],1,\"Wouter de Kort\"],[0,[],0,\". I just happened to be working with a customer that is migrating from Azure DevOps Server to Azure DevOps Services and they had some SSRS reports that I knew could fairly easily be created using OData and PowerBI. In this post I’ll go over some of my experiences with the OData service and share a PowerBI template so that you can start creating some simple build reports yourself.\"]]],[1,\"h2\",[[0,[],0,\"TL;DR\"]]],[1,\"p\",[[0,[],0,\"If you just want the PowerBI template, then head over to this \"],[0,[3],1,\"Github repo\"],[0,[],0,\" and have at it!\"]]],[1,\"h2\",[[0,[],0,\"Exploring Metadata\"]]],[1,\"p\",[[0,[],0,\"If you want to see what data you can grab from the OData endpoint for your Azure DevOps account, then navigate to this URL: \"],[0,[4],1,\"https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/$metadata\"],[0,[],0,\" (you’ll need to replace {organization} with your organization name). This gives an XML document that details the entities and relationships. Here’s a screenshot of what it looks like in Chrome:\"]]],[10,0],[1,\"p\",[[0,[],0,\"To get the data for an entity, you need to use OData queries. Some of these are pretty obscure, but powerful. First tip: pluralize the entity to get the entries. For example, the entity “Build” is queried by navigating to \"],[0,[5],1,\"https://analytics.dev.azure.com/{organization}/_odata/v3.0-preview/Builds\"],[0,[],0,\". You definitely want to learn how to apply $filter (for filtering data), $select (for specifying which columns you want to select), $apply (for grouping and aggregating) and $expand (for expanding fields from related entities). Once you have some of these basics down, you’ll be able to get some pretty good data out of your Azure DevOps account.\"]]],[1,\"p\",[[0,[],0,\"Here’s an example. Let’s imagine you want a list of all builds (build runs) from Sep 1st to today. The Build entity has the ProjectSK (an identifier to the project), but you’ll probably want to expand to get the Project name. Similarly, the Build entity includes a reference to the Build Definition ID, but you’ll have to expand to get the Build Definition Name. Here’s what the request would look like:\"]]],[10,1],[1,\"p\",[[0,[],0,\"If you look at the metadata for the Build entity, you’ll see that there are navigation properties for Project, BuildPipeline, Branch and a couple others. These are the names I use in the $expand directive, using an internal $select to specify which fields of the related entities I want to select.\"]]],[1,\"h2\",[[0,[],0,\"Connecting with PowerBI\"]]],[1,\"p\",[[0,[],0,\"To connect with PowerBI, you just connect to an OData field. You then have to expand some of the columns and do some other cleanup. Here’s what the M query looks like (view it by navigating to the “Advanced editor” for a query) for getting all the Builds since September 1st:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 2: Connect to the OData feed Build entities (the #”AzureDevOpsOrg” is a parameter so that the account can be changed in a single place)\"]],[[0,[],0,\"Line 3: Use Date.ToText and other M functions to get dates going back 2 weeks\"]],[[0,[],0,\"Line 6: Standard OData feed arguments\"]],[[0,[],0,\"Lines 7-11: Update some column types, rename some columns and expand some record columns to make the data easier to work with\"]]]],[1,\"p\",[[0,[],0,\"You can see how we can use PowerBI functions (like DateTime.LocalNow) and so on. This allows us to create dynamic reports.\"]]],[1,\"h2\",[[0,[],0,\"Performance – Be Mindful\"]]],[1,\"p\",[[0,[],0,\"Be careful with your queries – try to aggregate where you can. For detail reports, make sure you limit the result sets using filters like date, team or team project and so on. You don’t want to be bringing millions of records back each time you refresh a report! For my particular report, I limit the date range to the builds completed in the last 2 weeks. In my case, that’s not a lot of data – but if you run hundreds of builds every day, even that date range might be too broad.\"]]],[1,\"h2\",[[0,[],0,\"Limitations\"]]],[1,\"p\",[[0,[],0,\"There are still some gaps when using the OData feeds. For example, you can get TestRun and TestResult entities – both for automated as well as manual tests. This data is sufficient for doing some reporting on automated tests – but it’s impossible to tie the TestResults back to test plans and suites. The TestResult actually has a TestCaseReferenceId so you can get back to the Test Case, but there’s no way to aggregate these to Suites and Plans since these entities are entirely absent from the OData model. Or the Build entity has a relationship to the Branch entity, which contains a RepositoryId, but no repository name – and there isn’t an entity for Repo in the OData model either.\"]]],[1,\"h2\",[[0,[],0,\"API Calls From PowerBI\"]]],[1,\"p\",[[0,[],0,\"Two other limitations that I found was that there’s no queue information in the OData fields (so you can’t see which queue a build was routed to) and there’s no code coverage information either. So doing any analysis on code coverage statistics or queues isn’t possible using pure OData. Wouter makes the same discovery in his blog post, where he calls out using PowerShell to call the Azure DevOps REST APIs to get some additional queue data.\"]]],[1,\"p\",[[0,[],0,\"However, you can call REST APIs from PowerBI. I wanted a report where users could filter by repo, so I wanted a list of repositories in my organization. I also wanted to include queue and code coverage information on the Build entities.\"]]],[1,\"p\",[[0,[],0,\"Before we look at how to do this in PowerBI, there is a caveat to doing API calls, especially if you’re looping over records: don’t do this for large datasets! When I was trying to aggregate test runs to test suites and plans, I was actually able to get a list of test plans and test suites in an organization using REST APIs. But then I wanted a list of test IDs in each Test Suite – and that’s when my dream died. The organization I was doing this for had over 20,000 Test Suites – that means that PowerBI would have to make over 20,000 REST API calls to get all the Tests in Test Suites in an organization. I was forced to abandon that plan. In short, be mindful of where you use your REST API calls, and try to limit the number of rows you’re making the calls for!\"]]],[1,\"p\",[[0,[],0,\"Another caveat is that while you can authenticate to the OData feed using org credentials, you need a PAT for the REST API call! So there are now two authentication mechanisms for the report – org account and PAT.\"]]],[1,\"p\",[[0,[],0,\"Enough caveats – let’s get to it!\"]]],[1,\"h3\",[[0,[],0,\"Create a REST API Function\"]]],[1,\"p\",[[0,[],0,\"The first step is to create a function that can call the Azure DevOps API. Here’s the function to get a list of repositories for a give Team Project:\"]]],[10,3],[1,\"p\",[[0,[],0,\"This function takes a single arg called “project” of type text.\"]]],[1,\"p\",[[0,[],0,\"Now that we have the function defined, we can use it to expand a table with a list of Team Projects to end up with a list of all the repos in an org. Add a new Data Source, open the advanced editor and paste in this query:\"]]],[10,4],[1,\"p\",[[0,[],0,\"If it runs, you’ll get a table of projects in your Azure DevOps organization:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Now comes the magic:\"]]],[3,\"ol\",[[[0,[],0,\"Click on Add Column in the ribbon\"]],[[0,[],0,\"Click on “Invoke Custom Function”\"]],[[0,[],0,\"Enter “Repos” as the new column name\"]],[[0,[],0,\"Select “GetGitRepos” (the function we created earlier) from the list of functions\"]],[[0,[],0,\"Make sure the type is set to column so that PowerBI will loop through each row in the table, calling the function\"]],[[0,[],0,\"Change the column to ProjectName – this is the value for the project arg for the function\"]]]],[10,6],[1,\"p\",[[0,[],0,\"Once you click OK, PowerBI will call the function for each row in the table – this is why you don’t want to do this on a table with more than a few hundred rows! Here’s what the result will look like:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Now we want to expand the Record in each row, so click on the expand glyph to the right of the column name. We don’t really care about count, we just want value expanded and we don’t need the prefix:\"]]],[10,8],[1,\"p\",[[0,[],0,\"This expands, but we’ll need to expand “value” once more, since it too is a complex object. Click the expand glyph again and select “Expand to Rows”. You can now filter out nulls – I could only do this by adding a line in the Advanced Editor:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Don’t forget to change the “in” to #“Filter nulls”. You will then need to expand value again:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Now we can finally see the fields for the repo itself – I just selected name, size, defaultBranch and webUrl. Now you can update any types and rename columns as you need. We now have a list of repos! Here’s the final M query:\"]]],[10,11],[1,\"p\",[[0,[],0,\"For adding queue information to builds, I created a function to get build detail for a build number (so that I could extract the queue). For code coverage, I created a function to call the coverage API for a build – again expanding the records that came back. You can see the final queries in the template.\"]]],[1,\"h2\",[[0,[],0,\"Relating Entities\"]]],[1,\"p\",[[0,[],0,\"Now that I have a few entities, PowerBI detects most of the relationships. I added a CalendarDate table so that I could filter all builds/tests on a particular date (the CompletedDate column is a DateTime field, so this is necessary to group on a day). The final ERD looks like this:\"]]],[10,12],[1,\"p\",[[0,[],0,\"I had some trouble relating branch to repo, so I eventually just added a LOOKUP function to lookup the repo name for the branch via the repositoryId. That’s why Repo isn’t related to other entities in the ERD. Similarly, I originally had a Project entity, but found that creating slicers on the Project column in the build worked just fine and kept the ERD simple.\"]]],[1,\"h2\",[[0,[],0,\"Charts\"]]],[1,\"p\",[[0,[],0,\"I created two simple reports in the template – one showing a Build Summary and another showing a test summary. Feel free to start from these and go make some pretty reports!\"]]],[10,13],[1,\"p\",[[0,[],0,\"To open the template, you can get it from this \"],[0,[3],1,\"Github repo\"],[0,[],0,\". There’s also instructions on how to update the auth.\"]]],[1,\"h1\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"The OData feed for Azure DevOps is getting better – mixing in some REST API calls allows you to fill in some gaps. If you’re careful about your filtering and what data you’re querying, you’ll be able to make some compelling reports. Go forth and measure…\"]]],[1,\"p\",[[0,[],0,\"Happy reporting!\"]]]]}","published_at":1568696786000,"status":"published","published_by":1},{"id":"877ccf49-570e-4250-8cc4-8be24114dfc2","title":"Azure Outage – I was a victim too, dear Reader","slug":"azure-outage--i-was-a-victim-too-dear-reader","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://github.com/madskristensen/MiniBlog\"]],[\"a\",[\"href\",\"http://azure.microsoft.com/blog/2014/11/19/update-on-azure-storage-service-interruption/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This morning I went to check on my blog – the very blog you’re busy reading – and I was greeted with a dreaded YSOD (Yellow Screen of Death). What? That can’t be! I haven’t deployed anything since about 10 days ago, so I know it wasn’t my code! What gives?\"]]],[1,\"p\",[[0,[],0,\"It turns out that I had some garbled post files. My blog is built on \"],[0,[0],1,\"MiniBlog\"],[0,[],0,\" which stores all posts as xml files. One of the changes I made to my engine is to increment a view counter on each post so that I can track which posts are being hit. I suppose there is some risk in doing this, since there is a lot of writing to the files. Turns out about 6 files in my posts directory were either empty or partially empty – I suspect that IIS was writing the files when \"],[0,[1],1,\"the outage happened a couple of days ago\"],[0,[],0,\". Anyway, turns out MiniBlog isn’t that resilient when the xml files it’s reading are not well-formed xml! That just goes to show you that even though your code has been stable for months, stuff can still go wrong!\"]]],[1,\"p\",[[0,[],0,\"So I applied a fix (which moves the broken file out the way and sends me an email with the exception information) and it looks like the site is up again. Although at time of writing, the site is dog slow and I can’t seem to WebDeploy – I’ve had to use FTP to fix the posts and update my code, and I can’t see the files using the Azure SDK File Explorer (though I am able to see the logs, fortunately). I suspect there are still Azure infrastructure problems.\"]]],[1,\"p\",[[0,[],0,\"So other than having my counts go wonky, I may have lost a couple of comments. If one of your comments disappeared, Dear Reader, I humbly apologize.\"]]],[1,\"p\",[[0,[],0,\"Also, this is my first post from my Surface 3 Pro! Woot!\"]]],[1,\"p\",[[0,[],0,\"Happy reading-as-long-as-Azure-stays-up!\"]]]]}","published_at":1416609302000,"status":"published","published_by":1},{"id":"12e92756-f244-471c-ada5-81b6f17889bc","title":"Azure Pipeline Parameters","slug":"azure-pipeline-parameters","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"variables: { string: string }\\n\\nparameters: { string: any }\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"# valid syntax\\nkey: $(value)\\nkey: $[variables.value]\\n${{ parameters.key }} : ${{ parameters.value }}\\n\\n# invalid syntax\\n$(key): value\\n$[variables.key]: value\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"# templates/step-deploy-container-service.yml\\nparameters:\\n  serviceName: ''  # product-api\\n  serviceShortName: '' # productapi\\n  environment: dev\\n  imageRepo: ''  # product.api\\n  ...\\n  services: []\\n\\nsteps:\\n- ${{ each s in parameters.services }}:\\n  - ${{ if eq(s.skip, 'false') }}:\\n    - task: KubernetesManifest@0\\n      displayName: Bake ${{ s.serviceName }} manifest\\n      name: bake_${{ s.serviceShortName }}\\n      inputs:\\n        action: bake\\n        renderType: helm2\\n        releaseName: ${{ s.serviceName }}-${{ parameters.environment }}\\n        ...\\n    - task: KubernetesManifest@0\\n      displayName: Deploy ${{ s.serviceName }} to k8s\\n      inputs:\\n        manifests: $(bake_${{ s.serviceShortName }}.manifestsBundle)\\n        imagePullSecrets: $(imagePullSecret)\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"...\\n  - template: templates/step-deploy-container-service.yml\\n    parameters:\\n      acrName: $(acrName)\\n      environment: dev\\n      ingressHost: $(IngressHost)\\n      tag: $(tag)\\n      autoscale: $(autoscale)\\n      services:\\n      - serviceName: 'products-api'\\n        serviceShortName: productsapi\\n        imageRepo: 'product.api'\\n        skip: false\\n      - serviceName: 'coupons-api'\\n        serviceShortName: couponsapi\\n        imageRepo: 'coupon.api'\\n        skip: false\\n      ...\\n      - serviceName: 'rewards-registration-api'\\n        serviceShortName: rewardsregistrationapi\\n        imageRepo: 'rewards.registration.api'\\n        skip: true\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"# templates/steps.yml\\nparameters:\\n  services: []\\n\\nsteps:\\n- ${{ each s in parameters.services }}:\\n  - ${{ if eq(s.skip, 'false') }}:\\n    - script: echo 'Deploying ${{ s.name }}'\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"jobs:\\n- job: deploy\\n  - steps: templates/steps.yml\\n    parameters:\\n      services:\\n      - name: foo\\n        skip: false\\n      - name: bar\\n        skip: true\\n      - name: baz\\n        skip: false\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Deploying foo<br>\\nDeploying baz</font>\"}],[\"code\",{\"code\":\"# templates/steps.yml\\nparameters:\\n  environment: ''\\n  middleSteps: []\\n\\nsteps:\\n- script: echo 'Prestep'\\n- ${{ parameters.middleSteps }}\\n- script: echo 'Post-step'\\n\\n# pipelineA\\njobs:\\n- job: A\\n  - steps: templates/steps.yml\\n    parameters:\\n      middleSteps:\\n      - script: echo 'middle A step 1'\\n      - script: echo 'middle A step 2'\\n\\n# pipelineB\\njobs:\\n- job: B\\n  - steps: templates/steps.yml\\n    parameters:\\n      middleSteps:\\n      - script: echo 'This is job B middle step 1'\\n      - task: ...  # some other task\\n      - task: ...  # some other task\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"# template.yml\\nparameters:\\n- name: usersteps\\n  type: stepList\\n  default: []\\nsteps:\\n- ${{ each step in parameters.usersteps }}:\\n  - ${{ each pair in step }}:\\n    ${{ if ne(pair.key, 'script') }}:\\n      ${{ pair.key }}: ${{ pair.value }}\\n\\n# azure-pipelines.yml\\nextends:\\n  template: template.yml\\n  parameters:\\n    usersteps:\\n    - task: MyTask@1\\n    - script: echo This step will be stripped out and not run!\\n    - task: MyOtherTask@2\\n\",\"language\":\"javascript;\"}]],\"markups\":[[\"a\",[\"href\",\"https://colinsalmcorner.com/post/azure-pipeline-variables\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&tabs=yaml%2Cbatch#understand-variable-syntax\"]],[\"a\",[\"href\",\"http://pipelinehttps://github.com/10thmagnitude/TailwindTraders-Backend/blob/master/Pipeline/azure-pipeline.yaml\"]],[\"a\",[\"href\",\"https://github.com/10thmagnitude/MLOpsDemo/blob/master/templates/job-train-model.yml\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/security/templates?view=azure-devops#use-extends-templates\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In a \"],[0,[0],1,\"previous post\"],[0,[],0,\", I did a deep dive into Azure Pipeline variables. That post turned out to be longer than I anticipated, so I left off the topic of parameters until this post.\"]]],[1,\"h2\",[[0,[],0,\"Type: Any\"]]],[1,\"p\",[[0,[],0,\"If we look at the YML schema for variables and parameters, we’ll see this definition:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Parameters are essentially the same as variables, with the following important differences:\"]]],[3,\"ul\",[[[0,[],0,\"Parameters are dereferenced using “${{}}” notation\"]],[[0,[],0,\"Parameters can be complex objects\"]],[[0,[],0,\"Parameters are expanded at queue time, not at run time\"]],[[0,[],0,\"Parameters can only be used in templates (you cannot pass parameters to a pipeline, only variables)\"]]]],[1,\"p\",[[0,[],0,\"Parameters allow us to do interesting things that we cannot do with variables, like if statements and loops. Before we dive in to some examples, let’s consider \"],[0,[1],1,\"variable dereferencing\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Variable Dereferencing\"]]],[1,\"p\",[[0,[],0,\"The \"],[0,[2],1,\"official documentation\"],[0,[],0,\" specifies three methods of dereferencing variables: macros, template expressions and runtime expressions:\"]]],[3,\"ul\",[[[0,[],0,\"Macros: this is the “$(var)” style of dereferencing\"]],[[0,[],0,\"Template parameters use the syntax “${{ parameter.name }}”\"]],[[0,[],0,\"Runtime expressions, which have the format “$[variables.var]”\"]]]],[1,\"p\",[[0,[],0,\"In practice, the main thing to bear in mind \"],[0,[1],1,\"is when the value is injected\"],[0,[],0,\". “$()” variables are expanded at runtime, while “${{}}” parameters are expanded at \"],[0,[1],1,\"compile\"],[0,[],0,\" time. Knowing this rule can save you some headaches.\"]]],[1,\"p\",[[0,[],0,\"The other notable difference is left vs right side: variables can only expand on the right side, while parameters can expand on left or right side. For example:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here's a real-life example from a \"],[0,[3],1,\"TailWind Traders\"],[0,[],0,\" I created. In this case, the repo contains several microservices that are deployed as Kubernetes services using Helm charts. Even though the code for each microservice is different, the \"],[0,[1],1,\"deployment\"],[0,[],0,\" for each is identical, except for the path to the Helm chart and the image repository.\"]]],[1,\"p\",[[0,[],0,\"Thinking about this scenario, I wanted a template for deployment steps that I could parameterize. Rather than copy the entire template, I used a “for” expression to iterate over a map of complex properties. For each service deployment, I wanted:\"]]],[3,\"ul\",[[[0,[],0,\"serviceName: The path to the service Helm chart\"]],[[0,[],0,\"serviceShortName: Required because the deployment requires two steps: “bake” the manifest, and then “deploy” the baked manifest. The “deploy” task references the output of the “bake” step, so I needed a name that wouldn't collide as I expanded it multiple times in the “for” loop\"]]]],[1,\"p\",[[0,[],0,\"Here's a snippet of the template steps:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Here's a snippet of the pipeline that references the template:\"]]],[10,3],[1,\"p\",[[0,[],0,\"In this case, “services” could not have been a variable since variables can only have “string” values. Hence I had to make it a parameter.\"]]],[1,\"h2\",[[0,[],0,\"Parameters and Expressions\"]]],[1,\"p\",[[0,[],0,\"There are a number of expressions that allow us to create more complex scenarios, especially in conjunction with parameters. The example above uses both the “each” and the “if” expressions, along with the boolean function “eq”. Expressions can be used to loop over steps or ignore steps (as an equivalent of setting the “condition” property to “false”). Let's look at an example in a bit more detail. Imagine you have this template:\"],[1,[],0,0]]],[10,4],[1,\"p\",[[0,[],0,\"Then if you specify the following pipeline:\"]]],[10,5],[1,\"p\",[[1,[],0,1],[0,[],0,\"you should get the following output from the steps:\"]]],[10,6],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"Parameters can also be used to inject steps. Imagine you have a set of steps that you want to repeat with different parameters - except that in some cases, a slightly different middle step needs to be executed. You can create a template that has a parameter called “middleSteps” where you can pass in the middle step(s) as a parameter!\"]]],[10,7],[1,\"p\",[[0,[],0,\"For a real world example of this, see this \"],[0,[4],1,\"template file\"],[0,[],0,\". This is a demo where I have two scenarios for machine learning: a manual training process and an AutoML training process. The pre-training and post-training steps are the same, but the training steps are different: the template reflects this scenario by allowing me to pass in different “TrainingSteps” for each scenario.\"]]],[1,\"h2\",[[0,[],0,\"Extends Templates\"]]],[1,\"p\",[[0,[],0,\"Passing steps as parameters allows us to create what Azure DevOps calls “\"],[0,[5],1,\"extends templates\"],[0,[],0,\"”. These provide rails around what portions of a pipeline can be customized, allowing template authors to inject (or remove) steps. The following example from the documentation demonstrates this:\"]]],[10,8],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Parameters allow us to pass and manipulate complex objects, which we are unable to do using variables. They can be combined with expressions to create complex control flow. Finally, parameters allow us to control how a template is customized using extends templates.\"]]],[1,\"p\",[[0,[],0,\"Happy parameterizing!\"]]]]}","published_at":1582788944000,"status":"published","published_by":1},{"id":"7849d529-0d25-465f-a56f-e1c681830d8b","title":"Azure Pipeline Variables","slug":"azure-pipeline-variables","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"variables:\\n  name: colin\\n\\nsteps:\\n- script: echo \\\"Hello, $(name)!\\\"\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"trigger:\\n- master\\n\\npool:\\n  vmImage: ubuntu-latest\\n\\nvariables:\\n  imageName: myregistry/api-image\\n\\nsteps:\\n- task: Docker@2\\n  displayName: Build an image\\n  inputs:\\n    repository: $(imageName)\\n    command: build\\n    Dockerfile: api/Dockerfile\\n\\n- task: Docker@2\\n  displayName: Push image\\n  inputs:\\n    containerRegistry: $(ACRRegistry)\\n    repository: $(imageName)\\n    command: push\\n    tags: $(Build.BuildNumber)\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/704856b2-fdc7-4dc5-b318-6ef8a5af42a6.png\\\" target=\\\"_blank\\\"><img width=\\\"312\\\" height=\\\"200\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a42362d3-34a7-4a89-a075-00632243cbcb.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"name: 1.0$(Rev:.r)\\n\\ntrigger:\\n- master\\n\\npool:\\n  vmImage: ubuntu-latest\\n  \\njobs:\\n- job: echo\\n  steps:\\n  - script: echo \\\"BuildConfiguration is $(buildConfiguration)\\\"\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e5016918-35d6-4037-a237-2669c32a66c4.png\\\" target=\\\"_blank\\\"><img width=\\\"327\\\" height=\\\"210\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/03d237c0-0360-47e9-927e-040dc5def391.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b47f930b-20dd-484a-ba87-3cf9078655a6.png\\\" target=\\\"_blank\\\"><img width=\\\"323\\\" height=\\\"208\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ff8674b4-bfcf-49d0-83fd-5ae5a906ded8.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"- script: |\\n    curUser=$(whoami)\\n    echo \\\"##vso[task.setvariable variable=currentUser;]$curUser\\\"\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"- script: echo $(Build.BuildNumber)\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"- script: echo $buildNum\\n  env:\\n    buildNum: $(Build.BuildNumber)\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"echo \\\"##vso[task.setvariable variable=currentUser;issecret=true]$curUser\\\"\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"- powershell: |\\n    Write-Host \\\"##vso[task.setvariable variable=currentUser;]$env:UserName\\\"\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"- script: |\\n    buildNum=$(...)  # calculate the build number somehow\\n    echo \\\"##vso[build.updatebuildnumber]$buildNum\\\"\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a4887fed-9394-4f2f-8742-1ffd05c8bc26.png\\\" target=\\\"_blank\\\"><img width=\\\"314\\\" height=\\\"190\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/623a4b38-40b4-41ba-b9da-3352a1ab04a1.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/faa615fb-4f15-4eb2-bc61-d5f22f370e59.png\\\" target=\\\"_blank\\\"><img width=\\\"313\\\" height=\\\"177\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d4c5c376-a471-4aab-8f03-7452eaa9a112.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7e8565f1-a6a3-4836-8348-80a83c4d46fe.png\\\" target=\\\"_blank\\\"><img width=\\\"249\\\" height=\\\"290\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/766709c4-081e-4bed-8251-22f9e5fd6613.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"trigger:\\n- master\\n\\npool:\\n  vmImage: ubuntu-latest\\n  \\njobs:\\n- job: DEV\\n  variables:\\n  - group: WebApp-DEV\\n  - name: environment\\n    value: DEV\\n  steps:\\n  - script: echo \\\"ConStr is $(ConStr) in enviroment $(environment)\\\"\\n\\n- job: QA\\n  variables:\\n  - group: WebApp-QA\\n  - name: environment\\n    value: QA\\n  steps:\\n  - script: echo \\\"ConStr is $(ConStr) in enviroment $(environment)\\\"\\n\\n- job: Prod\\n  variables:\\n  - group: WebApp-Prod\\n  - name: environment\\n    value: Prod\\n  steps:\\n  - script: echo \\\"ConStr is $(ConStr) in enviroment $(environment)\\\"\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"# templates/variables.yml\\nvariables:\\n- name: buildConfiguration\\n  value: debug\\n- name: buildArchitecture\\n  value: x64\\n\\n# pipelineA.yml\\nvariables:\\n- template: templates/variables.yml\\nsteps:\\n- script: build x ${{ variables.buildArchitecture }} ${{ variables.buildConfiguration }}\\n\\n# pipelineB.yml\\nvariables:\\n- template: templates/variables.yml\\nsteps:\\n- script: echo 'Arch: ${{ variables.buildArchitecture }}, config ${{ variables.buildConfiguration }}'\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"jobs:\\n- job: A\\n  variables:\\n    a: 10\\n  steps:\\n    - bash: |\\n        echo $(a)    # This will be 10\\n        echo '##vso[task.setvariable variable=a]20'\\n        echo $(a)    # This will also be 10, since the expansion of $(a) happens before the step\\n    - bash: echo $(a)        # This will be 20, since the variables are expanded just before the step\\n\",\"language\":\"javascript;\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml\"]],[\"strong\"],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/scripts/logging-commands?view=azure-devops&tabs=bash\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/scripts/logging-commands?view=azure-devops&tabs=bash#build-commands\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&tabs=yaml%2Cbatch#expansion-of-variables\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I am a big fan of Azure Pipelines. Yes it’s YAML, but once you get over that it’s a fantastic way to represent pipelines as code. It would be tough to achieve any sort of sophistication in your pipelines without variables. There are several types of variables, though this classification is partly mine and pipelines don’t distinguish between these types. However, I’ve found it useful to categorize pipeline variables to help teams understand some of the nuances that occur when dealing with them.\"]]],[1,\"p\",[[0,[],0,\"Every variable is really a key:value pair. The key is the name of the variable, and it has a string value. To dereference a variable, simply wrap the key in `$()`. Let’s consider this simple example:\"]]],[10,0],[1,\"p\",[[0,[],0,\"This will write “Hello, colin!” to the log.\"]]],[1,\"h2\",[[0,[],0,\"Inline Variables\"]]],[1,\"p\",[[0,[],0,\"Inline variables are variables that are hard coded into the pipeline YML file itself. Use these for specifying values that are not sensitive and that are unlikely to change. A good example is an image name: let’s imagine you have a pipeline that is building a Docker container and pushing that container to a registry. You are probably going to end up referencing the image name in several steps (such as tagging the image and then pushing the image). Instead of using a value in-line in each step, you can create a variable and use it multiple times. This keeps to the DRY (Do not Repeat Yourself) principle and ensures that you don’t inadvertently misspell the image name in one of the steps. In the following example, we create a variable called “imageName” so that we only have to maintain the value once rather than in multiple places:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Note that you obviously you cannot create \\\"secret\\\" inline variables. If you need a variable to be secret, you’ll have to use pipeline variables, variable groups or dynamic variables.\"]]],[1,\"h2\",[[0,[],0,\"Predefined Variables\"]]],[1,\"p\",[[0,[],0,\"There are several predefined variables that you can reference in your pipeline. Examples are:\"]]],[3,\"ul\",[[[0,[],0,\"Source branch: “Build.SourceBranch”\"]],[[0,[],0,\"Build reason: “Build.Reason”\"]],[[0,[],0,\"Artifact staging directory: “Build.ArtifactStagingDirectory”\"]]]],[1,\"p\",[[0,[],0,\"You can find a full list of predefined variables \"],[0,[0],1,\"here\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Pipeline Variables\"]]],[1,\"p\",[[0,[],0,\"Pipeline variables are specified in Azure DevOps in the pipeline UI when you create a pipeline from the YML file. These allow you to abstract the variables out of the file. You can specify defaults and/or mark the variables as \\\"secrets\\\" (we’ll cover secrets a bit later). This is useful if you plan on triggering the pipeline manually and want to set the value of a variable at queue time.\"]]],[1,\"p\",[[0,[],0,\"One thing to note: if you specify a variable in the YML variables section, you cannot create a pipeline variable with the same name. If you plan on using pipeline variables, you must \"],[0,[1],1,\"not\"],[0,[],0,\" specify them in the \\\"variables\\\" section!\"]]],[1,\"p\",[[0,[],0,\"When should you use pipeline variables? These are useful if you plan on triggering the pipeline manually and want to set the value of a variable at queue time. Imagine you sometimes want to build in “DEBUG” and other times in “RELEASE”: you could specify “buildConfiguration” as a pipeline variable when you create the pipeline, giving it a default value of “debug”:\"]]],[10,2],[1,\"p\",[[0,[],0,\"If you specify “Let users override this value when running this pipeline” then users can change the value of the pipeline when they manually queue it. Specifying “Keep this value secret” will make this value a secret (Azure DevOps will mask the value).\"]]],[1,\"p\",[[0,[],0,\"Let's look at a simple pipeline that consumes the pipeline variable:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Running the pipeline without editing the variable produces the following log:\"]]],[10,4],[1,\"p\",[[0,[],0,\"If the pipeline is not manually queued, but triggered, any pipeline variables default to the value that you specify in the parameter when you create it.\"]]],[1,\"p\",[[0,[],0,\"Of course if we update the value when we queue the pipeline to “release”, of course the log reflects the new value:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Referencing a pipeline variable is exactly the same as referencing an inline variable – once again, the distinction is purely for discussion.\"]]],[1,\"h2\",[[0,[],0,\"Secrets\"]]],[1,\"p\",[[0,[],0,\"At some point you’re going to want a variable that isn’t visible in the build log: a password, an API Key etc. As I mentioned earlier, inline variables are never secret. You must mark a pipeline variable as secret in order to make it a secret, or you can create a dynamic variable that is secret.\"]]],[1,\"p\",[[0,[],0,\"\\\"Secret\\\" in this case just means that the value is masked in the logs. It is still possible to expose the value of a secret if you really want to. A malicious pipeline author could “echo” a secret to a file and then open the file to get the value of the secret.\"]]],[1,\"p\",[[0,[],0,\"All is not lost though: you can put controls in place to ensure that nefarious developers cannot simply run updated pipelines – you should be using Pull Requests and Branch Policies to review changes to the pipeline itself (an advantage to having pipelines as code). The point is, you still need to be careful with your secrets!\"]]],[1,\"h2\",[[0,[],0,\"Dynamic Variables and Logging Commands\"]]],[1,\"p\",[[0,[],0,\"Dynamic variables are variables that are created and/or calculated at run time. A good example is using the “az cli” to retrieve the connection string to a storage account so that you can inject the value into a web.config. Another example is dynamically calculating a build number in a script.\"]]],[1,\"p\",[[0,[],0,\"To create or set a variable dynamically, you can use \"],[0,[2],1,\"logging commands\"],[0,[],0,\". Imagine you need to get the username of the current user for use in subsequent steps. Here’s how you can create a variable called “currentUser” with the value:\"]]],[10,6],[1,\"p\",[[0,[],0,\"When writing bash or PowerShell commands, don’t confuse “$(var)” with “$var”. “$(var)” is interpolated by Azure DevOps when the step is executed, while “$var” is a bash or PowerShell variable. I often use “env” to create environment variables rather than dereferencing variables inline. For example, I could write:\"]]],[10,7],[1,\"p\",[[0,[],0,\"but I can also use environment variables:\"]]],[10,8],[1,\"p\",[[0,[],0,\"This may come down to personal preference, but I’ve avoided confusion by consistently using env for my scripts!\"]]],[1,\"p\",[[0,[],0,\"To make the variable a secret, simple add “issecret=true” into the logging command:\"]]],[10,9],[1,\"p\",[[0,[],0,\"You could do the same thing using PowerShell:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Note that there are two flavors of PowerShell: “powershell” is for Windows and “pwsh” is for PowerShell Core which is cross-platform (so it can run on Linux and Mac!).\"]]],[1,\"p\",[[0,[],0,\"One special case of a dynamic variable is a calculated build number. For that, calculate the build number however you need to and then use the “build.updatebuildnumber” logging command:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Other logging commands are documented \"],[0,[3],1,\"here\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Variable Groups\"]]],[1,\"p\",[[0,[],0,\"Creating inline variables is fine for values that are not sensitive and that are not likely to change very often. Pipeline variables are useful for pipelines that you want to trigger manually. But there is another option that is particularly useful for multi-stage pipelines (we'll cover these in more detail later).\"]]],[1,\"p\",[[0,[],0,\"Imagine you have a web application that connects to a database that you want to build and then push to DEV, QA and Prod environments. Let's consider just one config setting - the database connection string. Where should you store the value for the connection string? Perhaps you could store the DEV connection string in source control, but what about QA and Prod? You probably don't want those passwords stored in source control.\"]]],[1,\"p\",[[0,[],0,\"You could create them as pipeline variables - but then you'd have to prefix the value with an environment or something to distinguish the QA value from the Prod value. What happens if you add in a STAGING environment? What if you have other settings like API Keys? This can quickly become a mess.\"]]],[1,\"p\",[[0,[],0,\"This is what Variable Groups are designed for. You can find variable groups in the “Library\\\" hub in Azure DevOps:\"]]],[10,12],[1,\"p\",[[0,[],0,\"The image above shows two variable groups: one for DEV and one for QA. Let's create a new one for Prod, specifying the same variable name (“ConStr”) but this time entering in the value for Prod:\"]]],[10,13],[1,\"p\",[[0,[],0,\"Security is beyond the scope of this post- but you can specify who has permission to view/edit variable groups, as well as which pipelines are allowed to consume them. You can of course mark any value in the variable group as secret by clicking the padlock icon next to the value.\"]]],[1,\"p\",[[0,[],0,\"The trick to making variable groups work for environment values is to keep the names the same in each variable group. That way the only setting you need to update between environments is the variable group name. I suggest getting the pipeline to work completely for one environment, and then “Clone” the variable group - that way you're assured you're using the same variable names.\"]]],[1,\"h3\",[[0,[],0,\"KeyVault Integration\"]]],[1,\"p\",[[0,[],0,\"You can also integrate variable groups to Azure KeyVaults. When you create the variable group, instead of specifying values in the variable group itself, you connect to a KeyVault and specify which keys from the vault should be synchronized when the variable group is instantiated in a pipeline run:\"]]],[10,14],[1,\"h3\",[[0,[],0,\"Consuming Variable Groups\"]]],[1,\"p\",[[0,[],0,\"Now that we have some variable groups, we can consume them in a pipeline. Let's consider this pipeline:\"]]],[10,15],[1,\"p\",[[0,[],0,\"When this pipeline runs, we’ll see the DEV, QA and Prod values from the variable groups in the corresponding jobs.\"]]],[1,\"p\",[[0,[],0,\"Notice that the format for inline variables alters slightly when you have variable groups: you have to use the “- name/value” format.\"]]],[1,\"h2\",[[0,[],0,\"Variable Templates\"]]],[1,\"p\",[[0,[],0,\"There is another type of template that can be useful - if you have a set of inline variables that you want to share across multiple pipelines, you can create a template. The template can then be referenced in multiple pipelines:\"]]],[10,16],[1,\"h2\",[[0,[],0,\"Precedence and Expansion\"]]],[1,\"p\",[[0,[],0,\"Variables can be defined at various scopes in a pipeline. When you define a variable with the same name at more than one scope, you need to be aware of the precedence. You can read the documentation on precedence \"],[0,[4],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"You should also be aware of \"],[0,[5],1,\"when\"],[0,[],0,\" variables are expanded. They are expanded at the beginning of the run, as well as before each step. This example shows how this works:\"]]],[10,17],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Azure Pipelines variables are powerful – and with great power comes great responsibility! Hopefully you understand variables and some of their gotchas a little better now. There’s another topic that needs to be covered to complete the discussion on variables – \"],[0,[5],1,\"parameters\"],[0,[],0,\". I’ll cover parameters in a follow up post.\"]]],[1,\"p\",[[0,[],0,\"For now – happy building!\"]]]]}","published_at":1581478528000,"status":"published","published_by":1},{"id":"fd91132f-63dd-4673-8776-0d6938e9a758","title":"Azure Web Apps, Kudu Console and TcpPing","slug":"azure-web-apps-kudu-console-and-tcpping","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/282b101d-ea87-4453-a22b-8e95a23c236d.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9d0dc450-7511-4540-901c-a39b7bf74d4b.png\\\" width=\\\"338\\\" height=\\\"200\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/01756426-fddc-496c-92c3-1e9924ede690.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bd5c54ff-2e65-4319-94a0-99d7381a4a6f.png\\\" width=\\\"312\\\" height=\\\"223\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"3\\\" face=\\\"Courier New\\\">tcpping &lt;enter&gt;</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9c7294f7-0294-40f4-832f-bc91f69dd34f.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/cf213b2f-ce32-45e9-ad58-9784cc048915.png\\\" width=\\\"318\\\" height=\\\"119\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"3\\\" face=\\\"Courier New\\\">tcpping 192.168.0.1:1443 &lt;enter&gt;</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/00312b65-b46a-49ce-9414-b9411175fa4e.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e613500b-5d37-4a2c-90d5-6884dd4ff870.png\\\" width=\\\"345\\\" height=\\\"290\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/web-sites-integrate-with-vnet/\"]],[\"a\",[\"href\",\"http://<yoursite>.scm.azurewebsites.net\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/benjaminperkins/archive/2014/03/24/using-kudu-with-windows-azure-web-sites.aspx\"]],[\"a\",[\"href\",\"https://channel9.msdn.com/Shows/Azure-Friday/What-is-Kudu-Azure-Web-Sites-Deployment-with-David-Ebbo\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I was working with a customer recently that put a website into Azure Web Apps. This site needed to connect to their backend databases (which they couldn’t move to Azure because legacy systems still needed to connect to it). We created an Azure VNet and configured site-to-site connectivity that created a secure connection between the Azure VNet and their on-premises network.\"]]],[1,\"p\",[[0,[],0,\"We then had to configure point-to-site connections for the VNet \"],[0,[0],1,\"so that we could put the Azure Web App onto the VNet\"],[0,[],0,\". This would (in theory) allow the website to access their on-premises resources such as the database. We also had to upgrade the site to Standard pricing in order to do this.\"]]],[1,\"p\",[[0,[],0,\"We had to reconfigure the site-to-site gateway to allow dynamic routing in order to do this, which meant deleting and recreating the gateway. A bit of a pain, but not too bad. We then configured static routing from the on-premises network to the point-to-site addresses on the VNet.\"]]],[1,\"h2\",[[0,[],0,\"Ping from Azure Web App?\"]]],[1,\"p\",[[0,[],0,\"Once we had that all configured, we wanted to test connectivity. If we had deployed a VM, it would have been simple – just open a cmd prompt and ping away. However, we didn’t have a server, since we were deploying an Azure Web App. So initially we deployed a dummy Azure Web App onto the VNet to test the connection. This became a little bit of a pain. However, I remembered reading about Kudu and decided to see if that would be easier.\"]]],[1,\"h2\",[[0,[],0,\"Kudu to the Rescue\"]]],[1,\"p\",[[0,[],0,\"If you browse to \"],[0,[1],1,\"http://<yoursite>.scm.azurewebsites.net\"],[0,[],0,\" (where <yoursite> is the name of your Azure Web App) then you’ll see the Kudu site.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Once you’ve opened the Kudu site, you can do all sorts of interesting things (see \"],[0,[2],1,\"this blog post\"],[0,[],0,\" and this \"],[0,[3],1,\"Scott Hanselman and David Ebbo video\"],[0,[],0,\"). If you open the Debug console (you can go CMD or PowerShell) then you get to play! I opened the CMD console and typed “help” – to my surprise I got a list of commands I could run:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Unfortunately I didn’t see anything that would help me with testing connectivity. However, I remembered that I had read \"],[0,[4],1,\"somewhere\"],[0,[],0,\" about the command “tpcping”. So I tried it:\"]]],[10,2],[10,3],[1,\"p\",[[0,[],0,\"Looks promising! Even better than the “ping” command, you can also test for a specific port, not just the ip address. So I want to test if my site can reach my database server on port 1443, no problem:\"]]],[10,4],[10,5],[1,\"p\",[[0,[],0,\"Hmm, seems that address isn’t working.\"]]],[1,\"p\",[[0,[],0,\"After troubleshooting for a while, we managed to sort the problem and tcpping gave us a nice “Success” message, so we knew we were good to go. Kudu saved us a lot of time!\"]]],[1,\"p\",[[0,[],0,\"Happy troubleshooting!\"]]]]}","published_at":1438918373000,"status":"published","published_by":1},{"id":"03668746-5f30-4079-93cf-00ba2ecc9847","title":"BITE–Branch Info Team Explorer Extension","slug":"bitebranch-info-team-explorer-extension","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-gvgJxO3NjxI/UdJ1VaPQlvI/AAAAAAAAA8g/Z1aLvetvQhU/s1600-h/image2.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-cFPhFgEFITg/UdJ1WvjRyaI/AAAAAAAAA8o/cDryt8aM4vI/image_thumb.png?imgmax=800\\\" width=\\\"203\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-UeRKHCH7Lk4/UdJ1XMB_rUI/AAAAAAAAA8w/Si3T8G_ARZU/s1600-h/image9.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-CpT7_b_rqPc/UdJ1YKi0eRI/AAAAAAAAA84/iCklL1PlQhs/image_thumb3.png?imgmax=800\\\" width=\\\"283\\\" height=\\\"206\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/85185516-dfe6-44e6-aa64-892cbff0e98a\"]],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/1d61464c-65af-4d25-af15-3b6b6919c56e\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Update 2013-08-30: The extension is now available for \"],[0,[0],1,\"VS 2013\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Branching is something that you should definitely be doing if you’re a modern developer. It doesn’t matter if you have branch-per-release or dev-main-live kind of branching – you need to be able to separate development streams.\"]]],[1,\"p\",[[0,[],0,\"So let’s pick on the classic dev-main-live scenario. You create a solution in Visual Studio (in a MAIN folder), check it into Source Control in TFS and then create branches to DEV and LIVE. Now you have three versions of the same solution – one on each branch. I always recommend opening from Source Control so that you know which branch you’re on. However, if you’ve opened the solution and then been working for a while, you may want to double-check which branch you’re working from. Hmmm, you’re stuck – there’s no way to do this other than checking the folder path for the solution.\"]]],[1,\"h2\",[[0,[],0,\"Enter BITE\"]]],[1,\"p\",[[0,[],0,\"Wouldn’t it be cool if you could instantly see which branch your solution is on? And how about selecting one of the other branches and clicking a “Switch” button to switch to the same solution on another branch? Now you can – using BITE – the Branch Info Team Explorer Extension! (Yes, I know it’s cheesy, but I couldn’t help it once I’d seen the acronym).\"]]],[1,\"p\",[[0,[],0,\"Once you’ve installed the extension from the \"],[0,[1],1,\"VS Gallery\"],[0,[],0,\", you’ll see a new Link under the Pending Changes section:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Click on “Branch Info” to see the extension in action.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here I’ve opened a solution on the MAIN branch (see the Current Branch label). I can see both the local and server paths for the solution. Also, there’s a drop-down labelled “Other Branches”. If I select one of the other branches, I can click the “Switch” button and the corresponding solution opens.\"]]],[1,\"p\",[[0,[],0,\"Let me know if you have any issues using this extension. (In case you missed the link, get the extension from the \"],[0,[1],1,\"VS Gallery\"],[0,[],0,\").\"]]],[1,\"p\",[[0,[],0,\"Happy branching!\"]]]]}","published_at":1372779480000,"status":"published","published_by":1},{"id":"3f78dc36-09f6-4f09-982e-03d07180b196","title":"Branch Info Team Explorer Extension (BITE) Now Available for VS 2013","slug":"branch-info-team-explorer-extension-(bite)-now-available-for-vs-2013","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-_cFZK458Psc/UiDjZPBkdgI/AAAAAAAABDU/_By03NAdx18/s1600-h/image%25255B2%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-hkEo-H5clLM/UiDjZwDMIWI/AAAAAAAABDc/7NNclLymTdA/image_thumb.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"196\\\"></a>\"}]],\"markups\":[[\"strong\"],[\"u\"],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/33a5274b-271b-45dd-8dc5-734d927a19dd\"]],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/1d61464c-65af-4d25-af15-3b6b6919c56e\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/07/bitebranch-info-team-explorer-extension.html\"]],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/85185516-dfe6-44e6-aa64-892cbff0e98a\"]]],\"sections\":[[1,\"p\",[[0,[0,1],2,\"Update 2013-09-12:\"],[0,[],0,\" I’ve updated the \"],[0,[2],1,\"extension\"],[0,[],0,\" to work with VS 2013 RC (since there were some breaking changes from Preview).\"]]],[1,\"p\",[[0,[],0,\"I wrote a Team Explorer Extension (\"],[0,[3],1,\"BITE\"],[0,[],0,\") \"],[0,[4],1,\"a few months ago\"],[0,[],0,\" to show you which branch your solution is on and how to easily change to the same solution on another branch.\"]]],[1,\"p\",[[0,[],0,\"Today I opened up and converted the extension for \"],[0,[5],1,\"VS 2013\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Since the architecture of the Home page in Team Explorer has changed a little, it wasn’t simply open and recompile for VS 2013. The BITE page actually stayed the same – but the classes that allow me to hook into the Team Explorer had to change quite a lot. And of course there’s scant documentation – even for extending 2012, never mind the dearth of information about extending Team Explorer 2013. Anyway, nothing that Reflector couldn’t help me with…\"]]],[1,\"p\",[[0,[],0,\"Here’s what the extension looks like in the VS 2013 Team Explorer Home page:\"]]],[10,0],[1,\"p\",[[0,[],0,\"While I was at it, I cleaned up the UI a little especially when you don’t have a solution open or the solution you have open is not branched. Other than that, it works exactly like before – only for TFVC, of course!\"]]],[1,\"p\",[[0,[],0,\"Happy branching!\"]]]]}","published_at":1377919440000,"status":"published","published_by":1},{"id":"2baae65d-d243-4a40-b6ae-aa9a7b12e2ef","title":"Branch Is Not Equal to Environment: CODE-PROD Branching Strategy","slug":"branch-is-not-equal-to-environment","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d69693b1-37e9-4174-8398-156cf40bd367.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c1c98eb0-32c0-4f38-ba58-0c1f0c8aade8.png\\\" width=\\\"380\\\" height=\\\"164\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/461eec0f-dc9e-4e5b-afc3-f5040958df71.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/dadadd62-0806-4d08-ac1b-f7411074874c.png\\\" width=\\\"243\\\" height=\\\"192\\\"></a>\"}],[\"code\",{\"code\":\"public class Forecast\\n{\\n    public int ID { get; set; }\\n\\n    public DateTime Date { get; set; }\\n    public DayOfWeek Day \\n    {\\n        get { return Date.DayOfWeek; } \\n    }\\n\\n    public int Min { get; set; }\\n\\n    public int Max { get; set; }\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0084ce46-7e76-48bf-9028-35997b93628b.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/27816db5-0d10-4914-8a49-cdd85f993639.png\\\" width=\\\"644\\\" height=\\\"291\\\"></a>\"}],[\"code\",{\"code\":\"public class Forecast\\n{\\n    public int ID { get; set; }\\n\\n    public DateTime Date { get; set; }\\n    public DayOfWeek Day \\n    {\\n        get { return Date.DayOfWeek; } \\n    }\\n\\n    public int Min { get; set; }\\n\\n    public int Max { get; set; }\\n\\n    public int CODEProperty { get; set; }\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a10581e9-c553-4ce8-8e88-0fc7bc5a0a1b.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/863f6109-a563-4bf5-9b16-b7dd5075e30e.png\\\" width=\\\"644\\\" height=\\\"298\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6242b8a1-2b86-4389-9d3f-b8c29459ba8a.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e7589b8f-bcb9-422c-9b9e-d730412407ae.png\\\" width=\\\"571\\\" height=\\\"329\\\"></a>\"}],[\"code\",{\"code\":\"public class Forecast\\n{\\n    public int ID { get; set; }\\n\\n    public DateTime Date { get; set; }\\n    public DayOfWeek Day \\n    {\\n        get { return Date.DayOfWeek; } \\n    }\\n\\n    public int Min { get; set; }\\n\\n    public int Max { get; set; }\\n\\n    public int HotfixProperty { get; set; }\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b247201f-a7d1-4e6b-8f99-3e3b312c86be.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ef8dd46d-ca85-4627-9f52-bf30daa455e4.png\\\" width=\\\"644\\\" height=\\\"164\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b91d162a-7920-4289-96f6-2d77f9f76596.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bee670af-da10-4178-8c78-19073e35ead5.png\\\" width=\\\"435\\\" height=\\\"484\\\"></a>\"}],[\"code\",{\"code\":\"public class Forecast\\n{\\n    public int ID { get; set; }\\n\\n    public DateTime Date { get; set; }\\n    public DayOfWeek Day \\n    {\\n        get { return Date.DayOfWeek; } \\n    }\\n\\n    public int Min { get; set; }\\n\\n    public int Max { get; set; }\\n\\n    public int CODEProperty { get; set; }\\n\\n    public int HotfixProperty { get; set; }\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/417517e7-1f8f-42fa-ba21-ae63060ec5e7.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5a65e063-be49-4938-81c8-5ec5b3f9b6df.png\\\" width=\\\"545\\\" height=\\\"450\\\"></a>\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/matching-binary-version-to-build-number-version-in-tfs-2013-builds\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/webdeploy-and-release-management--the-proper-way\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-absolutely-need-to-unit-test\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Over the last couple of months I’ve done several implementations and upgrades of TFS 2013. Most organizations I work with are not developing boxed software – they’re developing websites or apps for business. The major difference is that boxed software often has more than one version of a product “in production” – some customers will be on version 1.0 while others will be on version 2.0 and so on. In this model, branches for each major version, with hot-fix branches where necessary – are a good way to keep these code bases separate while still being able to merge bug fixes across versions. However, I generally find that this is overkill for a “product” that only ever has one version in production at any one time – like internal applications or websites.\"]]],[1,\"p\",[[0,[],0,\"In this case, a well-established branching model is Dev-Main-Live.\"]]],[1,\"h2\",[[0,[],0,\"Dev-Main-Live\"]]],[10,0],[1,\"p\",[[0,[],0,\"Dev-Main-Live (or sometimes Dev-Integration-Prod or other variants) is a fairly common branching model – new development is performed on the Dev branch (with multiple developers coding simultaneously). When changes are to be tested, they are merged to Main. There code is tested in a test or UAT environment, and when testing is complete the changes are merged to Live before being deployed to production. This means that if there are production issues (what? we have bugs?!?) those can be fixed on the Live branch – thus they can be tested and deployed independently from the Dev code which may not be production ready.\"]]],[1,\"p\",[[0,[],0,\"There are some issues with this approach:\"]]],[3,\"ol\",[[[0,[],0,\"You shouldn’t be taking so long to test that you need a separate Main branch. I only advise this for extensive test cycles – but you should be aiming to shorten your test cycles anyway. This makes the Main branch fairly obsolete – I’ve seen teams who always “merge through” Main to get changes from Dev to Live – so I’ve started advising getting rid of the Main branch altogether.\"]],[[0,[],0,\"If you build code from Main, deploy it to Test and sign-off, you have to merge to Live before doing a build from the Live branch. This means that what you’re deploying isn’t what you tested (since you tested pre-merge). I’ve seen some teams deploy from the Main branch build, wait for several days, and then merge to the Live branch. Also a big no-no!\"]],[[0,[],0,\"Usually bug fixes that are checked in on the Live branch don’t make it back to the Dev branch since you have to merge through Main – so the merge of new dev and bug fixes on the Live branch get done when Dev gets merged onto Live (through Main). This is too late in the cycle and can introduce merge bugs or rework.\"]]]],[1,\"p\",[[0,[],0,\"This model \"],[0,[0],1,\"seems\"],[0,[],0,\" to work nicely since the branches “represent” the environments – what I have in Dev is in my dev environment, what’s on Main is in my Test environment and what’s in Live is in production, right? This “branch equals environment” mindset is actually hard to manage, so I’m starting to recommend a new approach.\"]]],[1,\"h2\",[[0,[],0,\"The Solution: Code-Prod with Builds\"]]],[10,1],[1,\"p\",[[0,[],0,\"So how should you manage code separation as well as know what code is in which environment at any time? The answer is to simplify the branching model and make use of builds.\"]]],[1,\"p\",[[0,[],0,\"In this scenario new development is done on the CODE branch (the name is to consciously separate the idea of the branch from the environment). When you’re ready to go to production, merge into PROD and do a build. The TFS build will (by default) label the code that is used to build the binaries. You’ll be able to tie the binary version to the build label if you use my \"],[0,[1],1,\"versioning script you can always match binaries to builds\"],[0,[],0,\". So you’ll be able to recreate a build, even if you lose the binaries somehow.\"]]],[1,\"p\",[[0,[],0,\"So now you have built “the bits” – notice how there is no mention of environment yet. You should be thinking of build and deploy as separate activities. Why? Because then you’ll be able to build a single package that can be deployed (and tested) in a number of environments. Of course you’re going to have to somehow manage configuration files for your different environments – for web projects you can refer to \"],[0,[2],1,\"my post\"],[0,[],0,\" about how to parameterize the web.config so that you can deploy to any environment (the post is specific to Release Management, but the principles are the same for other deployment mechanisms and for any type of application that needs different configurations for different environments).\"]]],[1,\"h3\",[[0,[],0,\"Deployment – To Lab or To Release?\"]]],[1,\"p\",[[0,[],0,\"Let’s start off considering the “happy path” – you’ve done some coding in CODE, merged to PROD and produced a “production build”. It needs to be tested (of course you’ve already \"],[0,[3],1,\"unit tested\"],[0,[],0,\" as part of your build). Now you have two choices – Lab Management or Release Management. I like using a combination of Lab and Release, since each has a some good benefits. You can release to test using Lab Management (including automated deploy and test) so that your testers have an environment to test against – Lab Management allows rich data diagnostic collection during both automated and manual testing. You then use Release Management to get the bits into the release pipeline for deployment to UAT and Production environments, including automated deployment workflows and sign-offs. This way you only get builds into the release pipeline that have passed several quality gates (unit testing, automated UI testing and even manual testing) before getting into UAT. Irrespective of what approach you take, make sure you can take one build output and deploy it to multiple environments.\"]]],[1,\"h2\",[[0,[],0,\"But What About Bugs in Production?\"]]],[1,\"p\",[[0,[],0,\"If you get bugs in production \"],[0,[4],1,\"before\"],[0,[],0,\" you do the merge, the solution is simple – fix the bug on the PROD branch, then build, test and release back to production. No messy untested dev CODE anywhere.\"]]],[1,\"p\",[[0,[],0,\"But what do you do if you have bugs after your merge, but before you’ve actually deployed to production? Hopefully you’re moving towards shorter release / test cycles, so this window should be short (and rare). But even if you do hit this scenario, there is a way to do the bug fix and keep untested code out. It’s a bit complicated (so you should be trying to avoid this scenario), but let me walk you through the scenario.\"]]],[1,\"p\",[[0,[],0,\"Let’s say we have a file in a web project called “Forecast.cs” that looks like this:\"]]],[10,2],[1,\"p\",[[0,[],0,\"We’ve got a PROD build (1.0.0.4) and the label for 1.0.0.4 shows this file to be on version 51.\"]]],[10,3],[1,\"p\",[[0,[],0,\"We now make a change and add a property called “CODEProperty” (line 15) on the CODE branch:\"]]],[10,4],[1,\"p\",[[0,[],0,\"We then check-in, merge to PROD and do another build (1.0.0.5). This version is then deployed out for testing in our UAT environment. Forecast.cs is now on version 53 in the 1.0.0.5 label, while all other files are on 51.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Suddenly, the proverbial paw-paw hits the fan and there’s an urgent business-stopping bug in our currently deployed production version (1.0.0.4). So we go to source control, search for the 1.0.0.4 label in the PROD branch that the build created and select “Get This Version” to get the 1.0.0.4 version locally.\"]]],[10,6],[1,\"p\",[[0,[],0,\"We fix the bug (by adding a property called “HotfixProperty” – line 15 below). Note how there is no “CODEProperty” since this version of Forecast is before the CODEProperty checkin.\"]]],[10,7],[1,\"p\",[[0,[],0,\"Since we’re not on the latest version (we did a “Get-label”) we won’t be able to check in. So we shelve the change (calling the shelveset “1.0.0.4 Hotfix”). We then open the build template and edit the Get Version property and tell the build to get 1.0.0.4 too by specifying L followed by the label name – so the full “Get version” value is LPROD_1.0.0.4:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Next we queue the build, telling the build to apply the Shelveset too:\"]]],[10,9],[1,\"p\",[[0,[],0,\"We won’t be able to “Check in changes after successful build” since the build won’t be building with the Latest version. We’ll have to do that ourselves later. The build completes – we now have build 1.0.0.6 which can be deployed straight to production to “handle” the business-stopping bug.\"]]],[1,\"p\",[[0,[],0,\"Finally we do a Get Latest of the solution in PROD, unshelve the changeset to merge the Hotfix with the development code, clear the Get version property on the build and queue the next build that includes both the changes from CODE as well as the hotfix from PROD. This build is now 1.0.0.7. Meanwhile, testing is completed on 1.0.0.5, and so we can then fast-track the testing for 1.0.0.7 to release the new CODEProperty feature, including the hotfix from build 1.0.0.6.\"]]],[1,\"p\",[[0,[],0,\"Here’s a summary of what code is in what build:\"]]],[3,\"ul\",[[[0,[],0,\"1.0.0.4 – baseline PROD code\"]],[[0,[],0,\"1.0.0.5 – CODEProperty change coming from a merge from CODE branch into PROD branch\"]],[[0,[],0,\"1.0.0.6 – baseline PROD plus the hotfix shelveset (no CodeProperty at all) which includes the HotfixProperty\"]],[[0,[],0,\"1.0.0.7 – CODEProperty merged with HotfixProperty\"]]]],[1,\"p\",[[0,[],0,\"Here’s the 1.0.0.7 version of Forecast.cs (see lines 15 and 17):\"]]],[10,10],[1,\"p\",[[0,[],0,\"If we turn on Annotation, you’ll see that CODEProperty is changeset 52 (in green below), and HotfixProperty is changeset 54 (in red below):\"]]],[10,11],[1,\"p\",[[0,[],0,\"Yes, it’s a little convoluted, but it’ll work – the point is that this is possible without a 3rd branch in Source Control. Also, you should be aiming to shorten your test / release cycles so that this situation is very rare. If you hit this scenario often, you could introduce the 3rd branch (call it INTEGRATION or MAIN or something) that can be used to isolate bug-fixes in PROD from new development in CODE that isn’t ready to go out to production.\"]]],[1,\"p\",[[0,[],0,\"Here’s a summary of the steps if there is a bug in current production when you haven’t deployed the PROD code (after a merge from CODE) to production yet:\"]]],[3,\"ol\",[[[0,[],0,\"PROD code is built (1.0.0.4) and released to production.\"]],[[0,[],0,\"CODE is merged to PROD and build 1.0.0.5 is created, but not deployed to production yet\"]],[[0,[],0,\"Get by Label – the current PROD label (1.0.0.4)\"]],[[0,[],0,\"Fix the bug and shelve your changes\"]],[[0,[],0,\"Edit the build to change the Get version to the current PROD label (1.0.0.4)\"]],[[0,[],0,\"Queue the build with your hotfix shelveset (this will be build 1.0.0.6)\"]],[[0,[],0,\"Test and deploy the hotfix version (1.0.0.6) to production\"]],[[0,[],0,\"Get Latest and unshelve to merge the CODE code and the hotfix\"]],[[0,[],0,\"Clear the Get version field of the build and queue the new build (1.0.0.7)\"]],[[0,[],0,\"Test and deploy to production\"]]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"The key to good separation of work streams is to not mistake the branch for the environment, nor confuse build with deploy. Using the CODE-PROD branching scenario, builds with versioning and labels, parameterized configs and Lab/Release management you can:\"]]],[3,\"ul\",[[[0,[],0,\"Isolate development code from production code, so that you can do new features while still fixing bugs in production and not have untested development pollute the hotfixes\"]],[[0,[],0,\"Track which code is deployed where (using binary versions and labels)\"]],[[0,[],0,\"Recreate builds from labels\"]],[[0,[],0,\"Deploy a single build to multiple environments, so that what you test in UAT is what you deploy to production\"]]]],[1,\"p\",[[0,[],0,\"Happy building and deploying!\"]]]]}","published_at":1409182445000,"status":"published","published_by":1},{"id":"1051dd8a-6bc8-423e-98d9-6cc6993d476d","title":"Build-Deploy-Test Workflow for Physical Environments","slug":"build-deploy-test-workflow-for-physical-environments","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TV-i61iKBAI/AAAAAAAAAO0/1qpy3bbu7dM/s1600-h/clip_image0024.jpg\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"clip_image002\\\" border=\\\"0\\\" alt=\\\"clip_image002\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TV-jKpq_ovI/AAAAAAAAAO4/jPWG461VXno/clip_image002_thumb1.jpg?imgmax=800\\\" width=\\\"322\\\" height=\\\"155\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TV-jLX3HqiI/AAAAAAAAAO8/CbkTW5gTAjs/s1600-h/clip_image0044.jpg\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"clip_image004\\\" border=\\\"0\\\" alt=\\\"clip_image004\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TV-jbahYV7I/AAAAAAAAAPA/xHHPJHDut5c/clip_image004_thumb1.jpg?imgmax=800\\\" width=\\\"301\\\" height=\\\"216\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TV-jcDplFCI/AAAAAAAAAPE/zYzRUWjp9K8/s1600-h/clip_image0065.jpg\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"clip_image006\\\" border=\\\"0\\\" alt=\\\"clip_image006\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TV-kZbR-nXI/AAAAAAAAAPI/r7dv1pGfTcU/clip_image006_thumb2.jpg?imgmax=800\\\" width=\\\"373\\\" height=\\\"310\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TV-kaL4-vsI/AAAAAAAAAPM/x9OruRIC55Q/s1600-h/clip_image0084.jpg\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"clip_image008\\\" border=\\\"0\\\" alt=\\\"clip_image008\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TV-lXJ71PdI/AAAAAAAAAPQ/rbg-w9udgJ8/clip_image008_thumb1.jpg?imgmax=800\\\" width=\\\"346\\\" height=\\\"238\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TV-lYRQRLPI/AAAAAAAAAPU/bApVU7dtPvI/s1600-h/image3.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TV-loSQIP4I/AAAAAAAAAPY/BTXWG9Tlxbc/image_thumb1.png?imgmax=800\\\" width=\\\"389\\\" height=\\\"325\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/lab_management/archive/2011/02/16/running-build-deploy-test-workflow-on-physical-environments.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This week, Darshan Desai published a \"],[0,[0],1,\"post\"],[0,[],0,\" about a build-deploy-test workflow for Physical environments. The solution is entirely XAML based – you don’t need any custom assemblies. However, the design-time experience is not as rich as the wizard that you get when you do a Lab workflow for build-deploy-test using the LabDefault.xaml template.\"]]],[1,\"p\",[[0,[],0,\"I’ve been working on building a workflow that includes a wizard similar to that of the Lab workflow. Seeing Darshan’s solution allowed me to iron out a few kinks in my solution, as well as overcome some of his solution’s limitations. Obviously, since this is a scenario for a Physical environment, there is no ability to do snapshots or restores - you'll have to make sure you have some clean up scripts to run pre-deployment to get to a \\\"clean-ish\\\" state.\"]]],[1,\"h2\",[[0,[],0,\"Notions Physical Build-Deploy-Test Solution\"]]],[1,\"p\",[[0,[],0,\"Setting up the environment is the same for my solution as it is for Darshan – you need to install and configure both a Build agent (or workflow agent, as it’s called in the Lab scenario) as well as a Test agent. Both agents need to be connected to controllers in your TFS environment. The name of the Build agent is important, since this is the way that you configure where deployment scripts are run.\"]]],[1,\"p\",[[0,[],0,\"If you’re going to test, you need to create a test plan with a test suite that contains tests that have automation associate with them. Also, you’ll need a “regular” build that can compile (and optionally unit test) the code as well as the dll’s that contain the automated tests (call this the “Source build”). Then you’ll need to create some automated test settings for your test plan. This test setup is exactly the same setup you’d need if you’re using the Lab workflow or Darshan’s workflow.\"]]],[1,\"p\",[[0,[],0,\"To use our workflow, you need the PhysicalDefaultWorkflow in source control somewhere, as well as a custom assembly. You’re controller needs to be configured to point to the folder in source control that contains this custom assembly. In contrast, Darshan’s workflow doesn’t require a custom assembly.\"],[1,[],0,0],[0,[],0,\"Here’s a walkthrough of what my workflow looks like once you’ve configured the physical environment, the Source build and the test plan.\"]]],[1,\"p\",[[0,[],0,\"1. Create a new Build Definition and set the workspace, drop location, trigger and retention policy just as you would for any other build.\"]]],[1,\"p\",[[0,[],0,\"2. Change the Build Process Template on the Process tab to the PhysicalDefaultTemplate.xaml.\"]]],[1,\"p\",[[0,[],0,\"3. You’ll see the familiar “Click here to edit details…” for the Workflow Process Details argument. Clicking the button with the ellipsis will launch the Physical Workflow Parameters wizard. You’ll see a welcome screen – click next to start configuring the build.\"]]],[1,\"p\",[[0,[],0,\"4. On the “Select Environment” screen, select the Physical environment that you want to deploy to \"],[1,[],0,1]]],[10,0],[1,\"p\",[[0,[],0,\"5. On the “Configure Build” screen, configure the Source build. Here I overcome some of Darshan’s build’s limitations – you can choose a custom drop location, queue a new build or select the latest available build for your build definition.\"]]],[10,1],[1,\"p\",[[0,[],0,\"\"],[1,[],0,2],[0,[],0,\"6. The next screen is “Deployment Scripts”. Here you can configure scripts for the deployment. This screen is slightly different from the same screen in the Lab Workflow. Instead of “Role” for the deployment script, you need to target the Build (Workflow) agent to run the script on (you may see agents that are not part of the environment in this list, so make sure you select the correct agents). You can use $(BuildLocation) as a parameter for the build drop folder of the Source build. I’ve also created a Machine_ parameter that’s similar to the Computer_ (and InternalComputer_) parameters of the Lab workflow. You use $(Machine_AgentName) as a variable for the physical machine name that the agent with name AgentName resides on. For example, if you have an agent called “MyAgent” on a machine called “MyMachine”, then you can use $(Machine_MyAgent) as the variable and when the build runs, this variable will be expanded to “MyMachine”.\"]]],[1,\"p\",[[0,[],0,\"7. You’ll need to configure an account to perform the deployment under. In the Lab Workflow, this is the account that the Lab Agent is configured with. This can be any account as long as it has permissions to execute the script and to the Source build drop folder. Warning: the password is not stored securely!\"]]],[10,2],[1,\"p\",[[0,[],0,\"8. Finally, configure testing on the “Configure Testing” screen. This screen is exactly the same as the “Configure Testing” screen in the Lab Workflow.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now you can run your build! Here’s the output of one of my builds:\"]]],[10,4]]}","published_at":1298149920000,"status":"published","published_by":1},{"id":"9a28514f-b873-4d2c-8f7d-b5b3b6ec7b54","title":"Build Fails: Path Limit Exceeded","slug":"build-fails-path-limit-exceeded","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Exception Message: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-eQNhGS0pzL4/UXp47qTp-EI/AAAAAAAAAtI/luBrKVAZSMo/s1600-h/image%25255B4%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-ehJSuaZ-iZ8/UXp49GeDtiI/AAAAAAAAAtQ/lhKBlndIVYM/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"676\\\" height=\\\"89\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">c:\\\\Builds\\\\1\\\\FabrikamFiber\\\\FabrikamFiber.CallCenter MAIN\\\\src</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-5MotCXdpb6g/UXp4-MNAX4I/AAAAAAAAAtY/c2sP7n2_cQw/s1600-h/image%25255B8%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-8ZM6lMwJi2Y/UXp4_Y0GzVI/AAAAAAAAAtg/ujjdwXaCruU/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"391\\\" height=\\\"118\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">c:\\\\Builds\\\\1\\\\FabrikamFiber\\\\FabrikamFiber.CallCenter MAIN\\\\src\\\\BuildLibs</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-mnyCUBYQQmE/UXp5Ag7OM7I/AAAAAAAAAto/iIZgqj8fbfY/s1600-h/image%25255B12%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-vUhsirYDlK8/UXp5B9eIajI/AAAAAAAAAtw/QGDK6l2oXRM/image_thumb%25255B6%25255D.png?imgmax=800\\\" width=\\\"347\\\" height=\\\"184\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-XWq18c09NYQ/UXp5DsVVZ9I/AAAAAAAAAt4/GvX_JTOrijo/s1600-h/image%25255B16%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-PD04i9o8hIE/UXp5FAq2BwI/AAAAAAAAAuA/Tat0v1tXPLw/image_thumb%25255B8%25255D.png?imgmax=800\\\" width=\\\"315\\\" height=\\\"297\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">c:\\\\b\\\\$(BuildAgentId)\\\\$(BuildDefinitionId)</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">c:\\\\b\\\\1\\\\10\\\\src</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$/FabrikamFiber/Code In Some Really/Long Folder/That has SubFolders/Within SubFolders</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$/FabrikamFiber/Code In Some Really/Long Folder/Libs</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$(SourceDir)/T/W</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$(SourceDir)/Libs</font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/bb399135(v=vs.100).aspx\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I had a customer who mailed me about their builds failing. The error message was\"]]],[10,0],[10,1],[1,\"p\",[[0,[],0,\"The problem was the path of the source files got too long. There are 2 contributing factors for path length on a build server:\"]]],[3,\"ol\",[[[0,[],0,\"The Build Agent Working Directory setting\"]],[[0,[],0,\"The Source Setting Workspace mapping\"]]]],[1,\"p\",[[0,[],0,\"When the build agent checks out code, it checks it out to (WorkingDirectory)\\\\Mapping for each mapping. By default, the build agent working directory is set to $(SystemDrive)\\\\Builds\\\\$(BuildAgentId)\\\\$(BuildDefinitionPath) – more on these macros later – but this usually defaults the working directory that the source gets checked out to something like\"]]],[10,2],[1,\"p\",[[0,[],0,\"(where the team project name is “FabrikamFiber” and the build definition name is “FabrikamFiber.CallCenter MAIN”).\"]]],[1,\"p\",[[0,[],0,\"Then, if you look at the Source Setting workspace mapping for the build, you’ll see the mappings for what source code the build is supposed to check out:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Here, $(SourceDir) equates to the root working folder for the build agent that ends up running the build. You’ll see I have a 2 mapping here – one to $(SourceDir) and one to $(SourceDir)\\\\BuildLibs. That means that my BuildLibs will get checked out to:\"]]],[10,4],[1,\"p\",[[0,[],0,\"which is already 69 characters. If I have lots of subdirectories below that, I could start hitting the 260 character path limit.\"]]],[1,\"h2\",[[0,[],0,\"Customizing the Build Agent Working Directory\"]]],[1,\"p\",[[0,[],0,\"So let’s shorten the build agent working directory. In Team Explorer, click on the Build hub. Click “Actions” and select “Manage Build Controllers”.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Then find the build agent(s) that are going to build your code and click “Properties”. You want to set the working directory to something shorter:\"]]],[10,6],[1,\"p\",[[0,[],0,\"There are 4 macros you can use here (according to \"],[0,[0],1,\"this page\"],[0,[],0,\"):\"]]],[3,\"ul\",[[[0,[],0,\"\"]]]],[1,\"p\",[[0,[1],1,\"$(BuildAgentId)\"],[0,[],0,\": An automatically generated integer that uniquely identifies a build agent within a team project collection.\"]]],[3,\"ul\",[[[0,[],0,\"\"]]]],[1,\"p\",[[0,[1],1,\"$(BuildAgentName)\"],[0,[],0,\": The Display Name of the build agent.\"]]],[3,\"ul\",[[[0,[],0,\"\"]]]],[1,\"p\",[[0,[1],1,\"$(BuildDefinitionId)\"],[0,[],0,\": An automatically generated integer that uniquely identifies a build definition within a team project collection.\"]]],[3,\"ul\",[[[0,[],0,\"\"],[0,[1],1,\"$(BuildDefinitionPath)\"],[0,[],0,\": The team project name and the build definition name, separated by a backslash.\"]]]],[1,\"p\",[[0,[],0,\"So let’s change the Working Directory to:\"]]],[10,7],[1,\"p\",[[0,[],0,\"That means the root of the working directory will be something like:\"]]],[10,8],[1,\"p\",[[0,[],0,\"which is only 13 characters.\"]]],[1,\"h2\",[[0,[],0,\"Shorten the Build Source Setting Workspace Mapping\"]]],[1,\"p\",[[0,[],0,\"Nothing says that the mapped folder for the build has to have the same name as the folder in source control. For example, if you have\"]]],[10,9],[1,\"p\",[[0,[],0,\"and you need to map a path to\"]]],[10,10],[1,\"p\",[[0,[],0,\"for referencing a library, then you could map those 2 folders to\"]]],[10,11],[1,\"p\",[[0,[],0,\"and\"]]],[10,12],[1,\"p\",[[0,[],0,\"respectively. As long as you keep the same relative path “distances”, any relative path references you have will just work (assuming you haven’t hard coded them).\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1367013240000,"status":"published","published_by":1},{"id":"72f2a5ba-171f-4501-ac5f-6ee0152fba8b","title":"Build Script Hooks for TFS 2012 Builds","slug":"build-script-hooks-for-tfs-2012-builds","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-A0SlTFrXZK8/Ut-woxu6NEI/AAAAAAAABLk/LgyhQ3CP7zM/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-n3XDr5QsH10/Ut-wpsUu8gI/AAAAAAAABLs/2mMlrsLAvYo/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"435\\\" height=\\\"177\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-6Caj4I539nE/Ut-wqToYgPI/AAAAAAAABL0/d94g2x3w-G8/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-UljBJ6TNYAQ/Ut-wq-Dg2CI/AAAAAAAABL8/ePeDUlft-2w/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"329\\\" height=\\\"273\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">Not String.IsNullOrEmpty(PreBuildScriptPath)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-vIOv0L3w1IA/Ut-wrs-GLLI/AAAAAAAABME/7-8_QsbEE7M/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-aS6oqjAyftg/Ut-wsXlgPgI/AAAAAAAABMM/IgNvSQMJLlY/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"249\\\" height=\\\"343\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">Set-ExecutionPolicy RemoteSigned</font>\"}],[\"code\",{\"code\":\"Param(\\n  [string]$pathToSearch = $env:TF_BUILD_SOURCESDIRECTORY,\\n  [string]$buildNumber = $env:TF_BUILD_BUILDNUMBER,\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-ZDfuvAp7Sy4/Ut-wtSBw6aI/AAAAAAAABMU/DptMqrg6gAc/s1600-h/image%25255B15%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-EUWdOsDWxzI/Ut-wuAp2r1I/AAAAAAAABMc/i8yS-yCdVLA/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"256\\\" height=\\\"342\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-KvKAIkIe4AM/Ut-wusSu33I/AAAAAAAABMk/Cc82aqusQsU/s1600-h/image%25255B19%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-78axFRtXjIA/Ut-wvY49xeI/AAAAAAAABMs/p-6sSH_MTs8/image_thumb%25255B9%25255D.png?imgmax=800\\\" width=\\\"412\\\" height=\\\"139\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-UacgKu522uo/Ut-wwHm4IiI/AAAAAAAABM0/mLHR81iOT5A/s1600-h/image%25255B23%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-3V0EjbZKtsU/Ut-wwmH38OI/AAAAAAAABM8/6Ljc-YStxY4/image_thumb%25255B11%25255D.png?imgmax=800\\\" width=\\\"420\\\" height=\\\"85\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2014/01/error-handling-poor-mans-runscript-in.html\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/07/matching-binary-version-to-build-number.html\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/vstudio/dn376353.aspx#scripts\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"EDIT: My colleague Tyler Doerksen pointed out in his comments that my solution doesn’t do any error checking of the scripts. If your script fails, the build happily continues. I’ve added \"],[0,[0],1,\"another post\"],[0,[],0,\" to show how to add error handling.\"]]],[1,\"p\",[[0,[],0,\"One of my favorite features about the TFS 2013 Builds is the script hooks – there are pre- and post-build as well as pre- and post-test hooks. These make customizing build a whole lot easier. For example, customizing the build so that your assembly versions match your build number \"],[0,[1],1,\"is a snap\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"I set out to implement the same logic in a 2012 build this morning – unfortunately, the RunScript build activity from the 2013 template is only in the 2013 TFS Build assembly.\"]]],[1,\"p\",[[0,[],0,\"So I came up with a “poor-man’s” run-script equivalent for 2012 builds (with the best part being you don’t need any custom assemblies, so the edit can be applied directly to your build template without having to pull it into a solution). I’ll walk you through the steps of customizing the default build – in this example I’m only doing pre- and post-build scripts, but the principles would be the same for pre- and post-test scripts.\"]]],[1,\"h2\",[[0,[],0,\"Challenge 1 – Invoking PowerShell\"]]],[1,\"p\",[[0,[],0,\"The first challenge is how do you invoke a PowerShell script from within the build? It’s fairly easy: use the InvokeProcess activity.\"]]],[1,\"p\",[[0,[],0,\"First you’ll need to add workflow arguments for the pre- and post-build script paths as well as their corresponding args. Open your build workflow and click on “Arguments”. Enter 4 “In String” arguments as follows:\"]]],[10,0],[1,\"p\",[[0,[],0,\"You can add defaults if you like.\"]]],[1,\"p\",[[0,[],0,\"I always like to put custom arguments in a separate section of the build parameters, so that anyone creating a build from the template can see them and read their descriptions. You do this by clicking the “…” button in the Default Value column of the “Metadata” argument and filling in some metadata for your arguments:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now go to the workflow and find the MSBuild Activity in the heart of the workflow that does the compilation (be careful – there’s one that does a clean of the workspace too – you don’t want that one). Just above the ForEach (For Each Project in BuildSettings.ProjectsToBuild) activity, add an If activity (this will automatically add in a sequence to wrap the activities we’re adding) and set its condition to\"]]],[10,2],[1,\"p\",[[0,[],0,\"and in the Then of the If activity add a “ConvertWorkspaceItem” activity and an InvokeProcess activity. In the InvokeProcess activity, drag a “WriteBuildMessage” and “WriteBuildError” activity onto the area below stdOutput and errOutput respectively. Set the “Message” property of each Write activity to stdOutput and errOutput respectively.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Click on the sequence activity that your activities are in. We’ll need two local variables: preBuildScriptLocalPath and postBuildScriptLocalPath (both strings).\"]]],[1,\"p\",[[0,[],0,\"Now in the ConvertWorkspaceItem activity, set the following properties:\"]]],[3,\"ul\",[[[0,[],0,\"DisplayName: “Get pre-build script local path”\"]],[[0,[],0,\"Input: PreBuildScriptPath\"]],[[0,[],0,\"Output: preBuildScriptLocalPath\"]],[[0,[],0,\"Workspace: Workspace\"]]]],[1,\"p\",[[0,[],0,\"Set the following properties in the InvokeProcess activity:\"]]],[3,\"ul\",[[[0,[],0,\"Arguments: String.Format(\\\" \\\"\\\"& '{0}' {1}\\\"\\\" \\\", prebuildScriptLocalPath, PreBuildScriptArgs)\"]],[[0,[],0,\"DisplayName: “Run pre-build script”\"]],[[0,[],0,\"FileName: “PowerShell”\"]]]],[1,\"p\",[[0,[],0,\"Now you can copy this whole “If” activity and paste it below the “ForEach” (the one that does contains the MSBuild activity) and rename pre to post – this implements your post-build hook.\"]]],[1,\"p\",[[0,[],0,\"Don’t forget that you’ll need to change PowerShell’s execution policy on your build server. Log in to your build server and run PowerShell as an administrator. Run the following:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now you’re almost set…\"]]],[1,\"h2\",[[0,[],0,\"Challenge 2 – Environment Variables\"]]],[1,\"p\",[[0,[],0,\"When I created a script for a 2013 build to version the assemblies, I relied on the fact that the 2013 build sets some environment variables that you can use in your scripts. Here’s a snippet showing 2 environment variables I used in my \"],[0,[1],1,\"versioning script\"],[0,[],0,\":\"]]],[10,5],[1,\"p\",[[0,[],0,\"You can see I’m getting $env:TF_BUILD_BUILDNUMBER. Well, in the 2012 workflow, these variables aren’t set, so you have to add an activity to do it.\"]]],[1,\"p\",[[0,[],0,\"Just above your “If” activity for the pre-build script invocation, add an InvokeMethod activity (this is in the “Primitives” section of the workflow designer toolbox).\"]]],[10,6],[1,\"p\",[[0,[],0,\"Set the following properties:\"]]],[3,\"ul\",[[[0,[],0,\"MethodName: SetEnvironmentVariable\"]],[[0,[],0,\"TargetType: System.Environment\"]]]],[1,\"p\",[[0,[],0,\"Then you need to set different parameters for each environment variable you want to set. Click on the “…” next to the value of the Parameters property:\"]]],[10,7],[1,\"p\",[[0,[],0,\"In this one I set 2 parameters: “In String TF_BUILD_SOURCESDIRECTORY” and “In String SourcesDirectory” to set the sources directory environment variable. I did the same for binaries directory and build number, each time using the name from \"],[0,[2],1,\"this list\"],[0,[],0,\" (see the TF_BUILD environment variables section) and the value from the corresponding workflow argument or variable. Then I could use the same PowerShell script that I used in my 2013 builds without having to modify it.\"]]],[1,\"h2\",[[0,[],0,\"Creating a Build Definition\"]]],[1,\"p\",[[0,[],0,\"Now when you create a build definition, make sure that you include the folder that contains your scripts into the build workspace. Then set your script paths (using the source control paths) and arguments appropriately, for example:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Happy customizing!\"]]]]}","published_at":1390427460000,"status":"published","published_by":1},{"id":"74311567-f3e9-4920-b4ef-d344b4465f58","title":"Build vNext and SonarQube Runner: Dynamic Version Script","slug":"build-vnext-and-sonarqube-runner-dynamic-version-script","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"Param(\\n  [string]$pathToSearch = $env:BUILD_SOURCESDIRECTORY,\\n  [string]$buildNumber = $env:BUILD_BUILDNUMBER,\\n  [string]$searchFilter = \\\"AssemblyInfo.*\\\",\\n  [regex]$pattern = \\\"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\"\\n)\\n \\nif ($buildNumber -match $pattern -ne $true) {\\n    Write-Error \\\"Could not extract a version from [$buildNumber] using pattern [$pattern]\\\"\\n    exit 1\\n} else {\\n    try {\\n        $extractedBuildNumber = $Matches[0]\\n        Write-Host \\\"Using version $extractedBuildNumber in folder $pathToSearch\\\"\\n \\n        $files = gci -Path $pathToSearch -Filter $searchFilter -Recurse\\n\\n        if ($files){\\n            $files | % {\\n                $fileToChange = $_.FullName  \\n                Write-Host \\\"  -&gt; Changing $($fileToChange)\\\"\\n                \\n                # remove the read-only bit on the file\\n                sp $fileToChange IsReadOnly $false\\n \\n                # run the regex replace\\n                (gc $fileToChange) | % { $_ -replace $pattern, $extractedBuildNumber } | sc $fileToChange\\n            }\\n        } else {\\n            Write-Warning \\\"No files found\\\"\\n        }\\n \\n        Write-Host \\\"Done!\\\"\\n        exit 0\\n    } catch {\\n        Write-Error $_\\n        exit 1\\n    }\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"Param(\\n  [string]$buildNumber = $env:BUILD_BUILDNUMBER,\\n  [regex]$pattern = \\\"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\",\\n  [string]$key,\\n  [string]$name\\n)\\n \\n$version = \\\"1.0\\\"\\nif ($buildNumber -match $pattern -ne $true) {\\n    Write-Verbose \\\"Could not extract a version from [$buildNumber] using pattern [$pattern]\\\" -Verbose\\n} else {\\n    $version = $Matches[0]\\n}\\n\\nWrite-Verbose \\\"Using args: begin /v:$version /k:$key /n:$name\\\" -Verbose\\n$cmd = \\\"MSBuild.SonarQube.Runner.exe\\\"\\n\\n&amp; $cmd begin /v:$version /k:$key /n:$name\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8f4dad3d-6859-4220-a674-89b99f03542b.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/54dbdda6-9975-48dd-875c-858fa7c55b0d.png\\\" width=\\\"331\\\" height=\\\"210\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1173de53-ef4d-4ea9-bc36-04d29ffd8f9a.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/995b513c-35c3-4919-be0b-e129c25cd40e.png\\\" width=\\\"336\\\" height=\\\"213\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7f59f375-b594-4b3a-bee7-adff7b134e47.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/11a02d4c-7fd8-418c-9d81-1404f74f2ee4.png\\\" width=\\\"356\\\" height=\\\"159\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.sonarqube.org/\"]],[\"a\",[\"href\",\"http://www.sonarqube.org/announcing-sonarqube-integration-with-msbuild-and-team-build/\"]],[\"a\",[\"href\",\"https://github.com/SonarSource/sonar-.net-documentation\"]],[\"a\",[\"href\",\"https://github.com/SonarSource/sonar-.net-documentation/blob/master/doc/analyze-from-tfs.md#analyzing-projects-using-the-new-tfs-2015-build-system\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/matching-binary-version-to-build-number-version-in-tfs-2013-builds\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/Library/vs/alm/Build/scripts/variables\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/47d1049d-bb27-454e-aab8-24566c85e548?SRC=Home\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"SonarQube\"],[0,[],0,\" is a fantastic tool for tracking technical debt, and it’s starting to make some inroads into the .NET world as SonarSource \"],[0,[1],1,\"collaborates with Microsoft\"],[0,[],0,\". I’ve played around with it a little to start getting my hands dirty.\"]]],[1,\"h2\",[[0,[],0,\"Install Guidance\"]]],[1,\"p\",[[0,[],0,\"If you’ve never installed SonarQube before, then I highly recommend this \"],[0,[2],1,\"eGuide\"],[0,[],0,\". Just one caveat that wasn’t too clear: you need to create the database manually before running SonarQube for the first time. Just create an empty database (with the required collation) and go from there.\"]]],[1,\"h2\",[[0,[],0,\"Integrating into TeamBuild vNext – with Dynamic Versioning\"]]],[1,\"p\",[[0,[],0,\"Once you’ve got the server installed and configured, you’re ready to integrate with TeamBuild. It’s easy enough using \"],[0,[3],1,\"build VNext Command Line task\"],[0,[],0,\". However, one thing bugged me as I was setting this up – hard-coding the version number. I like to \"],[0,[4],1,\"version my assemblies from the build number\"],[0,[],0,\" on the build using a PowerShell script. Here’s the 2015 version (since the \"],[0,[5],1,\"environment variable names\"],[0,[],0,\" have changed):\"]]],[10,0],[1,\"p\",[[0,[],0,\"So now that I get dll’s versions matching my build number, why not SonarQube too? So I used the same idea to wrap the “begin” call into a PowerShell script which can get the build number too:\"]]],[10,1],[1,\"p\",[[0,[],0,\"I drop this into the same folder as the MsBuild.SonarQube.Runner.exe so that I don’t have to fiddle with more paths. Here’s the task in my build:\"]]],[10,2],[1,\"p\",[[0,[],0,\"The call to the SonarQube runner “end” doesn’t need any arguments, so I’ve left that as a plain command line call:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now when the build runs, the version number passed to SonarQube matches the version number of my assemblies which I can tie back to my builds. Sweet!\"]]],[10,4],[1,\"p\",[[0,[],0,\"One more change you could make is to specify the key and name arguments as variables. That way you can manage them as build variables instead of managing them in the call to the script on the task.\"]]],[1,\"p\",[[0,[],0,\"Finally, don’t forget to install the Roslyn SonarQube \"],[0,[6],1,\"SonarLint extension\"],[0,[],0,\". This will give you the same analysis that SonarQube uses inside VS.\"]]],[1,\"p\",[[0,[],0,\"Happy SonarQubing!\"]]]]}","published_at":1439321698000,"status":"published","published_by":1},{"id":"c56c1592-d9f0-4215-b213-d912bdf42d29","title":"Build with a Hosted Build Controller: A First Attempt","slug":"build-with-a-hosted-build-controller-a-first-attempt","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-zFldcgQM414/T_Kxsm-tjlI/AAAAAAAAAZY/hPVamI148dc/s1600-h/image%25255B3%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/--x0jdTD6PuQ/T_KxvqsJiHI/AAAAAAAAAZg/y-PmP57mtLk/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"366\\\" height=\\\"123\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-OV4n_yWmn38/T_KxwtUwHlI/AAAAAAAAAZo/DzC6GlYBG-s/s1600-h/image%25255B7%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/--cSWlRr8E6U/T_Kxxs9vKhI/AAAAAAAAAZw/sr-SW1uEWhU/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"372\\\" height=\\\"182\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-qBj8hfLL79o/T_Kxz6B-XcI/AAAAAAAAAZ4/jwTDHxqC_sc/s1600-h/image%25255B22%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-Su-A8GAGPx8/T_Kx08yoWYI/AAAAAAAAAaA/MYFd3aCaCqk/image_thumb%25255B10%25255D.png?imgmax=800\\\" width=\\\"328\\\" height=\\\"165\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-vuIiMfzmijw/T_Kx18Vz5uI/AAAAAAAAAaI/AmJIOXVXnIY/s1600-h/image%25255B14%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-nt9nCLmJcjQ/T_Kx2842D8I/AAAAAAAAAaQ/tputL10JR0Q/image_thumb%25255B6%25255D.png?imgmax=800\\\" width=\\\"234\\\" height=\\\"281\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-hHJqOokv75c/T_Kx3gy8i-I/AAAAAAAAAaY/jo2VHctJT1o/s1600-h/image%25255B18%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-_OkJVhkUuRw/T_Kx5Z7N1QI/AAAAAAAAAag/qTxGfxBhIY8/image_thumb%25255B8%25255D.png?imgmax=800\\\" width=\\\"282\\\" height=\\\"359\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/72576517-821b-46c2-aa1a-fab940752292\"]],[\"a\",[\"href\",\"http://www.improvingsoftwarequality.com/\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/tfs/hh549175(v=vs.110).aspx\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/04/more-on-fakes-beta-has-issues.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/06/vs-2012-rc-fakes-bugs-fixed.html\"]],[\"strong\"],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/07/code-coverage-doesnt-work-with-fakes-on.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I am working on some code for the \"],[0,[0],1,\"TFS Tester Power Tool\"],[0,[],0,\" with my colleague \"],[0,[1],1,\"Anna Russo\"],[0,[],0,\" (who just got her first MVP award!) and we’re using TFS Preview for source control and work item tracking. From the start I wanted to get some unit tests and builds up and running. The challenge for the unit testing side was that the tool works against a Team Foundation Server, so testing required some sort of mocking or faking.\"]]],[1,\"p\",[[0,[],0,\"At around the same time I started looking at the unit tests, Microsoft released a beta of the \"],[0,[2],1,\"Fakes Framework\"],[0,[],0,\". I liked the look of the framework (especially since I had done some stuff with its predecessor, Moles), and decided to write the tests using it. However, I hit a snag with a \"],[0,[3],1,\"bug in the beta\"],[0,[],0,\". Fortunately, the \"],[0,[4],1,\"RC fixed the issue\"],[0,[],0,\", and so I was able to get the tests running.\"]]],[1,\"p\",[[0,[],0,\"So once Anna had granted me EditBuildDefinition rights on the hosted Team Project, I was ready to spin my first hosted build.\"]]],[1,\"p\",[[0,[],0,\"Here are my experiences.\"]]],[1,\"h2\",[[0,[],0,\"Cloaked Build Drops folder\"]]],[10,0],[1,\"p\",[[0,[],0,\"I never check-out my entire source control folder for a build (and neither should you!). When creating a build, make sure the workspace is the minimum codebase that you need. Since I didn’t have a solution open, the workspace defaulted to the root of the Team Project, and you can see that it adds a cloak for the source controlled drops folder. At first I was a little puzzled at this, and then I realised that if you do check out your entire repository for your build, you probably don’t need the previous build outputs, so this default makes sense. However, when I changed the root folder to a subfolder a bit deeper in, and didn’t delete the cloak of the drops folder – the build failed since the cloak didn’t have an active parent folder. So I just deleted the cloaked entry.\"]]],[1,\"h2\",[[0,[],0,\"Symbols\"]]],[10,1],[1,\"p\",[[0,[],0,\"I like that you can copy the output of the build to a source controlled folder – this makes sense for TFS in the cloud. However, I tried to then add a source control folder as the destination for the symbols, and of course the build failed since the symbols target must be a valid UNC (I wonder if there’s a way to get the symbols into Source Control…).\"]]],[1,\"h2\",[[0,[],0,\"Hosted Build Queue Summary\"]]],[1,\"p\",[[0,[],0,\"I like the Queue Summary page that comes up when you queue a 2012 build – it tells you how long the average build time for this build definition is and where your build is in the queue.\"]]],[10,2],[1,\"h2\",[[0,[],0,\"Enabling Code Coverage\"]]],[1,\"p\",[[0,[],0,\"I think that if you have unit tests, you may as well enable code coverage. Enabling code coverage in a 2012 build is a little different than it was in 2010 (where you did it via a runsettings file). In a 2012 build (this applies to on-premises as well as hosted), you need to edit the “Automated Tests” parameter of the build (press the ellipsis button in the right-hand column for this parameter). In the “General” tab, you’ll see a drop-down under “Options” that you can select to enable code coverage.\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Parameter name: Input (Value cannot be null)\"]]],[1,\"p\",[[0,[],0,\"So I ran the build again and I bumped into a strange error.\"]]],[10,4],[1,\"p\",[[0,[],0,\"The error text wasn’t too helpful, so I queued the build again and changed the logging level to “Diagnostic”. That wasn’t too helpful either, except that I could see the error was happening in the catch of the Try-Catch around the testing activities.\"]]],[1,\"p\",[[0,[],0,\"After digging around in my ChampsList mail (one of the advantages of being an MVP) I came across Neno Loje’s mail about the same issue. Turns out there was a bug in the workflow which was fixed – make sure you use the DefaultTemplate11.1.xaml (as opposed to DefaultTemplate11.xaml).\"]]],[1,\"h2\",[[0,[],0,\"Code Coverage and Fakes – not friends\"]]],[1,\"p\",[[0,[],0,\"So now I noticed that the tests failed – even though I’ve run them on a local on-premises TeamBuild with no issues. I disabled Code Coverage, and the tests pass! So it seems at this point in time I can get the tests to pass, as long as I don’t enable Code Coverage. I’ve mailed the ChampsList, so hopefully I’ll get a resolution to this soon.\"]]],[1,\"p\",[[0,[5],1,\"Update:\"],[0,[],0,\" see the follow up to this in \"],[0,[6],1,\"this post\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Summary\"]]],[1,\"p\",[[0,[],0,\"All in all, it wasn’t too different getting a hosted build to work than getting an on-premises build to work. Once I get some resolution to the code coverage issue, I’ll get exactly the same functionality (I just have to figure out how to handle debug symbols elegantly…).\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1341337680000,"status":"published","published_by":1},{"id":"2b2a6a00-ab03-400e-a4e5-e95b7bc63d95","title":"Building VS 2015 Setup Projects in Team Build","slug":"building-vs-2015-setup-projects-in-team-build","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/34c056cc-67e1-43dd-99b7-3271105b883c.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4ceb0068-6af9-4b28-947a-21de05cc1a69.png\\\" width=\\\"752\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/fcd94210-91dd-43e5-8209-6237066cd7d6.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4375ef99-63ed-4533-ad01-3bf743d77616.png\\\" width=\\\"308\\\" height=\\\"353\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/d9f53d62-d230-442e-b917-7bd29dbf2e0b.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a35d538f-a743-487b-a9c4-aca62c8549f5.png\\\" width=\\\"453\\\" height=\\\"300\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a1613f42-5c56-4643-8f8a-32bc4fdd3cde.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/2ad1710a-8b51-47ad-b25f-3f837c2177dc.png\\\" width=\\\"483\\\" height=\\\"116\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://wixtoolset.org/\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/f1cc3f3e-c300-40a7-8797-c509fb8933b9\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/library/t71a733d.aspx\"]],[\"a\",[\"href\",\"http://www.flexerasoftware.com/producer/products/software-installation/installshield-software-installer/\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Remember when Visual Studio had a setup project template? And then it was removed? Then you moved to \"],[0,[0],1,\"WiX\"],[0,[],0,\" and after learning it for 3 months and still being confused, you just moved to Web Apps?\"]]],[1,\"p\",[[0,[],0,\"Well everyone complained about the missing setup project templates and MS finally added it back in as an \"],[0,[1],1,\"extension\"],[0,[],0,\". Which works great if you build out of Visual Studio – but what about automated builds? Turns out they don’t understand the setup project, so you have to do some tweaking to get it to work.\"]]],[1,\"h2\",[[0,[],0,\"Setup Project Options\"]]],[1,\"p\",[[0,[],0,\"There are a couple of options if you’re going to use setup projects.\"]]],[3,\"ol\",[[[0,[2],1,\"ClickOnce\"],[0,[],0,\". This is a good option if you don’t have a deployment solution that can deploy new versions of your application (like System Center or the like). It requires fudging on the builds to get versioning to work in some automated fashion. At least it’s free.\"]],[[0,[0],1,\"WiX\"],[0,[],0,\". Free and very powerful, but really hard to learn and you end up programming in XML – which is a pain. However, if you need your installer to do “extra” stuff (like create a database during install) then this is a good option. Automation is also complicated because you have to invoke Candle.exe and Light.exe to “build” the WiX project.\"]],[[0,[],0,\"VS Setup Projects. Now that they’re back in VS, you can use these projects to create installers. You can’t do too much crazy stuff – this just lays down the exe’s and gets you going. It’s easy to maintain, but you need to tweak the build process to build these projects. Also free.\"]],[[0,[3],1,\"InstallShield\"],[0,[],0,\" and other 3rd party paid installer products. These are typically powerful, but expensive. Perhaps the support you get is worth the price, but you’ll have to decide if the price is worth the support and other features you don’t get from the other free solutions.\"]]]],[1,\"h2\",[[0,[],0,\"Tweaking Your Build Agent\"]]],[1,\"p\",[[0,[],0,\"You unfortunately won’t be able to build setup projects on the Hosted build agent because of these tweaks. So if you’ve got a build agent, here’s what you have to do:\"]]],[3,\"ol\",[[[0,[],0,\"Install Visual Studio 2015 on the build machine.\"]],[[0,[],0,\"Install the \"],[0,[1],1,\"extension\"],[0,[],0,\" onto your build machine.\"]],[[0,[],0,\"Configure the build agent service to run under a known user account (not local service, but some user account on the machine).\"]],[[0,[],0,\"Apply a registry hack – you have to edit HKCU\\\\SOFTWARE\\\\Microsoft\\\\VisualStudio\\\\14.0_Config\\\\MSBuild\\\\EnableOutOfProcBuild to have a DWORD of 0 (I didn’t have the key, so I just added it). If you don’t do this step, then you’ll probably get an obscure error like this: “ERROR: An error occurred while validating.  HRESULT = '8000000A'”\"]],[[0,[],0,\"Customize the build template (which I’ll show below).\"]]]],[1,\"p\",[[0,[],0,\"It’s fairly nasty, but once you’ve done it, your builds will work without users having to edit the project file or anything crazy.\"]]],[1,\"h2\",[[0,[],0,\"Customizing the Build Definition\"]]],[1,\"p\",[[0,[],0,\"You’ll need to configure the build to compile the entire solution first, and then invoke Visual Studio to create the setup package.\"]]],[1,\"p\",[[0,[],0,\"Let’s walk through creating a simple build definition to build a vdproj.\"]]],[3,\"ol\",[[[0,[],0,\"Log in to VSTS or your TFS server and go to the build hub. Create a new build definition and select the Visual Studio template. Select the source repo and set the default queue to the queue that your build agent is connected to.\"]],[[0,[],0,\"Just after the Visual Studio Build task, add a step and select the “Command Line” task from the Utility section.\"]],[[0,[],0,\"Enter the path to devenv.com for the Tool parameter (this is typically “C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\devenv.com”).\"]],[[0,[],0,\"The arguments have the following format: \"],[0,[4],1,\"solutionPath\"],[0,[],0,\" /build \"],[0,[4],1,\"configuration\"],[0,[],0,\" \"],[0,[4],1,\"projectPath\"],[0,[],0,\"\"]],[[0,[],0,\"solutionPath is the path to the solution file\"]],[[0,[],0,\"configuration is the config (debug, release etc.)\"]],[[0,[],0,\"projectPath is the path to the vdproj file\"]],[[0,[],0,\"Finally, expand the “Advanced” group and set the working folder to the path of the sln file and check the “Fail on Standard Error” checkbox.\"]]]],[1,\"p\",[[0,[],0,\"Here’s an example:\"]]],[10,0],[1,\"p\",[[0,[],0,\"For reference, here’s how my source is structured:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can then publish the setup exe or msi if you need to. You can run tests or scripts or anything else during the build (for ease I delete the unit test task in the above example).\"]]],[1,\"p\",[[0,[],0,\"I now have a successful build:\"]]],[10,2],[1,\"p\",[[0,[],0,\"And the msi is in my drop, ready to be deployed in Release Management:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Happy setup building!\"]]]]}","published_at":1452662000000,"status":"published","published_by":1},{"id":"de940a83-6934-46bd-bafc-ed24c3b7bb08","title":"Bulk Migrate Work Item Comments, Links and Attachments","slug":"bulk-migrate-work-item-comments-links-and-attachments","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"$oldTpcUrl = \\\"http://localhost:8080/tfs/oldCollection\\\"\\n$newTpcUrl = \\\"http://localhost:8080/tfs/newCollection\\\"\\n\\n$csvFile = \\\".\\\\map.csv\\\" #format: oldId, newId\\n$user = \\\"domain\\\\user\\\"\\n$pass = \\\"password\\\"\\n\\n[Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Common')\\n[Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Client')\\n[Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.WorkItemTracking.Client')\\n\\n$oldTpc = [Microsoft.TeamFoundation.Client.TfsTeamProjectCollectionFactory]::GetTeamProjectCollection($oldTpcUrl)\\n$newTpc = [Microsoft.TeamFoundation.Client.TfsTeamProjectCollectionFactory]::GetTeamProjectCollection($newTpcUrl)\\n\\n$oldWorkItemStore = $oldTpc.GetService([Microsoft.TeamFoundation.WorkItemTracking.Client.WorkItemStore])\\n$newWorkItemStore = $newTpc.GetService([Microsoft.TeamFoundation.WorkItemTracking.Client.WorkItemStore])\\n\\n$list = Import-Csv $csvFile\\n$cred = new-object System.Net.NetworkCredential($user, $pass)\\n\\nforeach($map in $list) {\\n    $oldItem = $oldWorkItemStore.GetWorkItem($map.oldId)\\n    $newItem = $newWorkItemStore.GetWorkItem($map.newId)\\n\\n    Write-Host \\\"Processing $($map.oldId) -&gt; $($map.newId)\\\" -ForegroundColor Cyan\\n    \\n    foreach($oldLink in $oldItem.Links | ? { $_.BaseType -eq \\\"HyperLink\\\" }) {\\n        Write-Host \\\"   processing link $($oldLink.Location)\\\" -ForegroundColor Yellow\\n\\n        if (($newItem.Links | ? { $_.Location -eq $oldLink.Location }).count -gt 0) {\\n            Write-Host \\\"      ...link already exists on new work item\\\"\\n        } else {\\n            $newLink = New-Object Microsoft.TeamFoundation.WorkItemTracking.Client.Hyperlink -ArgumentList $oldLink.Location\\n            $newLink.Comment = $oldLink.Comment\\n            $newItem.Links.Add($newLink)\\n        }\\n    }\\n\\n    if ($oldItem.Attachments.Count -gt 0) {\\n        foreach($oldAttachment in $oldItem.Attachments) {\\n            mkdir $oldItem.Id | Out-Null\\n            Write-Host \\\"   processing attachment $($oldAttachment.Name)\\\" -ForegroundColor Magenta\\n\\n            if (($newItem.Attachments | ? { $_.Name.Contains($oldAttachment.Name) }).count -gt 0) {\\n                Write-Host \\\"      ...attachment already exists on new work item\\\"\\n            } else {\\n                $wc = New-Object System.Net.WebClient\\n                $file = \\\"$pwd\\\\$($oldItem.Id)\\\\$($oldAttachment.Name)\\\"\\n\\n                $wc.Credentials = $cred\\n                $wc.DownloadFile($oldAttachment.Uri, $file)\\n\\n                $newAttachment = New-Object Microsoft.TeamFoundation.WorkItemTracking.Client.Attachment -ArgumentList $file, $oldAttachment.Comment\\n                $newItem.Attachments.Add($newAttachment)\\n            }\\n        }\\n    \\n        try {\\n            $newItem.Save();\\n            Write-Host \\\"   Attachments and links saved\\\" -ForegroundColor DarkGreen\\n        }\\n        catch {\\n            Write-Error \\\"Could not save work item $newId\\\"\\n            Write-Error $_\\n        }\\n    }\\n\\n    $comments = $oldItem.GetActionsHistory() | ? { $_.Description.length -gt 0 } | % { $_.Description }\\n    if ($comments.Count -gt 0){\\n        Write-Host \\\"   Porting $($comments.Count) comments...\\\" -ForegroundColor Yellow\\n        foreach($comment in $comments) {\\n            Write-Host \\\"      ...adding comment [$comment]\\\"\\n            $newItem.History = $comment\\n            $newItem.Save()\\n        }\\n    }\\n    \\n    Write-Host \\\"Done!\\\" -ForegroundColor Green\\n}\\n\\nWrite-Host\\nWrite-Host \\\"Migration complete\\\"\\n\",\"language\":\"ps;\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://1drv.ms/1vFDpr5\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I was working at a customer that had set up a test TFS environment. When we set up their “real” TFS, they did a get-latest of their code and imported their code – easy enough. They did have about 100 active work items that they also wanted to migrate. Not being a big fan of TFS Integration Platform, I usually recommend using Excel to port work items \"],[0,[0],1,\"en masse\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"There are a couple of “problems” with the Excel approach:\"]]],[3,\"ol\",[[[0,[],0,\"When you create work items in the new Team Project, they have to go into the “New” state (or the first state for the work item)\"]],[[0,[],0,\"You can’t migrate test cases (since the steps don’t play nicely in Excel) – and you can’t migrate test results either.\"]],[[0,[],0,\"You can’t migrate comments, hyperlinks or attachments in Excel (other than opening each work item one by one)\"]]]],[1,\"p\",[[0,[],0,\"You can mitigate the “new state” limitation by creating several sheets – one for “New” items, one for “Active” items, one for “Resolved” items and so on. The “New” items are easy – just import “as-is”. For the other states, import them into the “New” state and then bulk update the state to the “target” state. Keeping the sheets separated by state makes this easier to manage. Another tip I advise is to add a custom field to the new Team Project (you don’t have to expose it on the forms if you don’t want to) called “OldID” that you set to the id of the old work item – that way you’ve always got a link back to the original work item if you need it.\"]]],[1,\"p\",[[0,[],0,\"For test case, you have to go to the API to migrate them over to the new team project – I won’t cover that topic in this post.\"]]],[1,\"p\",[[0,[],0,\"For comments, hyperlinks and attachments I quickly wrote a PowerShell script that does exactly that! I’ve uploaded it to OneDrive so you can download it \"],[0,[1],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Here’s the script itself:\"]]],[10,0],[1,\"p\",[[0,[],0,\"When you run this, open the script and fix the top 5 lines (the variables for this script). Enter in the Team Project Collection URL’s (these can be the same if you’re migrating links from one Team Project to another in the same Collection). The person running the script needs to have read permissions on the old server and contributor permission on the new server. You then need to make a cvs file with 2 columns: oldId and newId. Populate this with the mapping from the old work item Id to the new work item Id. Finally, enter a username and password (this is simply for fetching the attachments) and you can run the script.\"]]],[1,\"p\",[[0,[],0,\"Happy migrating!\"]]]]}","published_at":1409021457000,"status":"published","published_by":1},{"id":"fbaa5899-b4a8-4de4-aad3-accd3245840f","title":"ClickOnce Versioning and Deployment in a TeamBuild","slug":"clickonce-versioning-and-deployment-in-a-teambuild","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TU6mm9ayV8I/AAAAAAAAAM4/5gJJEueEjYY/s1600-h/image8.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TU6mp0IpFDI/AAAAAAAAAM8/QhbvuEBwMj8/image_thumb4.png?imgmax=800\\\" width=\\\"569\\\" height=\\\"483\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TU6mx8JP44I/AAAAAAAAANA/lpmZYeUn1OQ/s1600-h/image13.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TU6m2b4t0uI/AAAAAAAAANE/Q1pREKrCWh4/image_thumb7.png?imgmax=800\\\" width=\\\"583\\\" height=\\\"526\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TU6m4Poxz3I/AAAAAAAAANI/kAxjxoAvCAA/s1600-h/image20.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TU6m6M0trZI/AAAAAAAAANM/rPBq1LW9jno/image_thumb15.png?imgmax=800\\\" width=\\\"379\\\" height=\\\"562\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TU6m7vM3FoI/AAAAAAAAANQ/pgFOvVG8KTg/s1600-h/image25.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TU6m9TkNcDI/AAAAAAAAANU/dR9OVFE58a0/image_thumb23.png?imgmax=800\\\" width=\\\"425\\\" height=\\\"510\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TU6m-hShNKI/AAAAAAAAANY/naO1_n59miU/s1600-h/image36.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TU6nBJM5x_I/AAAAAAAAANc/MTLCadbLM_g/image_thumb33.png?imgmax=800\\\" width=\\\"589\\\" height=\\\"368\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/jimlamb/archive/2009/11/18/how-to-create-a-custom-workflow-activity-for-tfs-build-2010.aspx\"]],[\"a\",[\"href\",\"http://vishaljoshi.blogspot.com/2010/05/applying-xdt-magic-to-appconfig.html\"]],[\"a\",[\"href\",\"http://cid-64a24e0938d6d062.office.live.com/self.aspx/Colin%5E4s%20ALM%20Corner/ClickOnceBuild/ClickOnceBuildActivityPack.zip\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"TeamBuild in TFS 2010 is an incredibly powerful engine – it’s based on Windows Workflow 4 (WF4). Once you get over the initial learning curve of WF4, you can get your team builds to do some impressive stuff.\"]]],[1,\"p\",[[0,[],0,\"One common scenario is supporting a ClickOnce deployment from a build. Essentially, you need to add an MSBuild activity into your build, tweaking the command line parameters that are passed to MSBuild. That’s not too difficult in and of itself – but I wanted to go further and tie the ClickOnce version into the build version.\"]]],[1,\"p\",[[0,[],0,\"To do that, I used a custom build activity from \"],[0,[0],1,\"Jim Lamb’s post on versioning assemblies\"],[0,[],0,\". However, I had to tweak the “UpdateVersionInfo” activity to accept 2 regex patterns instead of Jim’s 1. Jim uses the regex pattern to extract version info from the build number (so if your build is ‘MyBuild_4.0.0.2’ he extracts 4.0.0.2). He uses the same pattern to find/replace in the AssemblyInfo.* files (or whatever files you specify). Usually this is fine.\"]]],[1,\"h2\",[[0,[],0,\"Creating the Custom Assembly\"]]],[1,\"p\",[[0,[],0,\"However, the assembly version in the project file for a ClickOnce app can have a different format – if you select “increment revision whenever I deploy” in the Publish properties of a ClickOnce project, the version places a * in the revision number – making the version 1.0.0.* (or if you look in the project file, 1.0.0.%2a). So I needed to separate the “source” regex pattern (the pattern used to extract the version from the build number) and the “target” regex – the pattern used for the search/replace in the target files.\"]]],[1,\"p\",[[0,[],0,\"I opened the UpdateVersionInfo.xaml and added a new argument (called “SourceRegeEx”). I then changed the “Extract Version Info” assignment activity to use SourceRegex instead of Regex.\"]]],[10,0],[1,\"p\",[[0,[],0,\"In the custom build workflow (make sure you follow Jim’s method of copying and then branching your custom workflow) I used two UpdateVersionInfo activities – one to update the version info in the AssemblyInfo.* files and one to update the ClickOnce vcproj file (this is where the ClickOnce version resides).\"]]],[1,\"h2\",[[0,[],0,\"Customizing the Default Build Template\"]]],[1,\"p\",[[0,[],0,\"I stared from the DefaultTemplate.xaml added a number of arguments (I also updated the metadata to make the arguments prettier when you create builds):\"]]],[3,\"ul\",[[[0,[],0,\"VersioningFileSpec – the file search pattern to update assembly versions (AssemblyInfo.* by default)\"]],[[0,[],0,\"VersioningRegularExpression – the regex pattern to use to update assembly versions (“\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+” by default)\"]],[[0,[],0,\"ClickOnceDeployIfTestsFail – true to deploy even if tests fail, false otherwise\"]],[[0,[],0,\"ClickOncePublishDir – the UNC share to publish to\"]],[[0,[],0,\"ClickOncePublishURL – the ClickOnce URL (can be the same as the PublishDir)\"]],[[0,[],0,\"ClickOnceCertThumbprint – the thumbprint of the certificate for signing manifests (note: this certificate needs to be installed on any build agent machines – you can copy the pfx file (if you made a temporary certificate in VS) to the build machine and import it into the store by double clicking it – then use mmc to view the certificate and get its SHA1 for this field when you create a build)\"]],[[0,[],0,\"ClickOnceSolutionPath – the source control path of the solution that contains the ClickOnce project\"]],[[0,[],0,\"ClickOnceVersionFileSpec – the file search pattern to update ClickOnce version – usually the name of the csproj file that you want to publish – e.g. ClickOnceProject.csproj)\"]],[[0,[],0,\"ClickOnceVersionRegEx – the regex target pattern for doing ClickOnce version replacements (usually either “\\\\d+.\\\\d+\\\\.\\\\d+\\\\.%2a” or “\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+”)\"]]]],[1,\"p\",[[0,[],0,\"I then added two UpdateVersionInfo activities – one right after the “GetWorkspace Activity” in the Process->Overall Build Process->Run On Agent->Initialize Workspace sequence. I used the VersioningFileSpec and RegularExpression arguments and hardcoded the source regex to \\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+”.\"]]],[10,1],[1,\"p\",[[0,[],0,\"The next step was the ClickOnce versioning customizations. I located the step “If TestStatus = Unknown” in the “Try Compile and Test” sequence. Right after that activity I added a sequence activity called “ClickOnce Deployment”.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Here is the sequence and explanation:\"]]],[10,3],[3,\"ul\",[[[0,[],0,\"Add an “If” activity\"]],[[0,[],0,\"Condition: ClickOnceDeployIfTestsFail Or BuildDetail.TestStatus = BuildPhaseStatus.Succeeded\"]],[[0,[],0,\"This will ensure that the “deploy even if failed” condition is obeyed\"]],[[0,[],0,\"Add a “WriteBuildMessage” to the Else side of the If Activity to just log that tests failed and the deployment will be skipped\"]],[[0,[],0,\"Add a sequence to the “Then” side of the If\"]],[[0,[],0,\"Add an UpdateVersionInfo Activity (this will apply the build version to the ClickOnce deployment) and set the following:\"]],[[0,[],0,\"SourcesDirectory = SourcesDirectory\"]],[[0,[],0,\"FileSpec = ClickOnceVersionFileSpec\"]],[[0,[],0,\"SourceRegEx = \\\"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\"\"]],[[0,[],0,\"RegularExpression = ClickOnceVersionRegEx\"]],[[0,[],0,\"Add a ConvertWorkspaceItem (to convert the source path of the click once solution to a local path) and set:\"]],[[0,[],0,\"Direction = ServerToLocal\"]],[[0,[],0,\"Input = ClickOnceSolutionPath\"]],[[0,[],0,\"Result = localSolution (you’ll need to add this as a string variable on the sequence)\"]],[[0,[],0,\"Workspace = Workspace\"]],[[0,[],0,\"Add an MSBuild Activity to publish\"]],[[0,[],0,\"The only argument worthy of note here is the CommandLineArguments which should be set to:String.Format(\\\"/p:SkipInvalidConfigurations=true {0} /Target:Publish /property:PublishDir=\\\"\\\"{1}\\\"\\\" /property:PublishUrl=\\\"\\\"{2}\\\"\\\" /property:InstallUrl=\\\"\\\"{2}\\\"\\\" /property:SignManifests=true /property:ManifestCertificateThumbprint=\\\"\\\"{3}\\\"\\\"\\\", MSBuildArguments, ClickOncePublishDir, ClickOncePublishURL, ClickOnceCertThumbprint)\"]],[[0,[],0,\"Add a WriteBuildMessage to say that the deployment was executed\"]]]],[1,\"p\",[[0,[],0,\"Now you need to compile the custom activity project and put the dll into source control for your controller to find (again refer to Jim’s blog post).\"]]],[1,\"p\",[[0,[],0,\"Finally, you can set up a build:\"]]],[10,4],[1,\"p\",[[0,[],0,\"NOTE: If you have different configurations (for Production and Staging for example) use \"],[0,[1],1,\"Vishal Joshi’s blog\"],[0,[],0,\" to enable config transforms on your app.config file – then target the config you want in the build.\"]]],[1,\"p\",[[0,[],0,\"Download the project (including the DefaultClickOnce.xaml template – see the test project’s Templates folder) from my \"],[0,[2],1,\"skydrive\"],[0,[],0,\".\"]]]]}","published_at":1297036140000,"status":"published","published_by":1},{"id":"0f8e3e1a-5ab6-4101-85a9-0ad520d99372","title":"Code Coverage doesn’t work with Fakes on Hosted Build","slug":"code-coverage-doesnt-work-with-fakes-on-hosted-build","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/07/build-with-hosted-build-controller.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In my \"],[0,[0],1,\"post about hosted build\"],[0,[],0,\", I discovered that if you enable code coverage on unit tests that use the Fakes framework, the unit tests fail (even though the tests pass without code coverage turned on). The error is a “ShimNotSupportedException”.\"]]],[1,\"p\",[[0,[],0,\"I mailed the ChampsList, and it turns out that there is a problem with the hosted build servers for this scenario.In short, the problem has to do with the mix of RC and RTM on the build agent machines, which are running VS 2012 RC, and TfsPreview, that is running a later build (closer to RTM) of TFS.\"]]],[1,\"p\",[[0,[],0,\"When VS goes to RTM and the build agents are upgraded, this problem should go away. Until then, you’ll have to build the code on a local build server if you need code coverage and use the Fakes framework in your tests.\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1342206060000,"status":"published","published_by":1},{"id":"1df2bacc-0591-4168-9724-c9d9154075b9","title":"Code Monkey?","slug":"code-monkey","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/charles_sterling/archive/2012/03/01/code-monkey-presentation-style.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve just spent 4 days in Seattle at my first Global MVP Summit – it’s been great meeting a lot of the other ALM MVPs and putting faces to email addresses! It’s also been great getting an “inside scope” on some of the strategic directions that the TFS and VS product teams are taking.\"]]],[1,\"p\",[[0,[],0,\"On Tuesday, Chuck Sterling did a presentation and “snagged me” to be his live Code Monkey demonstrator (he spoke and I clicked around). Tiago Pascoal later joked about a t-shirt, and I whipped out my Photoshop and created a Code Monkey t-shirt, which I sent to Chuck. He blogged about it \"],[0,[0],1,\"in this post\"],[0,[],0,\".\"]]]]}","published_at":1330747920000,"status":"published","published_by":1},{"id":"615ed0fa-3132-44d7-b2b0-c06e9dd3867f","title":"Colin’s ALM Corner – New Theme and Live Tiles","slug":"colins-alm-corner--new-theme-and-live-tiles","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/colins-alm-corner--updated-blog-engine\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Last week I \"],[0,[0],1,\"updated my blog engine from Blogger to MiniBlog\"],[0,[],0,\". The last couple of days I’ve been updating the theme and style. Every good blog needs some good bling!\"]]],[1,\"p\",[[0,[],0,\"Apart from the new look and feel, you can now also pin my site to Windows 8 or Windows Phone 8.1 Preview. When you pin the tile, you get an update of the latest posts on the Live Tile.\"]]],[1,\"p\",[[0,[],0,\"Let me know what you think of the new design!\"]]],[1,\"p\",[[0,[],0,\"Happy reading!\"]]]]}","published_at":1398896974000,"status":"published","published_by":1},{"id":"1bbcc27b-a919-4085-b8fb-13972c6d09a0","title":"Colin’s ALM Corner – Updated Blog Engine","slug":"colins-alm-corner--updated-blog-engine","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"public class BloggerATOMFormatter\\n{\\n    public void Format(string originalFolderPath, string targetFolderPath)\\n    {\\n        FormatPosts(originalFolderPath, targetFolderPath);\\n    }\\n\\n    private void FormatPosts(string originalFolderPath, string targetFolderPath)\\n    {\\n        var oldPostList = new Dictionary&lt;string, string&gt;();\\n        foreach (string file in Directory.GetFiles(originalFolderPath, \\\"*.xml\\\").Where(s =&gt; !s.EndsWith(\\\"comments.xml\\\")))\\n        {\\n            var originalDoc = LoadDocument(file);\\n            XNamespace atomNS = @\\\"http://www.w3.org/2005/Atom\\\";\\n\\n            var entry = originalDoc.Element(atomNS + \\\"entry\\\");\\n\\n            var title = entry.Element(atomNS + \\\"title\\\").Value;\\n            var oldUrl = (from link in entry.Elements(atomNS + \\\"link\\\")\\n                          where link.Attributes().ToList().Any(a =&gt; a.Name == \\\"rel\\\" &amp;&amp; a.Value == \\\"alternate\\\")\\n                          select link).First().Attribute(\\\"href\\\").Value.Replace(\\\"http://www.colinsalmcorner.com\\\", \\\"\\\");\\n            \\n            var content = FixContent(entry.Element(atomNS + \\\"content\\\").Value);\\n            var publishDate = DateTime.Parse(entry.Element(atomNS + \\\"published\\\").Value);\\n            var lastModDate = DateTime.Parse(entry.Element(atomNS + \\\"updated\\\").Value);\\n            var slug = FormatterHelpers.FormatSlug(title);\\n            var categories = from cat in entry.Elements(atomNS + \\\"category\\\")\\n                             select cat.Attribute(\\\"term\\\").Value;\\n\\n            var post = new Post();\\n            post.Author = \\\"Colin Dembovsky\\\";\\n            post.Categories = categories.ToArray();\\n            post.Content = content;\\n            post.IsPublished = true;\\n            post.PubDate = publishDate;\\n            post.Title = title;\\n            post.Slug = slug;\\n            post.LastModified = lastModDate;\\n            post.Comments = GetCommentsForPost(file);\\n\\n            var newId = Guid.NewGuid().ToString();\\n            Storage.Save(post, Path.Combine(targetFolderPath, newId + \\\".xml\\\"));\\n            oldPostList[oldUrl] = newId;\\n        }\\n        SaveOldPostMap(targetFolderPath, oldPostList);\\n    }\\n\\n    private void SaveOldPostMap(string targetFolderPath, Dictionary&lt;string, string&gt; oldPostList)\\n    {\\n        var mapElement = new XElement(\\\"OldPostMap\\\");\\n        foreach(var key in oldPostList.Keys)\\n        {\\n            mapElement.Add(\\n                new XElement(\\\"OldPost\\\",\\n                    new XAttribute(\\\"oldUrl\\\", key),\\n                    new XAttribute(\\\"postId\\\", oldPostList[key])\\n                )\\n            );\\n        }\\n        var doc = new XDocument(mapElement);\\n        doc.Save(Path.Combine(targetFolderPath, \\\"oldPosts.map\\\"));\\n    }\\n\\n    private List&lt;Comment&gt; GetCommentsForPost(string file)\\n    {\\n        var commentsFile = file.Replace(\\\".xml\\\", \\\".comments.xml\\\");\\n        if (!File.Exists(commentsFile))\\n        {\\n            return new List&lt;Comment&gt;();  \\n        }\\n\\n        var commentsDoc = LoadDocument(commentsFile);\\n        XNamespace atomNS = @\\\"http://www.w3.org/2005/Atom\\\";\\n\\n        var list = new List&lt;Comment&gt;();\\n        foreach (var originalComment in commentsDoc.Descendants(atomNS + \\\"entry\\\"))\\n        {\\n            var authorElement = originalComment.Element(atomNS + \\\"author\\\");\\n            var name = authorElement.Element(atomNS + \\\"name\\\").Value;\\n            var email = authorElement.Element(atomNS + \\\"email\\\").Value;\\n            var uriElement = authorElement.Element(atomNS + \\\"uri\\\");\\n            string website = null;\\n            if (uriElement != null)\\n            {\\n                website = uriElement.Value;\\n            }\\n\\n            var content = originalComment.Element(atomNS + \\\"content\\\").Value;\\n            var publishDate = DateTime.Parse(originalComment.Element(atomNS + \\\"published\\\").Value);\\n\\n            var comment = new Comment();\\n            comment.Author = name;\\n            comment.Email = email;\\n            comment.PubDate = publishDate;\\n            comment.Content = content;\\n            comment.IsAdmin = false;\\n            comment.Website = website;\\n            list.Add(comment);\\n        }\\n\\n        return list.OrderBy(c =&gt; c.PubDate).ToList();\\n    }\\n\\n    private string FixContent(string originalContent)\\n    {\\n        var regex = new Regex(\\\"&lt;pre class=\\\\\\\"brush: \\\\\\\\w*;\\\\\\\"&gt;(.*?)&lt;/pre&gt;\\\", RegexOptions.IgnoreCase);\\n        foreach(Match match in regex.Matches(originalContent))\\n        {\\n            var formatted = match.Groups[1].Value.Replace(\\\"&lt;br /&gt;\\\", Environment.NewLine);\\n            originalContent = originalContent.Replace(match.Groups[1].Value, formatted);\\n        }\\n        return originalContent.Replace(\\\"&lt;p&gt;&lt;/p&gt;&lt;br /&gt;\\\", \\\"\\\").Replace(\\\"&lt;p&gt;&lt;/p&gt;\\\", \\\"\\\").Replace(\\\"&lt;h3&gt;\\\", \\\"&lt;h2&gt;\\\").Replace(\\\"&lt;/h3&gt;\\\", \\\"&lt;/h2&gt;\\\");\\n    }\\n\\n    private XDocument LoadDocument(string file)\\n    {\\n        return XDocument.Parse(File.ReadAllText(file));\\n    }\\n}\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public class OldPostHandler : IHttpHandler\\n{\\n    public bool IsReusable\\n    {\\n        get { return false; }\\n    }\\n\\n    public void ProcessRequest(HttpContext context)\\n    {\\n        var oldUrl = context.Request.RawUrl;\\n        var oldPost = Storage.GetOldPost(oldUrl);\\n\\n        if (oldPost == null)\\n        {\\n            throw new HttpException(404, \\\"The post does not exist\\\");\\n        }\\n\\n        var newUrl = \\\"/post/\\\" + oldPost.Slug;\\n        context.Response.Status = \\\"301 Moved Permanently\\\";\\n        context.Response.AddHeader(\\\"Location\\\", newUrl);\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public static Post GetOldPost(string url)\\n{\\n    var map = GetOldPostMap();\\n    if (map.ContainsKey(url))\\n    {\\n        return GetAllPosts().SingleOrDefault(p =&gt; p.ID == map[url]);\\n    }\\n    return null;\\n}\\n\\npublic static Dictionary&lt;string, string&gt; GetOldPostMap()\\n{\\n    GetAllPosts();\\n\\n    if (HttpRuntime.Cache[\\\"oldPostMap\\\"] != null)\\n    {\\n        return (Dictionary&lt;string, string&gt;)HttpRuntime.Cache[\\\"oldPostMap\\\"];\\n    }\\n    return new Dictionary&lt;string, string&gt;();\\n}\\n\\nprivate static void LoadOldPostMap()\\n{\\n    var map = new Dictionary&lt;string, string&gt;();\\n    var mapFile = Path.Combine(_folder, \\\"oldPosts.map\\\");\\n    if (File.Exists(mapFile))\\n    {\\n        var doc = XDocument.Load(mapFile);\\n        foreach (var mapping in doc.Descendants(\\\"OldPost\\\"))\\n        {\\n            var oldUrl = mapping.Attribute(\\\"oldUrl\\\").Value;\\n            var newId = mapping.Attribute(\\\"postId\\\").Value;\\n            map[oldUrl] = newId;\\n        }\\n    }\\n    HttpRuntime.Cache.Insert(\\\"oldPostMap\\\", map);\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"&lt;handlers&gt;\\n  &lt;remove name=\\\"CommentHandler\\\"/&gt;\\n  &lt;add name=\\\"CommentHandler\\\" verb=\\\"*\\\" type=\\\"CommentHandler\\\" path=\\\"/comment.ashx\\\"/&gt;\\n  &lt;remove name=\\\"PostHandler\\\"/&gt;\\n  &lt;add name=\\\"PostHandler\\\" verb=\\\"POST\\\" type=\\\"PostHandler\\\" path=\\\"/post.ashx\\\"/&gt;\\n  &lt;remove name=\\\"MetaWebLogHandler\\\"/&gt;\\n  &lt;add name=\\\"MetaWebLogHandler\\\" verb=\\\"POST,GET\\\" type=\\\"MetaWeblogHandler\\\" path=\\\"/metaweblog\\\"/&gt;\\n  &lt;remove name=\\\"FeedHandler\\\"/&gt;\\n  &lt;add name=\\\"FeedHandler\\\" verb=\\\"GET\\\" type=\\\"FeedHandler\\\" path=\\\"/feed/*\\\"/&gt;\\n  &lt;remove name=\\\"FeedsHandler\\\"/&gt;\\n  &lt;add name=\\\"FeedsHandler\\\" verb=\\\"GET\\\" type=\\\"FeedHandler\\\" path=\\\"/feeds/*\\\"/&gt;\\n  &lt;remove name=\\\"CssHandler\\\"/&gt;\\n  &lt;add name=\\\"CssHandler\\\" verb=\\\"GET\\\" type=\\\"MinifyHandler\\\" path=\\\"*.css\\\"/&gt;\\n  &lt;remove name=\\\"JsHandler\\\"/&gt;\\n  &lt;add name=\\\"JsHandler\\\" verb=\\\"GET\\\" type=\\\"MinifyHandler\\\" path=\\\"*.js\\\"/&gt;\\n  &lt;remove name=\\\"OldPostHandler\\\"/&gt;\\n  &lt;add name=\\\"OldPostHandler\\\" verb=\\\"GET\\\" type=\\\"OldPostHandler\\\" path=\\\"*.html\\\"/&gt;\\n&lt;/handlers&gt;\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"public static Dictionary&lt;string, int&gt; GetTags()\\n{\\n    var categories = Storage.GetAllPosts().SelectMany(p =&gt; p.Categories).Distinct();\\n    var tags = new Dictionary&lt;string, int&gt;();\\n    foreach(var cat in categories)\\n    {\\n        var count = Storage.GetAllPosts().Where(p =&gt; p.Categories.Any(c =&gt; c.Equals(cat, StringComparison.OrdinalIgnoreCase))).Count();\\n        tags[cat] = count;\\n    }\\n    return tags;\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"&lt;div id=\\\"tagcloud\\\"&gt;\\n    @{\\n        var tags = Blog.GetTags();\\n        foreach (var tag in tags.Keys)\\n        {\\n            &lt;a href=\\\"/category/@tag\\\" rel=\\\"@tags[tag]\\\"&gt;@tag&lt;/a&gt;\\n        }\\n    }\\n&lt;/div&gt;\\n\\n&lt;script type=\\\"text/javascript\\\"&gt;\\n    // tag cloud script\\n    $(\\\"#tagcloud a\\\").tagcloud({\\n        size: {\\n            start: 0.8,\\n            end: 1.75,\\n            unit: 'em'\\n        },\\n        color: {\\n            start: \\\"#7cc0f4\\\",\\n            end: \\\"#266ca2\\\"\\n        }\\n    });\\n&lt;/script&gt;\\n\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"public static List&lt;Post&gt; Search(string term)\\n{\\n    term = term.ToLower();\\n    return (from p in Storage.GetAllPosts()\\n            where p.Title.ToLower().Contains(term) || p.Content.ToLower().Contains(term) || p.Comments.Any(c =&gt; c.Content.ToLower().Contains(term))\\n            select p).ToList();\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"@{\\n    var term = Request.QueryString[\\\"term\\\"];\\n\\n    Page.Title = Blog.Title;\\n    Layout = \\\"~/themes/\\\" + Blog.Theme + \\\"/_Layout.cshtml\\\";\\n    \\n    if (string.IsNullOrEmpty(term))\\n    {\\n        &lt;h1&gt;Oops!&lt;/h1&gt;\\n        &lt;p&gt;Something went wrong with your search. Try again...&lt;/p&gt;\\n    }\\n    else\\n    {\\n        &lt;h1&gt;Results for search: '@term'&lt;/h1&gt;\\n        \\n        var list = Blog.Search(term);\\n        if (list.Count == 0)\\n        {\\n            &lt;p&gt;No matches...&lt;/p&gt;\\n        }\\n        else\\n        {\\n            foreach(var p in list)\\n            {\\n                @RenderPage(\\\"~/themes/\\\" + Blog.Theme + \\\"/PostSummary.cshtml\\\", p);\\n            }\\n        }\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"input {\\n    outline: none;\\n}\\ninput[type=search] {\\n    -webkit-appearance: textfield;\\n    -webkit-box-sizing: content-box;\\n    font-family: inherit;\\n    font-size: 80% !important;\\n}\\ninput::-webkit-search-decoration,\\ninput::-webkit-search-cancel-button {\\n    display: none; /* remove the search and cancel icon */\\n}\\n\\n/* search input field */\\ninput[type=search] {\\n    background: #ededed url(images/search-icon.png) no-repeat 9px center;\\n    border: solid 1px #ccc;\\n    padding: 5px 5px 5px 10px;\\n    width: 130px;\\n    \\n    -webkit-border-radius: 10em;\\n    -moz-border-radius: 10em;\\n    border-radius: 10em;\\n    \\n    -webkit-transition: all .5s;\\n    -moz-transition: all .5s;\\n    transition: all .5s;\\n}\\ninput[type=search]:focus {\\n    width: 100%;\\n    background-color: #fff;\\n    border-color: #6dcff6;\\n    \\n    -webkit-box-shadow: 0 0 5px rgba(109,207,246,.5);\\n    -moz-box-shadow: 0 0 5px rgba(109,207,246,.5);\\n    box-shadow: 0 0 5px rgba(109,207,246,.5);\\n}\\n\\n/* placeholder */\\ninput:-moz-placeholder {\\n    color: #999;\\n}\\ninput::-webkit-input-placeholder {\\n    color: #999;\\n}\\n\",\"language\":\"css;\"}],[\"code\",{\"code\":\"&lt;section&gt;\\n    &lt;br /&gt;\\n    &lt;form action=\\\"/search\\\" method=\\\"get\\\" role=\\\"form\\\" id=\\\"searchForm\\\"&gt;\\n        &lt;fieldset&gt;\\n            &lt;input type=\\\"search\\\" placeholder=\\\"Search this blog\\\" name=\\\"term\\\"&gt;\\n        &lt;/fieldset&gt;\\n    &lt;/form&gt;\\n    &lt;hr /&gt;\\n&lt;/section&gt;\\n\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"if (!context.User.Identity.IsAuthenticated)\\n{\\n    // was throwing 403 here\\n    FormsAuthentication.RedirectToLoginPage();\\n    return;\\n}\",\"language\":\"csharp;\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogger.com\"]],[\"a\",[\"href\",\"https://www.hanselman.com/\"]],[\"a\",[\"href\",\"http://madskristensen.net/\"]],[\"a\",[\"href\",\"https://github.com/madskristensen/miniblog\"]],[\"a\",[\"href\",\"http://bloggerbackup.codeplex.com/\"]],[\"a\",[\"href\",\"https://github.com/madskristensen/MiniBlogFormatter\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/bb387061.aspx\"]],[\"a\",[\"href\",\"http://alexgorbatchev.com/SyntaxHighlighter/\"]],[\"a\",[\"href\",\"https://github.com/addywaddy/jquery.tagcloud.js\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have been using \"],[0,[0],1,\"Blogger\"],[0,[],0,\" ever since I started my blog back in 2010. Once you get the template right (and set up a domain) it’s not a bad hosting platform. It works nicely with Windows Live Writer (as every self-respecting blog engine should). However, I felt it was time for a change – I wanted to take charge of my own blogging platform.\"]]],[1,\"p\",[[0,[],0,\"A couple of week’s ago I read a post by \"],[0,[1],1,\"Scott Hanselman\"],[0,[],0,\" about \"],[0,[2],1,\"Mad’s Krisensen’s\"],[0,[],0,\" \"],[0,[3],1,\"MiniBlog\"],[0,[],0,\" engine. I had a look and liked it instantly – but there was no way to port from Blogger to MiniBlog. So I left it to stew in the back of my mind (*ominous chuckle - BWAHAHAHA*).\"]]],[1,\"h2\",[[0,[],0,\"Porting to MiniBlog from Blogger\"]]],[1,\"p\",[[0,[],0,\"I finally had another look a few days ago to see if I could port my existing blog posts over. While there was no native way to do this, I found a util (\"],[0,[4],1,\"BloggerBackup\"],[0,[],0,\") that let me export my blog posts (in ATOM format). I promptly exported all my posts.\"]]],[1,\"p\",[[0,[],0,\"The next trick was to import them into MiniBlog format. Fortunately there’s a little util that converts from BlogEngine.NET (or WordPress) to MiniBlog called \"],[0,[5],1,\"MiniBlogFormatter\"],[0,[],0,\". I cloned the repo and wrote my own formatter. This wasn’t too hard – using some \"],[0,[6],1,\"Linq-to-XML\"],[0,[],0,\" I had something going pretty quickly. Here’s the code:\"]]],[10,0],[1,\"p\",[[0,[],0,\"There is a bit of “colinsALMcorner” specific code here, but if you’re looking to move from Blogger to MiniBlog you should be able to use most of this code. I had some issues with the formatting of the <pre> sections for \"],[0,[7],1,\"Syntax Highlighter\"],[0,[],0,\" – once I had that sorted, the formatter worked flawlessly.\"]]],[1,\"h2\",[[0,[],0,\"Redirecting Existing Posts\"]]],[1,\"p\",[[0,[],0,\"One of the challenges I had was what about search engines that already reference existing posts? Since I wanted to host MiniBlog on Azure and point my domain to the new site, I wanted to preserve any existing reference. However, the naming scheme for posts in Blogger is different from that in MiniBlog.\"]]],[1,\"p\",[[0,[],0,\"What I ended up doing was creating a map file as part of my convert-from-blogger-file-to-MiniBlog-file in the MiniBlogFormatter. I then created a simple HttpHandler that can server a “301 Moved Permanently” redirect when you hit an old post. Here’s the code:\"]]],[10,1],[1,\"p\",[[0,[],0,\"It’s small, neat and quick – keeping in line with the MiniBlog philosophy. Here’s the Storage.GetOldPost() method:\"]]],[10,2],[1,\"p\",[[0,[],0,\"GetAllPosts() add’s a call to LoadOldPostMap() which finds the map file and reads it into memory. I only have 87 posts, so it’s not too heavy.\"]]],[1,\"p\",[[0,[],0,\"Here’s the code to invoke the handler in web.config:\"]]],[10,3],[1,\"p\",[[0,[],0,\"You’ll see that I also added a “FeedsHandler” as well to work with the blogger feeds format, so that existing subscribers wouldn’t be affected by the switch (hopefully).\"]]],[1,\"p\",[[0,[],0,\"I then styled the site (since it’s based on bootstrap that wasn’t a problem). I also added a tag-cloud function and a search function. Both turned out to be really simple.\"]]],[1,\"h3\",[[0,[],0,\"Tag Cloud\"]]],[1,\"p\",[[0,[],0,\"I needed a method that would return all the categories and their frequency for the tag cloud. Here’s the code in the backend:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Next I had to find a way to present a tag cloud on the page using javascript. There are lots of ways of doing this – I ended up using this \"],[0,[8],1,\"jQuery tagcloud script\"],[0,[],0,\". Here’s the html for my tag cloud:\"]]],[10,5],[1,\"h3\",[[0,[],0,\"Search\"]]],[1,\"p\",[[0,[],0,\"I regularly search my own blog – it’s a “working journal” of sorts. Having a search function was pretty important to me. Again the solution was really simple. Here’s the search code:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Once I had the results, I created a new search.cshtml page that shows just the first few lines of the blog post:\"]]],[10,7],[1,\"p\",[[0,[],0,\"The final bit was to get a search control. I ended up doing one entirely in css:\"]]],[10,8],[1,\"p\",[[0,[],0,\"And here’s the search control in my side-bar:\"]]],[10,9],[1,\"h3\",[[0,[],0,\"Approve or Delete Comments from the Alert Mail\"]]],[1,\"p\",[[0,[],0,\"When someone writes a comment on a post, MiniBlog sends you an email. I like to moderate comments, so that’s how I’ve configured MiniBlog. In the mail there are 2 links – one to approve and one to delete the comment. However, I kept getting 403 “unauthorized” then clicking the links if I wasn’t logged in on the site. I made a small tweak to the CommentHandler Accept and Delete methods to redirect me to the login page instead of throwing a 403:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Now when I hit the link from my mail, I get redirected to the login screen. Once logged in, the comment is approved/deleted and all’s well.\"]]],[1,\"h2\",[[0,[],0,\"Publishing to Azure\"]]],[1,\"p\",[[0,[],0,\"After testing posting from Windows Live Writer (no issues there) I then published the site to Azure. I changed my DNS records from Blogger to Azure and hey presto – new site is up!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I’m really happy with the new look & feel and with the other modern web benefits (like SEO optimization and of course, speed) that MiniBlog brings. Thanks Mads!\"]]],[1,\"p\",[[0,[],0,\"I expect there may be a glitch or two for the switch over, but hopefully everything works well. Let me know in the comments if you experience any issues.\"]]],[1,\"p\",[[0,[],0,\"Happy reading!\"]]]]}","published_at":1398450368000,"status":"published","published_by":1},{"id":"fa6f24cc-c3eb-4880-bf7f-77e8fc93b968","title":"Config Per Environment vs Tokenization in Release Management","slug":"config-per-environment-vs-tokenization-in-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"{\\n    \\\"$schema\\\": \\\"http://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\\\",\\n    \\\"contentVersion\\\": \\\"1.0.0.0\\\",\\n    \\\"parameters\\\": {\\n        \\\"WebsiteName\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"PartsUnlimitedServerName\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"PartsUnlimitedHostingPlanName\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"CdnStorageAccountName\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"CdnStorageContainerName\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"CdnStorageAccountNameForDev\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"CdnStorageContainerNameForDev\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"CdnStorageAccountNameForStaging\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        },\\n        \\\"CdnStorageContainerNameForStaging\\\": {\\n            \\\"value\\\": \\\"\\\"\\n        }\\n    }\\n}\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/aa60a661-c377-4ffa-8db6-ccff14e51c38.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4ae9d98a-cb72-430c-ba46-59d5b7c0a367.png\\\" width=\\\"725\\\" height=\\\"256\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/1a9b2619-525f-49c9-9033-781e183fadb9.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a359a967-b5f7-48fd-8bbf-442ef8624828.png\\\" width=\\\"612\\\" height=\\\"235\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/28c3cfb6-c24f-4e50-b4a9-d6255443e10d.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/456da677-1025-40e2-81d3-1487394b8db0.png\\\" width=\\\"690\\\" height=\\\"224\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c8de9fec-749a-48ef-9066-5e5ec90827d6.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/7f716985-f73f-430b-861a-9b107ed084e4.png\\\" width=\\\"681\\\" height=\\\"224\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/fc295a17-a03a-4802-a4eb-23f7f79e95f5.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/44ff7e2a-2917-48d3-a9fd-9293c63bf063.png\\\" width=\\\"636\\\" height=\\\"288\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/webdeploy-configs-and-web-release-management\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vso-agent-tasks/tree/master/Tasks/AzureWebPowerShellDeployment\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/PartsUnlimited/blob/hands-on-labs/docs/HOL_PartsUnlimited_WebSite_Continuous_Deployment/HOL_Continuous_Deployment_Release_Management.md\"]],[\"a\",[\"href\",\"http://github.com/microsoft/partsunlimited\"]],[\"em\"],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/resource-group-authoring-templates/\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/PartsUnlimited/tree/master/env/PartsUnlimited.Environment/PartsUnlimited.Environment/Templates\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vso-agent-tasks/tree/master/Tasks/DeployAzureResourceGroup\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/ReplaceTokens\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/AzureWebDeploy\"]],[\"a\",[\"href\",\"https://octopus.com/\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vso-agent-tasks/tree/master/Tasks/IISWebAppDeployment\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In my \"],[0,[0],1,\"previous post\"],[0,[],0,\" I experimented with WebDeploy to Azure websites. My issue with the out-of-the-box \"],[0,[1],1,\"Azure Web App Deploy task\"],[0,[],0,\" is that you can specify the WebDeploy zip file, but you can’t specify any environment variables other than connection strings. I showed you how to tokenize your configuration and then use some PowerShell to get values defined in the Release to replace the tokens at deploy time. However, the solution still felt like it needed some more work.\"]]],[1,\"p\",[[0,[],0,\"At the same time that I was experimenting with Release Management in VSTS, I was also writing a \"],[0,[2],1,\"Hands On Lab for Release Management\"],[0,[],0,\" using the \"],[0,[3],1,\"PartsUnlimited repo\"],[0,[],0,\". While writing the HOL, I had some debate with the Microsoft team about how to manage environment variables. I like a clean separation between \"],[0,[4],1,\"build\"],[0,[],0,\" and \"],[0,[4],1,\"deploy\"],[0,[],0,\". To achieve that, I recommend tokenizing configuration, as I showed in my previous post. That way the build produces a single logical package (this could be a number of files, but logically it’s a single output) that has tokens instead of values for environment config. The deployment process then fills in the values at deployment time. The Microsoft team were advocating hard-coding environment variables and checking them into source control – \"],[0,[4],1,\"a la\"],[0,[],0,\" “infrastructure as code”. The debate, while friendly, quickly seemed to take on the the feel of an unwinnable debate like “Git merge vs rebase”. I think having both techniques in your tool belt is good, allowing you to select the one which makes sense for any release.\"]]],[1,\"h2\",[[0,[],0,\"Config Per Environment vs Tokenization\"]]],[1,\"p\",[[0,[],0,\"There are then (at least) two techniques for handling configuration. I’ll call them “config per environment” and “tokenization”.\"]]],[1,\"p\",[[0,[],0,\"In “config per environment”, you essentially hard-code a config file per environment. At deploy time, you overwrite the target environment config with the config from source control. This could be an xcopy operation, but hopefully something a bit more intelligent – like an \"],[0,[5],1,\"ARM Template\"],[0,[],0,\" param.json file. When you define an ARM template, you define parameters that are passed to the template when it is executed. You can also then define a param.json file that supplies the parameters. For example, look at the FullEnvironmentSetup.json and FullEnvironmentSetup.param.json file in \"],[0,[6],1,\"this folder\"],[0,[],0,\" of the PartsUnlimited repo. Here’s the param.json file:\"]]],[10,0],[1,\"p\",[[0,[],0,\"You can see how the parameters match the parameters defined in the template json file. In this case, since the repo is public, the values are just empty strings – but you can imagine how you could define “dev.param.json” and “staging.param.json” and so on – each environment gets its own param.json file. Then at deploy time, you specify to the release which param.json file to use for that environment in the \"],[0,[7],1,\"Deploy Azure Resource Group\"],[0,[],0,\" task.\"]]],[1,\"p\",[[0,[],0,\"I’m still not sure I like hard-coding values and committing them to source control. The Microsoft team argued that this is “config as code” – but I still think that defining values in Release Management constitutes config as code, even if the code isn’t committed into source control. I’m willing to concede if you’re deploying to Azure using ARM – but I don’t think too many people are at present. Also, there’s the issue of sensitive information going to source control – in this case, the template actually requires a password field (not defined in the param file) – are you going to hardcode usernames/passwords into source control? And even if you do, if you just want to change a value, you need to create a new build since there’s no way to use the existing build – which is probably not what you want!\"]]],[1,\"p\",[[0,[],0,\"Let’s imagine you’re deploying your web app to IIS instead of Azure. How do you manage your configuration in that case? “Use config transformations!” you cry. The problem – as I pointed out in my previous post – is that if you have a config transform for each environment, you have to build a package for each environment, since the transformation occurs at \"],[0,[4],1,\"build\"],[0,[],0,\" time, not at \"],[0,[4],1,\"deploy\"],[0,[],0,\" time. Hence my preference for a single transform that inserts tokens into the WebDeploy package at build time that can be filled in with actual values at deploy time. This is what I call “tokenization”.\"]]],[1,\"p\",[[0,[],0,\"So when do you use config-per-environment and when do you use tokenization? I think that if you’ve got ARM templates, use config-per-environment. It’s powerful and elegant. However, even if you’re using ARM, if you have numerous environments, and environment configs change frequently, you may want to opt for tokenization. When you use config-per-environment, you’ll have to queue a new build to get the new config files into the drop that the release is deploying – while tokenization lets you change the value in Release Management and re-deploy an existing package. So if you prefer not to rebuild your binaries just to change an environment variable, then use tokenization. Also, if you don’t want to store usernames/passwords or other sensitive data in source control, then tokenization is better – sensitive information can be masked in Release Management. Of course you could do a combination – storing some config in source code and then just using Release Management for defining sensitive values.\"]]],[1,\"h3\",[[0,[],0,\"Docker Environment Variables\"]]],[1,\"p\",[[0,[],0,\"As an aside, I think that Docker encourages tokenization. Think about how you wouldn’t hard-code config into the Dockerfile – you’d “assume” that certain environment variables are set. Then when you run an instance of the image, you would specify the environment variable values as part of the run command. This is (conceptually anyway) tokenization – the image has “placeholders” for the config that are “filled in” at deploy time. Of course, nothing stops you from specifying a Dockerfile per environment, but it would seem a bit strange to do so in the context of Docker.\"]]],[1,\"p\",[[0,[],0,\"You, dear reader, will have to decide which is better for yourself!\"]]],[1,\"h2\",[[0,[],0,\"New Custom Build Tasks – Replace Tokens and Azure WebDeploy\"]]],[1,\"p\",[[0,[],0,\"So I still like WebDeploy with tokenization – but the PowerShell-based solution I hacked out in my previous post still felt like it could use some work. I set about seeing if I could wrap the PowerShell scripts into custom Tasks. I also felt that I could improve on the arguments passed to the WebDeploy cmd file – specifically for Azure Web Apps. Why should you download the Web App publishing profile manually if you can specify credentials to the Azure subscription as a Service Endpoint? Surely it would be possible to suck down the publishing profile of the website automatically? So I’ve created two new build tasks – \"],[0,[8],1,\"Replace Tokens\"],[0,[],0,\" and \"],[0,[9],1,\"Azure WebDeploy\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"Replace Tokens Task\"]]],[1,\"p\",[[0,[],0,\"I love how \"],[0,[10],1,\"Octopus Deploy\"],[0,[],0,\" automatically replaces web.config keys if you specify matching environment variables in a deployment project. I did something similar in my previous post with some PowerShell. The \"],[0,[8],1,\"Replace Tokens task\"],[0,[],0,\" does exactly that – using some Regex, it will replace any matching token with the environment variable (if defined) in Release Management. It will work nicely on the WebDeploy SetParams.xml file, but could be used to replace tokens on any file you want. Just specify the path to the file (and optionally configure the Regex) and you’re done. This task is implemented in node, so it’ll work on any platform that the VSTS agent can run on.\"]]],[1,\"h3\",[[0,[],0,\"Azure WebDeploy Task\"]]],[1,\"p\",[[0,[],0,\"I did indeed manage to work out how to get the publishing username and password of an Azure website from the context of an Azure subscription. So now you drop a “Replace Tokens” task to replace tokens in the SetParams.xml file, and then drop an \"],[0,[9],1,\"Azure WebDeploy task\"],[0,[],0,\" into the Release. This looks almost identical to the out-of-the-box “Azure Web App Deployment” task except that it will execute the WebDeploy command using the SetParams.xml file to override environment variables.\"]]],[1,\"h2\",[[0,[],0,\"Using the Tasks\"]]],[1,\"p\",[[0,[],0,\"I tried the same hypothetical deployment scenario I used in my previous post – I have a website that needs to be deployed to IIS for Dev, to a staging deployment slot in Azure for staging, and to the production slot for Production. I wanted to use the same tokenized build that I produced last time, so I didn’t change the build at all. Using my two new tasks, however, made the Release a snap.\"]]],[1,\"h3\",[[0,[],0,\"Dev Environment\"]]],[1,\"p\",[[0,[],0,\"Here’s the definition in the Dev environment:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can see the “Replace Tokens” task – I just specified the path to the SetParams.xml file as the “Target File”. The environment variables look like this:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Note how I define the app setting (CoolKey), the connection string (EntityDB) and the site name (the IIS virtual directory name of the website). The “Replace Tokens” path finds the corresponding tokens and replaces them with the values I’ve defined.\"]]],[1,\"p\",[[0,[],0,\"To publish to IIS, I can just use the “Batch Script” task:\"]]],[10,3],[1,\"p\",[[0,[],0,\"I specify the path to the cmd file (produced by the build) and then add the arguments “/Y” to do the deployment (as opposed to a what-if) and use the “/M” argument to specify the IIS server I’m deploying to. Very clean!\"]]],[1,\"h3\",[[0,[],0,\"Staging and Production Environments\"]]],[1,\"p\",[[0,[],0,\"For the staging environment, I use the same “Replace Tokens” task. The variables, however, look as follows:\"]]],[10,4],[1,\"p\",[[0,[],0,\"The SiteName variable has been removed. This is because the “Azure WebDeploy” task will work out the site name internally before invoking WebDeploy.\"]]],[1,\"p\",[[0,[],0,\"Here’s what the Azure WebDeploy task looks like in Staging:\"]]],[10,5],[1,\"p\",[[0,[],0,\"The parameters are as follows:\"]]],[3,\"ul\",[[[0,[],0,\"Azure Subscription – the Azure subscription Service Endpoint – this sets the context for the execution of this task\"]],[[0,[],0,\"Web App Name – the name of the Web App in Azure\"]],[[0,[],0,\"Web App Location – the Azure region that the site is in\"]],[[0,[],0,\"Slot – the deployment slot (leave empty for production slot)\"]],[[0,[],0,\"Web Deploy Package Path – the path to the webdeploy zip, SetParams.xml and cmd files\"]]]],[1,\"p\",[[0,[],0,\"Internally, the task connects to the Azure subscription using the Endpoint credentials. It then gets the web app object (via the name) and extracts the publishing username/password and site name, taking the slot into account (the site name is different for each slot). It then replaces the SiteName variable in the SetParametes.xml file before calling WebDeploy via the cmd (which uses the zip and the SetParameters.xml file). Again, this looks really clean.\"]]],[1,\"p\",[[0,[],0,\"The production environment is the same, except that the Slot is empty, and the variables have production values.\"]]],[1,\"h3\",[[0,[],0,\"IIS Web Application Deployment Task\"]]],[1,\"p\",[[0,[],0,\"After my last post, a reader tweeted me to ask why I don’t use the out-of-the-box \"],[0,[11],1,\"IIS Web Application Deployment task\"],[0,[],0,\". The biggest issue I have with this task is that it uses WinRM to remote to the target machine and then invokes WebDeploy “locally” in the WinRM session. That means you have to install and configure WinRM on the target machine before deploying. On the plus side, it does allow you to specify the SetParameters.xml file and even override values at deploy time. It can work against Azure Web Apps too. You can use it if you wish – just remember to use the “Replace Tokens” task before to get environment variables into your SetParameters.xml file!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Whichever method you prefer – config per environment or tokenization – Release Management makes your choice a purely philosophical debate. Due to its customizable architecture, there’s not too much technical difference between the methods when it comes to defining the Release Definition. That, to my mind, assures me that Release Management in VSTS is a fantastic tool.\"]]],[1,\"p\",[[0,[],0,\"So make your choice and happy releasing!\"]]]]}","published_at":1450197469000,"status":"published","published_by":1},{"id":"2b4355d9-bfb3-4f92-bc2b-323249145854","title":"Configuring AAD Authentication to Azure SQL Databases","slug":"configuring-aad-authentication-to-azure-sql-databases","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"# Azure CLI 2.0\\naz ad sp create-for-rbac --name CoolAppSP --password SomeStrongPassword\\n\\n# PowerShell\\n# get the application we want a service principal for\\n$app = Get-AzureRmADApplication -DisplayNameStartWith MyDemoApp\\nNew-AzureRmADServicePrincipal -ApplicationId $app.ApplicationId -DisplayName CoolAppSP -Password SomeStrongPassword\\n\\n\",\"language\":\"plain\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/48392dc1-e100-4568-8122-413720245376.png\\\"><img width=\\\"245\\\" height=\\\"297\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5c7acf37-8eb9-4ff5-b582-05b5016600ee.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"CREATE USER [CoolAppSP] FROM EXTERNAL PROVIDER\\nEXEC sp_addrolemember 'db_owner', 'CoolAppSP'\\n\",\"language\":\"plain\"}],[\"code\",{\"code\":\"public async Task&lt;string&gt; GetAccessTokenAsync(string clientId, string clientSecret, string authority, string resource, string scope)\\n{\\n\\tvar authContext = new AuthenticationContext(authority, TokenCache.DefaultShared);\\n\\tvar clientCred = new ClientCredential(clientId, clientSecret);\\n\\tvar result = await authContext.AcquireTokenAsync(resource, clientCred);\\n\\n\\tif (result == null)\\n\\t{\\n\\t\\tthrow new InvalidOperationException(\\\"Could not get token\\\");\\n\\t}\\n\\n\\treturn result.AccessToken;\\n}\",\"language\":\"csharp\"}],[\"code\",{\"code\":\"public async Task&lt;SqlConnection&gt; GetSqlConnectionAsync(string tenantId, string clientId, string clientSecret, string dbServer, string dbName)\\n{\\n\\tvar authority = string.Format(\\\"https://login.windows.net/{0}\\\", tenantId);\\n\\tvar resource = \\\"https://database.windows.net/\\\";\\n\\tvar scope = \\\"\\\";\\n\\tvar token = await GetTokenAsync(clientId, clientSecret, authority, resource, scope);\\n\\n\\tvar builder = new SqlConnectionStringBuilder();\\n\\tbuilder[\\\"Data Source\\\"] = $\\\"{dbServer}.database.windows.net\\\";\\n\\tbuilder[\\\"Initial Catalog\\\"] = dbName;\\n\\tbuilder[\\\"Connect Timeout\\\"] = 30;\\n\\tbuilder[\\\"Persist Security Info\\\"] = false;\\n\\tbuilder[\\\"TrustServerCertificate\\\"] = false;\\n\\tbuilder[\\\"Encrypt\\\"] = true;\\n\\tbuilder[\\\"MultipleActiveResultSets\\\"] = false;\\n\\n\\tvar con = new SqlConnection(builder.ToString());\\n\\tcon.AccessToken = token;\\n\\treturn con;\\n}\",\"language\":\"csharp\"}],[\"code\",{\"code\":\"public class CoolAppDataContext : DbContext\\n{\\n\\tpublic CoolAppDataContext(SqlConnection con)\\n\\t\\t: base(con, true)\\n\\t{\\n\\t\\tDatabase.SetInitializer&lt;CoolAppDataContext&gt;(null);\\n\\t}\\n\\n\\tpublic DbSet&lt;Product&gt; Products { get; set; }\\n\\n\\t...\\n}\",\"language\":\"csharp\"}]],\"markups\":[[\"a\",[\"href\",\"https://blogs.msdn.microsoft.com/sqlsecurity/2016/02/09/token-based-authentication-support-for-azure-sql-db-using-azure-ad-auth/\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Azure SQL is a great service - you get your databases into the cloud without having to manage all that nasty server stuff. However, one of the problems with Azure SQL is that you have to authenticate using SQL authentication - a username and password. However, you can also authenticate \"],[0,[0],1,\"via Azure Active Directory (AAD) tokens\"],[0,[],0,\". This is analogous to integrated login using Windows Authentication - but instead of Active Directory, you're using AAD.\"]]],[1,\"p\",[[0,[],0,\"There are a number of advantages to AAD Authentication:\"]]],[3,\"ul\",[[[0,[],0,\"You no longer have to share logins since users log in with their AAD credentials, so auditing is better\"]],[[0,[],0,\"You can manage access to databases using AAD groups\"]],[[0,[],0,\"You can enable \\\"app\\\" logins via Service Principals\"]]]],[1,\"p\",[[0,[],0,\"In order to get this working, you need:\"]]],[3,\"ul\",[[[0,[],0,\"To enable AAD authentication on the Azure SQL Server\"]],[[0,[],0,\"A Service Principal\"]],[[0,[],0,\"Add logins to the database granting whatever rights required to the service principal\"]],[[0,[],0,\"Add code to get an auth token for accessing the database\"]],[[0,[],0,\"If you're using Entity Framework (EF), create a new constructor for your DbContext\"]]]],[1,\"p\",[[0,[],0,\"In this post I'll walk through creating a service principal, configuring the database for AAD auth, creating code for retrieving a token and configuring an EF DbContext for AAD auth.\"]]],[1,\"h2\",[[0,[],0,\"Create a Service Principal\"]]],[1,\"p\",[[0,[],0,\"Azure lets you configure service principals - these are like service accounts on an Active Directory. The advantage to this is that you can configure access to resources for the service and not have to worry about users leaving the org (or domain) and having to change creds and so on. Service principals get keys that can be rotated for better security too. You'll need the service principal when you configure your app to connect to the database.\"]]],[1,\"p\",[[0,[],0,\"You can create a service principal using the portal or you can do it easily using:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Of course you need to provide a proper strong password! Take a note of the servicePrincipalNames property - the one that looks like a GUID. We'll need this later.\"]]],[1,\"h2\",[[0,[],0,\"Configuring AAD on the Database\"]]],[1,\"p\",[[0,[],0,\"In order to use AAD against the SQL Server, you'll need to configure an AAD admin (user or group) for the database. You can do this in the portal by browsing to the Azure SQL Server (not the database) and clicking \\\"Active Directory Admin\\\". In the page that appears, click \\\"Set Admin\\\" and assign a user or group as the AAD admin.\"]]],[1,\"p\",[[0,[],0,\"Once you've done that, you need to grant Azure AD users (or groups) permissions in the databases (not the server). To do that you have to connect to the database using an Azure AD account. Open Visual Studio or SQL Server Management Studio and connect to the database as the admin (or a member of the admin group) using \\\"Active Directory Password Authentication\\\" or \\\"Azure Directory Integrated Authentication\\\" from the Authentication dropdown:\"]]],[10,1],[1,\"p\",[[0,[],0,\"If you don't see these options, then you'll need to update your SQL Management Studio or SSDT. If you're domain joined to the Azure Active Directory domain, you can use the integrated method - in my case my laptop isn't domain joined so I used the password method. For username and password, I used my Azure AD (org account) credentials. Once you're logged in and connected to the database, execute the following T-SQL:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Of course you'll use the name of the Service Principal you created earlier - the name for the Login is the same name as the service principal you created, or can be the email address of a specific user or group display name if you're granting access to specific AAD users or groups so that they can access the db directly. And of course the role doesn't have to be dbowner - it can be whatever role you need it to be.\"]]],[1,\"h2\",[[0,[],0,\"Authenticating using the Service Principal\"]]],[1,\"p\",[[0,[],0,\"There are a couple of pieces we need in order to authenticate an application to the Azure SQL database using AAD credentials. The first is a token (it's an OAuth token) that identifies the service principal. Secondly, we need to construct a database connection that uses the token to authenticate to the server.\"]]],[1,\"h3\",[[0,[],0,\"Retrieve a Token from AAD\"]]],[1,\"p\",[[0,[],0,\"To get a token, we'll need to call Azure AD and request one. For this, you'll need the Microsoft.IdentityModel.Clients.ActiveDirectory Nuget package.\"]]],[1,\"p\",[[0,[],0,\"Here's the code snippet I used to get a token from AAD:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: We need some information in order to get the token: ClientId and ClientSecret are from the service principal. The authority, resource and scope will need to be passed in too (more on this later).\"]],[[0,[],0,\"Line 3: We're getting a token from the \\\"authority\\\" or tenant in Azure\"]],[[0,[],0,\"Line 4: We create a new client credential using the id and secret of the \\\"client\\\" (in this case, the service principal)\"]],[[0,[],0,\"Line 5: We get a token for this client onto the \\\"resource\\\"\"]],[[0,[],0,\"Line 7: We throw if we don't get a token back\"]],[[0,[],0,\"Line 12: If we do get a token, return it to the caller\"]]]],[1,\"p\",[[0,[],0,\"The client id is the \\\"application ID\\\" of the service principal (the guid in the servicePrincipalNames property of the service principal). To get the secret, log in to the portal and click in the Active Directory blade. Click on \\\"App Registration\\\" and search for your service principal. Click on the service principal to open it. Click on Keys and create a key - make a note of the key so that you can add this to configurations. This key is the clientSecret that the GetAccessToken method needs.\"]]],[1,\"p\",[[0,[],0,\"For authority, you'll need to supply the URL to your Azure tenant. You can get this by running \\\"az account show\\\" (Azure CLI 2.0) or \\\"Get-AzureRmSubscription\\\" (PowerShell). Make a note of the tenantId of the subscription (it's a GUID). Once you have that, the authority is simply \\\"https://login.windows.net/{tenantId}\\\". The final piece of info required is the resource - for Azure SQL access, this is simply \\\"https://database.windows.net/\\\". The scope is just empty string - for databases, the security is configured per user (using the role assignments on the DB you configured earlier). The \"],[0,[1],1,\"authentication\"],[0,[],0,\" is done using Azure AD via the token - the database is doing \"],[0,[1],1,\"authorization\"],[0,[],0,\". In other words, Azure lets an Azure AD user in when they present a valid token - the database defines what the user can do once they're in via roles.\"]]],[1,\"h3\",[[0,[],0,\"Creating a SQL Connection\"]]],[1,\"p\",[[0,[],0,\"We've now got a way to get a token - so we can create a SQL Connection to the database. Here's a code snippet:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: All the info we need for the connection\"]],[[0,[],0,\"Lines 3 - 5: Prepare the info for the call to get the token\"]],[[0,[],0,\"Line 6: Get the access token\"]],[[0,[],0,\"Lines 8-15: Prepare the SQL connection string to the Azure SQL database - tweak the properties (like Connect Timeout) appropriately.\"]],[[0,[],0,\"Line 17: Create the connection\"]],[[0,[],0,\"Line 18: Inject the token into the connection object\"]]]],[1,\"p\",[[0,[],0,\"You'd now be able to use the connection just like you would any SqlConnection object.\"]]],[1,\"h2\",[[0,[],0,\"Entity Framework DataContext Changes\"]]],[1,\"p\",[[0,[],0,\"If you're using Entity Framework for data access, you'll notice there's no obvious way to use the SqlConnection object that's now configured to access the Azure SQL database. You'll need to create a constructor on your DbContext:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3: A constructor that accepts a SqlConnection object\"]],[[0,[],0,\"Line 4: Call the base constructor method\"]],[[0,[],0,\"Line 5: Override the initializer for the context's Database object\"]]]],[1,\"p\",[[0,[],0,\"Now you can use the above methods to construct a SqlConnection to an Azure SQL database using AAD credentials and pass it in to the DbContext - and you're good to go!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Configuring an application to use Azure AD credentials to connect to an Azure SQL database is straightforward once you have all the pieces in place. There's some configuration you need to ensure is in place, but once it's configured you can stop using SQL Authentication to access your cloud databases - and that's a win!\"]]],[1,\"p\",[[0,[],0,\"Happy connecting!\"]]]]}","published_at":1503381820000,"status":"published","published_by":1},{"id":"3685326d-516d-4b11-bfc5-b886ef9aa339","title":"Container DevOps: Beyond Build (Part 1)","slug":"container-devops-beyond-build-part-1","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-2---traefik\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-3---canary-testing\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-4---telemetry-with-prometheus\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-5---prometheus-operator\"]],[\"a\",[\"href\",\"https://github.com/microsoft/partsunlimited\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/net-core-multi-stage-dockerfile-with-test-and-code-coverage-in-azure-pipelines\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/testing-in-production-routing-traffic-during-a-release\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/ingress/\"]],[\"a\",[\"href\",\"https://istio.io/\"]],[\"a\",[\"href\",\"https://linkerd.io/\"]],[\"a\",[\"href\",\"https://traefik.io/\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview\"]],[\"a\",[\"href\",\"https://prometheus.io/\"]],[\"a\",[\"href\",\"https://grafana.com/\"]],[\"a\",[\"href\",\"https://github.com/coreos/prometheus-operator\"]],[\"a\",[\"href\",\"https://prometheus.io/docs/alerting/alertmanager/\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/kubernetes-service/\"]],[\"a\",[\"href\",\"https://helm.sh/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series:\"]]],[3,\"ul\",[[[0,[],0,\"Part 1: Intro (this post)\"]],[[0,[0],1,\"Part 2: Traefik Basics\"]],[[0,[1],1,\"Part 3: Canary Testing\"]],[[0,[2],1,\"Part 4: Telemetry with Prometheus\"]],[[0,[3],1,\"Part 5: Prometheus Operator\"]]]],[1,\"p\",[[0,[],0,\"I've written before that I think that containers - and Kubernetes (k8s) - are the way of the future. I was fortunate enough to attend my first KubeCon last year in Seattle, and I was happy to see the uptake of k8s and the supporting cloud native technologies around k8s are robust and healthy. But navigating the myriad of services, libraries and techniques is a challenge! This is going to be the first in a series of posts about Container DevOps - and I don't just mean building images and deploying them. What about monitoring? And A/B testing? And all the other stuff that successful DevOps teams are supposed to be doing? We'll look at how you can implement some of these tools and techniques in this series.\"]]],[1,\"h2\",[[0,[],0,\"PartsUnlimited 1.0\"]]],[1,\"p\",[[0,[],0,\"For the last couple of years I've had the opportunity to demo DevOps using Azure DevOps probably a few hundred times. I built a demo in Azure DevOps using a fork of Microsoft's \"],[0,[4],1,\"PartsUnlimited repo\"],[0,[],0,\". When I originally built the demo, the .NET Core tooling was a bit of a mess, so I just stuck with the full framework version. The demo targets Azure App Services and shows how you can make a change in code, then submit a Pull Request, which triggers a build that compiles, runs static code analysis and unit tests and triggers a release, which deploys the new version of the app to a staging slot in the Azure web app, routing some small percentage of all traffic to the slot for canary testing. All the while metrics are being collected in Application Insights, and after doing the canary deployment, you can analyze the metrics and decide if the canary is successful or not - and then either promote it to the rest of the site or just shift all traffic to the existing prod version in the case of a failure.\"]]],[1,\"p\",[[0,[],0,\"But how do you do the same sort of thing with k8s? That's what I set out to discover. But before we get there, let's take a step back and consider what we should be investigating in the world of Container DevOps in the first place!\"]]],[1,\"h2\",[[0,[],0,\"Components of Container DevOps\"]]],[1,\"p\",[[0,[],0,\"Here are some of the components of Container DevOps that I think need to be considered:\"]]],[3,\"ol\",[[[0,[],0,\"Building Quality - including multi-stage image builds, reportable unit testing and publishing to secure image repositories\"]],[[0,[],0,\"Environment isolation - isolating Dev, Test and Prod environments\"]],[[0,[],0,\"Canary Testing - testing changes on a small set or percentage of users before promoting to everyone (also called Testing in Production or Blue/Green testing)\"]],[[0,[],0,\"Monitoring - producing, consuming and analyzing metrics about your services\"]],[[0,[],0,\"Security - securing your services using TLS\"]],[[0,[],0,\"Resiliency - making services resilient through throttling or circuit-breakers\"]],[[0,[],0,\"Infrastructure and Configuration as Code\"]]]],[1,\"p\",[[0,[],0,\"There are some more that I think should be on the list that I haven't yet gotten to exploring in detail - such as vulnerability scanning. Hopefully I get around to adding to the above list, but we'll start here for now.\"]]],[1,\"h3\",[[0,[],0,\"Building Quality\"]]],[1,\"p\",[[0,[],0,\"I've \"],[0,[5],1,\"previously blogged\"],[0,[],0,\" about how to run unit tests - and publish the test and code coverage results - in Azure DevOps pipelines. It was a good exercise, but as I look at it now I realize why I prefer to build code outside the container and copy the binaries in: it's hard to do ancillary work (like unit test, code scans etc.) in a Dockerfile. One advantage to the multi-stage Dockerfile that you'll lose is the dependency management - you have to manage that on the build machine (or container) if you're building the code outside the Dockerfile. But I think the dependency management ends up being simpler than trying to run (and publish) tests and test coverage and static analysis inside the Dockerfile. My post covered how to do unit testing/code coverage, but when I thought about adding SonarQube analysis or vulnerability scanning with WhiteSource, I realized the Dockerfile starts becoming clumsy. I think it's easier to just drop in the SonarQube and WhiteSource tasks into a pipeline and build on the build machine - and then just have a Dockerfile copy the compiled binaries in to create the final light-weight container image.\"]]],[1,\"h3\",[[0,[],0,\"Environment Isolation\"]]],[1,\"p\",[[0,[],0,\"There are a couple of ways to do this: the most isolated (and expensive) is to spin up a k8s cluster per environment - but you can achieve good isolation using k8s namespaces. You could also use a combination: have a Prod cluster and a Dev/Test cluster that uses namespaces to separate environments within that cluster.\"]]],[1,\"h3\",[[0,[],0,\"Canary Testing\"]]],[1,\"p\",[[0,[],0,\"This one took a while for me to wrap my head around. Initially I struggled with this concept because I was too married to the Azure Web App version, which works as follows:\"]]],[3,\"ol\",[[[0,[6],1,\"Traffic Manager\"],[0,[],0,\" routes 0% traffic to the \\\"blue\\\" slot\"]],[[0,[],0,\"Deploy the new build to the blue slot - it doesn't matter if this slot is down, since no traffic is incoming anyway\"]],[[0,[],0,\"Update Traffic Manager to divert some percentage of traffic to the blue slot\"]],[[0,[],0,\"Monitor metrics\"]],[[0,[],0,\"If successful, swap the \\\"blue\\\" slot with the production slot (an instantaneous action) and update Traffic Manager to route 100% of traffic to the new production slot\"]]]],[1,\"p\",[[0,[],0,\"I started trying to achieve the same thing in k8s, but k8s doesn't have the notion of slots. An easy enough solution is to have a separate Deployment (and Service) for \\\"blue\\\" and \\\"green\\\" versions. But then there's no Traffic Manager - so I stated investigating various \"],[0,[7],1,\"Ingresses\"],[0,[],0,\" and Ingress Controllers to see if I could get the same sort of functionality. I initially got a POC running in \"],[0,[8],1,\"Istio\"],[0,[],0,\" - but was still \\\"swapping slots\\\" in my mental model. Unfortunately, Istio, while very capable, is large and complicated - it felt like a sledge-hammer when all I needed was a screwdriver. I then tried \"],[0,[9],1,\"linkerd\"],[0,[],0,\" - which was fine until I hit some limitations. Finally, I tried \"],[0,[10],1,\"Traefik\"],[0,[],0,\" - and I found I could do everything I wanted to (and more) using Traefik. There's definitely a follow on post here detailing this part of my Container DevOps journey - so stay tuned!\"]]],[1,\"p\",[[0,[],0,\"The other mental breakthrough came when I realized that Deployments (unlike slots in Azure Web Apps) are inherently highly available: that is, I can deploy a new version of a Deployment and k8s automatically does rolling updates to ensure the Deployment is never \\\"down\\\". So I didn't have to worry about diverting traffic away from the \\\"blue\\\" Deployment while I was busy updating it - and I can even keep the traffic split permanent. What that means is that I have two versions of the Deployment/Service: a \\\"blue\\\" one and a \\\"green\\\" one. I set up traffic splitting using Traefik and route say 20% of traffic to the \\\"blue\\\" service. When rolling out a new version, I simply update the tag of the image in the Deployment and k8s automatically does a rolling update for me - the blue slot never goes down and it's \\\"suddenly\\\" on the new version. Then I can monitor metrics, and if successful, I can update the \\\"green\\\" Deployment. Now both Blue and Green Deployments are on the new version, so while the Blue Deployment is getting 20% of the traffic, the versions are the same, so 100% of the traffic is now on the new version - and I had zero downtime! Of course I can simply revert the Blue Deployment back to the old version to get the same effect if the experiment is NOT a success.\"]]],[1,\"p\",[[0,[],0,\"One more snippet to whet your appetite for future posts on this topic: Traefik also handles TLS certs via LetsEncrypt, circuit-breaking and more.\"]]],[1,\"h3\",[[0,[],0,\"Monitoring\"]]],[1,\"p\",[[0,[],0,\"I am a huge fan of \"],[0,[11],1,\"Application Insights\"],[0,[],0,\" - and there's no reason not to continue logging to AppInsights from within your containers - assuming your containers can reach out to Azure. However, I wanted to see how I could do monitoring completely \\\"in-cluster\\\" and so I turned to \"],[0,[12],1,\"Prometheus\"],[0,[],0,\" and \"],[0,[13],1,\"Grafana\"],[0,[],0,\". This is also a subject for another blog post, but I managed to (without too much hassle) get the following to work:\"]]],[3,\"ol\",[[[0,[],0,\"Export Prometheus metrics from .NET Core containers\"]],[[0,[],0,\"Create a \"],[0,[14],1,\"Prometheus Operator\"],[0,[],0,\" in my k8s cluster to easily (and declaratively) add Prometheus scraping to new deployments\"]],[[0,[],0,\"Create custom dashboards in Grafana\"]],[[0,[],0,\"Export the dashboards to code so that I can deploy them from scratch for new clusters\"]]]],[1,\"p\",[[0,[],0,\"I didn't explore \"],[0,[15],1,\"AlertManager\"],[0,[],0,\" - but this would be essential for a complete monitoring solution. However, the building blocks are in place. I also found that \\\"business telemetry\\\" is difficult in Prometheus. By business telemetry I mean telemetry that has nothing to do with performance - things like how many products from category A were sold in a particular date range? AppInsights made \\\"business telemetry\\\" a breeze - the closest I could get in Prometheus was some proxy telemetry that gave me some idea of what was happening from a \\\"business\\\" perspective. Admittedly, Prometheus is a performance metric framework, so I wasn't too surprised.\"]]],[1,\"h3\",[[0,[],0,\"Security\"]]],[1,\"p\",[[0,[],0,\"There's a whole lot to security that I didn't fully explore - especially in-cluster Role Based Access Control (RBAC). What I did explore was how to secure your services using certificates - and how to do that declaratively and easily as you roll out a publicly accessible service. Again Traefik made this simple - I'll detail how I did it in my Traefik blogpost. As a corollary, I did also isolate \\\"internal\\\" from \\\"external\\\" services - the internal services are not accessible from outside the cluster at all - the simplest way to secure a service!\"]]],[1,\"h3\",[[0,[],0,\"Resiliency\"]]],[1,\"p\",[[0,[],0,\"I've already mentioned how using Deployments with rolling updates gives me zero-downtime deployments \\\"for free\\\". But what about throttling services that are being overwhelmed with connections? Or circuit-breakers? Again Traefik came to the rescue - and again, details are coming up in a post dedicated to how I configured Traefik.\"]]],[1,\"h3\",[[0,[],0,\"Infrastructure as Code\"]]],[1,\"p\",[[0,[],0,\"It (almost) goes without saying that all of these capabilities should be doable from scripts and templates - and that there shouldn't be any manual steps. I did that from the first - using \"],[0,[16],1,\"Azure CLI\"],[0,[],0,\" scripts to spin up my \"],[0,[17],1,\"Azure Kubernetes Service\"],[0,[],0,\" (AKS) clusters, and configure Public IPs and Load Balancers. I used some bash scripts for doing kubectl commands and finally used \"],[0,[18],1,\"Helm\"],[0,[],0,\" for packaging my applications so that deployment is a breeze - including creating Ingresses, ServiceMonitors, Deployments, Secrets and all the pieces you need to run your services.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I know I haven't showed very much - but I've gotten all of the pieces in place and will be blogging about how I did it - and sharing the code too! The point is that Container DevOps is more than just building images - there is far involved to do mature DevOps, and it's possible to achieve all of these mature practices using k8s. Traefik is definitely the star of the show! For now, I've hopefully prodded you into thinking about how to do some of these practices yourself.\"]]],[1,\"p\",[[0,[],0,\"Happy deploying!\"]]]]}","published_at":1557308177000,"status":"published","published_by":1},{"id":"af19a9ac-9c7a-4c8c-9bc0-08d34c2546ba","title":"Container DevOps Beyond Build: Part 2 - Traefik","slug":"container-devops-beyond-build-part-2---traefik","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">--set</font>\"}],[\"code\",{\"code\":\"image: traefik\\nimageTag: 1.7.7\\nserviceType: NodePort\\n\\nkubernetes:\\n  ingressClass: \\\"dev-traefik-internal\\\"\\nssl:\\n  enabled: false\\n\\nacme:\\n  enabled: false\\n\\ndashboard:\\n  enabled: true\\n  domain: traefik-internal-ui.aks\\nrbac:\\n  enabled: true\\nmetrics:\\n  prometheus:\\n    enabled: true\\n    restrictAccess: false\\n  datadog:\\n    enabled: false\\n  statsd:\\n    enabled: false\\n\",\"language\":\"plain\"}],[\"code\",{\"code\":\"image: traefik\\nimageTag: 1.7.7\\nserviceType: LoadBalancer\\nloadBalancerIP: \\\"101.22.98.189\\\"\\n\\nkubernetes:\\n  ingressClass: \\\"dev-traefik-external\\\"\\nssl:\\n  enabled: true\\n  enforced: true\\n  permanentRedirect: true\\n  upstream: false\\n  insecureSkipVerify: false\\n  generateTLS: false\\n\\nacme:\\n  enabled: true\\n  email: myemail@somewhere.com\\n  onHostRule: true\\n  staging: false\\n  logging: false\\n  domains:\\n    enabled: true\\n    domainsList:\\n      - main: \\\"mycoolaks.westus.cloudapp.azure.com\\\"\\n  challengeType: tls-alpn-01\\n  persistence:\\n    enabled: true\\n    \\ndashboard:\\n  enabled: true\\n  domain: traefik-external-ui.aks\\n  \\nrbac:\\n  enabled: true\\nmetrics:\\n  prometheus:\\n    enabled: true\\n    restrictAccess: false\\n  datadog:\\n    enabled: false\\n  statsd:\\n    enabled: false\\n\\ntracing:\\n  enabled: false\\n\",\"language\":\"plain\"}],[\"code\",{\"code\":\"apiVersion: v1\\nkind: Service\\nmetadata:\\n  name: dev-partsunlimitedwebsite\\n  namespace: dev\\nspec:\\n  type: NodePort\\n  selector:\\n    app: partsunlimited-website\\n    function: web\\n  ports:\\n  - name: http\\n    port: 80\\n\",\"language\":\"plain;\"}],[\"code\",{\"code\":\"apiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n  annotations:\\n    kubernetes.io/ingress.class: dev-traefik-external\\n  labels:\\n    app: partsunlimited-website\",\"language\":\"plain;\"}],[\"code\",{\"code\":\"    function: web\\n  name: dev-website-ingress\\n  namespace: dev\\nspec:\\n  rules:\\n  - host: mycoolaks.westus.cloudapp.azure.com\\n    http:\\n      paths:\\n      - backend:\\n          serviceName: dev-partsunlimitedwebsite\\n          servicePort: http\\n        path: /site\\n\",\"language\":\"plain;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ce3df455-3a45-4f06-90bc-c38e3b102abf.png\\\" target=\\\"_blank\\\"><img width=\\\"460\\\" height=\\\"318\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a0c1e4b5-1a1c-4088-93a8-4c079e55f85b.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-1\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-3---canary-testing\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-4---telemetry-with-prometheus\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-5---prometheus-operator\"]],[\"a\",[\"href\",\"https://github.com/microsoft/partsunlimited\"]],[\"a\",[\"href\",\"https://www.iis.net/downloads/microsoft/web-deploy\"]],[\"a\",[\"href\",\"https://www.npmjs.com/\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/reference/builder/\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/net-core-multi-stage-dockerfile-with-test-and-code-coverage-in-azure-pipelines\"]],[\"a\",[\"href\",\"https://www.sonarqube.org/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/\"]],[\"a\",[\"href\",\"https://istio.io/\"]],[\"a\",[\"href\",\"https://linkerd.io/\"]],[\"a\",[\"href\",\"https://traefik.io/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#how-pods-manage-multiple-containers\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/testing-in-production-routing-traffic-during-a-release\"]],[\"a\",[\"href\",\"https://letsencrypt.org/\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/aks/static-ip\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/istio-abtest\"]],[\"a\",[\"href\",\"https://istio.io/docs/reference/config/istio.networking.v1alpha3/#VirtualService\"]],[\"a\",[\"href\",\"https://linkerd.io/1/advanced/dtabs/\"]],[\"a\",[\"href\",\"https://www.datadoghq.com/\"]],[\"a\",[\"href\",\"https://zipkin.io/\"]],[\"a\",[\"href\",\"https://www.jaegertracing.io/\"]],[\"a\",[\"href\",\"https://prometheus.io/\"]],[\"a\",[\"href\",\"https://grafana.com/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/ingress/\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/helm/charts/tree/master/stable/traefik\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/kubernetes-service/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/service/#nodeport\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1: Intro\"]],[[0,[],0,\"Part 2: Traefik (this post)\"]],[[0,[1],1,\"Part 3: Canary Testing\"]],[[0,[2],1,\"Part 4: Telemetry with Prometheus\"]],[[0,[3],1,\"Part 5: Prometheus Operator\"]]]],[1,\"p\",[[0,[],0,\"In Part 1 of this series, I outlined some of my goals and some of the thinking around what I think Container DevOps is - it's far more than just being able to build and run a container or two. Beyond just automating builds, you have to think about how you're going to release. Zero downtime and canary testing, resiliency and monitoring are all table stakes - but while I understand how to do that using Azure Web Apps, I hadn't done a lot of these for containerized applications. After working for a couple of months on a Kubernetes (k8s) version of .NET Core \"],[0,[4],1,\"PartsUnlimited\"],[0,[],0,\", I have some thoughts to share on how I managed to put these practices into place.\"]]],[1,\"p\",[[0,[],0,\"When thinking about running containers in production, you have to think about the end to end journey, starting at building images right through deployment and into monitoring and tracing. I'm a firm believer in building quality into the pipeline early, so automated builds should unit test (with code coverage), do static analysis and finally package applications. In \\\"traditional web\\\" builds, the packaging usually means a zip or \"],[0,[5],1,\"WebDeploy\"],[0,[],0,\" package or \"],[0,[6],1,\"NPM\"],[0,[],0,\" package or even just a drop of files. When building container images, you're inevitably using a \"],[0,[7],1,\"Dockerfile\"],[0,[],0,\" - which makes compiling and packaging simple, but leaves a lot to be desired when you want to test code or do static analysis, package scanning and other quality controls. I've already blogged about how I was able to \"],[0,[8],1,\"add unit testing and code coverage to a multi-stage Dockerfile\"],[0,[],0,\" - I just got \"],[0,[9],1,\"SonarQube\"],[0,[],0,\" working too, so that's another post in the works.\"]]],[1,\"h2\",[[0,[],0,\"Works In My Orchestrator ™\"]]],[1,\"p\",[[0,[],0,\"However, assume that we have an image in our container registry that we want to deploy. You've probably run that image on your local machine to make sure it at least starts up and exposes the right ports, so it works on your machine. Bonus points if you ran it in a development Kubernetes cluster! But now how do you deploy this new container to a production cluster? If you just use k8s Deployment \"],[0,[10],1,\"rolling update\"],[0,[],0,\" strategy. you'll get zero-downtime for free, since k8s brings up the new container and replaces the existing ones only when the new ones are ready (assuming you have good liveness and readiness probes defined). But how do you test the new version for only a small percentage of users? Or secure traffic to that service? Or if you're deploying multiple services (microservices anyone?) how do you monitor traffic flow in the service mesh? Or cut out \\\"bad\\\" services so that they don't crash your entire system?\"]]],[1,\"p\",[[0,[],0,\"With these questions in mind, I started to investigate how one does these sorts of things with deployments to k8s. The rest of this post is about my experiences.\"]]],[1,\"h2\",[[0,[],0,\"Ops Criteria\"]]],[1,\"p\",[[0,[],0,\"Here's the list of criteria I had in mind to cover - and I'll evaluate three tools using these criteria:\"]]],[3,\"ol\",[[[0,[],0,\"Internal and External Routing - I want to be able to define how traffic \\\"external\\\" traffic (traffic originating outside the cluster) and \\\"internal\\\" traffic (traffic originating and terminating within the cluster) is routed between services.\"]],[[0,[],0,\"Secure Communication - I want communication to endpoints to be secure - especially external traffic.\"]],[[0,[],0,\"Traffic Shifting - I want to be able to shift traffic between services - especially for canary testing.\"]],[[0,[],0,\"Resiliency - I want to be able to throttle connections or implement circuit breaking to keep my app as a whole resilient.\"]],[[0,[],0,\"Tracing - I want to be able to see what's going on across my entire application.\"]]]],[1,\"p\",[[0,[],0,\"I explored three tools: \"],[0,[11],1,\"Istio\"],[0,[],0,\", \"],[0,[12],1,\"Linkerd\"],[0,[],0,\" and \"],[0,[13],1,\"Traefik\"],[0,[],0,\". I'll evaluate each tool against the five criteria above.\"]]],[1,\"p\",[[0,[],0,\"Spoiler: Traefik won the race!\"]]],[1,\"p\",[[0,[],0,\"Disclaimer: some of these tools do more than these five things, so this isn't a wholistic showdown between these tools - it's a showdown over these five criteria only. Also, Traefik is essentially a reverse proxy on steroids, while Istio and Linkerd are service meshes - so you may need some functionality of a service mesh that Traefik can't provide.\"]]],[1,\"h3\",[[0,[],0,\"Internal and External Routing\"]]],[1,\"p\",[[0,[],0,\"All three tools are capable of routing traffic. Istio and Linkerd both inject \"],[0,[14],1,\"sidecar\"],[0,[],0,\" proxies to your containers. I like this approach since you can abstract away the communication/traffic/monitoring from your application code. This seemed to be promising, and while I was able to get some of what I wanted using both Istio and Linkerd, both had some challenges. Firstly, Istio is huge, rich and complicated. It has a lot of \"],[0,[15],1,\"Custom Resource Definitions\"],[0,[],0,\" (CRDs) - more than k8s itself in fact! So while it worked for routing like I wanted, it seemed very heavy. Linkerd worked for external routing, but due to limitations in the current implementation, I couldn't get it working to route internal traffic.\"]]],[1,\"p\",[[0,[],0,\"Let's say you have a website and make a code change - you want to test that in production - but only to a small percentage of users. With Azure App Services, you can use \"],[0,[16],1,\"Traffic Manager and deployment slots\"],[0,[],0,\" for this kind of canary testing. Let's say you get the \\\"external\\\" routing working - most clients connecting to your website get the original version, while a small percentage get the new version. This is what I mean by \\\"external\\\" traffic. But what if you have a microservice architecture and your website code is calling internal services which call other internal services? Surely you want to be able to do the same sort of traffic shifting - that's \\\"internal\\\" routing - routing traffic internally within the cluster. Linkerd couldn't do that for me - mostly due to incompatibility between the control plane and the sidecars, I think.\"]]],[1,\"p\",[[0,[],0,\"Traefik did this easily via Ingress Controllers (abstractions native to k8s). I set up two controllers - one to handle \\\"external\\\" traffic and one to handle \\\"internal\\\" traffic - and it worked beautifully. More on this later.\"]]],[1,\"h3\",[[0,[],0,\"Secure Communication\"]]],[1,\"p\",[[0,[],0,\"I didn't explore this topic too deeply with either Istio or Linkerd, but Traefik made securing external endpoints with certificates via \"],[0,[17],1,\"LetsEncrypt\"],[0,[],0,\" really easy. I tried to get secure communication for my internal services, but I was trying with a self-signed cert and I think that's what prevented it from working. I'm sure that you could just as easily add this capability into internal traffic using Traefik if you really needed to. We'll see this later too - but using a \"],[0,[18],1,\"static IP and DNS on an Azure Load Balancer\"],[0,[],0,\", I was able to get secure external endpoints with very little fuss!\"]]],[1,\"h3\",[[0,[],0,\"Traffic Shifting\"]]],[1,\"p\",[[0,[],0,\"If you've got routing, then it follows that you should be able to shift traffic to different services (or more likely, different versions of the same service). I got this working in Istio (see my Github repo and mardown on how I did this \"],[0,[19],1,\"here\"],[0,[],0,\") and Linkerd only worked for external traffic. With Istio you shift by defining a \"],[0,[20],1,\"VirtualService\"],[0,[],0,\" - it's an Istio CRD that's a love-child between a Service and an Ingress. With Linkerd, traffic rules are specified using \"],[0,[21],1,\"dtabs\"],[0,[],0,\" - it's a cool idea (abstracting routes) but the implementation was horrible to work with - trying to learn the obscure format and debug it was not great.\"]]],[1,\"p\",[[0,[],0,\"By far the biggest problem with both Istio and Linkerd is that their network routing doesn't understand readiness or liveness probes since the work via their sidecar containers. This becomes a problem when you're deploying a new version of a service using a rolling upgrade - as soon as the service is created, Istio or Linkerd start sending traffic to the endpoint, irrespective of the readiness of that deployment. You can probably work around this, but I found that I didn't have to if I used Traefik.\"]]],[1,\"p\",[[0,[],0,\"Traefik lets you declaratively specify weight rules to shift traffic using simple annotations on a standard Ingress resource. It's clean and intuitive when you see it. The traffic shifting also obeys readiness/liveness probes, so you don't start getting traffic routed to services/deployments that are not yet ready. Very cool!\"]]],[1,\"h3\",[[0,[],0,\"Resiliency\"]]],[1,\"p\",[[0,[],0,\"First, there's a lot of things to discuss in terms of resiliency - for this post I'm just looking at features like zero-downtime deployment, circuit breaking and request limiting. Istio and Linkerd both have control planes for defining circuit breakers and request limiting - Traefik let's you define these as annotations. Again, this comparison is a little apples-for-oranges since Traefik is \\\"just\\\" a reverse proxy, while Istio and Linkerd are service meshes. However, the ease of declaration of these features is so simple in Traefik, it's compelling. Also, since Traefik builds \\\"over\\\" rolling updates in Deployments, you get zero-downtime deployment for free. If you're using Istio, you have to be careful about your deployments since you can get traffic to services that are not yet ready.\"]]],[1,\"h3\",[[0,[],0,\"Tracing\"]]],[1,\"p\",[[0,[],0,\"Traefik offloads monitoring to Prometheus and the helm chart has hooks into \"],[0,[22],1,\"DataDog\"],[0,[],0,\", \"],[0,[23],1,\"Zipkin\"],[0,[],0,\" or \"],[0,[24],1,\"Jaeger\"],[0,[],0,\" for tracing. For my experiments, I deployed \"],[0,[25],1,\"Prometheus\"],[0,[],0,\" and \"],[0,[26],1,\"Grafana\"],[0,[],0,\" for tracing and monitoring. Both Istio and Linkerd have control planes that include tracing - including mesh visualization - which can be really useful for tracing microservices since you can see traffic flow within the mesh. With Traefik, you need additional tools.\"]]],[1,\"h2\",[[0,[],0,\"Configuring Traefik Controllers\"]]],[1,\"p\",[[0,[],0,\"So now you know some of the reasons that I like Traefik - but how do you actually deploy it? There are a couple components to Traefik: the \"],[0,[27],1,\"Ingress Controller\"],[0,[],0,\" (think of this as a proxy) and then \"],[0,[28],1,\"ingresses\"],[0,[],0,\" themselves - these can be defined at the application level and specify how the controller should direct traffic to the services within the cluster. There's another component (conceptually) and that is the \"],[0,[29],1,\"ingress class\"],[0,[],0,\": you can have multiple Traefik ingress controllers, and if you do, you need to specify a class for each controller. When you create an ingress, you also annotate that ingress to specify which controller should handle its traffic - you're essentially carving the ingress space into multiple partitions, each handled by a different controller.\"]]],[1,\"p\",[[0,[],0,\"For the controller, there are some other \\\"under the hood\\\" components such as secrets, config maps, deployments and services - but all of that can be easily deployed and managed via the \"],[0,[30],1,\"Traefik Helm chart\"],[0,[],0,\". You can quite easily deploy Traefik with a lot of default settings using\"]]],[10,0],[1,\"p\",[[0,[],0,\"from the command line, but I found it started getting unwieldy. I therefore downloaded the default values.yml file and customized some of the values. When deploying Traefik, I simply pass in my customized values.yml file to specify my settings.\"]]],[1,\"p\",[[0,[],0,\"For my experiments I wanted two types of controller: an \\\"external\\\" controller that was accessible from the world and included SSL. I also wanted an \\\"internal\\\" controller that was not accessible outside of the cluster that I could use to do internal routing. I use \"],[0,[31],1,\"Azure Kubernetes Service\"],[0,[],0,\" (AKS), so the code for this series assumes that.\"]]],[1,\"p\",[[0,[],0,\"Let's take a look at the values file of the \\\"internal\\\" controller:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 1-3: The image it \\\"traefik\\\" and we want the 1.7.7 version. Since this is just internal, we only need a \"],[0,[32],1,\"NodePort\"],[0,[],0,\" service (I tried ClusterIP, but that didn't work).\"]],[[0,[],0,\"Line 6: we want this ingress controller to watch and manage traffic for Ingresses that have this class as their annotation. This is how we have multiple Traefik controllers within a cluster. I prepend the class with the namespace (dev) too!\"]],[[0,[],0,\"Lines 7,8: Since this is an internal ingress, we don't need SSL. I tried to get this working, but suspect I had issues with the certs. If you need internal SSL, this is where you'd set it.\"]],[[0,[],0,\"Lines 10,11: This is for generating a cert via LetsEncrypt. Not needed for our internal traffic.\"]],[[0,[],0,\"Lines 14,15: Enable the dashboard. I accessed via port-forwarding, so the domain isn't critical.\"]],[[0,[],0,\"Lines 16,17: RBAC is enabled.\"]],[[0,[],0,\"Lines 18-25: tracing options - I just enabled Prometheus.\"]]]],[1,\"p\",[[0,[],0,\"Let's now compare the values file for the \\\"external\\\" controller:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Most of the file is the same, but here are the differences:\"]]],[3,\"ul\",[[[0,[],0,\"Line 4: We specify the static IP we want the LoadBalancer to use - I have code that pre-creates this static IP (with DNS name) in Azure before I execute this script.\"]],[[0,[],0,\"Line 7: We specify a different class to divide the \\\"ingress space\\\".\"]],[[0,[],0,\"Lines 8-14: These are the LetsEncrypt settings, including the domain name, challenge type and persistence to store the cert settings.\"]]]],[1,\"p\",[[0,[],0,\"Now that we have the controllers (internal and external) deployed, we can deploy \\\"native\\\" k8s services and ingresses (with the correct annotations) and everything Will Just Work ™.\"]]],[1,\"h2\",[[0,[],0,\"Configuring Ingresses\"]]],[1,\"p\",[[0,[],0,\"Assuming you have the following service:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Then you can define the following ingress:\"]]],[10,4],[10,5],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 2: This resource is of type \\\"Ingress\\\"\"]],[[0,[],0,\"Lines 4,5: We define the class - this ties this Ingress to the controller with this class - for our case, this is the \\\"external\\\" Traefik controller\"]],[[0,[],0,\"Lines 12-18: We're specifying how the Ingress Controller (Traefik in this case) should route traffic. This is the simplest configuration - take requests coming to the host \\\"mycoolaks.westus.cloudapp.azure.com\\\" and route them to the \\\"dev-partsunlimitedwebsite\\\" service onto the \\\"http\\\" port (port 80 if you look at the service definition above).\"]],[[0,[],0,\"Line 19: We can use the Traefik controller to front multiple services - using the path helps to route effectively.\"]]]],[1,\"p\",[[0,[],0,\"When you access the service, you'll see the secure padlock in the browser window and be able to see details for the valid cert:\"]]],[10,6],[1,\"p\",[[0,[],0,\"The best thing is I didn't have to generate the cert myself - Traefik did it all for me.\"]]],[1,\"p\",[[0,[],0,\"There's more that we can configure on the Ingress - including the traffic shifting for canary or A/B testing. We can also annotate the service to include circuit-breaking - but I'll save that for another post now that I've laid out the foundation for traffic management using Traefik.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Container DevOps requires thinking about how traffic is going to flow in your cluster - and while there are many tools for doing this, I like the combination of simplicity and power you get with Traefik. There's still a lot more to explore in Container DevOps - hopefully this post gives you some insight into my thoughts.\"]]],[1,\"p\",[[0,[],0,\"Happy container-ing!\"]]]]}","published_at":1557308199000,"status":"published","published_by":1},{"id":"cbd1728d-c185-4adb-b46f-7efa5280f928","title":"Container DevOps: Beyond Build (Part 3) - Canary Testing","slug":"container-devops-beyond-build-part-3---canary-testing","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"kind: Ingress\\nmetadata:\\nannotations:\\n  kubernetes.io/ingress.class: traefik-external\\n  traefik.ingress.kubernetes.io/service-weights: |\\n    partsunlimited-website-blue: 5%\\n    partsunlimited-website-green: 95%\\nlabels:\\n  app: partsunlimited-website\\n  name: partsunlimited-website\\nspec:\\n  rules:\\n  - host: cdk8spu-dev.westus.cloudapp.azure.com\\n    http:\\n      paths:\\n      - backend:\\n          serviceName: partsunlimited-website-blue\\n          servicePort: http\\n        path: /site\\n      - backend:\\n          serviceName: partsunlimited-website-green\\n          servicePort: http\\n        path: /site\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"apiVersion: v1\\nkind: Service\\nmetadata:\\n  name: partsunlimited-website-blue\\n  labels:\\n    app: partsunlimited\\n    canary: blue\\nannotations:\\n  traefik.backend.circuitbreaker: \\\"NetworkErrorRatio() &gt; 0.2\\\"\\nspec:\\n  ...\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: partsunlimited-website-green\\n  labels:\\n    app: partsunlimited\\n    canary: green\\nspec:\\n  ...\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"canaries:\\n  - name: blue\\n    replicaCount: 1\\n    weight: 20\\n    tag: 1.0.0.0\\n    annotations:\\n      traefik.backend.circuitbreaker: \\\"NetworkErrorRatio() &gt; 0.2\\\"\\n  - name: green\\n    replicaCount: 2\\n    weight: 80\\n    tag: 1.0.0.0\\n    annotations: {}\",\"language\":\"javascript;\"}]],\"markups\":[[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-1\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-2---traefik\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-4---telemetry-with-prometheus\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-5---prometheus-operator\"]],[\"a\",[\"href\",\"https://istio.io/\"]],[\"a\",[\"href\",\"https://linkerd.io/\"]],[\"a\",[\"href\",\"https://traefik.io/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\"]],[\"a\",[\"href\",\"https://helm.sh/\"]],[\"a\",[\"href\",\"https://helm.sh/docs/glossary/#tiller\"]],[\"a\",[\"href\",\"https://github.com/helm/community/blob/master/helm-v3/000-helm-v3.md\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1: Intro\"]],[[0,[1],1,\"Part 2: Traefik Basics\"]],[[0,[],0,\"Part 3: Canary Testing (this post)\"]],[[0,[2],1,\"Part 4: Telemetry with Prometheus\"]],[[0,[3],1,\"Part 5: Prometheus Operator\"]]]],[1,\"p\",[[0,[],0,\"In my \"],[0,[1],1,\"previous post\"],[0,[],0,\" I compared \"],[0,[4],1,\"Istio\"],[0,[],0,\", \"],[0,[5],1,\"Linkerd\"],[0,[],0,\" and \"],[0,[6],1,\"Traefik\"],[0,[],0,\" and motivated why I preferred Traefik for Container DevOps. I showed how I was able to spin up Traefik controllers - one for internal cluster traffic routing, one for external cluster in-bound traffic routing. With that foundation in place, I can easily implement canary testing - both for external endpoints as well as internal services.\"]]],[1,\"h2\",[[0,[],0,\"Canary Testing\"]]],[1,\"p\",[[0,[],0,\"What is canary testing (sometimes referred to as A/B testing)? This is a technique of \\\"testing in production\\\" where you shift a small portion of traffic to a new version of a service to ensure it is stable, or that it is meeting some sort of business requirement, in the case of hypothesis-driven development. This is an important technique because no matter how good your test and staging environments are, there's no place like production. Sure, you can test performance in a test/stage environment, but you can only ever test user behavior in production! Being able to trickle a small amount of traffic to a new service limits exposure.\"]]],[1,\"p\",[[0,[],0,\"However, a lot of teams that do use canary testing tend to use it just for proving that a service is stable. I think that they're missing a trick - namely, telemetry and \\\"proving hypotheses\\\". Without good telemetry, you're never going to unlock the true potential of canary testing. Think of your canary as an experiment - and make sure you have a means to measure the success (or failure) of that experiment - otherwise you're just pointlessly mixing chemicals. I'll cover monitoring and telemetry in another post.\"]]],[1,\"h2\",[[0,[],0,\"Traffic Shifting Using Label Selectors\"]]],[1,\"p\",[[0,[],0,\"You can do canary testing \\\"natively\\\" in Kubernetes (k8s) by using good \"],[0,[7],1,\"label selectors\"],[0,[],0,\". Imagine you have service Foo and it has label selectors \\\"app=foo\\\". Any pods that you deploy (typically via \"],[0,[8],1,\"Deployments\"],[0,[],0,\", \"],[0,[9],1,\"DaemonSets\"],[0,[],0,\" or \"],[0,[10],1,\"StatefulSets\"],[0,[],0,\") that have the label \\\"app=foo\\\" get traffic routed to them when the service endpoint is targeted. Imagine you had a Deployment that spins up two replicas of a pod with labels \\\"app=foo,version=1.0\\\". Hitting the Service endpoint will cause k8s to route traffic between the two pods. Now you have a new version of the container image and you create a Deployment that spins up one pod with labels \\\"app=foo,version=1.1\\\". Now because all three pods match the Service label selector \\\"app=foo\\\" traffic is distributed between all three pods - you've effectively routed 33% of traffic to the new pod.\"]]],[1,\"p\",[[0,[],0,\"So far so good. But here's where things get tricky: say you're monitoring the pods and decide that version 1.1 is good to go - how do you \\\"promote\\\" it to production fully? You could update the labels on the original pods and remove \\\"app=foo\\\" - they'll no longer match and so now all traffic is going to the third version 1.1 pod. But now you only have one pod, where originally you had two. So you'd have to also scale the Deployment of version 1.1 to ensure it gets as many replicas as the original service. And now you have a Deployment that's missing some labels - so you'd have to dig to find out what those pods are.\"]]],[1,\"p\",[[0,[],0,\"Alternatively, you could just add \\\"version=1.1\\\" to the Service label selectors. Again you'd have to scale the version 1.1 Deployment, but at least you don't get \\\"dangling pods\\\". But what about deploying version 1.2? Now you have to remove the \\\"version=1.1\\\" label from the Service since just adding \\\"app=foo\\\" won't be good enough to get traffic onto pods with labels \\\"app=foo,version=1.2\\\".\"]]],[1,\"p\",[[0,[],0,\"And how would you go about testing traffic shifting of just 2%? You'd need to deploy 49 replicas of version 1.1 and a single version 1.2 just to get that percentage.\"]]],[1,\"p\",[[0,[],0,\"What it boils down to is that using label selectors proves to be too much cognitive load since you spend too much time juggling labels, and the dial is \\\"too course\\\" - you can't easily test traffic percentages lower that say 20% very easily. In contrast, if you use Traefik to do the traffic shifting, you get the added bonus of circuit breakers, SSL and other features too.\"]]],[1,\"h2\",[[0,[],0,\"Traffic Shifting Using Traefik\"]]],[1,\"p\",[[0,[],0,\"Let's see how we'd do traffic shifting using Traefik. Let's suppose that I've already deployed a Traefik controller with \\\"ingressClass=traefik.external\\\". To route traffic between two identical services (where the only difference between the services is the image version) I can create this ingress:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: the kind of resource in \\\"Ingress\\\" - nothing special about this, it's a native k8s Ingress resource\"]],[[0,[],0,\"Line 4: this is where we specify which IngressController should do the heavy lifting for this particular Ingress\"]],[[0,[],0,\"Lines 5-7: simple, intuitive and declarative - we want 5% of traffic to be routed to the \\\"blue\\\" Service\"]],[[0,[],0,\"Line 13: when inbound traffic has host \\\"cdk8spu-dev.westus.cloudapp.azure.com\\\" (the DNS for the LoadBalancer), then we want the ingress to use the following rules to direct the traffic\"]],[[0,[],0,\"Lines 16-23: we specify the backend Services and Ports that the Ingress should route to and can even specify custom paths to map different backends to different URL paths\"]]]],[1,\"h3\",[[0,[],0,\"The Services\"]]],[1,\"p\",[[0,[],0,\"This assumes that we have two services: partsunlimited-website-blue and partsunlimted-website-green. In my case these are exactly the same service - they will sometimes just have pods on different versions of the images I'm building. Let's look at the services:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 5-7, 17-19: these are out-of-box label selectors for services. There's the common \\\"app\\\" label and then a label for each canary \\\"slot\\\" that I have\"]],[[0,[],0,\"Lines 8-9: since I am using Traefik, I can easily create a circuit-breaker using the annotation. In this case, we instruct the controller to cease to send traffic to the blue service if its network failure rate rises above 20%\"]],[[0,[],0,\"The other lines are exactly what you would use for defining any k8s service\"]]]],[1,\"h2\",[[0,[],0,\"Helm\"]]],[1,\"p\",[[0,[],0,\"Now that I've shown you how to define the ingress and the services, I can discuss how I actually deployed my services. If you use \\\"native\\\" k8s yml manifests, it can become difficult to manage all your resources. Imagine you have several services, configmaps, secrets, ingresses, ingress controllers, persistence volumes - you'd need to manage each type of resource. \"],[0,[11],1,\"Helm\"],[0,[],0,\" simplifies that task by \\\"bundling\\\" the related resources. That way \\\"helm upgrade\\\" gives you a single command to install or upgrade all the resources - and similarly, \\\"helm status\\\" and \\\"helm delete\\\" let you inspect or destroy the app and all its resources quickly. So I built a helm package for my application that included the Traefik plumbing.\"]]],[1,\"h3\",[[0,[],0,\"Challenges with Helm\"]]],[1,\"p\",[[0,[],0,\"It's not all roses and unicorns though - helm has some disadvantages. Firstly, there's \"],[0,[12],1,\"Tiller\"],[0,[],0,\" - the \\\"server side\\\" component of helm. To use helm, you need to install Tiller on your k8s cluster, and give it some pretty beefy permissions. \"],[0,[13],1,\"Helm 3\"],[0,[],0,\" is abandoning Tiller, so this should improve in the near future.\"]]],[1,\"p\",[[0,[],0,\"The other (more pertinent) challenge is the way helm performs upgrades. Let's have a look at a snippet of the values file that I have for my service - this file is used to override (or supply) values to an actual deployment:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 4,10 - I define the weights for each canary. Helm injects this value into the Ingress resource.\"]],[[0,[],0,\"Lines 5,6 - I define annotations to apply to the service - in this case the Traefik circuit-breaker, but I could add others too\"]]]],[1,\"p\",[[0,[],0,\"Initially, I wanted to do a deployment with \\\"version=1.0.0.0\\\" for both canaries, and then just run \\\"helm upgrade --set-values canaries[0].imageTag=1.0.0.1\\\" to update the version of the blue canary. However, helm doesn't work this way and so I have to supply all the values for the chart, rather than just the ones I want to update. In a pipeline, the version to deploy to the blue canary is the latest build number - but I have to calculate the green canary version number or it will be overwritten with \\\"1.0.0.0\\\" every time. It's not a big deal since I can work it out, but it would be nice if helm had a way to only update a single value and leave all other current values \\\"as-is\\\".\"]]],[1,\"p\",[[0,[],0,\"In the end, the ease of managing the entire application (encompassing all the resources) using helm outweighed the minor paper-cuts. I still highly recommend helm to manage app deployment - even if they're simple apps!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Traffic shifting using Traefik is pretty easy - it's also intuitive since it's based on annotations and is specified over \\\"native\\\" k8s resources instead of having to rely on custom constructs or sidecars or other rule-language formats. This makes it an ideal tool for performing canary testing in k8s deployments.\"]]],[1,\"p\",[[0,[],0,\"Happy canary testing!\"]]]]}","published_at":1557308216000,"status":"published","published_by":1},{"id":"bd6e560e-ba9f-4ce5-a4ab-6b747c711372","title":"Container DevOps: Beyond Build (Part 4) - Telemetry with Prometheus","slug":"container-devops-beyond-build-part-4---telemetry-with-prometheus","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"using Prometheus;\\n\\npublic class Startup\\n{\\n    public void Configure(IApplicationBuilder app)\\n    {\\n        var basePath = Configuration[\\\"PathBase\\\"] ?? \\\"/\\\";\\n        ...\\n\\n        // prometheus\\n        var version = Assembly.GetEntryAssembly().GetCustomAttribute<assemblyfileversionattribute>()\\n                .Version.ToString();\\n        app.UseMethodTracking(version, Configuration[\\\"ASPNETCORE_ENVIRONMENT\\\"], Configuration[\\\"CANARY\\\"]);\\n        app.UseMetricServer($\\\"{basePath}/metrics\\\");\\n</assemblyfileversionattribute>\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public static class PrometheusAppExtensions\\n{\\n    static readonly string[] labelNames = new[] { \\\"version\\\", \\\"environment\\\", \\\"canary\\\", \\\"method\\\", \\\"statuscode\\\", \\\"controller\\\", \\\"action\\\" };\\n\\n    static readonly Counter counter = Metrics.CreateCounter(\\\"http_requests_received_total\\\", \\\"Counts requests to endpoints\\\", new CounterConfiguration\\n    {\\n        LabelNames = labelNames\\n    });\\n\\n    static readonly Gauge inProgressGauge = Metrics.CreateGauge(\\\"http_requests_in_progress\\\", \\\"Counts requests currently in progress\\\", new GaugeConfiguration\\n    {\\n        LabelNames = labelNames\\n    });\\n\\n    static readonly Histogram requestHisto = Metrics.CreateHistogram(\\\"http_request_duration_seconds\\\", \\\"Duration of requests to endpoints\\\", new HistogramConfiguration\\n    {\\n        LabelNames = labelNames\\n    });\\n\\n    public static void UseMethodTracking(this IApplicationBuilder app, string version, string environment, string canary)\\n    {\\n        app.Use(async (context, next) =&gt;\\n        {\\n            // extract values for this event\\n            var routeData = context.GetRouteData();\\n            var action = routeData?.Values[\\\"Action\\\"] as string ?? \\\"\\\";\\n            var controller = routeData?.Values[\\\"Controller\\\"] as string ?? \\\"\\\";\\n            var labels = new string[] { version, environment, canary,\\n                context.Request.Method, context.Response.StatusCode.ToString(), controller, action };\\n\\n            // start a timer for the histogram\\n            var stopWatch = Stopwatch.StartNew();\\n            using (inProgressGauge.WithLabels(labels).TrackInProgress()) // increments the inProgress, decrementing when disposed\\n            {\\n                try\\n                {\\n                    await next.Invoke();\\n                }\\n                finally\\n                {\\n                    // record the duration\\n                    stopWatch.Stop();\\n                    requestHisto.WithLabels(labels).Observe(stopWatch.Elapsed.TotalSeconds);\\n\\n                    // increment the counter\\n                    counter.WithLabels(labels).Inc();\\n                }\\n            }\\n        });\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public class ShoppingCartController : Controller\\n{\\n    static readonly string[] labelNames = new[] { \\\"category\\\", \\\"product\\\", \\\"version\\\", \\\"environment\\\", \\\"canary\\\"  };\\n\\n    readonly Counter productCounter = Metrics.CreateCounter(\\n        \\\"pu_product_add\\\", \\\"Increments when product is added to basket\\\", \\n        new CounterConfiguration\\n        {\\n            LabelNames = labelNames\\n        });\\n    \\n    public async Task<iactionresult> AddToCart(int id)\\n    {\\n        // Retrieve the product from the database\\n        var addedProduct = _db.Products\\n            .Include(product =&gt; product.Category)\\n            .Single(product =&gt; product.ProductId == id);\\n\\n        var labels = new string[] { properties[\\\"ProductCategory\\\"], properties[\\\"Product\\\"], version, environment, canary };\\n        productCounter.WithLabels(labels).Inc();\\n        ...\\n    }\\n    ...\\n}\\n</iactionresult>\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"# HELP process_open_handles Number of open handles\\n# TYPE process_open_handles gauge\\nprocess_open_handles 346\\n# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.\\n# TYPE process_start_time_seconds gauge\\nprocess_start_time_seconds 1556149570.76\\n# HELP dotnet_total_memory_bytes Total known allocated memory\\n# TYPE dotnet_total_memory_bytes gauge\\ndotnet_total_memory_bytes 8133304\\n# HELP process_virtual_memory_bytes Virtual memory size in bytes.\\n# TYPE process_virtual_memory_bytes gauge\\nprocess_virtual_memory_bytes 12298391552\\n# HELP http_request_duration_seconds Duration of requests to endpoints\\n# TYPE http_request_duration_seconds histogram\\nhttp_request_duration_seconds_sum{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\"} 6.6218794\\nhttp_request_duration_seconds_count{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\"} 926\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.005\\\"} 872\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.01\\\"} 909\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.025\\\"} 916\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.05\\\"} 919\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.075\\\"} 919\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.1\\\"} 922\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.25\\\"} 923\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.5\\\"} 925\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"0.75\\\"} 925\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"1\\\"} 925\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"2.5\\\"} 925\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"5\\\"} 926\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"7.5\\\"} 926\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"10\\\"} 926\\nhttp_request_duration_seconds_bucket{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\",le=\\\"+Inf\\\"} 926\\n# HELP process_num_threads Total number of threads\\n# TYPE process_num_threads gauge\\nprocess_num_threads 24\\n# HELP dotnet_collection_count_total GC collection count\\n# TYPE dotnet_collection_count_total counter\\ndotnet_collection_count_total{generation=\\\"0\\\"} 3\\ndotnet_collection_count_total{generation=\\\"2\\\"} 0\\ndotnet_collection_count_total{generation=\\\"1\\\"} 1\\n# HELP process_working_set_bytes Process working set\\n# TYPE process_working_set_bytes gauge\\nprocess_working_set_bytes 159961088\\n# HELP http_requests_received_total Counts requests to endpoints\\n# TYPE http_requests_received_total counter\\nhttp_requests_received_total{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\"} 926\\n# HELP process_private_memory_bytes Process private memory size\\n# TYPE process_private_memory_bytes gauge\\nprocess_private_memory_bytes 0\\n# HELP pu_product_add Increments when product is added to basket\\n# TYPE pu_product_add counter\\npu_product_add{category=\\\"Wheels &amp; Tires\\\",product=\\\"Disk and Pad Combo\\\",version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\"} 1\\npu_product_add{category=\\\"Oil\\\",product=\\\"Oil and Filter Combo\\\",version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\"} 1\\n# HELP http_requests_in_progress Counts requests currently in progress\\n# TYPE http_requests_in_progress gauge\\nhttp_requests_in_progress{version=\\\"1.0.0.45\\\",environment=\\\"Production\\\",canary=\\\"blue\\\",method=\\\"GET\\\",statuscode=\\\"200\\\",controller=\\\"\\\",action=\\\"\\\"} 1\\n# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.\\n# TYPE process_cpu_seconds_total counter\\nprocess_cpu_seconds_total 15.26\\n\",\"language\":\"plain;\"}]],\"markups\":[[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-1\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-2---traefik\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-3---canary-testing\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-5---prometheus-operator\"]],[\"a\",[\"href\",\"https://traefik.io/\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/appinsights-analytics-in-the-real-world\"]],[\"a\",[\"href\",\"https://prometheus.io/\"]],[\"a\",[\"href\",\"https://www.datadoghq.com/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/azure-monitor/app/proactive-diagnostics\"]],[\"a\",[\"href\",\"https://prometheus.io/docs/concepts/metric_types/\"]],[\"a\",[\"href\",\"https://grafana.com/\"]],[\"a\",[\"href\",\"https://prometheus.io/docs/prometheus/latest/querying/basics/\"]],[\"a\",[\"href\",\"https://www.nuget.org/packages/prometheus-net.AspNetCore/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1: Intro\"]],[[0,[1],1,\"Part 2: Traefik Basics\"]],[[0,[2],1,\"Part 3: Canary Testing\"]],[[0,[],0,\"Part 4: Telemetry with Prometheus (this post)\"]],[[0,[3],1,\"Part 5: Prometheus Operator\"]]]],[1,\"p\",[[0,[],0,\"In my \"],[0,[2],1,\"previous post\"],[0,[],0,\" in this series I wrote about how I used \"],[0,[4],1,\"Traefik\"],[0,[],0,\" to do traffic shifting and canary testing. I asserted that without proper telemetry, canary testing is (almost) pointless. Without some way to determine the efficacy of a canary deployment, you may as well just deploy straight out and not pretend.\"]]],[1,\"p\",[[0,[],0,\"I've also written about how I love and use \"],[0,[5],1,\"Application Insights to monitor .NET applications\"],[0,[],0,\". Application Insights (or AppInsights for short) is still my go-to telemetry tool. And it's not only a .NET tool - there are SDKs for Java, Javascript and Python among others. But since we're delving into container-land, I wanted to at least explore one of the popular k8s tools: \"],[0,[6],1,\"Prometheus\"],[0,[],0,\". There are other monitoring tools (like \"],[0,[7],1,\"Datadog\"],[0,[],0,\") and I think it'll be worth doing a compare/contrast of various monitoring tools at some stage. But for this post, I'll stick to Prometheus.\"]]],[1,\"h2\",[[0,[],0,\"Business vs Performance Telemetry\"]]],[1,\"p\",[[0,[],0,\"Most developers that are using any kind of telemetry understand \\\"performance\\\" telemetry - requests per second, read/writes per second, errors per second, memory and CPU usage - usual, bread-and-butter telemetry. However, I often encourage teams not to stop at \"],[0,[8],1,\"performance\"],[0,[],0,\" telemetry and to also start looking at how to instrument their applications with \\\"business\\\" telemetry. \"],[0,[8],1,\"Business telemetry\"],[0,[],0,\" is telemetry that has nothing to do with the running application - and everything to do with how the site or application is doing in business terms. For example, how many products of a certain category were sold today? What products are popular in which geos? And so on.\"]]],[1,\"p\",[[0,[],0,\"AppInsights is one of my go-to tools because you get performance telemetry \\\"for free\\\" - just add it to your project and you get all of the perf telemetry you need to have a good view of your application performance - and that's without changing a single line of code! However, if you do want business telemetry, you can add a few lines of code and it's simple to get business telemetry. Add to that the ability to connect PowerBI to your telemetry (something I've written about \"],[0,[5],1,\"before\"],[0,[],0,\") and you're able to produce the telemetry and have business users consume it using PowerBI - that's a recipe for success!\"]]],[1,\"p\",[[0,[],0,\"On the down-side, making sense of AppInsights telemetry definitely isn't simple, and the learning curve for analyzing your data is steep. The AppInsights query language is a delight though, and even has some built-in \"],[0,[9],1,\"machine learning capabilities\"],[0,[],0,\").\"]]],[1,\"h2\",[[0,[],0,\"Prometheus\"]]],[1,\"p\",[[0,[],0,\"Prometheus has long been a popular telemetry solution - however, as I was exploring it I came across some challenges. Firstly, integrating into .NET isn't simple - and you don't get anything \\\"for free\\\" - you have to code in the telemetry. Secondly, there are only four types of metrics you can \"],[0,[10],1,\"utilize\"],[0,[],0,\": Counter, Gauge, Histogram and Summary. These are great for performance telemetry, but are very difficult to use for business telemetry. However, creating graphs from Prometheus data is really simple (at least using \"],[0,[11],1,\"Grafana\"],[0,[],0,\", as I'll discuss in a later post) and there's a whole query language called \"],[0,[12],1,\"PromQL\"],[0,[],0,\" for querying Prometheus metrics.\"]]],[1,\"p\",[[0,[],0,\"In the remainder of this post I'll show how I used Prometheus in a .NET Core application.\"]]],[1,\"h2\",[[0,[],0,\"Performance Telemetry\"]]],[1,\"p\",[[0,[],0,\"To add performance telemetry to a .NET Core application, you have to add some middleware. You also need to expose the Prometheus endpoint. Here's a snippet from my Startup.cs file:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: Import the Prometheus namespace - this is from the \"],[0,[13],1,\"Prometheus NuGet package\"]],[[0,[],0,\"Line 7: We need to set a base path - this is for sharing Traefik frontends for multiple backend services\"]],[[0,[],0,\"Lines 10-11: Get the version of the application\"]],[[0,[],0,\"Line 12: Call the UseMethodTracking method (shown below) to configure middleware, passing in the version, environment and canary name\"]],[[0,[],0,\"Line 13: Tell Prometheus to expose an endpoint for the Prometheus server to scrape\"]]]],[1,\"p\",[[0,[],0,\"I want my metrics to be dimensioned by version, environment and canary. This is critical for successful canary testing! We also need a pathbase other than \\\"/\\\" since when we deploy services behind the Traefik router, we want to use path-based rules to route traffic to different backend services, even though there's only a single front-end base URL.\"]]],[1,\"p\",[[0,[],0,\"Here's the code for the UseMethodTracking method:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 2: set up the names of the dimensions I want to configure - note version, environment and canary\"]],[[0,[],0,\"Lines 5-8: set up a counter to count method hits\"]],[[0,[],0,\"Lines 10-13: set up a gauge to report how many requests are in progress\"]],[[0,[],0,\"Lines 15-18: set up a histogram to record duration of each method call\"]],[[0,[],0,\"Line 20: create a static extension method to inject Prometheus tracking into the middleware\"]],[[0,[],0,\"Line 22: add a new handler into the pipeline\"]],[[0,[],0,\"Lines 25-28: extract action, controller, method and response code from the current request if available\"]],[[0,[],0,\"Line 32: start a stopwatch\"]],[[0,[],0,\"Line 33: tell Prometheus that a method call is in progress - the \\\"end of operation\\\" is automatic at the end of the using (line 47)\"]],[[0,[],0,\"Line 35-38: invoke the actual request\"]],[[0,[],0,\"Line 39-43: stop the stopwatch and log the time recorded\"]],[[0,[],0,\"Line 46: increment the counter for this controller/action/method/version/environment/canary combination\"]]]],[1,\"p\",[[0,[],0,\"This code gives us performance metrics - we inject a step into the pipeline that starts a stopwatch, calls the operation, tells Prometheus an operation is in progress, and then when the operation completes, records the time taken and increments the call counter. Each \\\"log\\\" includes the \\\"withLabels()\\\" call that creates the context (dimensions) for the event.\"]]],[1,\"h2\",[[0,[],0,\"Total Sales by Product Category\"]]],[1,\"p\",[[0,[],0,\"Let's examine what telemetry looks like if we want to track a business metric: say, sales of products by category. For this to work, I'd need to track the product category and price of each item sold. I could add other dimensions too (such as user) so that I can extend my analytics. If I know which users are purchasing products, I can start slicing and dicing by geo or language or other user attributes. If I know when sales occur, I can slice and dice by day of week or hour or any other time-based dimensions. The more dimensions I have, the more insights I can drive.\"]]],[1,\"p\",[[0,[],0,\"Let's see how we would track this business metric using Prometheus. Firstly, which metric type do I need? If we use a Counter, we can count how many items are sold, but not track the price - because counters can only increment by 1, not anything else. I could try Gauge since Gauge lets me set an arbitrary number - but unfortunately that doesn't give me a running total - it's just a number at a point in time. Both Histogram and Summary are snapshots of observations in a time period (my wording) so they don't work either. In the end I decided to settle for number of products sold as a proxy for revenue - each time an item is sold, I want to log a counter for the product category and other dimensions so I get some idea of business telemetry.\"]]],[1,\"p\",[[0,[],0,\"Generally I like a logging framework with an interface that abstracts the logging mechanism or storage away from the application. I found that this was relatively easy to do using AppInsights - however, Prometheus doesn't really work that way because the metric types are very specific to a particular event or method.\"]]],[1,\"p\",[[0,[],0,\"Here's how I ended up logging some telemetry in Prometheus in my .NET Core application:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3: Set up a list of label names - again, these are dimensions for the telemetry\"]],[[0,[],0,\"Lines 5-10: Set up a Counter for counting when a product is added to a basket, using labelNames for the dimensions\"]],[[0,[],0,\"Line 19: Create an array of values that correspond to the labelNames array\"]],[[0,[],0,\"Line 20: Increment the counter, again using WithLabels()\"]]]],[1,\"h2\",[[0,[],0,\"Viewing Telemetry\"]]],[1,\"p\",[[0,[],0,\"Now that we have telemetry integrated, we can view the telemetry by browsing to the endpoint we configured Prometheus to expose. We'll get some of the metrics live:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 14-32: shows http request duration in buckets - notice how each bucket also has the version of the app, the environment, canary, statuscode, controller and action\"]],[[0,[],0,\"Lines 50-53: shows the number of products added to the basket by category, product, version, environment and canary\"]]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I didn't get round to showing how Prometheus scrapes the metrics from various services so that you can start to dashboard and analyze - that's the subject for the next post. While I find Prometheus is fairly painful to implement on the tracking side (certainly compared to AppInsights), the graphing and querying can be worth the pain. I'll show you how to do that in a k8s cluster in the next post.\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, though I experimented with using Prometheus for \\\"business\\\" telemetry, I can't say I recommend it. It's really meant for performance telemetry. So use AppInsights - which you can totally do even from within containers - if you need to do any business telemetry. And you do!\"]]],[1,\"p\",[[0,[],0,\"Happy monitoring!\"]]]]}","published_at":1557308231000,"status":"published","published_by":1},{"id":"0e91ed7c-8db2-4203-b7bf-b1a899fa5274","title":"Container DevOps: Beyond Build (Part 5) - Prometheus Operator","slug":"container-devops-beyond-build-part-5---prometheus-operator","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/\\nhelm upgrade --install prometheus-operator coreos/prometheus-operator --namespace monitoring\\nhelm upgrade --install kube-prometheus coreos/kube-prometheus --namespace monitoring\\n\",\"language\":\"bash\"}],[\"code\",{\"code\":\"alertmanager-operated                 ClusterIP   None           <none>        9093/TCP,6783/TCP   60d\\nkube-prometheus                       ClusterIP   10.0.240.33    <none>        9090/TCP            60d\\nkube-prometheus-alertmanager          ClusterIP   10.0.168.1     <none>        9093/TCP            60d\\nkube-prometheus-exporter-kube-state   ClusterIP   10.0.176.16    <none>        80/TCP              60d\\nkube-prometheus-exporter-node         ClusterIP   10.0.251.145   <none>        9100/TCP            60d\\nprometheus-operated                   ClusterIP   None           <none>        9090/TCP            60d\\n</none></none></none></none></none></none>\",\"language\":\"plain\"}],[\"code\",{\"code\":\"apiVersion: monitoring.coreos.com/v1\\nkind: ServiceMonitor\\nmetadata:\\n  name: website-monitor\\n  namespace: monitoring\\n  labels:\\n    prometheus: kube-prometheus\\n    tier: website\\nspec:\\n  jobLabel: app\\n  selector:\\n    matchLabels:\\n      release: pu-dev-website\\n      system: PartsUnlimited\\n      app: partsunlimited-website\\n  namespaceSelector:\\n    any: true\\n  endpoints:\\n  - port: http\\n    path: /site/metrics\\n    interval: 15s\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">kubectl port-forward svc/kube-prometheus -n monitoring 9090:9090</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">sum(increase(pu_product_add[2m])) by (category)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a8121922-b998-4260-b51a-abfc4869ce20.png\\\" target=\\\"_blank\\\"><img width=\\\"485\\\" height=\\\"249\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d4097067-5591-4088-a6ec-e27a09e68f3e.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-1\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-2---traefik\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-3---canary-testing\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/container-devops-beyond-build-part-4---telemetry-with-prometheus\"]],[\"a\",[\"href\",\"https://prometheus.io/\"]],[\"a\",[\"href\",\"https://github.com/coreos/prometheus-operator\"]],[\"a\",[\"href\",\"https://grafana.com/\"]],[\"a\",[\"href\",\"https://twitter.com/donovanbrown\"]],[\"a\",[\"href\",\"http://donovanbrown.com/post/what-is-devops\"]],[\"a\",[\"href\",\"https://github.com/helm/charts/tree/master/stable/prometheus\"]],[\"a\",[\"href\",\"https://github.com/coreos/kube-prometheus\"]],[\"a\",[\"href\",\"https://coreos.com/operators/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\"]],[\"a\",[\"href\",\"https://coreos.com/operators/prometheus/docs/latest/user-guides/getting-started.html#include-servicemonitors\"]],[\"a\",[\"href\",\"http://localhost:9090/graph\"]],[\"a\",[\"href\",\"https://prometheus.io/docs/prometheus/latest/querying/basics/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1: Intro\"]],[[0,[1],1,\"Part 2: Traefik Basics\"]],[[0,[2],1,\"Part 3: Canary Testing\"]],[[0,[3],1,\"Part 4: Telemetry with Prometheus\"]],[[0,[],0,\"Part 5: Prometheus Operator (this post)\"]]]],[1,\"p\",[[0,[],0,\"In \"],[0,[3],1,\"part 4\"],[0,[],0,\" of this series I showed how I created a metrics endpoint using \"],[0,[4],1,\"Prometheus\"],[0,[],0,\" in my .NET Core application. While not perfect for business telemetry, Prometheus is a standard for performance metrics. But that only exposes an endpoint for metrics - it doesn't do any visualization. In this post I'll go over how I used \"],[0,[5],1,\"Prometheus Operator\"],[0,[],0,\" in a k8s cluster to easily scrape metrics from services and then in the next post I'll cover how I configured \"],[0,[6],1,\"Grafana\"],[0,[],0,\"  to visualize those metrics - first by hand and then using infrastructure-as-code so that I can audit and/or recreate my entire monitoring environment from source code.\"]]],[1,\"h2\",[[0,[],0,\"Container DevOps Recap: The Importance of Monitoring\"]]],[1,\"p\",[[0,[],0,\"Monitoring is often the black sheep of DevOps - it's not stressed very much. I think that's partly because monitoring is hard - and often, contextual. Boards for work management and pipelines for build and release are generally more generic in concept and most teams starting with DevOps seem to start with these tools. However, Continuous Integration and Continuous Deployment should be complimented by Continuous Monitoring.\"]]],[1,\"p\",[[0,[],0,\"One of my DevOps heroes (and by luck of life, friend) \"],[0,[7],1,\"Donovan Brown\"],[0,[],0,\" coined the quintessential \"],[0,[8],1,\"definition of DevOps\"],[0,[],0,\" a few years ago: DevOps is the union of people, products and process to enable continuous delivery of value to end users. I've heard some folks criticize this definition for its lack of mention of monitoring among other things - but I think that a lot of Donovan's definition is implied (at least should be implied) in the phrase \\\"value\\\".\"]]],[1,\"p\",[[0,[],0,\"Most teams think of value in terms of features: I'd like to propose that monitoring as a mechanism of keeping systems stable, resilient and responsive is just as important as delivering features. So in a very real sense, his definition implies monitoring. I've also heard Donovan state that it doesn't matter how good your code is, if it's not in the hands of your users, it doesn't deliver value. In the same vein, it doesn't matter how good your features are: if you can't monitor for errors, scale or usage then you're missing delivering value for your users.\"]]],[1,\"p\",[[0,[],0,\"In a world of microservices and Kubernetes, the need for solid monitoring is paramount, and more difficult. Monoliths may be hard to change, but they are by and large easy to monitor. Microservices increase the complexity of monitoring, but there are some techniques that teams can use to manage the complexity.\"]]],[1,\"h2\",[[0,[],0,\"Prometheus Operator\"]]],[1,\"p\",[[0,[],0,\"In the last post I showed how I exposed Prometheus metrics from my services. Imagine you have 10 or 15 services - how do you monitor each one? Exposing metrics via Prometheus is all well and good, but how do you aggregate and visualize the metrics that are being produced? The first step is Prometheus itself - or the Prometheus instance (to be disambiguated by the Prometheus metrics endpoint that containers or services expose).\"]]],[1,\"p\",[[0,[],0,\"If you were manually setting up an instance of Prometheus, you would have to install the pods and services in a k8s cluster as well as configure Prometheus to tell it where to scrape metrics from. This manual process is complex, error prone and time-consuming: enter the \"],[0,[5],1,\"Prometheus Operator\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Installing the Prometheus operator (and instance) itself is simple thanks to the official \"],[0,[9],1,\"helm chart\"],[0,[],0,\".  This also (optionally) includes endpoints for monitoring the health of your k8s cluster components via \"],[0,[10],1,\"kube-prometheus\"],[0,[],0,\". It also installs AlertManager for automating alerts - I haven't played with this though.\"]]],[1,\"p\",[[0,[11],1,\"K8s Operators\"],[0,[],0,\" are a mechanism for deploying applications to a k8s cluster - but these applications tend to be \\\"smarter\\\" than regular k8s applications in that they can hook into the k8s lifecycle. Point-in-case: scraping telemetry for a newly deployed service. The Prometheus Operator will automagically update the Prometheus configuration via the k8s API when you declare that a new service has Prometheus endpoints. This is done via a \"],[0,[12],1,\"custom resource definition\"],[0,[],0,\" (CRD) that is created by the Prometheus helm chart: \"],[0,[13],1,\"ServiceMonitors\"],[0,[],0,\". When you create a service that exposes a Prometheus metrics endpoint, you simply declare a ServiceMonitor alongside your service to dynamically update Prometheus and let it know that you have a new service that can be scraped: including which port and how frequently to scrape.\"]]],[1,\"h3\",[[0,[],0,\"Configuring Prometheus Operator\"]]],[1,\"p\",[[0,[],0,\"The helm chart for Prometheus Operator is a beautiful thing: it means you can install and configure a Prometheus instance, the Prometheus Operator and kube-prometheus (for monitoring cluster components) with a couple lines of script. Here's how I do this in my release pipeline:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: Add the CoreOS repo for the stable Prometheus operator charts\"]],[[0,[],0,\"Line 2: Install (or upgrade) the Prometheus operator into a namespace called \\\"monitoring\\\"\"]],[[0,[],0,\"Line 3: Install the kube-prometheus components - this gives me cluster monitoring\"]]]],[1,\"p\",[[0,[],0,\"These commands are also idempotent so I can run them every time without worrying about current state - I always end up with the correct config. Querying the services in the monitoring namespace we see the following:\"]]],[10,1],[1,\"h3\",[[0,[],0,\"Configuring ServiceMonitors\"]]],[1,\"p\",[[0,[],0,\"Now that we have a Prometheus instance (and the Operator) configured we can examine how to tell the Operator that there's a new service to monitor. Fortunately, now that we have the ServiceMonitor CRD, it's pretty straight-forward: we just declare a ServiceMonitor resource alongside our service! Let's take a look at one:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 1-2: We're using the custom resource ServiceMonitor\"]],[[0,[],0,\"Line 7: We're using a label to ringfence this Service - we've configured the Prometheus service to look for ServiceMonitors with this label\"]],[[0,[],0,\"Lines 11-15: This ServiceMonitor applies to all services with these matching labels\"]],[[0,[],0,\"Lines 18-21: We configure the port (a named port in this case) and path for the Prometheus endpoint, as well as what frequency to scrape the metrics\"]]]],[1,\"p\",[[0,[],0,\"When we create this resource, the Operator picks up the creation of the ServiceMonitor resource via the k8s API and configures the Prometheus server to now scrape metrics from our service(s).\"]]],[1,\"h2\",[[0,[],0,\"PromQL\"]]],[1,\"p\",[[0,[],0,\"Now that we have some metrics going into Prometheus, we have the ability to query the metrics. We start by port-forwarding the Prometheus instance so that we can securely access it (you can also expose the instance publicly if you want to):\"]]],[10,3],[1,\"p\",[[0,[],0,\"Then we can browse to \"],[0,[14],1,\"http://localhost:9090/graph\"],[0,[],0,\" and see the Prometheus instance. We can then query metrics using \"],[0,[15],1,\"PromQL\"],[0,[],0,\" - a language to query, aggregate and visualize metrics. For example, to see the rate of increase in sales by category for the last 2 minutes, we can write\"]]],[10,4],[1,\"p\",[[0,[],0,\"And then we can visualize it by clicking on the graph button:\"]]],[10,5],[1,\"p\",[[0,[],0,\"This is a proxy for how well sales are doing on our site by category. And while this is certainly great, it's far better to visualize Prometheus metrics in Grafana dashboards - which we can do using the PromQL queries. But that's a topic for a future post in this series!\"]]],[1,\"h1\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Prometheus and the Prometheus Operator make configuring metric aggregation really easy. It's declarative, dynamic and intuitive. This makes it a really good framework for monitoring services within a k8s cluster. In the next post I'll show how you can hook Grafana up to the Prometheus server in order to make visualizations of the telemetry. Until then:\"]]],[1,\"p\",[[0,[],0,\"Happy monitoring!\"]]]]}","published_at":1557308123000,"status":"published","published_by":1},{"id":"381e0e17-64e3-42e4-a4b6-b259216df0a6","title":"Continuous Deployment of Service Fabric Apps using VSTS (or TFS)","slug":"continuous-deployment-of-service-fabric-apps-using-vsts-or-tfs","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a1859eb3-7f2a-4937-be6c-a49dba822a8d.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4a7acc11-01a8-47da-acaf-ad96ddc03ac0.png\\\" width=\\\"428\\\" height=\\\"426\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">1.0$(rev:.r)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4354826f-09b8-44e2-a2c5-d9d248012266.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8122a0ca-f3c9-4819-b976-fe95f71ab0ce.png\\\" width=\\\"522\\\" height=\\\"255\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/2f967a07-15ca-493c-bea0-2279e02acd5c.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/99b52c32-9907-4ad7-ac8b-674d86250a32.png\\\" width=\\\"525\\\" height=\\\"256\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/14608904-483f-496c-9218-15c366b5d685.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/dd5d1ce8-3ff4-4c9d-96c0-72481c6c2576.png\\\" width=\\\"504\\\" height=\\\"225\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/25e4f774-60ad-46a7-9258-38be03773928.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/7dc9fa90-26f4-4c43-824a-64b16fbe41d4.png\\\" width=\\\"498\\\" height=\\\"95\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/277b9095-4897-4410-b44a-67af02bc3389.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8eb44813-6d32-420a-8cc1-2421c9b7376e.png\\\" width=\\\"499\\\" height=\\\"95\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4f246bbb-567b-4338-95b1-86091360c51a.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/e7d5f48e-3e9f-4679-a22a-fb6e617badce.png\\\" width=\\\"497\\\" height=\\\"90\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/fe5104b1-f723-4667-b0bb-fabfe59965be.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/57a3cd79-ebc2-4fa8-bd8b-fc0ba3d6542e.png\\\" width=\\\"495\\\" height=\\\"92\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/d65f9c73-a569-4b82-887f-246cb6823c2b.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/17e165bf-d097-4bc0-96b9-fab4ca7e1304.png\\\" width=\\\"498\\\" height=\\\"82\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/38590fa3-9e30-455e-b727-601734367fe0.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/94c65e36-bd1f-425b-906a-c03cb0f048d6.png\\\" width=\\\"296\\\" height=\\\"322\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/7a0e92a5-c56d-43d4-a725-8cf01ddbf37d.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/f48745e5-8e7e-4a99-9fa3-4eade351e3dd.png\\\" width=\\\"310\\\" height=\\\"369\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/b581edba-501a-42f1-abb1-910c6d0d292c.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8419c3bc-7014-4c5e-a82d-082d37e88631.png\\\" width=\\\"452\\\" height=\\\"146\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/72527f6b-68ed-4604-935b-54bbccd2c276.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/ce833bce-d00f-44fb-9f68-0fe7269ec8fb.png\\\" width=\\\"554\\\" height=\\\"222\\\"></a>\"}],[\"code\",{\"code\":\"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?&gt;\\n&lt;PublishProfile xmlns=\\\"http://schemas.microsoft.com/2015/05/fabrictools\\\"&gt;\\n  &lt;!-- ClusterConnectionParameters allows you to specify the PowerShell parameters to use when connecting to the Service Fabric cluster.\\n       Valid parameters are any that are accepted by the Connect-ServiceFabricCluster cmdlet.\\n       \\n       For a remote cluster, you would need to specify the appropriate parameters for that specific cluster.\\n         For example: &lt;ClusterConnectionParameters ConnectionEndpoint=\\\"mycluster.westus.cloudapp.azure.com:19000\\\" /&gt;\\n\\n       Example showing parameters for a cluster that uses certificate security:\\n       &lt;ClusterConnectionParameters ConnectionEndpoint=\\\"mycluster.westus.cloudapp.azure.com:19000\\\"\\n                                    X509Credential=\\\"true\\\"\\n                                    ServerCertThumbprint=\\\"0123456789012345678901234567890123456789\\\"\\n                                    FindType=\\\"FindByThumbprint\\\"\\n                                    FindValue=\\\"9876543210987654321098765432109876543210\\\"\\n                                    StoreLocation=\\\"CurrentUser\\\"\\n                                    StoreName=\\\"My\\\" /&gt;\\n\\n  --&gt;\\n  &lt;!-- Put in the connection to the Prod cluster here --&gt;\\n  &lt;ClusterConnectionParameters ConnectionEndpoint=\\\"__ClusterName__.eastus.cloudapp.azure.com:19000\\\" /&gt;\\n  &lt;ApplicationParameterFile Path=\\\"..\\\\ApplicationParameters\\\\TestCloud.xml\\\" /&gt;\\n  &lt;UpgradeDeployment Mode=\\\"Monitored\\\" Enabled=\\\"true\\\"&gt;\\n    &lt;Parameters FailureAction=\\\"Rollback\\\" Force=\\\"True\\\" /&gt;\\n  &lt;/UpgradeDeployment&gt;\\n&lt;/PublishProfile&gt;\\n\",\"language\":\"xml; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/d8d5d37e-6787-4496-8388-1981bdc033d8.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b69d8875-9efd-46f8-9f0f-daa427cffec6.png\\\" width=\\\"539\\\" height=\\\"75\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/7b2458d3-f552-455e-9a56-c3590e7dd56f.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/fc9e4139-477b-425d-87f0-44383e9b8895.png\\\" width=\\\"442\\\" height=\\\"128\\\"></a>\"}],[\"code\",{\"code\":\"$IsUpgrade = ($publishProfile.UpgradeDeployment -and $publishProfile.UpgradeDeployment.Enabled -and $OverrideUpgradeBehavior -ne 'VetoUpgrade') -or $OverrideUpgradeBehavior -eq 'ForceUpgrade'\\n\\n# check if this application exists or not\\n$ManifestFilePath = \\\"$ApplicationPackagePath\\\\ApplicationManifest.xml\\\"\\n$manifestXml = [Xml] (Get-Content $ManifestFilePath)\\n$AppTypeName = $manifestXml.ApplicationManifest.ApplicationTypeName\\n$AppExists = (Get-ServiceFabricApplication | ? { $_.ApplicationTypeName -eq $AppTypeName }) -ne $null\\n\\nif ($IsUpgrade -and $AppExists)\\n\",\"language\":\"ps; first-line\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/e8408851-eccb-4d52-83b2-172eeeb05876.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/5e86b754-08cc-4227-b460-f17e2b6fe0eb.png\\\" width=\\\"754\\\" height=\\\"270\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$(Build.BuildNumber)-$(rev:r)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/898c74ec-07c4-41f6-9fb5-cbaddbf83957.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/f51dbe3a-f2c3-4eca-9e5b-f0d548557f88.png\\\" width=\\\"634\\\" height=\\\"364\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">src/VisualObjects/VisualObjects.ActorService/VisualObjectActor.cs</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">visualObject.Move(false);</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">visualObject.Move(true)</font>\"}]],\"markups\":[[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/service-fabric-overview/\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/azure-samples/service-fabric-dotnet-getting-started\"]],[\"a\",[\"href\",\"https://github.com/Azure-Samples/service-fabric-dotnet-web-reference-app\"]],[\"a\",[\"href\",\"https://github.com/Azure-Samples/service-fabric-dotnet-getting-started/tree/master/Actors/VisualObjects\"]],[\"a\",[\"href\",\"https://github.com/Azure-Samples/service-fabric-dotnet-getting-started\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/ServiceFabricBuildRelease\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/service-fabric-get-started/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/VersionAssemblies\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=colinsalmcorner.colinsalmcorner-buildtasks\"]],[\"a\",[\"href\",\"https://blogs.msdn.microsoft.com/visualstudioalm/2015/10/04/automating-azure-resource-group-deployment-using-a-service-principal-in-visual-studio-online-buildrelease-management/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/ReplaceTokens\"]],[\"strong\"],[\"u\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Azure’s \"],[0,[0],1,\"Service Fabric\"],[0,[],0,\" is breathtaking – the platform allows you to create truly “born in the cloud” apps that can \"],[0,[1],1,\"really\"],[0,[],0,\" scale. The platform takes care of the plumbing for you so that you can concentrate on business value in your apps. If you’re looking to create cloud apps, then make sure you take some time to investigate Service Fabric.\"]]],[1,\"h2\",[[0,[],0,\"Publishing Service Fabric Apps\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, most of the samples (like this \"],[0,[2],1,\"getting started one\"],[0,[],0,\" or this more \"],[0,[3],1,\"real-world one\"],[0,[],0,\") don’t offer any guidance around continuous deployment. They just wave hands and say, “Publish from Visual Studio” or “Publish using PowerShell”. Which is all well and good – but how do you actually do proper DevOps with ServiceFabric Apps?\"]]],[1,\"p\",[[0,[],0,\"Publishing apps to Service Fabric requires that you \"],[0,[1],1,\"package\"],[0,[],0,\" the app and then \"],[0,[1],1,\"publish\"],[0,[],0,\" it. Fortunately VSTS allows you to fairly easily package the app in an automated build and then publish the app in a release.\"]]],[1,\"p\",[[0,[],0,\"There are two primary challenges to doing this:\"]]],[3,\"ol\",[[[0,[],0,\"Versioning. Versioning is critical to Service Fabric apps, so your automated build is going to have to know how to version the app (and its constituent services) correctly\"]],[[0,[],0,\"Publishing – new vs upgrade. The out-of-the-box publish script (that you get when you do a File->New Service Fabric App project) needs to be invoked differently for new apps as opposed to upgrading existing apps. In the pipeline, you want to be able to publish the same way – whether or not the application already exists. Fortunately a couple modifications to the publish script do the trick.\"]]]],[1,\"p\",[[0,[],0,\"Finally, the cluster should be created or updated on the fly during the release – that’s what the ARM templates do.\"]]],[1,\"p\",[[0,[],0,\"To demonstrate a Service Fabric build/release pipeline, I’m going to use a “fork” of the original \"],[0,[4],1,\"VisualObjects\"],[0,[],0,\" sample from the \"],[0,[5],1,\"getting started repo\"],[0,[],0,\" (it’s not a complete fork since I just wanted this one solution from the repo). I’ve added an ARM template project to demonstrate how to create the cluster using ARM during the deployment and then I’ve added two publishing profiles – one for Test and one for Prod. The ARM templates and profiles for both Test and Prod are exactly the same in the repo – in real life you’ll have a beefier cluster in Prod (with different application parameters) than you will in test, so the ARM templates and profiles are going to look different. Having two templates and profiles gives you the idea of how to separate environments in the Release, which is all I want to demonstrate.\"]]],[1,\"p\",[[0,[],0,\"This entire flow works on TFS as well as VSTS, so I’m just going to show you how to do this using VSTS. I’ll call out differences for TFS when necessary.\"]]],[1,\"h3\",[[0,[],0,\"Getting the Code\"]]],[1,\"p\",[[0,[],0,\"The easiest way is to just fork \"],[0,[6],1,\"this repo\"],[0,[],0,\" on Github. You can of course clone the repo, then push it to a VSTS project if you prefer. For this post I’m going to use code that I’ve imported into a VSTS repo. If you’re on TFS, then it’s probably easiest to clone the repo and push it to your TFS server.\"]]],[1,\"h2\",[[0,[],0,\"Setting up the Build\"]]],[1,\"p\",[[0,[],0,\"Unfortunately the Service Fabric SDK isn’t installed on the hosted agent image in VSTS, so you’ll have to use a private agent. Make sure the Service Fabric SDK is installed on the build machine. Use this \"],[0,[7],1,\"help doc\"],[0,[],0,\" to get the bits.\"]]],[1,\"p\",[[0,[],0,\"The next thing you’ll need is my \"],[0,[8],1,\"VersionAssemblies custom build task\"],[0,[],0,\". I’ve bundled it into a \"],[0,[9],1,\"VSTS marketplace extension\"],[0,[],0,\". If you’re on VSTS, just click “Install” – if you’re on TFS, you’ll need to download it and upload it. You’ll only be able to do this on TFS 2015 Update 2 or later.\"]]],[1,\"p\",[[0,[],0,\"Now go to your VSTS account and navigate to the Code hub. Create a new Build definition using the Visual Studio template. Select the appropriate source repo and branch (I’m just going to use master) and select the queue with your private agent. Select Continuous Integration to queue the build whenever a commit is pushed to the repo:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Change the name of the build – I’ve called mine “VisualObjects”. Go to the General tab and change the build number format to be\"]]],[10,1],[1,\"p\",[[0,[],0,\"This will give the build number 1.0.1, then 1.0.2, 1.0.3 and so on.\"]]],[1,\"p\",[[0,[],0,\"Now we want to change the build so that it will match the ApplicationTypeVersion (from the application manifest) and all the Service versions within the ServiceManifests for each service within the application. So click “Add Task” and add two “VersionAssembly” tasks. Drag them to the top of the build (so that they are the first two tasks executed).\"]]],[1,\"p\",[[0,[],0,\"Configure the first one as follows:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Configure the second one as follows:\"]]],[10,3],[1,\"p\",[[0,[],0,\"The first task finds the ApplicationManifest.xml file and replaces the version with the build number. The second task recursively finds all the ServiceManifest.xml files and then also replaces the version number of each service with the build number. After the build, the application and service versions will all match the build number.\"]]],[1,\"p\",[[0,[],0,\"The next 3 tasks should be “NuGet Installer”, “Visual Studio Build” and “Visual Studio Test”. You can leave those as is.\"]]],[1,\"p\",[[0,[],0,\"Add a new “Visual Studio Build” task and place it just below the test task. Configure the Solution parameter to the path of the .sfproj in the solution (src/VisualObjects/VisualObjects/VisualObjects.sfproj). Make the MSBuild Arguments parameter “/t:Package). Finally, add $(BuildConfiguration) to the Configuration parameter. This task invokes Visual Studio to package the Service Fabric app:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now you’ll need to do some copying so that we get all the files we need into the artifact staging directory, ready for publishing. Add a couple “Copy” tasks to the build and configure them as follows:\"]]],[10,5],[1,\"p\",[[0,[],0,\"This copies the Service Fabric app package to the staging directory.\"]]],[10,6],[1,\"p\",[[0,[],0,\"This copies the Scripts folder to the staging directory (we’ll need this in the release to publish the app).\"]]],[10,7],[10,8],[1,\"p\",[[0,[],0,\"These tasks copy the Publish Profiles and ApplicationParameters files to the staging directory. Again, these are needed for the release.\"]]],[1,\"p\",[[0,[],0,\"You’ll notice that there isn’t a copy task for the ARM project – that’s because the ARM project automagically puts its output into the staging directory for you when building the solution.\"]]],[1,\"p\",[[0,[],0,\"You can remove the Source Symbols task if you want to – it’s not going to harm anything if it’s there. If you really want to keep the symbols you’ll have to specify a network share for the symbols to be copied to.\"]]],[1,\"p\",[[0,[],0,\"Finally, make sure that your “Publish Build Artifacts” task is configured like this:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Of course you can also choose a network folder rather than a server drop if you want. The tasks should look like this:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Run the build to make sure that it’s all happy. The artifacts folder should look like this:\"]]],[10,11],[1,\"h2\",[[0,[],0,\"Setting up the Release\"]]],[1,\"p\",[[0,[],0,\"Now that the app is packaged, we’re almost ready to define the release pipeline. There’s a decision to make at this point: to ARM or not to ARM. In order to create the Azure Resource Group containing the cluster from the ARM template, VSTS will need a secure connection to the Azure subscription (follow \"],[0,[10],1,\"these instructions\"],[0,[],0,\"). This connection is service principal based, so you need to have an AAD backing your Azure subscription and you need to have permissions to add new applications to the AAD (being an administrator or co-admin will work – there may be finer-grained RBAC roles for this, I’m not sure). However, if you don’t have an AAD backing your subscription or can’t create applications, you can manually create the cluster in your Azure subscription. Do so now if you’re going to create the cluster(s) manually (one for Test, one for Prod).\"]]],[1,\"p\",[[0,[],0,\"To create the release definition, go to the Release hub in VSTS and create a new (empty) Release. Select the VisualObjects build as the artifact link and set Continuous Deployment. This will cause the release to be created as soon as a build completes successfully. (If you’re on TFS, you will have to create an empty Release and then link the build in the Artifacts tab). Change the name of the release to something meaningful (I’ve called mine VisualObjects, just to be original).\"]]],[1,\"p\",[[0,[],0,\"Change the name of the first environment to “Test”. Edit the variables for the environment and add one called “AdminPassword” and another called “ClusterName”. Set the admin password to some password and padlock it to make it a secret. The name that you choose for the cluster is the DNS name that you’ll use to address your cluster. In my case, I’ve selected “colincluster-test” which will make the URL of my cluster “colincluster-test.eastus.cloudapp.azure.com”.\"]]],[10,12],[1,\"h3\",[[0,[],0,\"Create or Update the Cluster\"]]],[1,\"p\",[[0,[],0,\"If you created the cluster manually, skip to the next task. If you want to create (or update) the cluster as part of the deployment, then add a new “Azure Resource Group Deployment” task to the Test environment. Set the parameters as follows:\"]]],[3,\"ul\",[[[0,[],0,\"Azure Connection Type: Azure Resource Manager\"]],[[0,[],0,\"Azure RM Subscription: set this to the SPN connection you created from \"],[0,[10],1,\"these instructions\"],[0,[],0,\"\"]],[[0,[],0,\"Action: Create or Update Resource Group\"]],[[0,[],0,\"Resource Group: a name for the resource group\"]],[[0,[],0,\"Location: the location of your resource group\"]],[[0,[],0,\"Template: brows to the TestServiceFabricClusterTemplate.json file in the drop using the browse button (…)\"]],[[0,[],0,\"Template Parameters: brows to the TestServiceFabricClusterTemplate.parameters.json file in the drop using the browse button (…)\"]],[[0,[],0,\"Override Template Parameters: set this to -adminPassword (ConvertTo-SecureString '$(AdminPassword)' -AsPlainText -Force) –dnsName $(ClusterName)\"]]]],[1,\"p\",[[0,[],0,\"You can override any other parameters you need to in the Override parameters setting. For now, I’m just overriding the clusterName and adminPassword parameters.\"]]],[10,13],[1,\"h3\",[[0,[],0,\"Replace Tokens\"]]],[1,\"p\",[[0,[],0,\"The Service Fabric profiles contain the cluster connection information. Since you could be creating the cluster on the fly, I’ve tokenized the connection setting in the profile files as follows:\"]]],[10,14],[1,\"p\",[[0,[],0,\"You can see that there is a __ClusterName__ token (the highlighted line). You’ve already defined a value for cluster name that you used in the ARM task. Wouldn’t it be nice if you could simply replace the token called __ClusterName__ with the value of the variable called ClusterName? Since you’ve already installed the \"],[0,[9],1,\"Colin's ALM Corner Build and Release\"],[0,[],0,\" extension from the marketplace, you get the \"],[0,[11],1,\"ReplaceTokens\"],[0,[],0,\" task as well, which does exactly that! Add a ReplaceTokens task and set it as follows:\"]]],[10,15],[1,\"p\",[[0,[12,13],2,\"IMPORTANT NOTE!\"],[0,[],0,\" The templates I’ve defined are not secured. In production, you’ll want to secure your clusters. The connection parameters then need a few more tokens like the ServerCertThumbprint and so on. You can also make these tokens that the ReplaceTokens task can substitute. Just note that if you make any of them secrets, you’ll need to specify the secret values in the Advanced section of the task.\"]]],[1,\"h3\",[[0,[],0,\"Deploying the App\"]]],[1,\"p\",[[0,[],0,\"Now that we have a cluster and we have a profile that can connect to the cluster, and we have a package ready to deploy, we can invoke the PowerShell scrip to deploy! Add a “Powershell Script” task and configure it as follows:\"]]],[3,\"ul\",[[[0,[],0,\"Type: File Path\"]],[[0,[],0,\"Script filename: browse to the Deploy-FabricApplication.ps1 script in the drop folder (under drop/SFPackage/Scripts)\"]],[[0,[],0,\"Arguments: Set to -PublishProfileFile ../PublishProfiles/TestCloud.xml -ApplicationPackagePath ../Package\"]]]],[1,\"p\",[[0,[],0,\"The script needs to take at least the PublishProfile path and then the ApplicationPackage path. These paths are relative to the Scripts folder, so expand Advanced and set the working folder to the Scripts directory:\"]]],[10,16],[1,\"p\",[[0,[],0,\"That’s it! You can now run the release to deploy it to the Test environment. Of course you can add other tasks (like Cloud Load Tests etc.) and approvals. Go wild.\"]]],[1,\"h3\",[[0,[],0,\"Changes to the OOB Deploy Script\"]]],[1,\"p\",[[0,[],0,\"I mentioned earlier that this technique has a snag: if the release creates the cluster (or you’ve created an empty cluster manually) then the Deploy script will fail. The reason is that the profile includes an <UpgradeDeployment> tag that tells the script to upgrade the app. If the app exists, the script works just fine – but if the app doesn’t exist yet, the deployment will fail. So to work around this, I modified the OOB script slightly. I just query the cluster to see if the app exists, and if it doesn’t, the script calls the Publish-NewServiceFabricApplication cmdlet instead of the Publish-UpgradedServiceFabricApplication. Here are the changed lines:\"]]],[10,17],[1,\"p\",[[0,[],0,\"Lines 1 to 185 of the script are original, (I show line 185 as the first line of this snippet). The if statement alters slightly to take the $AppExists into account – the remainder of the script is as per the OOB script.\"]]],[10,18],[1,\"p\",[[0,[],0,\"Now that you have the Test environment, you can clone it to the Prod environment. Change the parameter values (and the template and profile paths) to make the prod-specific and you’re done! One more tip: if you change the release name format (under the general tab) to\"]]],[10,19],[1,\"p\",[[0,[],0,\"then you’ll get the build number as part of the release number.\"]]],[1,\"p\",[[0,[],0,\"Here you can see my cluster with the Application Version matching the build number:\"]]],[10,20],[1,\"p\",[[0,[],0,\"Sweet! Now I can tell which build was used for my application right from my cluster!\"]]],[1,\"h3\",[[0,[],0,\"See the Pipeline in Action\"]]],[1,\"p\",[[0,[],0,\"A fun demo to do is to deploy the app and then open up the VisualObjects url – that will be at clustername.eastus.cloudapp.azure.com:8082/VisualObjects (where clustername is the name of your cluster). When you see the bouncing triangles.\"]]],[1,\"p\",[[0,[],0,\"Then you can edit\"]]],[10,21],[1,\"p\",[[0,[],0,\"in Visual Studio or in the Code hub in VSTS. Look around line 50 for\"]]],[10,22],[1,\"p\",[[0,[],0,\"and change it to\"]]],[10,23],[1,\"p\",[[0,[],0,\". This will cause the triangle to start rotating. Commit the change and push it to trigger the build and the release. Then monitor the Service Fabric UI to see the upgrade trigger (from the release) and watch the triangles to see how they are upgraded in the Service Fabric rolling upgrade.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Service Fabric is awesome – and creating a build/release pipeline for Service Fabric apps in VSTS is a snap thanks to an amazing build/release engine – and some cool custom build tasks!\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1461872494000,"status":"published","published_by":1},{"id":"edab574f-cfe4-4f95-8afe-0db906a015d8","title":"Continuous Deployment with Docker and Build vNext","slug":"continuous-deployment-with-docker-and-build-vnext","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"function Set-DockerEnv {\\n    Write-Host \\\"Getting docker environment settings\\\" -ForegroundColor Yellow\\n    docker-machine env docker | ? { $_.Contains('export') } | % { $_.Replace('export ', '') } | `\\n        ConvertFrom-Csv -Delimiter \\\"=\\\" -Header \\\"Key\\\",\\\"Value\\\" | % { \\n            [Environment]::SetEnvironmentVariable($_.Key, $_.Value)\\n            Write-Host \\\"$($_.Key) = $($_.Value)\\\" -ForegroundColor Gray\\n        }\\n    Write-Host \\\"Done!\\\" -ForegroundColor Green\\n}\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/de79a882-4509-436e-b1fd-c16dd2b8e4d5.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/19648e75-30c9-4d00-921f-fcee25c486c4.png\\\" width=\\\"292\\\" height=\\\"230\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/772b5fae-c29c-48df-9ab2-1cbe4b3a444f.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/444c9664-94c1-4922-8968-90925e6b16e2.png\\\" width=\\\"294\\\" height=\\\"234\\\"></a>\"}],[\"code\",{\"code\":\"--tls --tlscert=c:\\\\users\\\\colin\\\\.docker\\\\machine\\\\certs\\\\cert.pem --tlskey=c:\\\\users\\\\colin\\\\.docker\\\\machine\\\\certs\\\\key.pem\\n\\n\",\"language\":\"\"}],[\"code\",{\"code\":\"FROM microsoft/aspnet:1.0.0-beta6\\n\\nADD . /app\\n\\nWORKDIR /app\\n\\nENTRYPOINT [\\\"./kestrel\\\"]\\n\\n\",\"language\":\"\"}],[\"code\",{\"code\":\"FROM windowsservercore\\n\\nADD . /app\\n\\nWORKDIR /app\\n\\nENTRYPOINT [\\\"cmd.exe\\\", \\\"/k\\\", \\\"web.cmd\\\"]\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/988047ed-7b44-412e-9df1-b71e4c75abfb.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fa59a5da-eba6-4b0c-91a8-c99ff20825e3.png\\\" width=\\\"238\\\" height=\\\"114\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/852aa52e-c937-4c92-85c0-51fb3f792a85.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/10654a6d-f1d0-46d4-bf83-bf194b8f5ca8.png\\\" width=\\\"313\\\" height=\\\"170\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f31824e0-7e6a-4f3d-8cef-092962a65d86.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f659f186-1934-4fb1-9048-c0f8b9b9247f.png\\\" width=\\\"244\\\" height=\\\"236\\\"></a>\"}],[\"code\",{\"code\":\"param (\\n    [string]$srcDir = $env.BUILD_SOURCESDIRECTORY\\n)\\n\\n# bootstrap DNVM into this session.\\n&amp;{$Branch='dev';iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/aspnet/Home/dev/dnvminstall.ps1'))}\\n\\n# load up the global.json so we can find the DNX version\\n$globalJsonFile = (Get-ChildItem -Path $srcDir -Filter \\\"global.json\\\" -Recurse | Select -First 1).FullName\\n$globalJson = Get-Content -Path $globalJsonFile -Raw -ErrorAction Ignore | ConvertFrom-Json -ErrorAction Ignore\\n\\nif($globalJson)\\n{\\n    $dnxVersion = $globalJson.sdk.version\\n}\\nelse\\n{\\n    Write-Warning \\\"Unable to locate global.json to determine using 'latest'\\\"\\n    $dnxVersion = \\\"latest\\\"\\n}\\n\\n# install DNX\\n# only installs the default (x86, clr) runtime of the framework.\\n# If you need additional architectures or runtimes you should add additional calls\\n&amp; $env:USERPROFILE\\\\.dnx\\\\bin\\\\dnvm install $dnxVersion -r coreclr\\n&amp; $env:USERPROFILE\\\\.dnx\\\\bin\\\\dnvm install $dnxVersion -Persistent\\n\\n # run DNU restore on all project.json files in the src folder including 2&gt;1 to redirect stderr to stdout for badly behaved tools\\nGet-ChildItem -Path $srcDir -Filter project.json -Recurse | ForEach-Object {\\n    Write-Host \\\"Running dnu restore on $($_.FullName)\\\"\\n    &amp; dnu restore $_.FullName 2&gt;1\\n}\\n\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/01b2f7d4-d731-43e3-a63d-2ca6a56b8664.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/da6bdd07-4250-40df-aaa8-1bd93802dfb0.png\\\" width=\\\"392\\\" height=\\\"189\\\"></a>\"}],[\"code\",{\"code\":\"/t:Build,FileSystemPublish /p:IgnoreDNXRuntime=true /p:PublishConfiguration=$(BuildConfiguration) /p:PublishOutputPathNoTrailingSlash=$(Build.StagingDirectory)\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7e50a160-67d5-461a-935a-28bbbd28a5b5.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/143822bb-ee5f-475e-8294-08f0e08a2b44.png\\\" width=\\\"283\\\" height=\\\"220\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/32b552c5-89b2-4b6d-b7b7-f7e5a3db2aff.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/526249ff-7982-4e7b-b2a1-11c73998d929.png\\\" width=\\\"376\\\" height=\\\"181\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0f4848a0-a301-4de8-bd0e-023e9838a242.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bc13b054-4caf-4ae8-8bc7-aad33bd00796.png\\\" width=\\\"357\\\" height=\\\"58\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c79fbd39-cb2f-4837-9ca5-f84f9cc1db96.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/84110c04-72de-42cc-8cc4-83d4e62395c9.png\\\" width=\\\"314\\\" height=\\\"231\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.docker.com/\"]],[\"a\",[\"href\",\"http://blog.nigelpoulton.com/\"]],[\"a\",[\"href\",\"http://www.pluralsight.com/courses/docker-deep-dive\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/virtualization/windowscontainers/about/about_overview\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-docker-ubuntu-quickstart/\"]],[\"a\",[\"href\",\"https://www.docker.com/toolbox\"]],[\"a\",[\"href\",\"http://boot2docker.io/\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/0f5b2caa-ea00-41c8-b8a2-058c7da0b3e4\"]],[\"em\"],[\"a\",[\"href\",\"https://hub.docker.com/r/microsoft/aspnet/\"]],[\"a\",[\"href\",\"https://hub.docker.com/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-should-switch-to-build-vnext\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/Library/vs/alm/Build/azure/deploy-aspnet5\"]],[\"a\",[\"href\",\"https://github.com/aspnet/dnvm\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/DockerPublish\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks.git\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I really like the idea of \"],[0,[0],1,\"Docker\"],[0,[],0,\". If you’re unfamiliar with Docker, then I highly recommend \"],[0,[1],1,\"Nigel Poulton’s\"],[0,[],0,\" \"],[0,[2],1,\"Docker Deep Dive\"],[0,[],0,\" course on Pluralsight. Containers have been around for quite a while in the Linux world, but Microsoft is jumping on the bandwagon with \"],[0,[3],1,\"Windows Server Containers\"],[0,[],0,\" too. This means that getting to grips with containers is a good idea – I think it’s the way of the future.\"]]],[1,\"h2\",[[0,[],0,\"tl;dr\"]]],[1,\"p\",[[0,[],0,\"If you’re just after the task, then go to my \"],[0,[4],1,\"Github repo\"],[0,[],0,\". You can get some brief details in the section below titled “Challenge 2: Publishing to Docker (a.k.a The Publish Build Task)”. If you want the full story read on!\"]]],[1,\"h2\",[[0,[],0,\"Deploying Apps to Containers\"]]],[1,\"p\",[[0,[],0,\"After hacking around a bit with containers, I decided to see if I could deploy some apps into a container manually. Turns out it’s not too hard. You need to have (at least) the Docker client, some code and a Dockerfile. Then you can just call a “docker build” (which creates an image) and then “docker run” to deploy an instance of the image.\"]]],[1,\"p\",[[0,[],0,\"Once I had that working, I wanted to see if I could bundle everything up into a build in Build vNext. That was a little harder to do.\"]]],[1,\"h3\",[[0,[],0,\"Environment and Tools\"]]],[1,\"p\",[[0,[],0,\"You can run a \"],[0,[5],1,\"Docker host in Azure\"],[0,[],0,\" but I wanted to be able to work locally too. So here is how I set up my local environment (on my Surface Pro 3):\"]]],[3,\"ol\",[[[0,[],0,\"I enabled Hyper-V (you can also use VirtualBox) so that I can run VMs\"]],[[0,[],0,\"I installed \"],[0,[6],1,\"Docker Toolbox\"],[0,[],0,\" (note: Docker Toolbox bundles VirtualBox – so you can use that if you don’t have or want Hyper-V, but otherwise, don’t install it)\"]],[[0,[],0,\"I then created a Docker host using “docker-machine create”. I used the hyper-v driver. This creates a Tiny Core Linux Docker host running the \"],[0,[7],1,\"boot2docker\"],[0,[],0,\" image.\"]],[[0,[],0,\"I set my environment variables to default to my docker host settings\"]],[[0,[],0,\"I could now execute “docker” commands – a good command to sanity check your environment is “docker info”\"]]]],[1,\"h4\",[[0,[],0,\"Aside: PowerShell to Set Docker Environment\"]]],[1,\"p\",[[0,[],0,\"I love using PowerShell. If you run “docker env” you get some settings that you could just “cat” to your profile (if you’re in Unix). However, the commands won’t work in PowerShell. So I created a small function that I put into my $PROFILE that I can run whenever I need to do any Docker stuff. Here it is:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now I can just run “Set-DockerEnv” whenever I need to set the environment.\"]]],[1,\"h3\",[[0,[],0,\"VS Tools for Docker\"]]],[1,\"p\",[[0,[],0,\"So I have a Docker environment locally – great. Now I need to be able to deploy something into a container! Since I (mostly) use Visual Studio 2015, I installed the \"],[0,[8],1,\"VS Tools for Docker\"],[0,[],0,\" extension. Make sure you follow the install instructions carefully – the preview toolset is a bit picky. I wanted to play with Windows containers, but for starters I was working with Unix containers, so I needed some code that could run on Unix. Fortunately, ASP.NET 5 can! So I did a File –> New Project and created an ASP.NET 5 Web application (this is a boilerplate MVC 6 application). Once I had the project created, I right-clicked the project and selected “Publish” to see the publish page. You’ll see the “Docker Containers” target:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can select an Azure Docker VM if you have one – in my case I wanted to deploy locally, so I checked “Custom Docker Host” and clicked OK. I entered in the server url for my local docker host (tcp://10.0.0.19:2376) and left all the rest as default. Clicking “Validate Connection” however, failed. After some trial and error, I realized that the default directory for certificates for the “docker-machine” command I used is different to the default directory the VS Tools for Docker expects. So I just supplied additional settings for “Auth Options” and voila – I could now validate the connection:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Here are the settings for “Auth Options” in full:\"]]],[10,3],[1,\"p\",[[0,[],0,\"I specifically left the Dockerfile setting to \"],[0,[9],1,\"(auto generate)\"],[0,[],0,\" to see what I would get. Here’s what VS generated:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ol\",[[[0,[],0,\"The FROM tells Docker which image to base this container on – it’s defaulting to the official image for \"],[0,[10],1,\"ASP.NET 5\"],[0,[],0,\" from \"],[0,[11],1,\"Docker hub\"],[0,[],0,\" (the public Docker image repository)\"]],[[0,[],0,\"ADD is copying all the files in the current directory (.) to a folder on the container called “/app”\"]],[[0,[],0,\"WORKDIR is changing directory into “/app”\"]],[[0,[],0,\"ENTRYPOINT tells Docker to run this command every time a container based on this image is fired up\"]]]],[1,\"h4\",[[0,[],0,\"Aside: Retargeting OS\"]]],[1,\"p\",[[0,[],0,\"Once you’ve generated the Dockerfile, you need to be careful if you want to deploy to a different OS (specifically Windows vs non-Windows). Rename the Dockerfile (in the root project directory) to “Docker.linux” or something and then clear the Dockerfile setting. VS will then auto generate a Dockerfile for deploying to Windows containers. Here’s the Windows flavored Dockerfile, just for contrast:\"]]],[10,5],[1,\"p\",[[0,[],0,\"VS is even smart enough to nest your Dockerfiles in your solution!\"]]],[10,6],[1,\"p\",[[0,[],0,\"So I could now publishing successfully from VS. Next up: deploying from Team Build!\"]]],[1,\"h2\",[[0,[],0,\"Docker Publish from Team Build vNext\"]]],[1,\"p\",[[0,[],0,\"(Just to make things a little simple, I going to use “build” interchangeably with “build vNext” or even “team build”. I’ve switched over completely from the old XAML builds – \"],[0,[12],1,\"so should you\"],[0,[],0,\").\"]]],[1,\"p\",[[0,[],0,\"If you’ve looked at the build tasks on VSO, you’ll notice that there is a “Docker” task:\"]]],[10,7],[1,\"p\",[[0,[],0,\"It’s a little unsatisfying, to be blunt. You can (supposedly) deploy a docker image – but there’s no way to get your code into the image (“docker build” the image) in the first place. Secondly, there doesn’t appear to be any security or advanced settings anywhere. Clicking “More Information” takes you to a placeholder markdown file – so no help there. Admittedly the team are still working on the “official” Docker tasks – but I didn’t want to wait!\"]]],[1,\"h3\",[[0,[],0,\"Prep: PowerShell Docker Publishing from the console\"]]],[1,\"p\",[[0,[],0,\"Taking a step back and delving into the files and scripts that VS Tools for Docker generated for me, I decided to take a stab at deploying using the PowerShell script in the PublishProfiles folder of my solution. I created a publish profile called “LocalDocker”, and sure enough VS had generated 3 files: the pubxml file (settings), a PowerShell publish file and a shell script publish file.\"]]],[10,8],[1,\"p\",[[0,[],0,\"To invoke the script, you need 3 things:\"]]],[3,\"ol\",[[[0,[],0,\"The path to the files that are going to be published\"]],[[0,[],0,\"The path to the pubxml file (contains the settings for the publish profile)\"]],[[0,[],0,\"(Optional) A hashtable of overrides for your settings\"]]]],[1,\"p\",[[0,[],0,\"I played around with the PowerShell script in my console – the first major flaw in the script is that it assumes you have already “packed” the project. So you first have to invoke msbuild with some obscure parameters. Only then can you invoke the Publish script. Also, the publish script does some hocus pocus with assuming that the script name and the pubxml file name are the same and working out the Dockerfile location is also a bit voodoo. It works nicely when you’re publishing from VS – but I found it not to be very “build friendly”.\"]]],[1,\"p\",[[0,[],0,\"I tried it in build vNext anyway. I managed to invoke the “LocalDocker-publish.ps1” file, but could not figure out how to pass a hashtable to a PowerShell task (to override settings)! Besides, even if it worked, there’d be a lot of typing and you have to know what the keys are for each setting. Enter the custom build task!\"]]],[1,\"p\",[[0,[],0,\"The way I saw it, I had to:\"]]],[3,\"ol\",[[[0,[],0,\"Compile the solution in such a way that it can be deployed into a Docker container\"]],[[0,[],0,\"Create a custom task that could invoke the PowerShell publish script, either from a pubxml file or some settings (or a combination)\"]]]],[1,\"h3\",[[0,[],0,\"Challenge 1: Building ASP.NET 5 Apps in Build vNext\"]]],[1,\"p\",[[0,[],0,\"Building ASP.NET 5 applications in build vNext isn’t as simple as you would think. I managed to find part of my answer in \"],[0,[13],1,\"this post\"],[0,[],0,\". You have to ensure that the \"],[0,[14],1,\"dnvm\"],[0,[],0,\" is installed and that you have the correct runtimes. Then you have to invoke “dnu restore” on all the projects (to install all the dependencies). That doesn’t get you all the way though – you need to also “dnu pack” the project.\"]]],[1,\"p\",[[0,[],0,\"Fortunately, you can invoke a simple script to get the dnvm and install the correct runtime. Then you can fight with supply some parameters to msbuild that tell it to pack the project for you. This took me dozens of iterations to get right – and even when I got it right, the “kestrel” command in my containers was broken. For a long time I thought that my docker publish task was broken – turns out I could fix it with a pesky msbuild argument. Be that as it may, my pain is your gain – hence this post!\"]]],[1,\"p\",[[0,[],0,\"You need to add this script to your source repo and invoke it in your build using a PowerShell task (note this is modified from the original in \"],[0,[13],1,\"this post\"],[0,[],0,\"):\"]]],[10,9],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ol\",[[[0,[],0,\"Lines 9,10: I had to change the way that the script searched for the “global.json” (where the runtime is specified). So I’m using BUILD_SOURCESDIRECTORY which the build engine passes in.\"]],[[0,[],0,\"Line 25: I added the coreclr (since I wanted to deploy to Linux)\"]],[[0,[],0,\"Lines 29-32: Again I changed the search path for the “dnu” commands\"]]]],[1,\"p\",[[0,[],0,\"Just add a PowerShell task into your build (before the VS Build task) and browse your repo to the location of the script:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Now you’re ready to call msbuild. In the VS Build task, set the solution to the xproj file (the project file for the ASP.NET project you want to publish). Then add the following msbuild arguments:\"]]],[10,11],[1,\"p\",[[0,[],0,\"You’re telling msbuild:\"]]],[3,\"ol\",[[[0,[],0,\"Run the build and FileSystemPublish targets\"]],[[0,[],0,\"Ignore the DXN runtime (this fixed my broken kestrel command)\"]],[[0,[],0,\"Use the $(BuildConfiguration) setting as the configuration (Debug, Release etc.)\"]],[[0,[],0,\"Publish (dnu pack) the site to the staging directory\"]]]],[1,\"p\",[[0,[],0,\"That now gets you ready to publish to Docker – hooray!\"]]],[1,\"h3\",[[0,[],0,\"Challenge 2: Publishing to Docker (a.k.a The Publish Build Task)\"]]],[1,\"p\",[[0,[],0,\"A note on the build agent machine: the agent must have access to the docker client, as well as your certificate files. You can supply the paths to the certificate files in the Auth options for the task, but you’ll need to make sure the docker client is installed on the build machine.\"]]],[1,\"p\",[[0,[],0,\"Now that we have some deployable code, we can finally turn our attention to publishing to Docker. I wanted to be able to specify a pubxml file, but then override any of the settings. I also wanted to be able to publish without a pubxml file. So I created a custom build task. The task has 3 internal components:\"]]],[3,\"ol\",[[[0,[],0,\"The task.json – this specifies all the parameters (including the pubxml path, the path to the “packed” solution, and all the overrides)\"]],[[0,[],0,\"A modified version of the publish PowerShell script that VS Tools for Docker generated when I created a Docker publish profile\"]],[[0,[],0,\"A PowerShell script that takes the supplied parameters, creates a hashtable, and invokes the publish PowerShell script\"]]]],[1,\"p\",[[0,[],0,\"The source code for the task is on \"],[0,[15],1,\"Github\"],[0,[],0,\". I’m not going to go through all the details here. To get the task, you’ll need to do the following:\"]]],[3,\"ol\",[[[0,[],0,\"Clone the repo (git clone \"],[0,[16],1,\"https://github.com/colindembovsky/cols-agent-tasks.git\"],[0,[],0,\")\"]],[[0,[],0,\"Install node and npm\"]],[[0,[],0,\"Install tfx-cli (npm install –g tfx-cli)\"]],[[0,[],0,\"Login (tfx login)\"]],[[0,[],0,\"Upload (tfx build tasks upload \"],[0,[9],1,\"pathToDockerPublish\"],[0,[],0,\")\"]]]],[1,\"p\",[[0,[],0,\"The \"],[0,[9],1,\"pathToDockerPublish\"],[0,[],0,\" is the path to Tasks/DockerPublish in the repo.\"]]],[1,\"p\",[[0,[],0,\"Once you’ve done that, you can then add a Docker Publish task. Set the path to your pubxml (if you have one), the path to the Dockerfile you want to use and set the Pack Output Path to \"],[0,[9],1,\"$(Build.StagingDirectory)\"],[0,[],0,\" (or wherever the code you want to deploy to the container is). If you don’t have a pubxml file, leave “Path to Pubxml” empty – you’ll have to supply all the other details. If you have a pubxml, but want to overwrite some settings, just enter the settings in accordingly. The script will take the pubxml setting unless you supply a value in an override.\"]]],[10,12],[1,\"p\",[[0,[],0,\"In this example, I’m overriding the docker image name (using the build number as the tag) and specifying “Build only” as false. That means the image will be built (using “docker build”) and a container will be spun up. Set this value to “true” if you just want to build the image without deploying a container. Here are all the settings:\"]]],[3,\"ul\",[[[0,[],0,\"Docker Server Url – url of your docker host\"]],[[0,[],0,\"Docker image name – name of the image to build\"]],[[0,[],0,\"Build Only – true to just run “docker build” – false if you want to execute “docker run” after building\"]],[[0,[],0,\"Host port – the port to open on the host\"]],[[0,[],0,\"Container port – the port to open on the container\"]],[[0,[],0,\"Run options – additional arguments passed to the “docker run” command\"]],[[0,[],0,\"App Type – can be empty or “Web”. Only required for ASP.NET applications (sets the \"],[0,[9],1,\"server.urls\"],[0,[],0,\" setting)\"]],[[0,[],0,\"Create Windows Container – set to “true” if you’re targeting a Windows docker host\"]],[[0,[],0,\"Auth Options – additional arguments to supply to docker commands (for example --tlscert)\"]],[[0,[],0,\"Remove Conflicting Containers – removes containers currently running on the same port when set to “true”\"]]]],[1,\"h2\",[[0,[],0,\"Success\"]]],[1,\"p\",[[0,[],0,\"Once the build completes, you’ll be able to see the image in “docker images”.\"]]],[10,13],[10,14],[1,\"p\",[[0,[],0,\"If you’ve set “build only” to false you’ll be able to access your application!\"]]],[10,15],[1,\"p\",[[0,[],0,\"Happy publishing!\"]]]]}","published_at":1442595579000,"status":"published","published_by":1},{"id":"37e6514b-1351-4e4a-b9fd-efa03efe5097","title":"Contributions to TFS Build Extensions","slug":"contributions-to-tfs-build-extensions","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://tfsbuildextensions.codeplex.com/\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/02/custom-build-task-include-merged.html\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/02/failing-builds-based-on-code-coverage.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Some of the code I’ve written before has made it into the \"],[0,[0],1,\"TFS Build Extensions\"],[0,[],0,\" latest release. They are my \"],[0,[1],1,\"“Include Merges (and associated work items) in a build\\\"\"],[0,[],0,\" and “\"],[0,[2],1,\"Fail code based on Code Coverage\"],[0,[],0,\"” activities.\"]]],[1,\"p\",[[0,[],0,\"If you use the activities, make sure to leave comments or suggestions on the codeplex site.\"]]],[1,\"p\",[[0,[],0,\"Happy build customizing!\"]]]]}","published_at":1378945380000,"status":"published","published_by":1},{"id":"c50beb53-e089-4ab2-9bde-d0eb2a91c82e","title":"CrossBrowser Testing: ChromeDriver Window Hangs after Test","slug":"crossbrowser-testing-chromedriver-window-hangs-after-test","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"[TestMethod]\\npublic void TestTimesheetIsDeployedChrome()\\n{\\n    BrowserWindow.CurrentBrowser = \\\"chrome\\\";\\n\\n    var testUrl = ConfigurationManager.AppSettings[\\\"TimesheetUrl\\\"];\\n    Assert.IsFalse(string.IsNullOrEmpty(testUrl), \\\"Could not find testUrl in App Settings\\\");\\n    var window = BrowserWindow.Launch(testUrl);\\n\\n    UIMap.Login();\\n    UIMap.ValidateLogonSuccess();\\n    // ...\\n    UIMap.LogOff();\\n    UIMap.ValidateLogoffSucceeded();\\n\\n    window.Close();\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-4hs5IJmraW8/UuuSFTZDWzI/AAAAAAAABOI/XdqQxWeJumg/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-vG1XGLrHts8/UuuSGdgHH7I/AAAAAAAABOQ/wTnFZ5Ewv1k/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"321\\\" height=\\\"175\\\"></a>\"}],[\"code\",{\"code\":\"// kill the process so we don't lock up - for some reason window.Close() locks\\nwindow.Process.Kill();\\n\\nvar procs = Process.GetProcesses().Where(p =&gt; p.ProcessName.ToLower().Contains(\\\"chromedriver\\\"));\\nforeach (var p in procs)\\n{\\n    p.Kill();\\n}\\n\",\"language\":\"csharp;\"}]],\"markups\":[[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/11cfc881-f8c9-4f96-b303-a2780156628d\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have been doing some coded UI testing and running tests using Chrome (via the \"],[0,[0],1,\"Selenium components\"],[0,[],0,\"). However, I noticed that when my test completed successfully, the Selenium (ChromeDriver) window stayed open and never terminated. Here’s a code snippet of my original code:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Pretty straight forward. Except that the call to “window.Close();” closed the browser, but not the ChromeDriver command window – so the test never terminated.\"]]],[1,\"p\",[[1,[],0,1]]],[10,1],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"So even though the test passes, I have to manually close the command window before the test run itself terminates.\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"After playing around a bit, I came up with this code to kill the command window (this works for Chrome – haven’t tested it for Firefox). Just replace “window.Close()” with this code:\"]]],[10,2],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"That did it nicely for me (and made my Build-Deploy-Test workflows in my lab terminate correctly).\"]]],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"Happy (cross-browser) testing!\"]]]]}","published_at":1391206020000,"status":"published","published_by":1},{"id":"2c998041-3f46-4481-8c07-11def62db972","title":"Custom Code Review Checkin Policy","slug":"custom-code-review-checkin-policy","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"http://i1.visualstudiogallery.msdn.s-msft.com/c476b708-77a8-4065-b9d0-919ab688f078/image/file/91116/1/capture.png\",\"alt\":\"\",\"title\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-bK5Hs1G5t2s/UNOEBvcXJtI/AAAAAAAAAiI/P1EkUhJXmkw/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-4zufP2hpTXg/UNOEC2eJ49I/AAAAAAAAAiQ/v4KUoVUk6Y4/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"291\\\" height=\\\"177\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/c476b708-77a8-4065-b9d0-919ab688f078\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Over the last couple of months, I’ve been doing a lot of VS / TFS 2012 demos, as well as installing / configuring / customizing TFS at customers. Everyone loves the Code Review Workflow, but inevitably the question gets asked, “Can I enforce code review on check-in”? Out of the box you can’t, but I created a custom policy for this.\"]]],[1,\"p\",[[0,[],0,\"You can get the policy from the \"],[0,[0],1,\"VS Gallery\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Note: this policy only works with “out of the box” Code Review Request and Response work item types in TFS 2012 and for VS 2012.\"]]],[1,\"p\",[[0,[],0,\"You can also configure how “strict” the review policy should be:\"]]],[3,\"ul\",[[[0,[],0,\"The policy will fail if the Code Review Request is not Closed\"]],[[0,[],0,\"The policy will fail if any response result is 'Needs Work'\"]],[[0,[],0,\"The policy will pass if all response results are 'Looks Good' or\"]],[[0,[],0,\"the policy will pass if all response results are 'Looks Good' or 'With Comments'\"]]]],[10,0],[1,\"p\",[[0,[],0,\"If the policy fails, you’ll get a friendly message reminding you to get a review!\"]]],[10,1],[1,\"p\",[[0,[],0,\"Happy reviewing!\"]]]]}","published_at":1356075180000,"status":"published","published_by":1},{"id":"d0d94ae6-97c3-48db-b341-bd173dc92270","title":"DacPac Change Report Task for VSTS Builds","slug":"dacpac-change-report-task-for-vsts-builds","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a86743a3-166a-4194-8981-d209dad9f5cc.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/ece36d55-850e-4d5b-9b6b-b61d9c2d5700.png\\\" width=\\\"557\\\" height=\\\"202\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/fa4526a0-91ef-4038-a50b-41ac2397b558.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b8aba5df-ee08-4199-b335-c31f88029ddf.png\\\" width=\\\"451\\\" height=\\\"255\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.red-gate.com/products/sql-development/readyroll/?gclid=CJDDxZ7Nqs4CFbEV0wodFIUCJA\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=redgatesoftware.redgate-readyroll\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=colinsalmcorner.colinsalmcorner-buildtasks\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Most development requires working against some kind of database. Some teams choose to use Object Relational Mappers (ORMs) like Entity Framework. I think that should be the preferred method of dealing with databases (especially code-first), but there are times when you just have to work with a database schema.\"]]],[1,\"p\",[[0,[],0,\"Recently I had to demo \"],[0,[0],1,\"ReadyRoll\"],[0,[],0,\" in VSTS. I have to be honest that I don’t like the paradigm of ReadyRoll – migration-based history seems like a mess compared to model-based history (which is the approach that SSDT takes). That’s a subject for another post (some day) or a discussion over beers. However, there was one thing that I really liked – the ability to preview database changes in a build. The \"],[0,[1],1,\"ReadyRoll extension\"],[0,[],0,\" on the VSTS marketplace allows you to do just that.\"]]],[1,\"p\",[[0,[],0,\"So I stole the idea and made a task that allows you to see SSDT schema changes from build to build.\"]]],[1,\"h2\",[[0,[],0,\"Using the Task\"]]],[1,\"p\",[[0,[],0,\"Let’s consider the scenario: you have an SSDT project in source control and you’re building the dacpac in a Team Build. What the task does is allow you to see what’s changed from one build to the next. Here’s what you need to do:\"]]],[3,\"ol\",[[[0,[],0,\"Install \"],[0,[2],1,\"Colin’s ALM Corner Build Tasks Extension\"],[0,[],0,\" from the VSTS Marketplace\"]],[[0,[],0,\"Edit the build definition and go to Options. Make sure “Allow Scripts to Access OAuth Token” is checked, since the task requires this. (If you forget this, you’ll see 403 errors in the task log).\"]],[[0,[],0,\"Make sure that the dacpac you want to compare is being published to a build drop.\"]],[[0,[],0,\"Add a “DacPac Schema Compare” task\"]]]],[1,\"p\",[[0,[],0,\"That’s it! Here’s what the task looks like:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Enter the following fields:\"]]],[3,\"ol\",[[[0,[],0,\"The name of the drop that your dacpac file is going to be published to. The task will look up the last successful build and download the drop in order to get the last dacpac as the source to compare.\"]],[[0,[],0,\"The name of the dacpac (without the extension). This is typically the name of the SSDT project you’re building.\"]],[[0,[],0,\"The path to the compiled dacpac for this build – this is the target dacpac path and is typically the bin folder of the SSDT project.\"]]]],[1,\"p\",[[0,[],0,\"Now run your build. Once the build completes, you’ll see a couple new sections in the Build Summary:\"]]],[10,1],[1,\"p\",[[0,[],0,\"The first section shows the schema changes, while the second shows a SQL-CMD file so you can see what would be generated by SqlPackage.exe.\"]]],[1,\"p\",[[0,[],0,\"Now you can preview schema changes of your SSDT projects between builds! As usual, let me know here, on Twitter or on Github if you have issues with the task.\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1470437844000,"status":"published","published_by":1},{"id":"1fa924fa-8e75-4153-83cd-de3667eae737","title":"Default Team Build to Invoke Instead of Build","slug":"default-team-build-to-invoke-instead-of-build","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TXfWvLzhc2I/AAAAAAAAAPc/P3t0dd1hZUM/s1600-h/image%5B4%5D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TXfWxcad5UI/AAAAAAAAAPg/hZpTSlhKjOk/image_thumb%5B2%5D.png?imgmax=800\\\" width=\\\"581\\\" height=\\\"266\\\"></a>\"}],[\"code\",{\"code\":\"<font color=\\\"#0000ff\\\">@echo off<br> <br><font color=\\\"#ff0000\\\">rem =============<br>rem do stuff here<br>rem =============</font><br> <br>IF %ERRORLEVEL% NEQ 0 GOTO Err<br>GOTO End<br> <br>:Err<br>@echo An error occurred<br>exit /B 1<br> <br>:End<br>@echo Done!</font>\",\"language\":\"\"}],[\"code\",{\"code\":\"<font face=\\\"Arial\\\">Obviously you’ll replace the actual “work” that you want this script to perform where the red “rems” are. That was where we put our xcopy.</font>\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TXfWy2zHuCI/AAAAAAAAAPk/qyW232vVSEw/s1600-h/image%5B14%5D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TXfW04xoDzI/AAAAAAAAAPo/CqKjFUnBKxY/image_thumb%5B20%5D.png?imgmax=800\\\" width=\\\"249\\\" height=\\\"750\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TXfW2l-JvnI/AAAAAAAAAPs/2mX3fsCxxaw/s1600-h/image%5B19%5D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TXfW41xltkI/AAAAAAAAAPw/9p39E25T3Yk/image_thumb%5B26%5D.png?imgmax=800\\\" width=\\\"575\\\" height=\\\"302\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TXfW6a6fgDI/AAAAAAAAAP0/pVoCtOfyeuU/s1600-h/image%5B26%5D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TXfW9Uob7TI/AAAAAAAAAP4/OFI43AxR0Sg/image_thumb%5B34%5D.png?imgmax=800\\\" width=\\\"328\\\" height=\\\"570\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://cid-64a24e0938d6d062.office.live.com/self.aspx/Colin%5E4s%20ALM%20Corner/NoCompileInvokeScriptTemplate.xaml\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Sometimes you need to do a “deployment” that doesn’t involve a build of source code – for example, I have been working at a customer in gorgeous Durban and they’ve got some Dynamix AX scripts that they need to source control – no problem for TFS. The “deployment” involves simply copying the entire source folder to a secure share.\"]]],[1,\"p\",[[0,[],0,\"So I sat down with Dave Pike, one of their senior developers / architects and we customized the default team build template. Essentially, we ripped out the MSBuild task that compiles, the testing tasks and some of the arguments that you specify on a default build. We still wanted the build to create the workspace, check out some folder (or folders), label source for the build and associate changesets and work items. Where the MSBuild task used to be, we wanted to invoke a script.\"]]],[1,\"p\",[[0,[],0,\"Finally, we changed the “copy binaries to drop folder” task to copy the sources to the drop location – this gives you the “build output” in the drop location.\"]]],[1,\"h2\",[[0,[],0,\"Source Control Structure\"]]],[1,\"p\",[[0,[],0,\"In our case, we wanted to check out a folder from source control and invoke a command within that folder to do an xcopy. Here’s how we structured the folders:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The tree in source control that we set as the workspace for the build (and hence the root SourcesDirectory on the build agent when the build is running) was $/TeamProject/DEV/Scripts. Within that folder, we had some other folders and the Deploy.cmd. When setting up the build definition, we specified the “Source Control Path to Script” as $/TeamProject/DEV/Scripts/Deploy.cmd – the build was able to invoke the script from there. The script is executed using the Sources directory (the root of the workspace mapping on the build agent) so you can work in relative paths from there if you’re copying or manipulating files in the workspace. In this case the working folder would be the local mapping of $/TeamProject/DEV/Scripts.\"]]],[1,\"h2\",[[0,[],0,\"The Deploy.cmd File\"]]],[1,\"p\",[[0,[],0,\"You’ll have to create a script (a Powershell script or a bat file or any other script you can invoke) that does the “deployment”. In our case, we wanted to xcopy the entire sources directory (the entire workspace that we set up when we create a build) to a UNC, so we just created a cmd file. We added this to the source control folder that held the scripts we wanted to deploy so that it would be checked out when the build runs. The source control path to this script is the only argument that you really need to specify when you set up one of these builds.\"]]],[1,\"p\",[[0,[],0,\"For example,\"]]],[1,\"p\",[[0,[],0,\"Here’s a stub for a generic deploy.cmd file that reports errors back to the build (so that the whole build is failed if the script fails):\"]]],[10,1],[10,2],[1,\"p\",[[1,[],0,0]]],[1,\"h2\",[[0,[],0,\"Gotchas For Associating Changesets and Work Items\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"When we had dropped in an Invoke build activity, we ran the build. The builds worked fine, but weren’t calculating the changesets or work items associated to this build. We opened the template again and for a while we were stumped – we had the AssociateChangeSetsAndWorkItems activity there and hadn’t messed with it at all – so why were we not seeing the associated changesets and work items?\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"We examined the build log of a few of the builds and realised that the AssociateChangeSetsAndWorkItems activity was logging a warning that said, “Cannot find label ‘’”. Reflecting the task revealed that the activity calculates that “last good label” for this build and then using dates, works out all the changesets and associated work items from the date of the last good build label to now and associates them with this current build. We could see that the logs were labelling the builds and we could see the labels in source control, but still the activity couldn’t seem to work out the “last good build label”. Then we realised that in the sequence of the Default Template where the MSBuild activity is executed, the compilation and test statuses are set. We’d ripped out that sequence when we added the Invoke, so we weren’t setting the compilation or test statuses at all. We simply added a SetBuildDetail activity just after our Invoke activity and set compilation, test and overall status on the BuildDetail to Successful – and voila, the builds were now able to work out the changesets and work items since the “last good build” and associate them with this build.\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"I suppose that makes sense – TFS only sets the build as “good” if the compilation and test status are both successful – we set that after our Invoke (if the Invoke fails because of error in the script, the status is not set to Success and the build is not classified as “good”).\"]]],[1,\"p\",[[1,[],0,4]]],[1,\"h2\",[[0,[],0,\"The Workflow\"]]],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"Here’s the Workflow “summary”:\"]]],[1,\"p\",[[1,[],0,6]]],[10,3],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"This is the detail of the parallel activity inside the “Try Invoke and Associate Changesets and Work Items:\"]]],[1,\"p\",[[1,[],0,8]]],[10,4],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"Finally, here’s the sequence that does the deployment:\"]]],[1,\"p\",[[1,[],0,10]]],[10,5],[1,\"p\",[[1,[],0,11]]],[1,\"p\",[[0,[],0,\"You can get the template from my \"],[0,[0],1,\"skydrive\"],[0,[],0,\".\"]]],[1,\"p\",[[1,[],0,12]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1299735420000,"status":"published","published_by":1},{"id":"750ebb13-c391-4e6a-921a-e8fa2856b923","title":"Developing a Custom Build vNext Task: Part 1","slug":"developing-a-custom-build-vnext-task-part-1","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/cb66c0a6-0d32-44bc-a6eb-0c3ab4062575.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2f3fdd02-3569-4e91-9998-ee9713ea7f62.png\\\" width=\\\"328\\\" height=\\\"320\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d00021c9-e383-412f-bbdd-43381afd22dd.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5040651a-b718-4667-a034-1d84de081578.png\\\" width=\\\"269\\\" height=\\\"192\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3f44a372-c022-49f8-b340-7178e4537de6.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bc0788ca-d4f0-4d51-b417-1d3db46adffd.png\\\" width=\\\"278\\\" height=\\\"165\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fdd59941-a862-4437-87ef-4bae7353437e.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5b850d6c-7dc6-4b74-8cfb-146f47f3698d.png\\\" width=\\\"280\\\" height=\\\"84\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">&gt;&gt; tfx build tasks create<br>Copyright Microsoft Corporation<br><br>Enter short name &gt; VersionAssemblies<br>Enter friendly name &gt; Version Assemblies<br>Enter description &gt; Match version assemblies to build number<br>Enter author &gt; Colin Dembovsky</font>\"}],[\"code\",{\"code\":\"{\\n  \\\"id\\\": \\\"5b4d14d0-3868-11e4-a31d-3f0a2d8202f4\\\",\\n  \\\"name\\\": \\\"VersionAssemblies\\\",\\n  \\\"friendlyName\\\": \\\"Version Assemblies\\\",\\n  \\\"description\\\": \\\"Updates the version number of the assemblies to match the build number\\\",\\n  \\\"author\\\": \\\"Colin Dembovsky (colinsalmcorner.com)\\\",\\n  \\\"helpMarkDown\\\": \\\"## Settings\\\\nThe task requires the following settings:\\\\n\\\\n1. **Source Path**: path to the sources that contain the version number files (such as AssemblyInfo.cs).\\\\n2. **File Pattern**: file pattern to search for within the `Source Path`. Defaults to 'AssemblyInfo.*'\\\\n3. **Build Regex Pattern**: Regex pattern to apply to the build number in order to extract a version number. Defaults to `\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+`.\\\\n4. **(Optional) Regex Replace Pattern**: Use this if the regex to search for in the target files is different from the Build Regex Pattern.\\\\n\\\\n## Using the Task\\\\nThe task should be inserted before any build tasks.\\\\n\\\\nAlso, you must customize the build number format (on the General tab of the build definition) in order to specify a format in such a way that the `Build Regex Pattern` can extract a build number from it. For example, if the build number is `1.0.0$(rev:.r)`, then you can use the regex `\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d\\\\\\\\.\\\\\\\\d+` to extract the version number.\\\\n\\\",\\n  \\\"category\\\": \\\"Build\\\",\\n  \\\"visibility\\\": [\\n    \\\"Build\\\"\\n  ],\\n  \\\"demands\\\": [],\\n  \\\"version\\\": {\\n    \\\"Major\\\": \\\"0\\\",\\n    \\\"Minor\\\": \\\"1\\\",\\n    \\\"Patch\\\": \\\"1\\\"\\n  },\\n  \\\"minimumAgentVersion\\\": \\\"1.83.0\\\",\\n  \\\"instanceNameFormat\\\": \\\"Version Assemblies using $(filePattern)\\\",\\n  \\\"groups\\\": [\\n    {\\n      \\\"name\\\": \\\"advanced\\\",\\n      \\\"displayName\\\": \\\"Advanced\\\",\\n      \\\"isExpanded\\\": false\\n    }\\n  ],\\n  \\\"inputs\\\": [\\n    {\\n      \\\"name\\\": \\\"sourcePath\\\",\\n      \\\"type\\\": \\\"filePath\\\",\\n      \\\"label\\\": \\\"Source Path\\\",\\n      \\\"defaultValue\\\": \\\"\\\",\\n      \\\"required\\\": true,\\n      \\\"helpMarkDown\\\": \\\"Path in which to search for version files (like AssemblyInfo.* files).\\\" \\n    },\\n    {\\n      \\\"name\\\": \\\"filePattern\\\",\\n      \\\"type\\\": \\\"string\\\",\\n      \\\"label\\\": \\\"File Pattern\\\",\\n      \\\"defaultValue\\\": \\\"AssemblyInfo.*\\\",\\n      \\\"required\\\": true,\\n      \\\"helpMarkDown\\\": \\\"File filter to replace version info. The version number pattern should exist somewhere in the file.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"buildRegex\\\",\\n      \\\"type\\\": \\\"string\\\",\\n      \\\"label\\\": \\\"Build Regex Pattern\\\",\\n      \\\"defaultValue\\\": \\\"\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\",\\n      \\\"required\\\": true,\\n      \\\"helpMarkDown\\\": \\\"Regular Expression to extract version from build number. This is also the default replace regex (unless otherwise specified in Advanced settings).\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"replaceRegex\\\",\\n      \\\"type\\\": \\\"string\\\",\\n      \\\"label\\\": \\\"Regex Replace Pattern\\\",\\n      \\\"defaultValue\\\": \\\"\\\",\\n      \\\"required\\\": false,\\n      \\\"helpMarkDown\\\": \\\"Regular Expression to replace with in files. Leave blank to use the Build Regex Pattern.\\\",\\n      \\\"groupName\\\": \\\"advanced\\\"\\n    }\\n  ],\\n  \\\"execution\\\": {\\n    \\\"Node\\\": {\\n      \\\"target\\\": \\\"versionAssemblies.js\\\",\\n      \\\"argumentFormat\\\": \\\"\\\"\\n    },  \\n    \\\"PowerShell\\\": {\\n      \\\"target\\\": \\\"$(currentDirectory)\\\\\\\\VersionAssemblies.ps1\\\",\\n      \\\"argumentFormat\\\": \\\"\\\",\\n      \\\"workingDirectory\\\": \\\"$(currentDirectory)\\\"\\n    }\\n  }\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"[CmdletBinding(DefaultParameterSetName = 'None')]\\nparam(\\n    [string][Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] $sourcePath,\\n    [string][Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] $filePattern,\\n    [string][Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()] $buildRegex,\\n    [string]$replaceRegex,\\n    [string]$buildNumber = $env:BUILD_BUILDNUMBER\\n)\\n\",\"language\":\"ps;\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-should-switch-to-build-vnext\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vso-agent-tasks/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/matching-binary-version-to-build-number-version-in-tfs-2013-builds\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/developing-a-custom-build-vnext-task-part-2\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/tfs-cli\"]],[\"a\",[\"href\",\"https://nodejs.org/\"]],[\"a\",[\"href\",\"https://www.npmjs.com/\"]],[\"a\",[\"href\",\"http://roadtoalm.com/2015/07/22/using-personal-access-tokens-to-access-visual-studio-online/\"]],[\"strong\"],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/library/aa833872.aspx\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/blob/master/Tasks/VersionAssemblies/VersionAssemblies.ps1\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/Library/vs/alm/Build/scripts/variables\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I love the \"],[0,[0],1,\"new build engine in VSO / TFS 2015\"],[0,[],0,\". You can get pretty far with the out of the box tasks, but there are cases where a custom task improves the user experience. The “Microsoft” version of this is SonarQube integration – you can run the SonarQube MSBuild Runner by using a “Command Line” task and calling the exe. However, there are two tasks on the \"],[0,[1],1,\"Microsoft Task Github repo\"],[0,[],0,\" that clean up the experience a little – SonarQube PreBuild and SonarQube PostTest. A big benefit of the tasks is that they actually “wrap” the exe within the task, so you don’t need to install the runner on the build machine yourself.\"]]],[1,\"p\",[[0,[],0,\"One customization I almost always make in my customers’ build processes is to \"],[0,[2],1,\"match binary versions to the build number\"],[0,[],0,\". In TFS 2012, this required a custom windows workflow task – a real pain to create and maintain. In 2013, you could enable it much more easily by invoking a PowerShell script. The same script can be invoked in Build vNext by using a PowerShell task.\"]]],[1,\"p\",[[0,[],0,\"The only down side to this is that the script has to be in source control somewhere. If you’re using TFVC, then this isn’t a problem, since all your builds (within a Team Project Collection) can use the same script. However, for Git repos it’s not so simple – you’re left with dropping the script into a known location on all build servers or committing the script to each Git repo you’re building. Neither option is particularly appealing. However, if we put the script “into” a custom build task for Build vNext, then we don’t have to keep the script anywhere else!\"]]],[1,\"h2\",[[0,[],0,\"TL;DR\"]]],[1,\"p\",[[0,[],0,\"I want to discuss creating a task in some detail, so I’m splitting this into two posts. This post will look at scaffolding a task and then customizing the manifest and PowerShell implementation. In the \"],[0,[3],1,\"next post\"],[0,[],0,\" I’m going to show the node implementation (along with some info on developing in TypeScript and VS Code) and how to upload the task.\"]]],[1,\"p\",[[0,[],0,\"If you just want the task, you can get the source at \"],[0,[4],1,\"this repo\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Create a Custom Task\"]]],[1,\"p\",[[0,[],0,\"In order to create a new task, you need to supply a few things: a (JSON) manifest file, an icon and either a PowerShell or Node script (or both). You can, of course, create these by hand – but there’s an easier way to scaffold the task: \"],[0,[5],1,\"tfx-cli\"],[0,[],0,\". tfx-cli is a cross-platform command line utility that you can use to manage build tasks (including creating, deleting, uploading and listing). You’ll need to install both \"],[0,[6],1,\"node\"],[0,[],0,\" and \"],[0,[7],1,\"npm\"],[0,[],0,\" before you can install tfx-cli.\"]]],[1,\"h3\",[[0,[],0,\"tfx login\"]]],[1,\"p\",[[0,[],0,\"Once tfx-cli is installed, you should be able to run “tfx” and see the help screen.\"]]],[10,0],[1,\"p\",[[0,[],0,\"You could authenticate each time you want to perform a command, but it will soon get tedious. It’s far better to cache your credentials.\"]]],[1,\"p\",[[0,[],0,\"For VSO, it’s simple. Log in to VSO and get a \"],[0,[8],1,\"Personal Access Token (pat)\"],[0,[],0,\". When you type “tfx login” you’ll be prompted for your VSO url and your pat. Easy as pie.\"]]],[1,\"p\",[[0,[],0,\"For TFS 2015, it’s a little more complicated. You need to first enable basic authentication on your TFS app tier’s IIS. Then you can log in using your windows account (note: the tfx-cli team is working on ntfs authentication, so this is just a temporary hack).\"]]],[1,\"p\",[[0,[],0,\"Here are the steps to enable basic auth on IIS:\"]]],[3,\"ul\",[[[0,[],0,\"Open Server Manager and make sure that the Basic Auth feature is installed (under the Security node)\"]]]],[10,1],[3,\"ul\",[[[0,[],0,\"If you have to install it, then you must reboot the machine before continuing\"]],[[0,[],0,\"Open IIS and find the “Team Foundation Server” site and expand the node. Then click on the “tfs” app in the tree and double-click the “Authentication” icon in the “Features” view to open the authentication settings for the app.\"]]]],[10,2],[3,\"ul\",[[[0,[],0,\"Enable “Basic Authentication” (note the warning!)\"]]]],[10,3],[3,\"ul\",[[[0,[],0,\"Restart IIS\"]]]],[1,\"p\",[[0,[9],1,\"DANGER WILL ROBINSON, DANGER!\"],[0,[],0,\" This is insecure since the passwords are sent in plaintext. You may want to \"],[0,[10],1,\"enable https\"],[0,[],0,\" so that the channel is secure.\"]]],[1,\"h3\",[[0,[],0,\"tfx build tasks create\"]]],[1,\"p\",[[0,[],0,\"Once login is successful, you can run “tfx build tasks create” – you’ll be prompted for some basic information, like the name, description and author of the task.\"]]],[10,4],[1,\"p\",[[0,[],0,\"That creates a folder (with the same name as the “short name”) that contains four files:\"]]],[3,\"ul\",[[[0,[],0,\"task.json – the json manifest file\"]],[[0,[],0,\"VersionAssemblies.ps1 – the PowerShell implementation of the task\"]],[[0,[],0,\"VersionAssemblies.js – the node implementation of the task\"]],[[0,[],0,\"icon.png – the generic icon for the task\"]]]],[1,\"h2\",[[0,[],0,\"Customizing the Task Manifest\"]]],[1,\"p\",[[0,[],0,\"The first thing you’ll want to do after getting the skeleton task is edit the manifest file. Here you’ll set things like:\"]]],[3,\"ul\",[[[0,[],0,\"demands – a list of demands that must be present on the agent in order to run the task\"]],[[0,[],0,\"visibility – should be “Build” or “Release” or both, if the task can be used in both builds and releases\"]],[[0,[],0,\"version – the version number of your task\"]],[[0,[],0,\"minimumAgentVersion – the minimum agent version this task requires\"]],[[0,[],0,\"instanceNameFormat – this is the string that appears in the build tasks list once you add it to a build. It can be formatted to use any of the arguments that the task uses\"]],[[0,[],0,\"inputs – input variables\"]],[[0,[],0,\"groups – used to group input variables together\"]],[[0,[],0,\"execution – used to specify the entry points for either Node or PowerShell (or both)\"]],[[0,[],0,\"helpMarkDown – the markdown that is displayed below the task when added to a build definition\"]]]],[1,\"h3\",[[0,[],0,\"Inputs and Groups\"]]],[1,\"p\",[[0,[],0,\"The inputs all have the following properties:\"]]],[3,\"ul\",[[[0,[],0,\"name – reference name of the input. This is the name of the input that is passed to the implementation scripts, so choose wisely\"]],[[0,[],0,\"type – type of input. Types include “pickList”, “filePath” (which makes the control into a source folder picker) and “string”\"]],[[0,[],0,\"label – the input label that is displayed to the user\"]],[[0,[],0,\"defaultValue – a default value (if any)\"]],[[0,[],0,\"required – true or false depending on whether the input is mandatory or not\"]],[[0,[],0,\"helpMarkDown – the markdown that is displayed when the user clicks the info icon next to the input\"]],[[0,[],0,\"groupName – specify the name of the group (do not specify if you want the input to be outside a group)\"]]]],[1,\"p\",[[0,[],0,\"The groups have the following format:\"]]],[3,\"ul\",[[[0,[],0,\"name – the group reference name\"]],[[0,[],0,\"displayName – the name displayed on the UI\"]],[[0,[],0,\"isExpanded – set to true for an open group, false for a closed group\"]]]],[1,\"p\",[[0,[],0,\"Another note: the markdown needs to be on a single line (since JSON doesn’t allow multi-line values) – so if your help markdown is multi-line, you’ll have to replace line breaks with ‘\\\\n’.\"]]],[1,\"p\",[[0,[],0,\"Of course, browsing the tasks on the Microsoft vso-agent-tasks repo lets you see what types are available, how to structure the files and so on.\"]]],[1,\"h3\",[[0,[],0,\"VersionAssembly Manifest\"]]],[1,\"p\",[[0,[],0,\"For the version assembly task I require a couple of inputs:\"]]],[3,\"ol\",[[[0,[],0,\"The path to the root folder where we start searching for files\"]],[[0,[],0,\"The file pattern to match – any file in the directory matching the pattern should have the build version replaced\"]],[[0,[],0,\"The regex to use to extract a version number from the build number (so if the build number is MyBuild_1.0.0.3, then we need regex to get 1.0.0.3)\"]],[[0,[],0,\"The regex to use for the replacement in the files – I want this under advanced, since most of the time this is the same as the regex specified previously\"]]]],[1,\"p\",[[0,[],0,\"I also need the build number – but that’s an environment variable that I will get within the task scripts (as we’ll see later).\"]]],[1,\"p\",[[0,[],0,\"Here’s the manifest file:\"]]],[10,5],[1,\"h2\",[[0,[],0,\"The PowerShell Script\"]]],[1,\"p\",[[0,[],0,\"Since I am more proficient in PowerShell that in Node, I decided to tackle the PowerShell script first. Also, I have a script that does this already! You can see the \"],[0,[11],1,\"full script\"],[0,[],0,\" in my \"],[0,[4],1,\"Github repo\"],[0,[],0,\" – but here’s the important bit – the parameters declaration:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3-5: these are the mandatory inputs. The name of the argument is the same as the name property of the inputs from the manifest file\"]],[[0,[],0,\"Line 6: the optional input (again with the name matching the input name in the manifest)\"]],[[0,[],0,\"Line 7: the build number is passed into the execution context as a \"],[0,[12],1,\"predefined variable\"],[0,[],0,\" which is set in the environment, which I read here\"]]]],[1,\"p\",[[0,[],0,\"While any of the predefined variables can be read anywhere in the script, I like to put the value as the default for a parameter. This makes debugging the script (executing it outside of the build environment) so much easier, since I can invoke the script and pass in the value I want to test with (as opposed to first setting an environment variable before I call the script).\"]]],[1,\"p\",[[0,[],0,\"Once I had the inputs (and the build number) I just pasted the existing script. I’ve included lots of “Write-Verbose –Verbose” calls so that if you set “system.debug” to “true” in your build variables, the task spits out some diagnostics. Write-Host calls end up in the console when the build is running.\"]]],[1,\"h2\",[[0,[],0,\"Wrap up\"]]],[1,\"p\",[[0,[],0,\"In this post I covered how to use tfx-cli to scaffold a task, then customize the manifest and implement a PowerShell script.\"]]],[1,\"p\",[[0,[],0,\"In the \"],[0,[3],1,\"next post\"],[0,[],0,\" I’ll show you how to write the node implementation of the task, using TypeScript and VS Code. I’ll also show you how to upload the task and use it in a build.\"]]],[1,\"p\",[[0,[],0,\"Happy customizing!\"]]]]}","published_at":1440110047000,"status":"published","published_by":1},{"id":"370cc512-abdc-4eda-8df2-945babb088fd","title":"Developing a Custom Build vNext Task: Part 2","slug":"developing-a-custom-build-vnext-task-part-2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/92b3709e-e900-4c30-bb25-4436a22094a4.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/707326d1-4148-4914-b0e0-ccb05ad44da9.png\\\" width=\\\"218\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9b52599a-c895-44da-86c2-8a08ac9cf503.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/34b1890b-acdd-4e77-a7e1-742e2eb275c8.png\\\" width=\\\"301\\\" height=\\\"230\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/11c7de5b-945d-4e5a-ae50-b52ed1e28fcd.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d82d3c94-4312-4e2a-aab7-afde0040c87e.png\\\" width=\\\"370\\\" height=\\\"33\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0e2ce84a-8c85-4db7-aa2e-e5998c443c03.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bff49cbf-5c12-4fee-8206-0b4087790bf6.png\\\" width=\\\"355\\\" height=\\\"178\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/dc320053-6317-443b-9edd-27640386d955.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f5450ec7-85d5-475c-a7ec-1a12311a4c6c.png\\\" width=\\\"295\\\" height=\\\"271\\\"></a>\"}],[\"code\",{\"code\":\"/// &lt;reference path=\\\"q/Q.d.ts\\\" /&gt;\\n/// &lt;reference path=\\\"node/node.d.ts\\\" /&gt;\\n/// &lt;reference path=\\\"vso-task-lib/vso-task-lib.d.ts\\\" /&gt;\\n/// &lt;reference path=\\\"shelljs/shelljs.d.ts\\\" /&gt;\\n\\ndeclare module \\\"vso-task-lib\\\" {\\n    export = VsoTaskLib;\\n}\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7c2d2b09-b024-4de1-83f1-5f8ef5a053fa.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/12b7971b-2895-41ed-9928-3bce045109fd.png\\\" width=\\\"538\\\" height=\\\"33\\\"></a>\"}],[\"code\",{\"code\":\"{\\n    \\\"version\\\": \\\"0.1.0\\\",\\n\\n    // The command is tsc. Assumes that tsc has been installed using npm install -g typescript\\n    \\\"command\\\": \\\"tsc\\\",\\n\\n    // The command is a shell script\\n    \\\"isShellCommand\\\": true,\\n\\n    // Show the output window only if unrecognized errors occur.\\n    \\\"showOutput\\\": \\\"silent\\\",\\n\\n    // Tell the tsc compiler to use the tsconfig.json from the open folder.\\n    \\\"args\\\": [\\\"-p\\\", \\\".\\\"],\\n\\n    // use the standard tsc problem matcher to find compile problems\\n    // in the output.\\n    \\\"problemMatcher\\\": \\\"$tsc\\\"\\n}\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4809b3c0-27c8-43ed-af1d-c21049799a43.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e8ab72b6-efd4-4a2a-bea1-3ef960d0de13.png\\\" width=\\\"362\\\" height=\\\"225\\\"></a>\"}],[\"code\",{\"code\":\"import * as tl from 'vso-task-lib';\\nimport * as sh from 'shelljs';\\n\\ntl.debug(\\\"Starting Version Assemblies step\\\");\\n\\n// get the task vars\\nvar sourcePath = tl.getPathInput(\\\"sourcePath\\\", true, true);\\nvar filePattern = tl.getInput(\\\"filePattern\\\", true);\\nvar buildRegex = tl.getInput(\\\"buildRegex\\\", true);\\nvar replaceRegex = tl.getInput(\\\"replaceRegex\\\", false);\\n\\n// get the build number from the env vars\\nvar buildNumber = tl.getVariable(\\\"Build.BuildNumber\\\");\\n\\ntl.debug(`sourcePath :${sourcePath}`);\\ntl.debug(`filePattern : ${filePattern}`);\\ntl.debug(`buildRegex : ${buildRegex}`);\\ntl.debug(`replaceRegex : ${replaceRegex}`);\\ntl.debug(`buildNumber : ${buildNumber}`);\\n\\nif (replaceRegex === undefined || replaceRegex.length === 0){\\n    replaceRegex = buildRegex;\\n}\\ntl.debug(`Using ${replaceRegex} as the replacement regex`);\\n\\nvar buildRegexObj = new RegExp(buildRegex);\\nif (buildRegexObj.test(buildNumber)) {\\n    var versionNum = buildRegexObj.exec(buildNumber)[0];\\n    console.info(`Using version ${versionNum} in folder ${sourcePath}`);\\n    \\n    // get a list of all files under this root\\n    var allFiles = tl.find(sourcePath);\\n\\n    // Now matching the pattern against all files\\n    var filesToReplace = tl.match(allFiles, filePattern, { matchBase: true });\\n    \\n    if (filesToReplace === undefined || filesToReplace.length === 0) {\\n        tl.warning(\\\"No files found\\\");\\n    } else {\\n        for(var i = 0; i &lt; filesToReplace.length; i++){\\n            var file = filesToReplace[i];\\n            console.info(`  -&gt; Changing version in ${file}`);\\n            \\n            // replace all occurrences by adding g to the pattern\\n            sh.sed(\\\"-i\\\", new RegExp(replaceRegex, \\\"g\\\"), versionNum, file);\\n        }\\n        console.info(`Replaced version in ${filesToReplace.length} files`);\\n    }\\n} else {\\n    tl.warning(`Could not extract a version from [${buildNumber}] using pattern [${buildRegex}]`);\\n}\\n\\ntl.debug(\\\"Leaving Version Assemblies step\\\");\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b67e1e31-d578-4db5-928e-686b99c6f489.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b0fbe679-4cab-44de-ac8c-542958f03f95.png\\\" width=\\\"324\\\" height=\\\"231\\\"></a>\"}],[\"code\",{\"code\":\"{\\n    \\\"version\\\": \\\"0.1.0\\\",\\n    // List of configurations. Add new configurations or edit existing ones.\\n    // ONLY \\\"node\\\" and \\\"mono\\\" are supported, change \\\"type\\\" to switch.\\n    \\\"configurations\\\": [\\n        {\\n            // Name of configuration; appears in the launch configuration drop down menu.\\n            \\\"name\\\": \\\"Launch versionAssemblies.js\\\",\\n            // Type of configuration. Possible values: \\\"node\\\", \\\"mono\\\".\\n            \\\"type\\\": \\\"node\\\",\\n            // Workspace relative or absolute path to the program.\\n            \\\"program\\\": \\\"Tasks/VersionAssemblies/versionAssemblies.ts\\\",\\n            // Automatically stop program after launch.\\n            \\\"stopOnEntry\\\": false,\\n            // Command line arguments passed to the program.\\n            \\\"args\\\": [],\\n            // Workspace relative or absolute path to the working directory of the program being debugged. Default is the current workspace.\\n            \\\"cwd\\\": \\\".\\\",\\n            // Workspace relative or absolute path to the runtime executable to be used. Default is the runtime executable on the PATH.\\n            \\\"runtimeExecutable\\\": null,\\n            // Optional arguments passed to the runtime executable.\\n            \\\"runtimeArgs\\\": [\\\"--nolazy\\\"],\\n            // Environment variables passed to the program.\\n            \\\"env\\\": { \\n                \\\"BUILD_BUILDNUMBER\\\": \\\"1.0.0.5\\\",\\n                \\\"INPUT_SOURCEPATH\\\": \\\"C:\\\\\\\\data\\\\\\\\ws\\\\\\\\col\\\\\\\\ColinsALMCornerCheckinPolicies\\\",\\n                \\\"INPUT_FILEPATTERN\\\": \\\"AssemblyInfo.*\\\",\\n                \\\"INPUT_BUILDREGEX\\\": \\\"\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\",\\n                \\\"INPUT_REPLACEREGEX\\\": \\\"\\\"\\n            },\\n            // Use JavaScript source maps (if they exist).\\n            \\\"sourceMaps\\\": true,\\n            // If JavaScript source maps are enabled, the generated code is expected in this directory.\\n            \\\"outDir\\\": \\\".\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"Attach\\\",\\n            \\\"type\\\": \\\"node\\\",\\n            // TCP/IP address. Default is \\\"localhost\\\".\\n            \\\"address\\\": \\\"localhost\\\",\\n            // Port to attach to.\\n            \\\"port\\\": 5858,\\n            \\\"sourceMaps\\\": false\\n        }\\n    ]\\n}\\n\",\"language\":\"js; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c9a466e6-795c-4836-8a70-e10ea8a09f68.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/56cf0266-19da-45a2-814f-89ee1fd2b920.png\\\" width=\\\"320\\\" height=\\\"284\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4fa466de-f864-4287-a314-3b8a7bfdfec6.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d1c5c01e-38b4-4b57-b9e5-7a679bba85e9.png\\\" width=\\\"355\\\" height=\\\"318\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">tfx-cli build tasks upload .\\\\VersionAssemblies</font>\"}],[\"image\",{\"src\":\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4816b75a-0511-4e88-b12c-08ee59ae9e8c.png\",\"alt\":\"image\",\"title\":\"image\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/32bddc88-1e06-44f0-ad43-bc902e1f9a77.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6e34eb6e-e1fd-4bcf-90c2-dd6c9e794078.png\\\" width=\\\"326\\\" height=\\\"123\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b06d383b-1f27-4716-a841-d27ad8930c19.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b95cc9b7-d3e2-4790-958a-769b43031c59.png\\\" width=\\\"320\\\" height=\\\"158\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/aa04348e-4299-45a1-9d2f-301d066a2588.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fdc584a6-fdd3-47eb-ad1b-69e68473df78.png\\\" width=\\\"287\\\" height=\\\"186\\\"></a>\"}],[\"image\",{\"src\":\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c1c33021-940f-4229-91a7-f11afa0898f1.png\",\"alt\":\"image\",\"title\":\"image\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/developing-a-custom-build-vnext-task-part-1\"]],[\"a\",[\"href\",\"https://github.com/microsoft/tfs-cli\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/VersionAssemblies\"]],[\"a\",[\"href\",\"https://code.visualstudio.com/\"]],[\"a\",[\"href\",\"https://channel9.msdn.com/Events/Visual-Studio/Visual-Studio-2015-Final-Release-Event/Introducing-Visual-Studio-Code\"]],[\"a\",[\"href\",\"http://github.com/colindembovsky/cols-agent-tasks\"]],[\"a\",[\"href\",\"http://github.com/microsoft/vso-agent-tasks\"]],[\"a\",[\"href\",\"https://www.npmjs.com/package/tsd\"]],[\"a\",[\"href\",\"https://github.com/borisyankov/DefinitelyTyped\"]],[\"a\",[\"href\",\"http://raathigesh.com/TypeScript-Development-in-VS-Code-with-Gulp/\"]],[\"a\",[\"href\",\"http://github.com/microsoft/tfs-cli\"]],[\"a\",[\"href\",\"http://roadtoalm.com/about/\"]],[\"a\",[\"href\",\"http://roadtoalm.com/2015/08/07/running-a-visual-studio-build-vnext-agent-in-a-docker-container/\"]],[\"a\",[\"href\",\"https://github.com/renevanosnabrugge/vsobuild-docker/blob/master/Dockerfile\"]],[\"a\",[\"href\",\"https://www.docker.com/toolbox\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"part 1\"],[0,[],0,\" I showed you how to scaffold a task using \"],[0,[1],1,\"tfx-cli\"],[0,[],0,\", how to customize the manifest and how to implement the PowerShell script for my \"],[0,[2],1,\"VersionAssemblies task\"],[0,[],0,\". In this post I’ll show you how I went about developing the Node version of the task and how I uploaded the completed task to my TFS server.\"]]],[1,\"h3\",[[0,[],0,\"VS Code\"]]],[1,\"p\",[[0,[],0,\"I chose to use \"],[0,[3],1,\"VS Code\"],[0,[],0,\" as the editor for my tasks. Partly because I wanted to become more comfortable using VS Code, and partly because the task doesn’t have a project file – it’s just some files in a folder – perfect for VS Code. If you’re unfamiliar with VS Code, then I highly recommend this \"],[0,[4],1,\"great intro video by Chris Dias\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Restructure and Initialize a Git Repo\"]]],[1,\"p\",[[0,[],0,\"It was time to get serious. I wanted to publish the task to a \"],[0,[5],1,\"Git repo\"],[0,[],0,\", so I decided to reorganize a little. I wanted the root of my repo to have a README.md file and then have a folder per task. Each task should also have a markdown file. So I created a folder called cols-agent-tasks and moved the VersionAssemblies task into a subfolder called Tasks. Then I initialized the Git repo.\"]]],[1,\"p\",[[0,[],0,\"Next I right clicked on my cols-agent-tasks folder and selected “Open with Code” to open the folder. Here’s how it looked:\"]]],[10,0],[1,\"p\",[[0,[],0,\"See the Git icon on the left? Clicking on it allows you to enter a commit message and commit. You can also diff files and undo changes. Sweet.\"]]],[1,\"h3\",[[0,[],0,\"Installing Node Packages\"]]],[1,\"p\",[[0,[],0,\"I knew that the VSO agent has a client library (vso-task-lib) from looking at the out of the box tasks in the \"],[0,[6],1,\"vso-agent-tasks repo\"],[0,[],0,\". I wanted to utilize that in my node task. The task lib is a node package, and so I needed to pull the package down from npm. So I opened up a PowerShell prompt and did an “npm init” to initialize a package.json file (required for npm) and walked through the wizard:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Since I chose MIT for the license, I added a license file too.\"]]],[1,\"p\",[[0,[],0,\"Now that the package.json file was initialized, I could run “npm install vso-task-lib –-save-dev” to install the vso-task-lib and save it as a dev dependency (since it’s not meant to be bundled with the completed task):\"]]],[10,2],[1,\"p\",[[0,[],0,\"The command also installed the q and shelljs libraries (which are dependencies for the vso-task-lib). I also noticed that a node_modules folder had popped up (where the packages are installed) and double checked that my .gitignore was in fact ignoring this folder (since I don’t want these packages committed into my repo).\"]]],[1,\"h3\",[[0,[],0,\"TypeScript Definitions Using tsd\"]]],[1,\"p\",[[0,[],0,\"I was almost ready to start diving into the code – but I wanted to make sure that VS Code gave me intellisense for the task library (and other libs). So I decided to play with \"],[0,[7],1,\"tsd\"],[0,[],0,\", a node TypeScript definition manager utility. I installed it by running “npm install tsd –g” (the –g is for global, since its a system-wide util). Once its installed, you can run “tsd” to see what you can do with the tool.\"]]],[1,\"p\",[[0,[],0,\"I first ran “tsd query vso-task-lib” to see if there is a TypeScript definition for vso-agent-lib. The font color was a bit hard to see, but I saw “zero results”. Bummer – the definition isn’t on \"],[0,[8],1,\"DefinitelyTyped\"],[0,[],0,\" yet. So what about q and shelljs? Both of them returned results, so I installed them (using the –-save option to save the definitions I’m using to a json file):\"]]],[10,3],[1,\"p\",[[0,[],0,\"On disk I could see a couple new files:\"]]],[3,\"ul\",[[[0,[],0,\"The “typings” folder with the definitions as well as a “global” definition file, tsd.d.ts, including q and shelljs (and node, which is installed when you install the type definition for shelljs)\"]],[[0,[],0,\"A tsd.json file in the root (which aggregates all the definitions)\"]]]],[1,\"p\",[[0,[],0,\"Opening the versionAssemblies.js file, I was able to see intellisense for q, shelljs and node:\"]]],[10,4],[1,\"p\",[[0,[],0,\"So what about the vso-task-lib? Since there was no definition on definitely typed, I had to import it manually. I copied the vso-task-lib.d.ts from the Microsoft repo into a folder called vso-task-lib in the typings folder and then updated the tsd.d.ts file adding another reference to this definition file. I also ended up declaring the vso-task-lib (since it doesn’t actaully declare an export) so that the imports worked correctly. Here’s the tsd.d.ts file:\"]]],[10,5],[1,\"h3\",[[0,[],0,\"Coverting versionAssemblies.js to TypeScript\"]]],[1,\"p\",[[0,[],0,\"Things were looking good! Now I wanted to convert the versionAssemblies.js file to TypeScript. I simply changed the extension from js to ts, and I was ready to start coding in TypeScript. But of course TypeScript files need to be transpiled to Javascript, so I hit “Ctrl-Shift-B” (muscle memory). To my surprise, VS Code informed me that there was “No task runner configured”.\"]]],[10,6],[1,\"p\",[[0,[],0,\"So I clicked on “Configure Task Runner” and VS Code created a .settings folder with a tasks.json file. There were some boilerplate examples of how to configure the task runner. After playing around a bit, I settled on the second example – which supposedly runs TypeScript using a tfconfig.json in the root folder:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Now I had to create a tsconfig.json file in the root, which I did. VS Code knows what to do with json files, so I just opened a { and was pleasantly surprised with schema intellisense for this file!\"]]],[10,8],[1,\"p\",[[0,[],0,\"I configured the “compilerOptions” and set the module to “commonjs”. Now pressing “Ctrl-Shift-B” invokes the task runner which transpiles my TypeScript file to Javascript – I saw the .js file appear on disk. Excellent! Setting sourceMaps to true will provide source mapping so that I can later debug.\"]]],[1,\"p\",[[0,[],0,\"One caveat – the build doesn't automatically happen when you save a TypeScript file. You can configure \"],[0,[9],1,\"gulp and then enable a watch\"],[0,[],0,\" so that when you change a TypeScript file the build kicks in – but I decided that was too complicated for this project. I just configured the keyboard shortcut “Ctrl-s” to invoke “workbench.action.tasks.build” in addition to saving the file (you can configure keyboard shortcuts by clicking File->Preferences->Keyboard Shortcuts. Surprise surprise it’s a json file…)\"]]],[1,\"h2\",[[0,[],0,\"Implementing the Build Task\"]]],[1,\"p\",[[0,[],0,\"Everything I’d done so far was just setup stuff. Now I was ready to actually code the task!\"]]],[1,\"p\",[[0,[],0,\"Here’s the complete script:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 1-2: import the library references\"]],[[0,[],0,\"Line 4: using the task library to log to the console\"]],[[0,[],0,\"Lines 6-13: using the task library to get the inputs (matching names from the task.json file) as well as getting the build number from the environment\"]],[[0,[],0,\"Lines 15-19: more debug logging\"]],[[0,[],0,\"Lines 21-23: default the replace regex to the build regex if the value is empty\"]],[[0,[],0,\"Line 26: compile the regex pattern into a regex object\"]],[[0,[],0,\"Line 27: test the build number to see if we can extract a version number using the regex pattern\"]],[[0,[],0,\"Lines 28-29: extract the version number from the build number and write the value to the console\"]],[[0,[],0,\"Line 32: get a list of all files in the sourcePath (recursively) using the task library method\"]],[[0,[],0,\"Line 35: filter the files to match the filePattern input, again using a task library method\"]],[[0,[],0,\"Lines 37-38: check if there are files that match – warn if there aren’t any\"]],[[0,[],0,\"Line 40: for each file that matches,\"]],[[0,[],0,\"Lines 41-45: use shelljs’s sed() method to do the regex replacement inline\"]],[[0,[],0,\"Line 45: I use the “g” option when compiling the regex to indicate that all matches should be replaced (as opposed to just the 1st match)\"]],[[0,[],0,\"Line 47: log to the console how many files were updated\"]],[[0,[],0,\"The remainder is just logging\"]]]],[1,\"p\",[[0,[],0,\"Using the task library really made developing the task straightforward. The setup involved in getting intellisense to work was worth the effort!\"]]],[1,\"h3\",[[0,[],0,\"Debugging from VS Code\"]]],[1,\"p\",[[0,[],0,\"Now that I had the code written, I wanted to test it. VS Code to the rescue again! Click on the “debug” icon on the left, and then click the gear icon at the top of the debug pane:\"]]],[10,10],[1,\"p\",[[0,[],0,\"That creates a new file called launch.json in the .settings folder. What – a json file? Who would have guessed! Here’s my final file:\"]]],[10,11],[1,\"p\",[[0,[],0,\"The changes I made have been highlighted. I changed the name and program settings. I also added some environment variables to simulate the values that the build agent is going to pass into the task. Finally, I changed “sourceMaps” to true and the output dir to “.” so that I could debug my TypeScript files. Now I just press F5:\"]]],[10,12],[1,\"p\",[[0,[],0,\"The debugger is working – but my code isn’t! Looks like I’m missing a node module – minimatch. No problem – just run “npm install minimatch -–save-dev” to add the module in and run again. Another module not found – this time shelljs. Run “npm install shelljs –-save-dev” and start again. Success! I can see watches in the left window, hover over variables to see their values, and start stepping through my code.\"]]],[10,13],[1,\"p\",[[0,[],0,\"My code ended up being perfect. Just kidding – I had to sort out some errors, but at least debugging made it a snap.\"]]],[1,\"h2\",[[0,[],0,\"Uploading the Task\"]]],[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"part 1\"],[0,[],0,\" I introduced \"],[0,[10],1,\"tfx-cli\"],[0,[],0,\". I now returned to the command line in order to test uploading the task. I changed to the cols-agent-tasks\\\\Tasks directory and ran\"]]],[10,14],[1,\"p\",[[0,[],0,\"I got a success, and so now I could test it in a build!\"]]],[1,\"h3\",[[0,[],0,\"Testing a Windows Build\"]]],[1,\"p\",[[0,[],0,\"Testing the windows build was fairly simple. I opened up an existing hosted build and replaced the PowerShell task that called my original PowerShell version assemblies task, and added in a brand new shiny “VersionAssemblies” task:\"]]],[10,15],[1,\"p\",[[0,[],0,\"The run worked perfectly too – I was able to see the version change in the build output. Just a tip – setting “system.debug” to “true” in the build variables caused the task to log verbose.\"]]],[10,16],[1,\"h3\",[[0,[],0,\"Testing a Linux build using Docker\"]]],[1,\"p\",[[0,[],0,\"Now I wanted to test the task in a Linux build. I’ve installed a couple of Ubuntu vms before, so I was prepared to spin one up when I came across an excellent post by my friend and fellow ALM MVP \"],[0,[11],1,\"Rene van Osnabrugge\"],[0,[],0,\". Rene shows how you can quickly spin up a \"],[0,[12],1,\"cross-platform build agent in a docker container\"],[0,[],0,\" – and even provides the \"],[0,[13],1,\"Dockerfile\"],[0,[],0,\" to be able to do it in 1 line! The timing was perfect – I downloaded \"],[0,[14],1,\"Docker Toolbox\"],[0,[],0,\" and installed a docker host (I couldn’t get the Hyper-V provider to work, so I had to resort to VirtualBox), then grabbed Rene’s Dockerfile and in no time at all I had a build agent on Ubuntu ready to test!\"]]],[1,\"p\",[[0,[],0,\"Here’s my x-plat agent in the default pool:\"]]],[10,17],[1,\"p\",[[0,[],0,\"Note the Agent.OS “capability”. In order to target this agent, I’m going to add it as a demand for the build:\"]]],[10,18],[1,\"p\",[[0,[],0,\"Here’s the successful run:\"]]],[10,19],[1,\"p\",[[0,[],0,\"I committed, pushed to \"],[0,[5],1,\"Github\"],[0,[],0,\" and now I can rest my weary brain!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Creating a custom task is, if not simple, at least easy. The agent architecture has been well thought out and overall custom task creation is a satisfying process, both for PowerShell and for Node. I look forward to seeing what custom tasks start creeping out of the woodwork. Hopefully task designers will follow Microsoft’s lead and make them open source.\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1440109971000,"status":"published","published_by":1},{"id":"413fe24a-0557-490f-8841-923326a54f4a","title":"Developing the Hybrid Lab Workflow","slug":"developing-the-hybrid-lab-workflow","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-MovEP7xbgkg/UJJK0jUXV9I/AAAAAAAAAgk/WiFnrVszGMc/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-QxGCUWGq5Sw/UJJK19OZXwI/AAAAAAAAAgs/QLK230DnMJc/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"355\\\" height=\\\"219\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-xOEhFGtrtQU/UJJK31gfsUI/AAAAAAAAAg0/O4AIO27f9Uo/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-avWuWrMO40s/UJJK5VxjfwI/AAAAAAAAAg8/5jA9XarzRNI/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"502\\\" height=\\\"216\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-j5ncgGxv6uI/UJJK7CvGPHI/AAAAAAAAAhE/LIHFCHogNIA/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-yQMwvqHHcxs/UJJK8f_q6-I/AAAAAAAAAhM/4i5fDqDHPXY/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"429\\\" height=\\\"259\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/10/hybrid-lab-workflow-standard-lab.html\"]],[\"a\",[\"href\",\"http://hybridlabworkflow.codeplex.com/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In my \"],[0,[0],1,\"previous post\"],[0,[],0,\" I talked about my \"],[0,[1],1,\"Hybrid Lab Workflow\"],[0,[],0,\" – this workflow allows you to do a Build-Deploy-Test workflow against a TFS 2012 Standard Environment, and as long as the environment is composed of VMs and you’re able to connect to the VM Host, then you can apply pre-deployment snapshots and take post-deployment snapshots.\"]]],[1,\"h2\",[[0,[],0,\"Why the Wizard needs the Interactive Agent Password\"]]],[1,\"p\",[[0,[],0,\"The trickiest bit of this workflow was getting the Environment ready for Deployment and Testing after a snapshot was applied. I discovered that if you took a snapshot of each the VMs in the Lab after you created the Environment, then if you applied the snapshot the Lab would look ready, but when you tried to Deploy into the Lab (specifically the machine that is running it’s lab agent as an interactive process for Coded UI tests) the deployment task failed.\"]]],[1,\"p\",[[0,[],0,\"I discovered (by chance) that if you restart the Interactive Agent on the test machine, the Deployment and Test worked as expected.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The problem was then achieving this in the workflow, since there is no API for doing this. I tried to achieve this from the controller side too (since if you “offline” and then “online” the agent from the controller in MTM, the lab worked too). Again, no luck, since there’s no API.\"]]],[10,1],[1,\"p\",[[0,[],0,\"So that’s why I had to add some nastiness to the Wizard and to the workflow. In the Wizard when you create a workflow, you need to provide the password for the Test Agent user – I can get the username from the Lab API.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Once I have that, I can use PsKill to remotely stop the Test Agent and then PsExec to remotely start the Agent again – that wires it up nicely and the rest of the workflow continues normally.\"]]],[1,\"p\",[[0,[],0,\"There was another alternative – I could invoke the LabEnvironment.Repair() method and specify that the agent needs to be installed on the test machine. This worked, but it took about 15 minutes for the Lab infrastructure to figure out that the Agent was already installed and then to wire it up. Resetting the Agent remotely only takes about 30 seconds, so I decided to go with that instead. I would have preferred a way to do this without a password or PsKill and PsExec, but since there’s no API for this, I went with the “quicker, dirtier” method.\"]]],[1,\"p\",[[0,[],0,\"Anyway, if you really don’t like that you can leave the password blank. The bit of the workflow that uses PsKill and PsExec will be bypassed – but then your Agent may be in some weird limbo state because of the snapshot. Maybe this only happens in my scenario and you don’t need to do it.\"]]],[1,\"p\",[[0,[],0,\"Happy workflowing!\"]]]]}","published_at":1351797120000,"status":"published","published_by":1},{"id":"4938af2d-98f1-4eaf-a02b-ec520fde6e0e","title":"DevOps Drives Better Architecture–Part 1 of 2","slug":"devops-drives-better-architecturepart-1-of-2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://bit.ly/devopsarch2\"]],[\"em\"],[\"a\",[\"href\",\"https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912/ref=asap_bc?ie=UTF8\"]],[\"a\",[\"href\",\"https://twitter.com/jezhumble\"]],[\"a\",[\"href\",\"http://www.davefarley.net/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"(Read \"],[0,[0],1,\"part 2\"],[0,[],0,\" here)\"]]],[1,\"p\",[[0,[],0,\"I haven’t blogged for a long while – it’s been a busy few months!\"]]],[1,\"p\",[[0,[],0,\"One of the things I love about being a DevOps consultant is that I have to be technically proficient – I can’t help teams develop best practices if I don’t know the technology to at least a reasonable depth – but I also get to be a catalyst for change. I love the cultural dynamics of DevOps. After all, as my friend Donovan Brown says, “DevOps is the union of \"],[0,[1],1,\"people\"],[0,[],0,\", processes and tools…”. When you involve people, then you get to watch (or, in my case, influence) culture. And it fascinates me.\"]]],[1,\"p\",[[0,[],0,\"I only recently read \"],[0,[2],1,\"Continuous Delivery\"],[0,[],0,\" by \"],[0,[3],1,\"Jez Humble\"],[0,[],0,\" and \"],[0,[4],1,\"David Farley\"],[0,[],0,\". I was pleased at how much of their insights I’ve been advocating “by instinct” over my years of ALM and DevOps consulting. Reading their book sparked the thoughts that I’ll put into this two-part post.\"]]],[1,\"p\",[[0,[],0,\"This part will introduce the thought that DevOps and architecture are symbiotic – good architecture makes for good DevOps, and good DevOps drives good architecture. Ill look at Builds and Source Control in particular. In part 2, I’ll discuss infrastructure as code, database design, automated testing and monitoring and how they relate to DevOps and vice versa.\"]]],[1,\"h2\",[[0,[],0,\"Tools, tools, tools\"]]],[1,\"p\",[[0,[],0,\"Over the past few months, the majority of my work has been to help teams implement Build/Release Pipelines. This seems inevitable to me given the state of DevOps in the market in general – most teams have made (or are in the process of making) a shift to agile, iterative frameworks for delivering their software. As they get faster, they need to release more frequently. And if they’ve got manual builds and deployments, the increasing frequency becomes a frustration because they can’t seem to deploy fast enough (or consistently enough). So teams are starting to want to automate their build/release flows.\"]]],[1,\"p\",[[0,[],0,\"It’s natural, therefore, to immediately look for a tool to help automation. And for a little help from your friends at Northwest Cadence to help you do it right!\"]]],[1,\"p\",[[0,[],0,\"Of course my tool of choice for build/release pipelines is Visual Studio Team Services (VSTS) or Team Foundation Server (TFS) for a number of reasons:\"]]],[3,\"ol\",[[[0,[],0,\"The build agent is cross platform (it’s built on .NET Core, so runs wherever .NET Core runs)\"]],[[0,[],0,\"The build agent is also the release agent\"]],[[0,[],0,\"The build agent can run tests\"]],[[0,[],0,\"The task-based system has a good Web-based UI, allowing authoring from wherever you have a browser\"]],[[0,[],0,\"The logging is great – allowing fast debugging of build issues\"]],[[0,[],0,\"Small custom logic can easily be handled with inline scripts\"]],[[0,[],0,\"If you can script it, the agent can do it – whether it’s bat, ps1 or sh\"]],[[0,[],0,\"Extensions are fairly easy to create\"]],[[0,[],0,\"There is a large and thriving marketplace for extensions\"]]]],[1,\"h2\",[[0,[],0,\"Good Architecture Means Easier DevOps\"]]],[1,\"p\",[[0,[],0,\"Inevitably implementing build automation impacts how you organize your source control. And implementing a release pipeline impacts how you test. And implementing continuous deployment impacts IT, since there’s suddenly a need to be able to spin up and configure and tear down environments on the fly. I love seeing this progression – but it’s often painful for the teams I’m working with. Why? Because teams start realizing that if their architecture was better, it would make other parts of the DevOps pipeline far easier to implement.\"]]],[1,\"p\",[[0,[],0,\"For example, if you start automating releases, pretty soon you start wanting to run automated tests since your tests start becoming the bottleneck to delivery. At this point, if you’ve used good architectural principles like interfaces and inversion of control, writing unit tests is far easier. If you haven’t, you have a far harder time writing tests.\"]]],[1,\"p\",[[0,[],0,\"Good architecture can make DevOps easier for you and your team. We’ve been told to do these things, and often we’ve found reasons not to do them (“I don’t have time to make an interface for everything!” or “I’ll refactor that class to make it more testable in the next sprint” etc. etc.). Hopefully I can show you how these architectural decisions, if done with DevOps in mind, will not only make your software better but help you to implement better DevOps, more easily!\"]]],[1,\"h2\",[[0,[],0,\"The Love Triangle: Source Control, Branches and Builds\"]]],[1,\"p\",[[0,[],0,\"I really enjoy helping teams implement their first automated builds. Builds are so foundational to good DevOps – and builds tend to force teams to reevaluate their code layout (structure), dependencies and branching strategy.\"]]],[1,\"p\",[[0,[],0,\"Most of the time, the teams have their source code in some sort of source control system. Time and time again, the teams that have a good structure and simple branching strategies have a far easier time getting builds to work well.\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, most repositories I look at are not very well structured. Or the branches represent environments so you see MAIN/DEV/PROD (which is horrible even though most teams don’t know why – read on if this is you). Or they have checked binaries into source control instead of using a package manager like NuGet. Or they have binary references instead of project references.\"]]],[1,\"p\",[[0,[],0,\"Anyway, as we get the build going, we start to uncover potential issues most teams don’t even know they have (like missing library references or conflicting package versions). After some work and restructuring, we manage to get a build to compile. Whoop!\"]]],[1,\"h3\",[[0,[],0,\"Branching\"]]],[1,\"p\",[[0,[],0,\"After the initial elation and once the party dies down, we take a look at the branching strategy. “We need a branch for development, then we promote to QA, and once QA signs off we promote to PROD. So we need branches for each of these environments, right?” This is still a very pervasive mindset. However, DevOps – specifically release pipelines – should operate on a simple principle: build once, deploy many times. In other words, the bits that you deploy to DEV should be the same bits that you deploy to QA and then to PROD. Don’t just take my work for it: read Continuous Delivery – the authors emphasize this over and over again!. You can’t do that if you have to merge and build each time you want to promote code between environments. So how do you track what code is where and promote parallel development?\"]]],[1,\"h3\",[[0,[],0,\"Builds and Versioning\"]]],[1,\"p\",[[0,[],0,\"I advocate for a master/feature branch strategy. That is, you have your stable code on master and then have multiple feature branches (1 to n at any one time) that developers work on. Development is done on the feature branch and then merged into master via Pull Request when it’s ready. At that point, a build is queued \"],[0,[1],1,\"which versions the assemblies and tags the source with the version\"],[0,[],0,\" (which is typically the build number).\"]]],[1,\"p\",[[0,[],0,\"That’s how you keep track of what code is where – by versions and tags that your build keeps the keys for. That way, you can do hotfixes directly onto master even if you’ve already merged code that is in the pipeline and not yet in production. For example, say you have 1.0.0.6 in prod and you merge some code in for a new feature. The build kicks in and produces version 1.0.0.7 which gets automatically deployed to the DEV environment for integration testing. While that’s going on, you get a bug report from PROD. Oh no! We’ve already merged in code that isn’t yet in PROD, so how do we fix it on master?!?\"]]],[1,\"p\",[[0,[],0,\"It’s easy – we know that 1.0.0.6 is in PROD, so we branch the code using tag 1.0.0.6 (which the build tagged in the repo when it ran the 1.0.0.6 build). We then fix the issue in the branch build off of this branch. A new build – 1.0.0.8. We take a quick look at this and fast-track it through until it’s deployed and business can continue. In the meantime, we can abandon the 1.0.0.7 build that’s currently in the deployment pipeline. We merge the hotfix branch back to the master and do a new build – 1.0.0.9 that now has the hotfix as well as the new feature. No sweat.\"]]],[1,\"p\",[[0,[],0,\"“Hang on. Pull Requests? Feature branches? That sounds like Git.” Yes, it does. If you’re not on Git, then you’d better have a convincing reason not to be. You can do a lot of this with TFVC, but it’s just harder. So just get to Git. And as a side benefit, you’ll get a far richer code review experience (in the form of Pull Requests) so your quality is likely to improve. And merging is easier. And you can actually cherry-pick. I could go on and on, but there’s enough Git bigots out there that I don’t need to add my voice too. But get to Git. Last word. Just Do It.\"]]],[1,\"h2\",[[0,[],0,\"Small Repos, Microservices and Package Management\"]]],[1,\"p\",[[0,[],0,\"So you’re on Git and you have a master/feature branch branching strategy. But you have multiple components or layers and you need them to live together for compilation, so you put them all into one repo, right? Wrong. You need to separate out your components and services into numerous small repos. Each repo should have Continuous Integration (CI) on it. This change forces teams to start decomposing their monolithic apps into shared libraries and microservices. “Wait – what? I need to get into microservices to get good DevOps?” I hear you yelling. Well, another good DevOps principle is releasing small amounts of change often. And if everything is mashed together in a giant repo, it’s hard to do that. So you need to split up your monoliths into smaller components that can be independently released. Yet again, good architecture (loose coupling, strict service boundaries) promotes good DevOps – or is it DevOps finally motivating you to Do It Right™ like you should have all along?\"]]],[1,\"p\",[[0,[],0,\"You’ve gone ahead and split out shared code and components. But now your builds don’t work because your common code (your internal libraries) are suddenly in different repos. Yes, you’re going to need some package management tool. Now, as a side benefit, teams can opt-in to changes in common libraries rather than being forced to update project references. This is a great example of how good DevOps influences good development practices! Even if you just use a file share as a NuGet source (you don’t necessarily need a full-blown package management system) you’re better off.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"In this post, we’ve looked at how good source control structure, branching strategies, loosely coupled architectures and package management can make DevOps easier. Or perhaps how DevOps pushes you to improve all of these. As I mentioned, good architecture and DevOps are symbiotic, feeding off each other (for good or bad). So make sure it’s for good! Now go and read \"],[0,[0],1,\"part 2\"],[0,[],0,\" of this post.\"]]],[1,\"p\",[[0,[],0,\"Happy architecting!\"]]]]}","published_at":1487480742000,"status":"published","published_by":1},{"id":"bdc0a35a-694f-413e-b679-800d6f84a242","title":"DevOps Drives Better Architecture–Part 2 of 2","slug":"devops-drives-better-architecturepart-2-of-2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://bit.ly/devopsarch1\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"part 1\"],[0,[],0,\" I introduced some thoughts as to how good architecture makes DevOps easier. And how good DevOps drives better architecture – a symbiotic relationship. I discussed how good source control structure, branching strategies, loosely coupled architectures and package management can make DevOps easier. In this post I’ll share some thoughts along the same lines for infrastructure as code, database design and management, monitoring and test automation.\"]]],[1,\"h2\",[[0,[],0,\"Infrastructure as Code\"]]],[1,\"p\",[[0,[],0,\"Let’s say you get your builds under control, you’re versioning, you get your repos sorted and you get package management in place. Now you’re starting to produce lots and lots of builds. Unfortunately, a build doesn’t add value to anyone until it’s in production! So you’ll need to deploy it. But you’ll need infrastructure to deploy to. So you turn to IT and fill in the forms and wait 2 weeks for the servers…\"]]],[1,\"p\",[[0,[],0,\"Even if you can quickly spin up tin (or VMs more likely) or you’re deploying to PaaS, you still need to handle configuration. You need to configure your servers (if you’re on IaaS) or your services (on PaaS). You can’t afford to do this manually each time you need infrastructure. You’re going to need to be able to automatically spin up (and down) resources when you need them.\"]]],[1,\"h3\",[[0,[],0,\"Spinning Up From Scratch\"]]],[1,\"p\",[[0,[],0,\"I was recently at a customer that were using AWS VMs for their DevTest infrastructure. We were evaluating if we could replicate their automated processes in Azure. The problem was they hadn’t scripted creation of their environments \"],[0,[1],1,\"from scratch\"],[0,[],0,\" – they would manually configure a VM until it was “golden” and then use that as a base image for spinning up instances. Now that we were wanting to change their cloud host, they couldn’t do it easily because someone would have to spin up an Azure VM and manually configure it. If they had rather used the principle of scripting and automating configuration, we could have used the existing scripts to quickly spin up test machines on any platform or host. Sometimes you don’t know you need something until you actually need it – so do the right things early and you’ll be better off in the long run. Get into the habit of configuring via code rather than via UI.\"]]],[1,\"h3\",[[0,[],0,\"Deploy Whole Components – Always\"]]],[1,\"p\",[[0,[],0,\"In Continuous Delivery, Humble and Farley argue that it’s easier to deploy your entire system each time than trying to figure out what the delta is and only deploy that. If you craft your scripts and deployments so that they are \"],[0,[1],1,\"idempotent\"],[0,[],0,\", then this shouldn’t be a problem. Try to prefer declarative scripting (such as PowerShell DSC) over imperative scripting (like pure PowerShell). Not only is it easier to “read” the configuration, but the system can check if a component is in the required state, and if it is, just “no-op”. Make sure your scripts work irrespective of the initial state of the system.\"]]],[1,\"p\",[[0,[],0,\"If you change a single class, should you just deploy that assembly? It’s far easier to deploy the entire component (be that a service or a web app) than trying to work out what changed and what need to be deployed. Tools can also help here – web deploy, for example, only deploys files that are different. You build the entire site and it calculates at deployment time what the differences are. Same with SDDT for database schema changes.\"]]],[1,\"p\",[[0,[],0,\"Of course, getting all the requirements and settings correct in a script in source control is going to mean that you need to cooperate and team up with the IT guys (and gals). You’re going to need to work together to make sure that you’re both happy with the resulting infrastructure. And that’s good for everyone.\"]]],[1,\"h2\",[[0,[],0,\"Good Database Design and Management\"]]],[1,\"h3\",[[0,[],0,\"Where does your logic live?\"]]],[1,\"p\",[[0,[],0,\"How does DevOps influence database design and management? I used to work for a company where the dev manager insisted that all our logic be in stored procedures. “If we need to make a change quickly,” he reasoned, “then we don’t need to recompile, we can just update the SP!” Needless to say, our code was virtually untestable, so we just deployed and hoped for the best. And spent a lot of time debugging and fighting fires. It wasn’t pretty.\"]]],[1,\"p\",[[0,[],0,\"Stored procedures are really hard to test reliably. And they’re hard to code and debug. So you’re better off leaving your database to \"],[0,[1],1,\"store data\"],[0,[],0,\". Placing logic into component or services lets you test the logic without having to spin up databases with test data – using mocks or fakes or doubles lets you abstract away where the data is stored and test the logic of your apps. And that makes DevOps a lot easier since you can test a lot more during the build phase. And the earlier you find issues (builds are “earlier” than releases) the lest it costs to fix them and the easier it is to fix them.\"]]],[1,\"h3\",[[0,[],0,\"Managing Schema\"]]],[1,\"p\",[[0,[],0,\"What about schema? Even if you don’t have logic in your database in the form of stored procedures, you’re bound to change the schema at some stage. Don’t do it using manual scripts. Start using SQL Server Data Tools (SSDT) for managing your schema. Would you change the code directly on a webserver to implement new features? Of course not – you want to have source control and testing etc. So why don’t we treat databases the same way? Most teams seem happy to “just fix it on the server” and hope they can somehow replicate changes made on the DEV databases to QA and PROD. If that’s you – stop it! Get your schema into an SSDT project and turn off DDL-write permissions so that they only way to change a database schema is to change the project, commit and let the pipeline make the change.\"]]],[1,\"p\",[[0,[],0,\"The advantage of this approach is that you get a full history (in source control) of changes made to your schema. Also, sqlpackage calculates the diff at deployment time between your model and the target database and only updates what it needs to to make the database match the model. Idempotent and completely uncaring as to the start state of the database. Which means hassle free deployments.\"]]],[1,\"h2\",[[0,[],0,\"Automated Testing\"]]],[1,\"p\",[[0,[],0,\"I’ve already touched on this topic – using interfaces and inversion of control makes your code testable, since you can easily mock out external dependencies. Each time you have code that interacts with an external system (be it a database or a web API) you should abstract it as an interface. Not only does this uncouple your development pace from the pace of the external system, it allows you to much more easily test your application by mocking/stubbing/faking the dependency. Teams that have well-architected code are more likely to test their code since the code is easier to test! And tested code produces fewer defects, which means more time delivering features rather than fighting fires. Once again, good architecture is going to ease your DevOps!\"]]],[1,\"p\",[[0,[],0,\"Once you’ve invested in unit testing, you’ll want to start doing some integration testing. This requires code to be deployed to environments so that it can actually hit the externals systems. If everything is a huge monolithic app, then as tests fail you won’t know why they failed. Smaller components will let you more easily isolate where issues occur, leading to faster mean time to detection (MTTD). And if you set up so that you can deploy components independently (since they’re loosely coupled, right!) then you can recover quickly, leading to faster mean time to recovery (MTTR).\"]]],[1,\"p\",[[0,[],0,\"You’ll want to have integration tests that operate “headlessly”. Prefer API calls and HTTP requests over UI tests since UI tests are notoriously hard to create correctly and tend to be fragile. However, if you do get to UI tests, then good architecture can make a big difference here too. Naming controls uniquely means UI test frameworks can find them more easily (and faster) so that UI testing is faster and more reliable. The point surfaces again that DevOps is making you think about how you structure even your UI!\"]]],[1,\"h2\",[[0,[],0,\"Monitoring\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, very few teams that I come across have really good monitoring in place. This is often the “forgotten half-breed” of the DevOps world – most teams get source code right, test right and deploy right – and then wash their hands. “Prod isn’t my responsibility – I’m a dev!” is a common culture. However, good monitoring means that you’re able to more rapidly diagnose issues, which is going to save you time and effort and keep you delivering value (debugging is not delivering value). So you’ll need to think about how to monitor your code, which is going to impact on your architecture.\"]]],[1,\"p\",[[0,[],0,\"Logging is just monitoring 1.0. What about utilization? How do you monitor how much resources your code is consuming? And how do you know when to spin up more resources for peak loads? Can you even do that – or do your web services require affinity? Ensuring that your code can run on 1 or 100 servers will make scaling a lot easier.\"]]],[1,\"p\",[[0,[],0,\"But beyond logging and performance monitoring, there’s a virtually untapped wealth of what I call “business monitoring” that very few (if any) teams seem to take advantage of. If you’re developing an e-commerce app, how can you monitor what product lines are selling well? And can you correlate user profiles to spending habits? The data is all there – if you can tap into it. Application Insights, coupled with analytics and PowerBI can empower a new level of insight that your business didn’t even know existed. DevOps (which included monitoring) will drive you to architect good “business monitoring” into your apps.\"]]],[1,\"h2\",[[0,[],0,\"Build and Release Responsibilities\"]]],[1,\"p\",[[0,[],0,\"One more nugget that’s been invaluable for successful pipelines: know what builds do and what releases do. Builds should take source code (and packages) as inputs, run quality checks such as code analysis and unit testing, and produce packaged binaries as outputs. These packages should be “environment agnostic” – that is, they should not need to know about environments or be tied to environments. Similarly your builds should not need connections strings or anything like that since the testing that occurs during a build should be unit tests that are fast and have no external dependencies.\"]]],[1,\"p\",[[0,[],0,\"This means that you’ll have to have packages that have “holes” in them where environment values can later be injected. Or you may decide to use environment variables altogether and have no configuration files. However you do it, architecting configuration correctly (and fit for purpose, since there can be many correct ways) will make deployment far easier.\"]]],[1,\"p\",[[0,[],0,\"Releases need to know about environments. After all, they’re going to be deploying to the environments, or perhaps even spinning them up and configuring them. This is where your integration and functional tests should be running, since some infrastructure is required.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Good architecture makes good DevOps a lot easier to implement – and good DevOps feeds back into improving the architecture of your application as well as your processes. The latest trend of “shift left” means you need to be thinking about more than just solving a problem in code – you need to be thinking beyond just coding. Think about how the code you’re writing is going to be tested. And how it’s going to be configured on different environments. And how it’s going to be deployed. And how you’re going to spin up the infrastructure you need to run it. And how you’re going to monitor it.\"]]],[1,\"p\",[[0,[],0,\"The benefits, however, of this “early effort” will pay off many times over in the long run. You’ll be faster, leaner and meaner than ever. Happy DevOps architecting!\"]]]]}","published_at":1487480773000,"status":"published","published_by":1},{"id":"b02f6455-0718-454f-887f-4a96b18f64ef","title":"DevOps is a Culture, Not a Team","slug":"devops-is-a-culture-not-a-team","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://blog.nwcadence.com/devops-culture-not-team/\"]],[\"a\",[\"href\",\"http://donovanbrown.com\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Ubuntu_(philosophy)\"]]],\"sections\":[[1,\"p\",[[0,[0],0,\"This post was originally posted on our \"],[0,[1],1,\"Northwest Cadence blog\"],[0,[],1,\" – but I feel it’s a really important post, so I’m cross-posting it here!\"]]],[1,\"p\",[[0,[],0,\"I recently worked at a customer that had created a DevOps team in addition to their Ops and Development teams. While I appreciated the attempt to improve their processes, inwardly I was horrified. Just as DevOps is not a product, I think it is bad practice to create a DevOps team. DevOps is a \"],[0,[0],1,\"culture\"],[0,[],0,\" and a \"],[0,[0],1,\"mindset\"],[0,[],0,\" that should pervade every member of your organization – beyond even developers and operations.\"]]],[1,\"h2\",[[0,[],0,\"What is DevOps?\"]]],[1,\"p\",[[0,[],0,\"So how do you define DevOps? \"],[0,[2],1,\"Donovan Brown\"],[0,[],0,\", a DevOps product manager at Microsoft, defines DevOps succinctly as follows: \"],[0,[0],1,\"DevOps is the union of people, process, and products to enable continuous delivery of value to end users\"],[0,[],0,\". Unfortunately, since the name is an amalgamation of \"],[0,[0],1,\"development\"],[0,[],0,\" and \"],[0,[0],1,\"operations\"],[0,[],0,\", most organizations get developers and ops to collaborate, and then boldly declare, “We do DevOps!”\"]]],[1,\"h2\",[[0,[],0,\"Wider than Dev and Ops\"]]],[1,\"p\",[[0,[],0,\"However, true DevOps is a culture that should involve everyone that is involved in \"],[0,[0],1,\"delivery of value to end users\"],[0,[],0,\". This means that business should understand their role in DevOps. Testers and Quality Assurance (QA) should understand their role in DevOps. Project Management Offices (PMOs), Human Resources (HR) and any other part of the organization that touches on the flow of value should be aware of their role in DevOps.\"]]],[1,\"p\",[[0,[],0,\"That’s why creating a DevOps team is a fundamentally bad decision. It distances people outside the “DevOps” team from being involved in the culture of DevOps. If there’s a DevOps team, and I’m not on it, why should I worry about DevOps? In the same manner, DevOps that is confined solely to dev and ops is indicative of the culture not pervading the organization. To fully benefit from DevOps, the entire organization needs to embrace the \"],[0,[0],1,\"mindset\"],[0,[],0,\" and \"],[0,[0],1,\"culture\"],[0,[],0,\" of DevOps.\"]]],[1,\"h2\",[[0,[],0,\"DevOps Values\"]]],[1,\"p\",[[0,[],0,\"What then is a DevOps culture? What values should be upheld by people as they improve their processes and utilize tools to aid in implementing DevOps practices? Here are a few:\"]]],[3,\"ol\",[[[0,[],0,\"Whatever we do should ultimately deliver value to the end users\"]],[[0,[],0,\"This is absolutely key to good DevOps – everyone, from stakeholders to developers, to testers and ops should be thinking of how to \"],[0,[0],1,\"deliver value\"],[0,[],0,\". If you can’t ship it, it’s not delivered value – so fix it until you can deliver.\"]],[[0,[],0,\"There’s no such thing as a DevOps Hero\"]],[[0,[],0,\"DevOps is not the domain of a single individual or team. Everyone needs to buy in to the culture, and everyone needs to own it. And we need to build a culture of “team”, an \"],[0,[3],1,\"ubuntu\"],[0,[],0,\" for value, within the entire organization.\"]],[[0,[],0,\"If we touch it, we own it\"]],[[0,[],0,\"If developers hand off their code to testers, then they ultimately assume “someone else” will check their code. If a developer is responsible for the after hours support calls, they’re more likely to ensure good quality. Of course enlisting the help of some testers will help that effort!\"]],[[0,[],0,\"We should examine everything we do for efficiencies\"]],[[0,[],0,\"Sometimes we need to step back and examine why we do certain things. Before we automated our deployments, we needed a change control board to wade through pages of “installation instructions”. Now we’ve automated deployments – so we do we still need the documentation or the checkpoint? We could go faster if we removed the “legacy” processes.\"]],[[0,[],0,\"We should be allowed to experiment\"]],[[0,[],0,\"Will automated deployment help us deliver value faster? Perhaps yes, perhaps no. We’ll never find out if we never have the permission (and time) to experiment. And we can learn from failed experiments too – so we should value experimentation\"]]]],[1,\"h2\",[[0,[],0,\"Everyone has a responsibility in DevOps\"]]],[1,\"p\",[[0,[],0,\"DevOps is more than just developers and ops getting together and automating a dew things. While this is certainly a fast-track to better DevOps, the DevOps mindset has to widen to include other teams not traditionally associated with DevOps – like HR, PMOs and Testers.\"]]],[1,\"h3\",[[0,[],0,\"Human Resources (HR)\"]]],[1,\"p\",[[0,[],0,\"Human Resources should be looking for people who are \"],[0,[0],1,\"passionate about delivering value\"],[0,[],0,\". DevOps culture is built faster when people have passion and care about their end users. Don’t just inspect a candidates technical competency – get a feel for how much of a team player they are and how much they will take responsibility for what they do. Don’t hire people who just want to clock in and out and do as little as possible. Also, you may have to “trim the fat” – get rid of positions that are not \"],[0,[0],1,\"delivering value\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"A further boost for developing DevOps culture is the right working environment. Make your workplace a place people love – but make sure they don’t burn out too! Force them to go home and decompress with friends and family. If your teams are always working overtime, it’s an indication that something isn’t right. Find and improve the erroneous practice so that your team members can have a life – this will reinforce the passion and loyalty they have to delivering value.\"]]],[1,\"h3\",[[0,[],0,\"PMOs (Project Management Offices)\"]]],[1,\"p\",[[0,[],0,\"The PMO needs to rethink in many areas – especially in \"],[0,[0],1,\"utilization\"],[0,[],0,\". Most PMOs strive to make sure that every team member is running at 100% utilization. However, there are problems with this approach:\"]]],[3,\"ol\",[[[0,[],0,\"Humans don’t multitask\"]],[[0,[],0,\"Humans don’t multitask – they switch quickly. However, unlike computers that can switch context perfectly to give the illusion of multitasking, humans have fallible memories. Switching costs, since our memories are not perfect. If there are no gaps in our schedules, we will inevitably run late since we don’t usually account for the cost of switching\"]],[[0,[],0,\"No time for thinking, discussion and experimenting\"]],[[0,[],0,\"If you’re at 100% utilization, you inevitably feel like you don’t have time to think. You can’t get involved in discussions with other team members about how to best solve a problem. And you can’t reflect on what is working and what is not. You certainly won’t have time to experiment. Over the long run, this will hamper delivery of value, since you won’t be innovating.\"]],[[0,[],0,\"High utilization increases wait-time\"]],[[0,[],0,\"The busier a resource is, the longer you have to wait to access it. A simple formula proves this – wait time = % busy / % idle. So if a resource is 50% utilized, wait time is 50/50 = 1 unit. If that same resource is 90% utilized, the wait time is 90/10 = 9 units. So you have to wait 9 times longer to access a resource that’s busy 90% of the time than when it’s busy 50% of the time. Long wait times means longer cycle times and lower throughput.\"]]]],[1,\"p\",[[0,[],0,\"PMOs need to embrace the innovative nature of DevOps – and that means giving team members time in their schedules. And it means embracing uncertainty – don’t be afraid to trust the team.\"]]],[1,\"h3\",[[0,[],0,\"Testers\"]]],[1,\"p\",[[0,[],0,\"As Infrastructure as Code, Continuous Integration (CI) and Continuous Deployment (CD) speed the delivery time, testers need to jump in and start automating their testing efforts. In fact, just as I think that a DevOps team is a bad idea, I think that a Testing team is just as bad. Testers should be part of the development/operations team, not a separate entity. And traditional “manual” testers need to beef up on their automation skills, since manual testing becomes a bottleneck in the delivery pipeline. Remember, testers that “find bugs” are not thinking DevOps – testers that aim to automate their tests so that results are faster and more accurate are thinking about real quality improvement – and that means they’re thinking about \"],[0,[0],1,\"delivering value\"],[0,[],0,\" to the end users.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"DevOps is not a team or a product – it is a culture that needs to pervade everyone in the organization. Everyone – from HR to PMOs to Testers, not just developers and ops – needs to embrace DevOps values, making sure that value is being delivered continually to their end users.\"]]]]}","published_at":1455834808000,"status":"published","published_by":1},{"id":"4419f29f-297a-49ea-9fa7-d5e0087503b2","title":"DevOps with Kubernetes and VSTS: Part 1","slug":"devops-with-kubernetes-and-vsts-part-1","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"c:\\ncd \\\\\\nminikube start --vm-driver hyperv --hyperv-virtual-switch minikube\",\"language\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">kubectl config set-cluster minikube --server=</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">https://kubernetes:8443</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\"> --certificate-authority=c:/users/&lt;user&gt;/.minikube/ca.crt</font>\"}],[\"code\",{\"code\":\"PS:\\\\&gt; kubectl get nodes\\nNAME       STATUS    AGE       VERSION\\nminikube   Ready     11m       v1.6.4\",\"language\":\"powershell\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a400f7c7-224c-4cd5-ae2c-477c0f6f379c.png\\\"><img width=\\\"326\\\" height=\\\"208\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/faed94b9-b164-4ea8-9648-b9e17ee67156.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">&amp; minikube docker-env | Invoke-Expression</font>\"}],[\"code\",{\"code\":\"version: '2'\\n\\nservices:\\n  api:\\n    image: api\\n    build:\\n      context: ./API\\n      dockerfile: Dockerfile\\n\\n  frontend:\\n    image: frontend\\n    build:\\n      context: ./frontend\\n      dockerfile: Dockerfile\",\"language\":\"plain\"}],[\"code\",{\"code\":\"FROM microsoft/aspnetcore:1.1\\nARG source\\nWORKDIR /app\\nEXPOSE 80\\nCOPY ${source:-obj/Docker/publish} .\\nENTRYPOINT [\\\"dotnet\\\", \\\"API.dll\\\"]\",\"language\":\"plain\"}],[\"code\",{\"code\":\"cd API\\ndotnet restore\\ndotnet build\\ndotnet publish -o obj/Docker/publish\\ncd ../frontend\\ndotnet restore\\ndotnet build\\ndotnet publish -o obj/Docker/publish\\ncd ..\\ndocker-compose -f docker-compose.yml build\",\"language\":\"powershell\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d449fcd1-3984-4a65-b06c-6f8493a266b9.png\\\"><img width=\\\"333\\\" height=\\\"71\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f93d81df-0b0d-4339-873b-d69e1ba27d9f.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"apiVersion: v1\\nkind: Service\\nmetadata:\\n  name: demo-backend-service\\n  labels:\\n    app: demo\\nspec:\\n  selector:\\n    app: demo\\n    tier: backend\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n      nodePort: 30081\\n  type: NodePort\\n---\\napiVersion: apps/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: demo-backend-deployment\\nspec:\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        app: demo\\n        tier: backend\\n    spec:\\n      containers:\\n      - name: backend\\n        image: api\\n        ports:\\n        - containerPort: 80\\n        imagePullPolicy: Never\",\"language\":\"plain\"}],[\"code\",{\"code\":\"spec:\\n  containers:\\n    - name: frontend\\n      image: frontend\\n      ports:\\n      - containerPort: 80\\n      env:\\n      - name: \\\"ASPNETCORE_ENVIRONMENT\\\"\\n        value: \\\"Production\\\"\\n      volumeMounts:\\n        - name: config-volume\\n          mountPath: /app/wwwroot/config/\\n      imagePullPolicy: Never\\n  volumes:\\n    - name: config-volume\\n      configMap:\\n        name: demo-app-frontend-config\",\"language\":\"plain;first-line:28\"}],[\"code\",{\"code\":\"apiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: demo-app-frontend-config\\n  labels:\\n    app: demo\\n    tier: frontend\\ndata:\\n  config.json: |\\n    {\\n      \\\"api\\\": {\\n        \\\"baseUri\\\": \\\"http://kubernetes:30081/api\\\"\\n      }\\n    }\",\"language\":\"plain\"}],[\"code\",{\"code\":\"PS:\\\\&gt; cd k8s\\nPS:\\\\&gt; kubectl apply -f .\\\\app-demo-frontend-config.yml\\nconfigmap \\\"demo-app-frontend-config\\\" created\\n\\nPS:\\\\&gt; kubectl apply -f .\\\\app-demo-backend-minikube.yml\\nservice \\\"demo-backend-service\\\" created\\ndeployment \\\"demo-backend-deployment\\\" created\\n\\nPS:\\\\&gt; kubectl apply -f .\\\\app-demo-frontend-minikube.yml\\nservice \\\"demo-frontend-service\\\" created\\ndeployment \\\"demo-frontend-deployment\\\" created\",\"language\":\"plain;highlight:[1,2,5,9]\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/116cd97a-ac84-400c-893a-157e9d1f503d.png\\\"><img width=\\\"320\\\" height=\\\"100\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5c5fb236-bf39-43f3-bac5-bf7912bf66f3.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9ab0a69f-c383-44e4-95d3-cb12152728cf.png\\\"><img width=\\\"244\\\" height=\\\"156\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/05a3d6ef-c5fd-4b57-936b-c6c25129a49e.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"PS:&gt; kubectl get pods\\nNAME                                       READY     STATUS    RESTARTS   AGE\\ndemo-backend-deployment-951716883-fhf90    1/1       Running   0          28m\\ndemo-backend-deployment-951716883-pw1r2    1/1       Running   0          28m\\ndemo-frontend-deployment-477968527-bfzhv   1/1       Running   0          14s\\ndemo-frontend-deployment-477968527-q4f9l   1/1       Running   0          24s\\n\\nPS:&gt; kubectl delete pods demo-backend-deployment-951716883-fhf90 demo\\n-backend-deployment-951716883-pw1r2 demo-frontend-deployment-477968527-bfzhv demo-frontend-deployment-477968527-q4f9l\\npod \\\"demo-backend-deployment-951716883-fhf90\\\" deleted\\npod \\\"demo-backend-deployment-951716883-pw1r2\\\" deleted\\npod \\\"demo-frontend-deployment-477968527-bfzhv\\\" deleted\\npod \\\"demo-frontend-deployment-477968527-q4f9l\\\" deleted\\n\\nPS:&gt; kubectl get pods\\nNAME                                       READY     STATUS    RESTARTS   AGE\\ndemo-backend-deployment-951716883-4dsl4    1/1       Running   0          3m\\ndemo-backend-deployment-951716883-n6z4f    1/1       Running   0          3m\\ndemo-frontend-deployment-477968527-j2scj   1/1       Running   0          3m\\ndemo-frontend-deployment-477968527-wh8x0   1/1       Running   0          3m\",\"language\":\"plain;highlight:[1,8,15]\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.docker.com/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/devops-with-kubernetes-and-vsts-part-2\"]],[\"a\",[\"href\",\"https://mesosphere.com/\"]],[\"a\",[\"href\",\"https://kubernetes.io/\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/swarm/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/tasks/configure-pod-container/configmap/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/configuration/secret/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/\"]],[\"a\",[\"href\",\"https://github.com/kubernetes/minikube\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/installation/\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/tasks/tools/install-kubectl/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/aurelia-azure-and-vsts\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/AzureAureliaDemo\"]],[\"a\",[\"href\",\"http://anthonychu.ca/post/aspnet-core-appsettings-secrets-kubernetes/\"]],[\"a\",[\"href\",\"https://github.com/aspnet/StaticFiles/issues/202\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/AzureAureliaDemo/blob/docker/frontend/SymlinkFileProvider.cs\"]],[\"a\",[\"href\",\"http://kubernetes:30080\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"If you've read my blog before, you'll probably know that I am huge fan of \"],[0,[0],1,\"Docker\"],[0,[],0,\" and containers. When was the last time you installed software onto bare metal? Other than your laptop, chances are you haven't for a long time. Virtualization has transformed how we think about resources in the datacenter, greatly increasing the density and utilization of resources. The next evolution in density is containers - just what VMs are to physical servers, containers are to VMs. Soon, almost no-one will work against VMs anymore - we'll all be in containers. At least, that's the potential.\"]]],[1,\"p\",[[0,[],0,\"However, as cool as containers are for packaging up apps, there's still a lot of uncertainty about how to actually run containers in production. Creating a single container is a cool and satisfying experience for a developer, but how do you run a cluster and scale containers? How do you monitor your containers? How do you manage faults? This is where we enter the world of container orchestration.\"]]],[1,\"p\",[[0,[],0,\"This post will cover the local development experience with Kubernetes and minikube. \"],[0,[1],1,\"Part 2\"],[0,[],0,\" covers the CI/CD pipeline to a Kubernetes cluster in Azure.\"]]],[1,\"h2\",[[0,[],0,\"Orchestrator Wars\"]]],[1,\"p\",[[0,[],0,\"There are three popular container orchestration systems - \"],[0,[2],1,\"Mesos\"],[0,[],0,\", \"],[0,[3],1,\"Kubernetes\"],[0,[],0,\" and \"],[0,[4],1,\"Docker Swarm\"],[0,[],0,\". I don't want to go into a debate on which one you should go with (yet) - but they're all conceptually similar.  They all work off configuration as code for spinning up lots of containers across lots of nodes. Kubernetes does have a couple features that I think are killer for DevOps: \"],[0,[5],1,\"ConfigMaps\"],[0,[],0,\", \"],[0,[6],1,\"Secrets\"],[0,[],0,\" and \"],[0,[7],1,\"namespaces\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"In short, namespaces allow you to segregate logical environments in the same cluster - the canonical example is a DEV namespace where you can run small copies of your PROD environment for testing. You could also use namespaces for different security contexts or multi-tenancy. ConfigMaps (and Secrets) allow you to store configuration outside of your containers - which means you can have the same image running in various contexts without having to bake environment-specific code into the images themselves.\"]]],[1,\"h2\",[[0,[],0,\"Kubernetes Workflow and Pipeline\"]]],[1,\"p\",[[0,[],0,\"In this post, I want to look at how you would develop with Kubernetes in mind. We'll start by looking at the developer workflow and then move on to how the DevOps pipeline looks in the next post. Fortunately, having \"],[0,[8],1,\"MiniKube\"],[0,[],0,\" (a one-node Kubernetes cluster that runs in a VM) means that you can develop against a fully features cluster on your laptop! That means you can take advantage of cluster features (like ConfigMaps) without having to be connected to a production cluster.\"]]],[1,\"p\",[[0,[],0,\"So what would the developer workflow look like? Something like this:\"]]],[3,\"ol\",[[[0,[],0,\"Develop code\"]],[[0,[],0,\"Build image from Dockerfile or docker-compose files\"]],[[0,[],0,\"Run service in MiniKube (which spins up containers from the images you just built)\"]]]],[1,\"p\",[[0,[],0,\"It turns out that Visual Studio 2017 (and/or VS Code), Docker and MiniKube make this a really smooth experience.\"]]],[1,\"p\",[[0,[],0,\"Eventually you're going to move to the DevOps pipeline - starting with a build. The build will take the source files and Dockerfiles and build images and push them to a private container registry. Then you'll want to push configuration to a Kubernetes cluster to actually run/deploy the new images. It turns out that using Azure and VSTS makes this DevOps pipeline smooth as butter! That will be the subject of \"],[0,[1],1,\"Part 2\"],[0,[],0,\" - for now, we'll concentrate on the developer workflow.\"]]],[1,\"h2\",[[0,[],0,\"Setting up the Developer Environment\"]]],[1,\"p\",[[0,[],0,\"I'm going to focus on a Windows setup, but the same setup would apply to Mac or Linux environments as well. To set up a local development environment, you need to install the following:\"]]],[3,\"ol\",[[[0,[9],1,\"Docker\"]],[[0,[10],1,\"Kubectl\"]],[[0,[8],1,\"MiniKube\"]]]],[1,\"p\",[[0,[],0,\"You can follow the links and run the installs. I had a bit of trouble with MiniKube on HyperV - by default, MiniKube start (the command that creates the MiniKube VM) just grabs the first HyperV virtual network it finds. I had a couple, and the one that MiniKube grabbed was an internal network, which caused MiniKube to fail. I created a new virtual network called minikube in the HyperV console and made sure it was an external network. I then used the following command to create the MiniKube VM:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Note: I had to cd to c:\\\\ - if I did not, MiniKube failed to create the VM.\"]]],[1,\"p\",[[0,[],0,\"My external network if connected to my WiFi. That means when I join a new network, my minikube VM gets a new IP. Instead of having to update the kubeconfig each time, I just added a hosts entry in my hosts file (c:\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts on Windows) using \\\"<IP> kubernetes\\\", where IP is the IP address of the minikube VM - obtained by running \\\"minikube ip\\\". To update the kubeconfig, run this command:\"]]],[10,1],[10,2],[10,3],[1,\"p\",[[0,[],0,\"where <user> is your username, so that the cert points to the ca.crt file generated into your .minikube directory.\"]]],[1,\"p\",[[0,[],0,\"Now if you join a new network, you just update the IP in the hosts file and your kubectl commands will still work. The certificate is generated for a hostname \\\"kubernetes\\\" so you have to use that name.\"]]],[1,\"p\",[[0,[],0,\"If everything is working, then you should get a neat response to \\\"kubectl get nodes\\\":\"]]],[10,4],[1,\"p\",[[0,[],0,\"To open the Kubernetes UI, just enter \\\"minikube dashboard\\\" and a browser will launch:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Finally, to \\\"re-use\\\" the minikube docker context, run the following command:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Now you are sharing the minikube docker socket. Running \\\"docker ps\\\" will return a few running containers - these are the underlying Kubernetes system containers. It also means you can create images here that the minikube cluster can run.\"]]],[1,\"p\",[[0,[],0,\"You now have a 1-node cluster, ready for development!\"]]],[1,\"h2\",[[0,[],0,\"Get Some Code\"]]],[1,\"p\",[[0,[],0,\"I recently blogged about \"],[0,[11],1,\"Aurelia development with Azure and VSTS\"],[0,[],0,\". Since I already had a couple of .NET Core sites, I thought I would see if I could get them running in a Kubernetes cluster. Clone \"],[0,[12],1,\"this repo\"],[0,[],0,\" and checkout the docker branch. I've added some files to the repo to support both building the Docker images as well as specifying Kubernetes configuration. Let's take a look.\"]]],[1,\"p\",[[0,[],0,\"The docker-compose.yml file specifies a composite application made up of two images: api and frontend:\"]]],[10,7],[1,\"p\",[[0,[],0,\"The Dockerfile for each service is straightforward: start from the ASP.NET Core 1.1 image, copy the application files into the container, expose port 80 and run \\\"dotnet app.dll\\\" (frontend.dll and api.dll for each site respectively) as the entry point for each container:\"]]],[10,8],[1,\"p\",[[0,[],0,\"To build the images, we need to dotnet restore, build and publish. Then we can build the images. Once we have images, we can configure a Kubernetes service to run the images in our minikube cluster.\"]]],[1,\"h2\",[[0,[],0,\"Building the Images\"]]],[1,\"p\",[[0,[],0,\"The easiest way to get the images built is to use Visual Studio, set the docker-compose project as the startup project and run. That will build the images for you. But if you're not using Visual Studio, then you can build the images by running the following commands from the root of the repo:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Now if you run \\\"docker images\\\" you'll see the minikube containers as well as images for the frontend and the api:\"]]],[10,10],[1,\"h2\",[[0,[],0,\"Declaring the Services - Configuration as Code\"]]],[1,\"p\",[[0,[],0,\"We can now define the services that we want to run in the cluster. One of the things I love about Kubernetes is that it pushes you to declare the environment you want rather than running a script. This declarative model is far better than an imperative model, and we can see that with the rise of Chef, Puppet and PowerShell DSC. Kubernetes allows us to specify the services we want exposed as well as how to deploy them. We can define various Kubernetes objects using a simple yaml file. We're going to declare two services: an api service and a frontend service. Usually, the backend services won't be exposed outside the cluster, but since the demo code we're deploying is a single page app (SPA), we need to expose the api outside the cluster.\"]]],[1,\"p\",[[0,[],0,\"The services are rarely going to change - they specify what services are available in the cluster. However, the underlying containers (or in Kubernetes speak, pods) that make up the service will change. They'll change as they are updated and they'll change as we scale out and then back in. To manage the containers that \\\"make up\\\" the service, we use a construct known as a Deployment. Since the service and deployment are fairly tightly coupled, I've placed them into the same file, so that we have a frontend service/deployment file (k8s/app-demo-frontend-minikube.yml) and an api service/deployment file (k8s/app-demo-backend-minikube.yml). The service and deployment definitions could live separately too if you want. Let's take a look at the app-demo-backend.yml file:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 1 - 15 declare the service\"]],[[0,[],0,\"Line 4 specified the service name\"]],[[0,[],0,\"Line 8 - 10 specify the selector for this service. Any pod that has the labels app=demo and tier=frontend will be load balanced for this service. As requests come into the cluster that target this service, the service will know how to route the traffic to its underlying pods. This makes adding, removing or updating pods easy since all we have to do is modify the selector. The service will get a static IP, but the underlying pods will get dynamic IPs that will change as they move through their lifecycle. However, this is transparent to us, since we just target the service and all is good.\"]],[[0,[],0,\"Line 14 - we want this service exposed on port 30081 (mapping to port 80 on the pods, as specified in line 13)\"]],[[0,[],0,\"Line 15 - the type NodePort specifies that we want Kubernetes to give the service a port on the same IP as the cluster. For \\\"real\\\" clusters (in a cloud provider like Azure) we would change this to get an IP from the cloud host.\"]],[[0,[],0,\"Lines 17 - 34 declare the Deployment that will ensure that there are containers (pods) to do the work for the service. If a pod dies, the Deployment will automatically start a new one. This is the construct that ensures the service is up and running.\"]],[[0,[],0,\"Line 22 specifies that we want 2 instances of the container for this service at all times\"]],[[0,[],0,\"Lines 26 and 27 are important: they must match the selector labels from the service\"]],[[0,[],0,\"Line 30 specifies the name of the container within the pod (in this case we only have a single container in this pod anyway, which is generally what you want to do)\"]],[[0,[],0,\"Line 31 specifies the name of the image to run - this is the same name as we specified in the docker-compose file for the backend image\"]],[[0,[],0,\"Line 33 exposes port 80 on this container to the cluster\"]],[[0,[],0,\"Line 34 specifies that we never want Kubernetes to pull the image since we're going to build the images into the minikube docker context. In a production cluster, we'll want to specify other policies so that the cluster can get updated images from a container registry (we'll see that in \"],[0,[1],1,\"Part 2\"],[0,[],0,\").\"]]]],[1,\"p\",[[0,[],0,\"The frontend definition for the frontend service is very similar - except there's also some \\\"magic\\\" for configuration. Let's take a quick look:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 30: name the container in the pod\"]],[[0,[],0,\"Line 31: specify the name of the image for this container - matching the name in the docker-compose file\"]],[[0,[],0,\"Lines 34 - 36: an example of how to specify environment variables for a service\"]],[[0,[],0,\"Lines 37 - 39: this is a reference to a volume mount (specified lower down) for mounting a config file, telling Kuberenetes where in the container file system to mount the file. In this case, Kubernetes will mount the volume with name \\\"config-volume\\\" to the path /app/wwwroot/config inside the container.\"]],[[0,[],0,\"Lines 41 - 44: this specifies a volume - in this case a configMap volume to use for the configuration (more on this just below). Here we tell Kubernetes to create a volume called config-volume (referred to by the container volumeMount) and to base the data for the volume off a configMap with the name demo-app-frontend-config\"]]]],[1,\"h2\",[[0,[],0,\"Handling Configuration\"]]],[1,\"p\",[[0,[],0,\"We now have a couple of container images and can start running them in minikube. However, before we start that, let's take a moment to think a little about configuration. If you've ever heard me speak or read my blog, you'll know that I am a huge proponent of \\\"build once, deploy many times\\\". This is a core principle of good DevOps. It's no different when you consider Kubernetes and containers. However, to achieve that you'll have to make sure you have a way to handle configuration outside of your compiled bits - hence mechanisms like configuration files. If you're deploying to IIS or Azure App Services, you can simply use the web.config (or for DotNet Core the appsettings.json file) and just specify different values for different environments. However, how do you do that with containers? The entire app is self-contained in the container image, so you can't have different versions of the config file - otherwise you'll need different versions of the container and you'll be violating the build once principle.\"]]],[1,\"p\",[[0,[],0,\"Fortunately, we can use volume mounts (a container concept) in conjunction with secrets and/or configMaps (a Kubernetes concept). In essence, we can specify configMaps (which are essentially key-value pairs) or secrets (which are masked or hidden key-value pairs) in Kubernetes and then just mount them via volume mounts into containers. This is really powerful, since the pod definition stays the same, but if we have a different configMap we get a different configuration! We'll see how this works when we deploy to a cloud cluster and use namespaces to separate dev and production environments.\"]]],[1,\"p\",[[0,[],0,\"The configMaps can also be specified using configuration as code. Here's the configuration for our configMap:\"]]],[10,13],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 2: we specify that this is a configMap definition\"]],[[0,[],0,\"Line 4: the name we can refer to this map by\"]],[[0,[],0,\"Line 9: we're specifying this map using a \\\"file format\\\" - the name of the file is \\\"config.json\\\"\"]],[[0,[],0,\"Lines 10 - 14: the contents of the config file\"]]]],[1,\"h3\",[[0,[],0,\"Aside: Static Files Symlink Issue\"]]],[1,\"p\",[[0,[],0,\"I did have one issue when mounting the config file using configMaps: inside the container the volume mount to /app/www/config/config.json ends up being a symlink. I got the idea of using a configMap in the container from \"],[0,[13],1,\"this excellent post\"],[0,[],0,\" by Anthony Chu, in which he mounts an application.json file that the Startup.cs file can consume. Apparently he didn't have any issues with the symlink in the Startup file. However, in the case of my demo frontend app, I am using a config file that is consumed by the SPA app - and that means, since it's on the client side, the config file needs to be served from the DotNet Core app, just like the html or js files. No problem - we've already got a UseStaticFiles call in Startup, so that should just serve the file, right? Unfortunately, it doesn't. At least, it only serves the first few bytes of the file.\"]]],[1,\"p\",[[0,[],0,\"I took a couple of days to figure this out - there's a \"],[0,[14],1,\"conversation on Github\"],[0,[],0,\" you can read if you're interested. In short, the symlink length is not the length of the file, but the length of the path to the file. The StaticFiles middleware reads FileInfo.Length bytes when the file is requested, but since the length isn't the full length of the file, only the first few bytes were being returned. I was able to create a \"],[0,[15],1,\"FileProvider\"],[0,[],0,\" that worked around the issue.\"]]],[1,\"h2\",[[0,[],0,\"Running the Images in Kubernetes\"]]],[1,\"p\",[[0,[],0,\"To run the services we just created in minikube, we can just use kubectl to apply the configurations. Here's the list of commands (the highlighted lines):\"]]],[10,14],[1,\"p\",[[0,[],0,\"And now we have some services! You can open the minikube dashboard by running \\\"minikube dashboard\\\" and check that the services are green:\"]]],[10,15],[1,\"p\",[[0,[],0,\"And you can browse to the frontend service by navigating to \"],[0,[16],1,\"http://kubernetes:30080\"],[0,[],0,\":\"]]],[10,16],[1,\"p\",[[0,[],0,\"The list (value1 and value2) are values coming back from the API service - so the frontend is able to reach the backend service in minikube successfully!\"]]],[1,\"h3\",[[0,[],0,\"Updating the Containers or Containers\"]]],[1,\"p\",[[0,[],0,\"If you update your code, you're going to need to rebuild the container(s). If you update the config, you'll have to re-run the \\\"kubectl apply\\\" command to update the configMap. Then, since we don't need high-availability in dev, we can just delete the running pods and let the replication set restart them - this time with updated config and/or code. Of course in production we won't do this - I'll show you how to do rolling updates in the next post when we do CI/CD to a Kubernetes cluster.\"]]],[1,\"p\",[[0,[],0,\"For dev though, I get the pods, delete them all and then watch Kubernetes magically re-start the containers again (with new IDs) and voila - updated containers.\"]]],[10,17],[1,\"p\",[[0,[],0,\"Note how the pods get updated IDs - since they're not the same pods! If we go to the frontend now, we'll see updated code.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I am really impressed with Kubernetes and how it encourages infrastructure as code. It's fairly easy to get a cluster running locally on your laptop using minikube, which means you can develop against a like-for-like environment that matched prod - which is always a good idea. You get to take advantage of secrets and configMaps, just like production containers will use. All in all this is a great way to do development, putting good practices into place right from the start of the development process.\"]]],[1,\"p\",[[0,[],0,\"Happy sailing! (Get it? Kubernetes = helmsman)\"]]]]}","published_at":1499168240000,"status":"published","published_by":1},{"id":"d126598f-09bb-4b0e-9972-f9c52074f5a2","title":"DevOps with Kubernetes and VSTS: Part 2","slug":"devops-with-kubernetes-and-vsts-part-2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"# set some variables\\nexport RG=\\\"cd-k8s\\\"\\nexport clusterName=\\\"cdk8s\\\"\\nexport location=\\\"westus\\\"\\n# create a folder for the cluster ssh-keys\\nmkdir cdk8s\\n\\n# login and create a resource group\\naz login\\naz group create --location $location --name $RG\\n\\n# create an ACS k8s cluster\\naz acs create --orchestrator-type=kubernetes --resource-group $RG --name=$ClusterName --dns-prefix=$ClusterName --generate-ssh-keys --ssh-key-value ~/cdk8s/id_rsa.pub --location $location --agent-vm-size Standard_DS1_v2 --agent-count 2\\n\\n# create an Azure Container Registry\\naz acr create --resource-group $RG --name $ClusterName --location $location --sku Basic --admin-enabled\\n\\n# configure kubectl\\naz acs kubernetes get-credentials --name $ClusterName --resource-group $RG --file ~/cdk8s/kubeconfig --ssh-key-file ~/cdk8s/id_rsa\\nexport KUBECONFIG=\\\"~/cdk8s/kubeconfig\\\"\\n\\n# test connection\\nkubectl get nodes\\nNAME                    STATUS                     AGE       VERSION\\nk8s-agent-96607ff6-0    Ready                      17m       v1.6.6\\nk8s-agent-96607ff6-1    Ready                      17m       v1.6.6\\nk8s-master-96607ff6-0   Ready,SchedulingDisabled   17m       v1.6.6\",\"language\":\"bash\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6bad8157-6f0f-4769-ad87-dba0559fd431.png\\\" target=\\\"_blank\\\"><img width=\\\"294\\\" height=\\\"172\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c6e7cdb2-8377-476e-b9cf-581ff29b0ddf.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"apiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: dev\\n---\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: prod\",\"language\":\"plain\"}],[\"code\",{\"code\":\"kubectl apply -f namespaces.yml\\nnamespace \\\"dev\\\" created\\nnamespace \\\"prod\\\" created\\n\\nkubectl get namespaces\\nNAME          STATUS    AGE\\ndefault       Active    27m\\ndev           Active    20s\\nkube-public   Active    27m\\nkube-system   Active    27m\\nprod          Active    20s\",\"language\":\"bash; highlight:[1,5]\"}],[\"code\",{\"code\":\"az acr credential show --name $ClusterName --output table\\nUSERNAME    PASSWORD                          PASSWORD2\\n----------  --------------------------------  --------------------------------\\ncdk8s       some-long-key-1                   some-long-key-2\\n\\nkubectl create secret docker-registry regsecret --docker-server=$ClusterName.azurecr.io --docker-username=$ClusterName --docker-password=&lt;some-long-key-1&gt; --docker-email=admin@azurecr.io\\nsecret \\\"regsecret\\\" created\",\"language\":\"bash; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b5f00719-4e56-4306-ba72-100952c2c478.png\\\" target=\\\"_blank\\\"><img width=\\\"297\\\" height=\\\"206\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a7511c54-2694-49d4-b38f-f0a71871ff39.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f34492c8-3acb-4069-af9c-5732a04293d5.png\\\" target=\\\"_blank\\\"><img width=\\\"301\\\" height=\\\"201\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4b2980ec-f363-45a0-8146-a9d4f735ddb4.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6672b99f-4f86-460e-9ed9-1ed5052a1125.png\\\" target=\\\"_blank\\\"><img width=\\\"327\\\" height=\\\"156\\\" title=\\\"image\\\" style=\\\"display: inline; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f0a853d8-05dc-4a74-83f4-5a4a21a17264.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b87cebd8-96ec-441d-9982-fbf88fa41643.png\\\" target=\\\"_blank\\\"><img width=\\\"218\\\" height=\\\"282\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3fd6a327-e654-4f27-8a96-85018b35995a.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a89cefb0-072e-4e9b-bc64-71db8a8c1f99.png\\\" target=\\\"_blank\\\"><img width=\\\"313\\\" height=\\\"156\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/eb80b904-b95d-418a-8fd9-c6ba74a1e81d.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0616d7c0-9c85-427b-a667-1fd8e1d69ee8.png\\\" target=\\\"_blank\\\"><img width=\\\"315\\\" height=\\\"200\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/df2dc709-8e56-4590-bde2-a3a801c785d9.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/559e2dc0-4108-44fd-a864-5fc66e23206b.png\\\" target=\\\"_blank\\\"><img width=\\\"244\\\" height=\\\"220\\\" title=\\\"image\\\" style=\\\"display: inline; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d6391629-5031-4dfa-b749-b8f276bbf890.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e7c5135d-5e5e-428f-8242-23bec4121a36.png\\\" target=\\\"_blank\\\"><img width=\\\"244\\\" height=\\\"220\\\" title=\\\"image\\\" style=\\\"display: inline; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/45cf619c-0939-4999-9169-220e6c2b18d4.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/48c2502e-771c-4b66-a09b-e897957ae39d.png\\\" target=\\\"_blank\\\"><img width=\\\"244\\\" height=\\\"224\\\" title=\\\"image\\\" style=\\\"display: inline; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/eb169cb5-c6b0-4f08-9573-8a154442eda0.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f3bcd024-b1bc-4850-bd39-d6a0fd066361.png\\\" target=\\\"_blank\\\"><img width=\\\"244\\\" height=\\\"145\\\" title=\\\"image\\\" style=\\\"display: inline; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5e2102f5-fb21-4442-a70a-d4f793a2f48b.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"apiVersion: v1\\nkind: Service\\nmetadata:\\n  name: demo-frontend-service\\n  labels:\\n    app: demo\\nspec:\\n  selector:\\n    app: demo\\n    tier: frontend\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n      nodePort: __FrontendServicePort__\\n  type: LoadBalancer\\n---\\napiVersion: apps/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: demo-frontend-deployment\\nspec:\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        app: demo\\n        tier: frontend\\n    spec:\\n      containers:\\n        - name: frontend\\n          image: __ContainerRegistry__/frontend\\n          ports:\\n          - containerPort: 80\\n          env:\\n          - name: \\\"ASPNETCORE_ENVIRONMENT\\\"\\n            value: \\\"__AspNetCoreEnvironment__\\\"\\n          volumeMounts:\\n            - name: config-volume\\n              mountPath: /app/wwwroot/config/\\n          imagePullPolicy: Always\\n      volumes:\\n        - name: config-volume\\n          configMap:\\n            name: demo-app-frontend-config\\n      imagePullSecrets:\\n        - name: regsecret\",\"language\":\"plain; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/085adf52-57c4-4402-ac69-73568d4d69d3.png\\\" target=\\\"_blank\\\"><img width=\\\"286\\\" height=\\\"147\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b60dce3f-cb90-4fda-8b40-0878e1cddb2b.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"az acs kubernetes browse -n $ClusterName -g $RG --ssh-key-file ~/cdk8s/id_rsa\\n\\nProxy running on 127.0.0.1:8001/ui\\nPress CTRL+C to close the tunnel...\\nStarting to serve on 127.0.0.1:8001\",\"language\":\"bash; highlight:[1]\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5b95aaac-6630-484d-839d-91f72a5ac868.png\\\" target=\\\"_blank\\\"><img width=\\\"289\\\" height=\\\"93\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2dc79a1f-343b-4604-a6e3-48d2101ac5d2.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8c6ad693-0c98-4913-84a2-943de4f9666c.png\\\" target=\\\"_blank\\\"><img width=\\\"283\\\" height=\\\"135\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6af595c5-dba2-4307-b362-9a1ad17593e7.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/251a6544-7bf1-4a88-9ea2-c60ba870e832.png\\\" target=\\\"_blank\\\"><img width=\\\"234\\\" height=\\\"244\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/201d8dd0-1864-456e-8579-502f9fbe8c44.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a0f5d264-eca5-4357-a140-82df86770e6f.png\\\" target=\\\"_blank\\\"><img width=\\\"288\\\" height=\\\"177\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e0f19be7-9584-4995-8438-d98e50172b76.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b5073661-75db-4c19-8c48-079a1310e1a0.png\\\" target=\\\"_blank\\\"><img width=\\\"274\\\" height=\\\"163\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/90eeb703-a23f-472a-99f1-c38fa433e989.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/08b983cf-05d9-40fc-aadb-b0b6ceb823cb.png\\\"><img width=\\\"311\\\" height=\\\"156\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ab172b16-ae83-44f1-825c-e901a4fa9438.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/devops-with-kubernetes-and-vsts-part-1\"]],[\"a\",[\"href\",\"https://kubernetes.io/\"]],[\"a\",[\"href\",\"https://github.com/kubernetes/minikube\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/AzureAureliaDemo\"]],[\"a\",[\"href\",\"http://aurelia.io/\"]],[\"a\",[\"href\",\"https://twitter.com/nigelpoulton\"]],[\"a\",[\"href\",\"https://www.pluralsight.com/courses/getting-started-kubernetes\"]],[\"a\",[\"href\",\"https://blogs.msdn.microsoft.com/devops/2017/06/09/deploying-applications-to-azure-container-service/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/cli/azure/overview\"]],[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/docs/build/concepts/library/variable-groups#link-secrets-from-an-azure-key-vault-as-variables\"]],[\"a\",[\"href\",\"http://bit.ly/cacbuildtasks\"]],[\"a\",[\"href\",\"http://localhost:8001/ui\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-tasks/pull/4657\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/AzureAureliaDemo/tree/docker/vsts-json\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"Part 1\"],[0,[],0,\" I looked at how to develop multi-container apps using \"],[0,[1],1,\"Kubernetes\"],[0,[],0,\" (k8s) - and more specifically, \"],[0,[2],1,\"minikube\"],[0,[],0,\", which is a full k8s environment that runs a single node on a VM on your laptop. In that post I walk through cloning \"],[0,[3],1,\"this repo\"],[0,[],0,\" (be sure to look at the docker branch) which contains two containers: a DotNet Core API container and a frontend SPA (\"],[0,[4],1,\"Aurelia\"],[0,[],0,\") container (also hosted as static files in a DotNet Core app). I show how to build the containers locally and get them running in minikube, taking advantage of ConfigMaps to handle configuration.\"]]],[1,\"p\",[[0,[],0,\"In this post, I will show you how to take the local development into CI/CD and walk through creating an automated build/release pipeline using VSTS. We'll create an Azure Container Registry and Azure Container Services using k8s as the orchestration mechanism.\"]]],[1,\"p\",[[0,[],0,\"I do recommend watching \"],[0,[5],1,\"Nigel Poulton\"],[0,[],0,\"'s excellent \"],[0,[6],1,\"Getting Started with Kubernetes\"],[0,[],0,\" PluralSight course and reading \"],[0,[7],1,\"this post\"],[0,[],0,\" by Atul Malaviya from Microsoft. Nigel's course was an excellent primer into Kubernetes and Atul's post was helpful to see how VSTS and k8s interact - but neither course nor post quite covered a whole \"],[0,[8],1,\"pipeline\"],[0,[],0,\". How do you update your images in a CI/CD pipeline was a question not answered to my satisfaction. So after some experimentation, I am writing this post!\"]]],[1,\"h2\",[[0,[],0,\"Creating a k8s Environment using Azure Container Services\"]]],[1,\"p\",[[0,[],0,\"You can run k8s on-premises, or in AWS or Google Cloud. However, I think Azure Container Services makes spinning up an k8s cluster really straightforward. However, the pipeline I walk through in this post is cloud-host agnostic - it will work against any k8s cluster. We'll also set up a private Container Registry in Azure, though once again, you can use any container registry you choose.\"]]],[1,\"p\",[[0,[],0,\"To spin up a k8s cluster you can use the portal, but the \"],[0,[9],1,\"Azure CLI\"],[0,[],0,\" makes it a snap and you get to save the keys you'll need to connect, so I'll use that mechanism. I'll also use Bash for Windows with kubectl, but any platform running kubectl and the Azure CLI will do.\"]]],[1,\"p\",[[0,[],0,\"Here are the commands:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 2-4: create some variables\"]],[[0,[],0,\"Line 6: create a folder for the ssh-keys and kubeconfig\"]],[[0,[],0,\"Line 9: login to Azure (this prompts you to open a browser with the device login - if you don't have an Azure subscription create a free one now!)\"]],[[0,[],0,\"Line 10: create a resource group to house all the resources we're going to create\"]],[[0,[],0,\"Line 13: create a k8s cluster using the resource group we just created and the name we pass in; generate ssh-keys and place them in the specified folder; we want 2 agents (nodes) with the specified VM size\"]],[[0,[],0,\"Line 16: create an Azure Container registry in the same resource group with admin access enabled\"]],[[0,[],0,\"Line 19: get the credentials necessary to connect to the cluster using kubectl; use the supplied ssh-key and save the creds to the specified kubeconfig file\"]],[[0,[],0,\"Line 20: tell kubectl to use this config rather than the default config (which may have other k8s clusters or minikube config)\"]],[[0,[],0,\"Line 23: test that we can connect to the cluster\"]],[[0,[],0,\"Lines 24-27: we are indeed connecting successfully!\"]]]],[1,\"p\",[[0,[],0,\"If you open a browser and navigate to the Azure portal and then open your resource group, you'll see how much stuff got created by the few preceding simple commands:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Don't worry - you'll not need to manage these resources yourself. Azure and the k8s cluster manage them for you!\"]]],[1,\"h2\",[[0,[],0,\"Namespaces\"]]],[1,\"p\",[[0,[],0,\"Before we actually create the build and release for our container apps, let's consider the promotion model. Typically there's Dev->UAT->Prod or something similar. In the case of k8s, minikube is the local dev environment - and that's great since this is a full k8s cluster on your laptop - so you get to run your code locally including using k8s \\\"meta-constructs\\\" such as configMaps. So what about UAT and Prod? You could spin up separate clusters, but that could end up being expensive. You can also share the prod cluster resources by leveraging \"],[0,[8],1,\"namespaces\"],[0,[],0,\". Namespaces in k8s can be security boundaries, but they can also be isolation boundaries. I can deploy new versions of my app to a dev namespace - and even though that namespace shares the resources of the prod namespace, it's completely invisible, including its own IPs etc. Of course I shouldn't load test in this configuration since loading the dev namespace is going to potentially steal resources from prod apps. This is conceptually similar to deployment slots in Azure App Services - they can be used to test apps lightly before promoting to prod.\"]]],[1,\"p\",[[0,[],0,\"When you spin up a k8s cluster, besides kube-system and kube-public namespaces (which house the k8s pods) there is a \\\"default\\\" namespace. If you don't specify otherwise, any services, deploymens or pods you create will go to this namespace. However, let's create two additional namespaces: dev and prod. Here's the yml:\"]]],[10,2],[1,\"p\",[[0,[],0,\"This file contains the definitions for both namespaces. Run the apply command to create the namespaces. Once completed, you can list all the namespaces in the cluster:\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Configuring the Container Registry Secret\"]]],[1,\"p\",[[0,[],0,\"One more piece of setup before we get to the code: when the k8s cluster is pulling images to run, we're going to want it to pull from the Container Registry we just created. Access to this registry is secured since this is a private registry. So we need to configure a registry secret that we can just reference in our deployment yml files. Here are the commands:\"]]],[10,4],[1,\"p\",[[0,[],0,\"The first command uses az to get the keys for the admin user (the admin username is the same as the name of the Container registry: so I created cdk8s.azurecr.io and so the admin username is cdk8s). Pass in one of the keys (it doesn't really matter which one) as the password. The email address is not used, so this can be anything. We now have a registry secret called \\\"regsecret\\\" that we can refer to when deploying to the k8s cluster. K8s will use this secret to authenticate to the registry.\"]]],[1,\"h2\",[[0,[],0,\"Configure VSTS Endpoints\"]]],[1,\"p\",[[0,[],0,\"We now have the k8s cluster and container registry configured. Let's add these endpoints to VSTS so that we can push containers to the registry during a build and perform commands against the k8s cluster during a release. The endpoints allow us to abstract away authentication so that we don't need to store credentials in our release definitions directly. You can also restrict who can view/consume the endpoints using endpoint roles.\"]]],[1,\"p\",[[0,[],0,\"Open VSTS and navigate to a Team Project (or just create a new one). Go to the team project and click the gear icon to navigate to the settings hub for that Team Project. Then click Services. Click \\\"+ New Services\\\" and create a new Docker Registry endpoint. Use the same credentials you used to create the registry secret in k8s using kubectl:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Next create a k8s endpoint. For the url, it will be https://$ClusterName.$location.cloudapp.azure.com (where clustername and location are the variables we used earlier to create the cluster). You'll need to copy the entire contents of the ~/cdk8s/kubeconfig file (or whatever you called it) that was output when you ran the az acs kubernetes get-credential command into the credentials textbox:\"]]],[10,6],[1,\"p\",[[0,[],0,\"We now have two endpoints that we can use in the build/release definitions:\"]]],[10,7],[1,\"h2\",[[0,[],0,\"The Build\"]]],[1,\"p\",[[0,[],0,\"We can now create a build that compiles/tests our code, creates docker images and pushes the images to the Container Registry, tagging them appropriately. Click on Build & Release and then click on Builds to open the build hub. Create a new build definition. Select the ASP.NET Core template and click apply. Here are the settings we'll need:\"]]],[3,\"ul\",[[[0,[],0,\"Tasks->Process: Set the name to something like k8s-demo-CI and select the \\\"Hosted Linux Preview\\\" queue\"]],[[0,[],0,\"Options: change the build number format to \\\"1.0.0$(rev:.r)\\\" so that the builds have a 1.0.0.x format\"]],[[0,[],0,\"Tasks->Get Sources: Select the Github repo, authorizing via OAuth or PAT. Select the AzureAureliaDemo and set the default branch to docker. You may have to fork the repo (or just import it into VSTS) if you're following along.\"]],[[0,[],0,\"Tasks->DotNet Restore - leave as-is\"]],[[0,[],0,\"Tasks->DotNet Build - add \\\"--version-suffix $(Build.BuildNumber)\\\" to the build arguments to match the assembly version to the build number\"]],[[0,[],0,\"Tasks->DotNet Test - disable this task since there are no DotNet tests in this solution (you can of course re-enable this task when you have tests)\"]],[[0,[],0,\"Tasks->Add an \\\"npm\\\" task. Set the working folder to \\\"frontend\\\" and make sure the command is \\\"install\\\"\"]],[[0,[],0,\"Tasks->Add a \\\"Command line\\\" task. Set the tool to \\\"node\\\", the arguments to \\\"node_modules/aurelia-cli/bin/aurelia-cli.js test\\\" and the working folder to \\\"frontend\\\". This will run Aurelia tests.\"]],[[0,[],0,\"Tasks->Add a \\\"Publish test results\\\" task. Set \\\"Test Results files\\\" to \\\"test*.xml\\\" and \\\"Search Folder\\\" to \\\"$(Build.SourcesDirectory)/frontend/testresults\\\". This publishes the Aurelia test results.\"]],[[0,[],0,\"Tasks->Add a \\\"Publish code coverage\\\" task. Set \\\"Coverage Tool\\\" to \\\"Cobertura\\\", \\\"Summary File\\\" to \\\"$(Build.SourcesDirectory)/frontend/reports/coverage/cobertura.xml\\\" and \\\"Report Directory\\\" to \\\"$(Build.SourcesDirectory)/frontend/reports/coverage/html\\\". This publishes the Aurelia test coverage results.\"]],[[0,[],0,\"Tasks->Add a \\\"Command line\\\" task. Set the tool to \\\"node\\\", the arguments to \\\"node_modules/aurelia-cli/bin/aurelia-cli.js build --env prod\\\" and the working folder to \\\"frontend\\\". This transpiles, processes and packs the Aurelia SPA app.\"]],[[0,[],0,\"Tasks->DotNet Publish. Change the Arguments to \\\"-c $(BuildConfiguration) -o publish\\\" and uncheck \\\"Zip Published Projects\\\"\"]],[[0,[],0,\"Tasks->Add a \\\"Docker Compose\\\" task. Set the \\\"Container Registry Type\\\" to \\\"Azure Container Registry\\\" and set your Azure subscription and container registry to the registry we created an endpoint for earlier. Set \\\"Additional Docker Compose Files\\\" to \\\"docker-compose.vsts.yml\\\", the action to \\\"Build service images\\\" and \\\"Additional Image Tags\\\" to \\\"$(Build.BuildNumber)\\\" so that the build number is used as the tag for the images.\"]],[[0,[],0,\"Clone the \\\"Docker Compose\\\" task. Rename it to \\\"Push service images\\\" and change the action to \\\"Push service images\\\". Check the \\\"Include Latest Tag\\\" checkbox.\"]],[[0,[],0,\"Tasks->Publish Artifact. Set both \\\"Path to Publish\\\" and \\\"Artifact Name\\\" to k8s. This publishes the k8s yml files so that they are available in the release.\"]]]],[1,\"p\",[[0,[],0,\"The final list of tasks looks something like this:\"]]],[10,8],[1,\"p\",[[0,[],0,\"You can now Save and Queue the build. When the build is complete, you'll see the test/coverage information in the summary.\"]]],[10,9],[1,\"p\",[[0,[],0,\"You can also take a look at your container registry to see the newly pushed service images, tagged with the build number.\"]]],[10,10],[1,\"h2\",[[0,[],0,\"The Release\"]]],[1,\"p\",[[0,[],0,\"We can now configure a release that will create/update the services we need. For that we're going to need to manage configuration. Now we could just hard-code the configuration, but that could mean sensitive data (like passwords) would end up in source control. I prefer to tokenize any configuration and have Release Management keep the sensitive data outside of source control. VSTS Release Management allows you to create secrets for individual environments or releases or you can create them in reusable Variable Groups. You can also now easily \"],[0,[10],1,\"integrate with Azure Key Vault\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"To replace the tokens with environment-specific values, we're going to need a task that can do token substitution. Fortunately, I've got a (cross-platform) ReplaceTokens task in \"],[0,[11],1,\"Colin's ALM Corner Build & Release Tasks\"],[0,[],0,\" extension on the VSTS Marketplace. Click on the link to navigate to the page and click install to install the extension onto your account.\"]]],[1,\"p\",[[0,[],0,\"From the build summary page, scroll down on the right hand side to the \\\"Deployments\\\" section and click the \\\"Create release\\\" link. You can also click on Releases and create a new definition from there. Start from an Empty template and select your team project and the build that you just completed as the source build. Check the \\\"Continuous Deployment\\\" checkbox to automatically trigger a release for every good build.\"]]],[1,\"p\",[[0,[],0,\"Rename the definition to \\\"k8s\\\" or something descriptive. On the \\\"General\\\" tab change the release number format to \\\"$(Build.BuildNumber)-$(rev:r)\\\" so that you can easily see the build number in the name of the release. Back on Environments, rename Environment 1 to \\\"dev\\\". Click on the \\\"Run on Agent\\\" link and make sure the Deployment queue is \\\"Hosted Linux Preview\\\". Add the following tasks:\"]]],[3,\"ul\",[[[0,[],0,\"Replace Tokens\"]],[[0,[],0,\"Source Path: browse to the k8s folder\"]],[[0,[],0,\"Target File Pattern: \\\"*-release.yml\\\". This performs token replacement on any yml file with a name that ends in \\\"-release.\\\" There's 3: back- and frontend service/deployment files and the frontend config file. This task finds the tokens in the file (with pre- and postfix __) and looks for variables with the same name. Each variable is replaced with its corresponding value. We'll create the variables shortly.\"]],[[0,[],0,\"Kubernetes Task 1 (apply frontend config)\"]],[[0,[],0,\"Set the k8s connection to the endpoint you created earlier. Also set the connection details for the Azure Container Registry. This applies to all the Kubernetes tasks. Set the Command to \\\"apply\\\", check the \\\"Use Configuration Files\\\" option and set the file to the k8s/app-demo-frontend-config-release.yml file using the file picker. Add \\\"--namespace $(namespace)\\\" to the arguments textbox.\"]]]],[10,11],[3,\"ul\",[[[0,[],0,\"Kubernetes Task 2 (apply backend service/deployment definition)\"]],[[0,[],0,\"Set the same connection details for the k8s service and Azure Container Registry. This time, set \\\"Secret Name\\\" to \\\"regsecret\\\" (this is the name of the secret we created when setting up the k8s cluster, and is also the name we refer to for the imagePullSecret in the Deployment definitions). Check the \\\"Force update secret\\\" setting. This ensures that the secret value in k8s matches the key from Azure. You could also skip this option since we created the key manually.\"]],[[0,[],0,\"Set the Command to \\\"apply\\\", check the \\\"Use Configuration Files\\\" option and set the file to the k8s/app-demo-backend-release.yml file using the file picker. Add \\\"--namespace $(namespace)\\\" to the arguments textbox.\"]]]],[10,12],[3,\"ul\",[[[0,[],0,\"Kubernetes Task 3 (apply frontend service/deployment definition)\"]],[[0,[],0,\"This is the same as the previous task except that the filename is k8s/app-demo-frontend-release.yml.\"]],[[0,[],0,\"Kubernetes Task 4 (update backend image)\"]],[[0,[],0,\"Set the same connection details for the k8s service and Azure Container Registry. No secret required here. Set the Command to \\\"set\\\" and specify Arguments as \\\"image deployment/demo-backend-deployment backend=$(ContainerRegistry)/api:$(Build.BuildNumber) --record --namespace=$(namespace)\\\".\"]],[[0,[],0,\"This updates the version (tag) of the container image to use. K8s will do a rolling update that brings new containers online and takes the old containers offline in such a manner that the service is still up throughout the bleed over.\"]]]],[10,13],[3,\"ul\",[[[0,[],0,\"Kubernetes Task 5 (update the frontend image)\"]],[[0,[],0,\"Same as the previous task except the Arguments are \\\"image deployment/demo-frontend-deployment frontend=$(ContainerRegistry)/frontend:$(Build.BuildNumber) --record --namespace=$(namespace)\\\"\"]],[[0,[],0,\"Click on the \\\"…\\\" button on the \\\"dev\\\" card and click Configure Variables. Set the following values:\"]],[[0,[],0,\"BackendServicePort: 30081\"]],[[0,[],0,\"FrontendServicePort: 30080\"]],[[0,[],0,\"ContainerRegistry: <your container reg>.azurecr.io\"]],[[0,[],0,\"namespace: $(Release.EnvironmentName)\"]],[[0,[],0,\"AspNetCoreEnvironment: development\"]],[[0,[],0,\"baseUri: http://$(BackendServiceIP)/api\"]],[[0,[],0,\"BackendServiceIP: 10.0.0.1\"]]]],[10,14],[1,\"p\",[[0,[],0,\"This sets environment-specific values for all the variables in the yml files. The Replace Tokens task takes care of injecting into the files for us. Let's take a quick look at one of the tokenized files (tokenized lines are highlighted):\"]]],[10,15],[1,\"p\",[[0,[],0,\"A note on the value for BackendServiceIP: we use 10.0.0.1 as a temporary placeholder, since Azure will create an IP for this service when k8s spins up the backend service (you'll see a public IP address in the resource group in the Azure portal). We will have to run this once to create the services and then update this to the real IP address so that the frontend service works correctly. We also use $(Release.EnvironmentName) as the value for namespace - so \\\"dev\\\" (and later \\\"prod\\\") need to match the namespaces we created, including casing.\"]]],[1,\"p\",[[0,[],0,\"If the service/deployment and config don't change, then the first 3 k8s tasks are essentially no-ops. Only the \\\"set\\\" commands are actually going to do anything. But this is great - since the service/deployment and config files can be applied idempotently! They change when they have to and don't mess anything up when they don't change - perfect for repeatable releases!\"]]],[1,\"p\",[[0,[],0,\"Save the definition. Click \\\"+ Release\\\" to create a new release. Click on the release number (it will be something like 1.0.0.1-1) to open the release. Click on logs to see the logs.\"]]],[10,16],[1,\"p\",[[0,[],0,\"Once the release has completed, you can see the deployment in the Kubernetes dashboard. To open the dashboard, execute the following command:\"]]],[10,17],[1,\"p\",[[0,[],0,\"The last argument is the path to the SSH key file that got generated when we created the cluster - adjust your path accordingly. You can now open a browser to \"],[0,[12],1,\"http://localhost:8001/ui\"],[0,[],0,\". Change the namespace dropdown to \\\"dev\\\" and click on Deployments. You should see 2 successful deployments - each showing 2 healthy pods. You can also see the images that are running in the deployments - note the build number as the tag!\"]]],[10,18],[1,\"p\",[[0,[],0,\"To see the services, click on Services.\"]]],[10,19],[1,\"p\",[[0,[],0,\"Now we have the IP address of the backend service, so we can update the variable in the release. We can then queue a new release - this time, the frontend configuration is updated with the correct IP address for the backend service (in this case 23.99.58.48). We can then browse to the frontend service IP address and see our service is now running!\"]]],[10,20],[1,\"h3\",[[0,[],0,\"Creating Prod\"]]],[1,\"p\",[[0,[],0,\"Now that we are sure that the dev environment is working, we can go back to the release and clone \\\"dev\\\" to \\\"prod\\\". Make sure you specify a post-approval on dev (or a pre-approval on prod) so that there's a checkpoint between the two environments.\"]]],[10,21],[1,\"p\",[[0,[],0,\"We can then just change the node ports, AspNetCoreEnvironment and BackendServiceIP variables and we're good to go! Of course we need to deploy once to the prod namespace before we see the k8s/Azure assigned IP address for the prod backend and then re-run the release to update the config.\"]]],[10,22],[1,\"p\",[[0,[],0,\"We could also remove the nodePort from the definitions altogether and let k8s decide on a node port - but if it's explicit then we know what port the service is going to run on within the cluster (not externally).\"]]],[1,\"p\",[[0,[],0,\"I did get irritated having to specify \\\"--namespace\\\" for each command - so irritated, in fact, that I've created a \"],[0,[13],1,\"Pull Request\"],[0,[],0,\" in the vsts-tasks Github repo to expose namespace as an optional UI element!\"]]],[1,\"h2\",[[0,[],0,\"End to End\"]]],[1,\"p\",[[0,[],0,\"Now that we have the dev and prod environments set up in a CI/CD pipeline, we can make a change to the code. I'll change the text below the version to \\\"K8s demo\\\" and commit the change. This triggers the build, creating a newer container image and running tests, which in turn triggers the release to dev. Now I can see the change in dev (which is on 1.0.0.3 or some newer version than 1.0.0.1), while prod is still on version 1.0.0.1.\"]]],[10,23],[1,\"p\",[[0,[],0,\"Approve dev in Release Management and prod kicks off - and a few seconds later prod is now also on 1.0.0.3.\"]]],[1,\"p\",[[0,[],0,\"I've exported the json definitions for both the build and the release into \"],[0,[14],1,\"this folder\"],[0,[],0,\" - you can attempt to import them (I'm not sure if that will work) but you can refer to them in any case.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"k8s shows great promise as a solid container orchestration mechanism. The yml infrastructure-as-code is great to work with and easy to version control. The deployment mechanism means you can have very minimal (if any) downtime when deploying and having access to configMaps and secrets makes the entire process secure. Using the Azure CLI you can create a k8s cluster and Azure Container registry with a couple simple commands. The VSTS integration through the k8s tasks makes setting up CI/CD relatively easy - all in all it's a great development workflow. Throw in minikube as I described in \"],[0,[0],1,\"Part 1\"],[0,[],0,\" of this series, which gives you a full k8s cluster for local development on your laptop, and you have a great dev/CI/CD workflow.\"]]],[1,\"p\",[[0,[],0,\"Of course a CI/CD pipeline doesn't battle test the actual applications in production! I would love to hear your experiences running k8s \"],[0,[8],1,\"in production\"],[0,[],0,\" - sound out in the comments if you have some experience of running apps in a k8s cluster in prod!\"]]],[1,\"p\",[[0,[],0,\"Happy k8sing!\"]]]]}","published_at":1499168046000,"status":"published","published_by":1},{"id":"1baf155e-0d4d-46cd-af32-a567ee2db446","title":"Docker DevOps","slug":"docker-devops","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/bf5182e4-d1ac-4e2b-9ae5-0bfcc00f631c.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/0fead412-36f9-4855-b0da-db677c5c1b58.png\\\" width=\\\"442\\\" height=\\\"280\\\"></a>\"}],[\"image\",{\"src\":\"http://wpup.codeimpossible.com/2009/06/works-on-my-machine-starburst.jpg\",\"alt\":\"\",\"title\":\"\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://docs.docker.com/\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/virtualization/windowscontainers/containers_welcome\"]],[\"a\",[\"href\",\"https://docs.docker.com/swarm/\"]],[\"a\",[\"href\",\"https://docs.docker.com/compose/\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/reference/glossary/#union-file-system\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/userguide/dockervolumes/\"]],[\"a\",[\"href\",\"http://mesos.apache.org/\"]],[\"a\",[\"href\",\"https://mesosphere.com/\"]],[\"a\",[\"href\",\"https://mesosphere.com/blog/2015/09/29/mesosphere-and-mesos-power-the-microsoft-azure-container-service/\"]],[\"a\",[\"href\",\"https://hub.docker.com/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Recently I attended the MVP Summit in Redmond. This is an annual event where MVPs from around the world converge on Microsoft to meet with each other and various product teams. It’s a highlight of the year (and one of the best benefits of being an MVP).\"]]],[10,0],[1,\"p\",[[0,[],0,\"The ALM MVPs have a tradition – we love to hear what other MVPs have been doing, so we have a pre-Summit session where we get 20 minute slots to share anything that’s interesting. This year I did a slide ware chat entitle \"],[0,[0],1,\"Docker DevOps\"],[0,[],0,\". It was just a collection of thoughts that I have on what Docker means for DevOps. I’d like to put some of those thoughts down in this post.\"]]],[1,\"h2\",[[0,[],0,\"Docker Means Containers\"]]],[1,\"p\",[[0,[1],1,\"Docker\"],[0,[],0,\" isn’t actually a technology per se. It’s just a containerization manager that happened to be at the right place at the right time – it’s made containers famous.\"]]],[1,\"p\",[[0,[],0,\"Container technology has been around for a fairly long time – most notably in the Linux kernel. Think of containers as the evolution of virtualization. When you have a physical server, it can be idle a lot of the time. So virtualization became popular, allowing us to create several virtual machines (VMs) on a single server. Apps running on the VM don’t know they’re on a VM – the VM has abstracted the physical hardware. Now most developers and IT Pros take virtualization for granted.\"]]],[1,\"p\",[[0,[],0,\"Containers take the abstraction deeper – they abstract the OS too. Containers are running instances of \"],[0,[0],1,\"images.\"],[0,[],0,\" The base layer of the image is typically a lightweight OS – only the bare essentials needed to run an app. Typically that means no UI or anything else that isn’t strictly needed. Images are also immutable. Under the hood, when you change an image, you actually create a differencing layer on top of the current layer. Containers also share layers – for example, if two containers have an ubuntu14.04 base layer, and then one has nginx and another has mySQL, there’s only one physical copy of the ubuntu14.04 image on disk. Shipping containers means just shipping the different top layer, which makes them easily portable.\"]]],[1,\"h3\",[[0,[],0,\"Windows Containers\"]]],[1,\"p\",[[0,[],0,\"So what about \"],[0,[2],1,\"Windows containers\"],[0,[],0,\"? Windows Server 2016 TP 4 (the latest release at the time of this article) has support for Windows containers – the first OS from Microsoft to support containerization. There are two flavors of Windows container – Windows Server containers and Hyper-V containers. Windows Server container processes are visible on the host, while Hyper-V containers are completely “black box” as far as the host is concerned – that makes the HyperV containers “more secure” than Windows Server containers. You can switch the mode at any time.\"]]],[1,\"p\",[[0,[],0,\"Windows container technology is still in its infancy, so there are a few rough edges, but it does show that Microsoft is investing in container technology. Another glaring sign is the fact that you can already create Docker hosts in Azure (both for Windows and Linux containers). Microsoft is also actively working on open-source Docker.\"]]],[1,\"h2\",[[0,[],0,\"What Containers Mean For You\"]]],[1,\"p\",[[0,[],0,\"So what does it all mean for you? Here’s the rub – just like you’ve probably not installed a \"],[0,[0],1,\"physical\"],[0,[],0,\" server for some years because of virtualization, I predict that pretty soon you won’t even install and manage VMs anymore. You’ll have a “cloud of hosts” somewhere (you won’t care where) and have the ability to spin up containers to your heart’s content. In short, it’s the way of the future.\"]]],[1,\"p\",[[0,[],0,\"So here are some things you need to be thinking about if you want to ride the wave of the future:\"]]],[3,\"ul\",[[[0,[],0,\"Microservices\"]],[[0,[],0,\"Infrastructure as Code\"]],[[0,[],0,\"Immutable machines\"]],[[0,[],0,\"Orchestration\"]],[[0,[],0,\"Docker Repositories\"]],[[0,[],0,\"It works on my machine\"]]]],[1,\"h3\",[[0,[],0,\"Microservices\"]]],[1,\"p\",[[0,[],0,\"The most important architectural change that containers bring is \"],[0,[0],1,\"microservices\"],[0,[],0,\". In order to use containers effectively, you have to (re-)architect your applications into small, loosely coupled services (each deployed into its own container). This makes each individual service simpler, but moves quite a bit of complexity into managing the services. Coordinating all these microservices is a challenge. However, I believe that the complexity at the management level is – well, more manageable. If done correctly, microservices can be deployed without much (or any) impact to other services, so you can isolate issues, deploy smaller units more frequently and gain scalability in the parts of the overall application that require it, as and when they require it (this is the job of the orchestration engine – something I’ll talk to later). This is much better than having to deploy an entire monolithic application every time.\"]]],[1,\"p\",[[0,[],0,\"So what about networking between the containers? Turns out that Docker is pretty good at managing how containers talk to each other (via \"],[0,[3],1,\"Docker Swarm\"],[0,[],0,\" and \"],[0,[4],1,\"Docker Compose\"],[0,[],0,\"). Each container must define which ports it exposes (if any). You can also link containers, so that they can communicate with each other. Furthermore, you have to explicitly define a “mapping” between the container ports and the host ports in order for the container to be exposed outside its host machine. So you have tight control over the surface area of each container (or group of containers). But it’s another thing to manage.\"]]],[1,\"h3\",[[0,[],0,\"Infrastructure as Code\"]]],[1,\"p\",[[0,[],0,\"When you create a Docker image, you specify it in a Dockerfile. The Dockerfile contains instructions in text that tell the Docker engine how to build up an image. The starting layer typically the (minimal) OS. Then follow instructions to install dependencies that the top app layers will need. Finally, the app itself is added.\"]]],[1,\"p\",[[0,[],0,\"Specifying your containers in this manner forces you to express your \"],[0,[0],1,\"infrastructure\"],[0,[],0,\" as code. This is a great practice, whether you’re doing it for Docker or not. After you’ve described your infrastructure as code, you can automate building the infrastructure – so Infrastructure as Code is a building block for automation. Automation is good – it allows rapid and reliable deployment, which means better quality, faster. It does mean that you’re going to have to embrace DevOps – and have both developers and operations (or better yet your DevOps team) work together to define and manage the infrastructure code. In this brave new world, no-one is installing OSs or anything else using GUIs. Script it baby, script it!\"]]],[1,\"h3\",[[0,[],0,\"Immutable Machines\"]]],[1,\"p\",[[0,[],0,\"Containers are essentially \"],[0,[0],1,\"immutable\"],[0,[],0,\". Under the hood, if you change a container, you actually freeze the current top layer (so that it’s immutable) and add a new “top layer” with the changes (this is enabled by \"],[0,[5],1,\"Union File Systems\"],[0,[],0,\"). In fact, if you do it correctly, you should never have a reason to change a container once it’s out of development. If you really do need to change something (or say, deploy some new code for your app within the container), you actually throw away the existing container and create a new one. Don’t worry though – Docker is super efficient – which means that you won’t need to rebuild the entire image from scratch – the interim layers are stored in the Docker engine, so Docker is smart enough to just use the common layers again and just create a new differencing layer for the new image.\"]]],[1,\"p\",[[0,[],0,\"Be that as it may, there is a shift in thinking about containers in production. They should essentially be viewed as immutable. Which means that your containers have to be \"],[0,[0],1,\"stateless\"],[0,[],0,\". That obviously won’t work for databases or any other persistent data. So Docker has the concept of \"],[0,[0,6],2,\"data volumes\"],[0,[],0,\", which are special directories that can be accessed (and shared) by containers but that are outside the containers themselves. This means you have to really think about where the data are located for containers and where they live \"],[0,[0],1,\"on the host\"],[0,[],0,\" (since they’re outside the containers). Migrating or upgrading data is a bit tricky with so many moving parts, so it’s something to think about carefully.\"]]],[1,\"h3\",[[0,[],0,\"Orchestration\"]]],[1,\"p\",[[0,[],0,\"So let’s imagine that you’ve architected an application composed of several microservices that can be deployed independently. You can spin them all up on a single machine and then – hang on, a single machine? Won’t that hit resource limitations pretty quickly? Yes it will. And what about the promise of scale – that if a container comes under pressure I can just spin another instance (container) up and voila – I’ve scaled out? Won’t that depend on how much host resources are available? Right again.\"]]],[1,\"p\",[[0,[],0,\"This is where tools like Docker Swarm come into play. Docker Swarm allows you to create and access a pool of Docker hosts. Ok, that’s great for deploying apps. But what about monitoring the resources available? And wouldn’t it be nice if the system could auto-scale? Enter \"],[0,[7],1,\"Apache Mesos\"],[0,[],0,\" and \"],[0,[8],1,\"Mesosphere\"],[0,[],0,\" (there are other products in this space too). Think of Mesos as a \"],[0,[0],1,\"distributed kernel\"],[0,[],0,\". It aggregates a bunch of machines – be they physical, virtual or cloud – into what appears to be a single machine that you can program against. Mesosphere is then a layer on top of Mesos that further abstracts, allowing much easier consumption and use of the Datacenter OS (dcos), which enables highly available, highly automated systems. Mesos uses containers natively, so Docker works in Mesos and Mesosphere. If you’re going to build scalable apps, then you are going to need an orchestration engine like Mesosphere. And it \"],[0,[9],1,\"runs in Azure\"],[0,[],0,\" too!\"]]],[1,\"h3\",[[0,[],0,\"Docker Repositories\"]]],[1,\"p\",[[0,[],0,\"Docker enables you to define a container (or image) using a Dockerfile. This file can be shared via some code repository. Then developers can code against that container (by building it locally) and when it’s ready for production, Ops can pull the file down and build the container. Sounds like a great way to share and automate! Docker repositories allow you to share Dockerfiles in exactly this manner. There are public repos, like \"],[0,[10],1,\"DockerHub\"],[0,[],0,\", and you can of course create (or subscribe) to private repos. This means that you get to share base images from official partners (for example, if you need nginx, no need to build it yourself – just pull down the official image from DockerHub that the nginx guys themselves have built). It also means that you have a great mechanism for moving code from dev to QA to Production. Just share the images in a public (or private) repo, and if a tester wants to test they can just spin up a container or two for themselves. The containers run exactly the same wherever they are run, so it could be the developer’s laptop, in a Dev/Test lab or in Production. And since only the delta’s are actually moved around (common images are shared) it’s quick and efficient to share code in this manner.\"]]],[1,\"h3\",[[0,[],0,\"It Works on My Machine\"]]],[10,1],[1,\"p\",[[0,[],0,\"“It works on my machine!” The classic exclamation heard by developer’s world over every time a bug is filed. And we all laugh since we know that between your dev environment and Production lie a whole slew of differences. Except that now, since the containers run the same wherever they are run, if it works in the developer’s container, it works in the Prod container.\"]]],[1,\"p\",[[0,[],0,\"Of course there are ways the containers may differ – for example, most real-world containers will have environment variables that have different values in different environments. But containers actually allow “It works on my machine” to become a viable statement once more.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Containers are the way of the future. You want to make sure that you’re getting on top of containers early (as in now) so that you don’t get left behind. Start re-architecting your application into microservices, and start investigating hosting options, starting with Docker and Docker Compose and moving towards dcos like Mesosphere. And be proud, once more, to say, “It works on my machine!”\"]]],[1,\"p\",[[0,[],0,\"Happy containering!\"]]]]}","published_at":1448294526000,"status":"published","published_by":1},{"id":"f24edcc9-4b00-47a6-b097-5afbfb7e0f63","title":"Don’t Just Fix It, Red-Green Refactor It!","slug":"dont-just-fix-it-red-green-refactor-it","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"protected string _name;\\npublic string Name\\n{\\n    get \\n    {\\n        if (string.IsNullOrEmpty(_name))\\n        {\\n            SplitKey();\\n        }\\n        return _name;\\n    }\\n    set \\n    {\\n        _name = value;\\n        CombineKey();\\n    }\\n}\\n\\nprotected ComponentType _type;\\npublic string Type\\n{\\n    get\\n    {\\n        return _type.ToString();\\n    }\\n    set\\n    {\\n        _type = ParseTypeEnum(value);\\n        CombineKey();\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"[TestMethod]\\npublic void SetsPropertiesCorrectlyFromKeys()\\n{\\n    var component = new Component()\\n    {\\n        Key = \\\"Logger_Log1\\\"\\n    };\\n\\n    Assert.AreEqual(\\\"Logger\\\", component.Type);\\n    Assert.AreEqual(\\\"Log1\\\", component.Name);\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"[TestMethod]\\npublic void SetsPropertiesCorrectlyFromKeys2()\\n{\\n    var component = new Component()\\n    {\\n        Key = \\\"Service_S0\\\"\\n    };\\n\\n    Assert.AreEqual(\\\"Service\\\", component.Type);\\n    Assert.AreEqual(\\\"S0\\\", component.Name);\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public enum ComponentType\\n{\\n    Unknown,\\n    Logger,\\n    Service\\n}\\n\\n...\\n\\nprotected ComponentType _type;\\npublic string Type\\n{\\n    get\\n    {\\n        if (_type == ComponentType.Unknown)\\n        {\\n            SplitKey();\\n        }\\n        return _type.ToString();\\n    }\\n    set\\n    {\\n        _type = ParseTypeEnum(value);\\n        CombineKey();\\n    }\\n}\\n\",\"language\":\"csharp;\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.asp.net/mvc\"]],[\"a\",[\"href\",\"http://nancyfx.org/\"]],[\"a\",[\"href\",\"http://www.visualstudio.com/en-us/get-started/share-your-code-in-git-vs.aspx\"]],[\"a\",[\"href\",\"http://myget.org\"]],[\"a\",[\"href\",\"https://www.nuget.org/\"]],[\"a\",[\"href\",\"https://octopusdeploy.com/\"]],[\"a\",[\"href\",\"http://www.hanselman.com/blog/OneOfMicrosoftsBestKeptSecretsPythonToolsForVisualStudioPTVS.aspx\"]],[\"a\",[\"href\",\"http://azure.microsoft.com\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dn481095.aspx\"]],[\"em\"],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-absolutely-need-to-unit-test\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’m back to doing some dev again – for a real-life, going-to-charge-for application! It’s great to be based from home again and to be on some very cutting edge dev.\"]]],[1,\"p\",[[0,[],0,\"I’m very comfortable with \"],[0,[0],1,\"ASP.NET MVC\"],[0,[],0,\", but this project is the first \"],[0,[1],1,\"Nancy\"],[0,[],0,\" project I’ve worked on. We’re also using \"],[0,[2],1,\"Git on VSO\"],[0,[],0,\" for source control and backlogs, \"],[0,[3],1,\"MyGet\"],[0,[],0,\" to host internal \"],[0,[4],1,\"NuGet\"],[0,[],0,\" packages, \"],[0,[5],1,\"Octopus deploy\"],[0,[],0,\" for deployment, \"],[0,[6],1,\"Python\"],[0,[],0,\" (with various libs, of course!) for number crunching and \"],[0,[7],1,\"Azure\"],[0,[],0,\" to host VMs and websites (which are monitored with \"],[0,[8],1,\"AppInsights\"],[0,[],0,\"). All in all it’s starting to shape up to a very cool application – details to follow as we approach go-live (play mysterious music here)…\"]]],[1,\"h2\",[[0,[],0,\"Ho Hum Dev\"]]],[1,\"p\",[[0,[],0,\"Ever get into a groove that’s almost too automatic? Ever been driving home and you arrive and think, “Wait a minute – how did I get here?”. You were thinking so intently on something else that you just drove “on automatic” without really paying attention to what you were doing.\"]]],[1,\"p\",[[0,[],0,\"Dev can sometimes get into this kind of groove. I was doing some coding a few days ago and almost missed a good quality improvement opportunity – fortunately, I was able to look up long enough to see a better way to do things, and hopefully save myself some pain down the line.\"]]],[1,\"p\",[[0,[],0,\"I was debugging some code, and something wasn’t working the way I expected. Here’s a code snippet showing two properties I was working with:\"]]],[10,0],[1,\"p\",[[0,[],0,\"See how the getter for the Type property doesn’t match the code for the getter for Name? Even though I have unit tests for this getter, the tests are all passing!\"]]],[1,\"p\",[[0,[],0,\"Now the simple thing to do would have been to simply add the missing call to SplitKey() and carry on – but I wanted to know why the tests weren’t failing. I knew there were issues with the code (I had hit them while debugging) so I decided to take a step back and try some good practices: namely red-green refactor.\"]]],[1,\"h2\",[[0,[],0,\"Working with Quality in Mind\"]]],[1,\"p\",[[0,[],0,\"When you’re coding you should be working with quality in mind – that’s why I love unit testing so much. If you’re doing dev without unit testing, you’re only setting yourself up for long hours of painful in-production debugging. Not fun. Build with quality up front – while it may \"],[0,[9],1,\"feel\"],[0,[],0,\" like it’s taking longer to deliver, \"],[0,[10],1,\"you’ll save time in the long run\"],[0,[],0,\" since you’ll be adding new features instead of debugging poor quality code.\"]]],[1,\"p\",[[0,[],0,\"Here’s what you *should* be doing when you come across “hanky” code:\"]]],[3,\"ol\",[[[0,[],0,\"Do some coding\"]],[[0,[],0,\"While running / debugging, find some bug\"]],[[0,[],0,\"BEFORE FIXING THE BUG, write a FAILING unit test that exposes the bug\"]],[[0,[],0,\"Refactor/fix till the test passes\"]]]],[1,\"p\",[[0,[],0,\"So I opened up the tests for this entity and found the issue: I was only testing one scenario. This highlights that while code coverage is important, it can give you a false sense of security!\"]]],[1,\"p\",[[0,[],0,\"Here’s the original test:\"]]],[10,1],[1,\"p\",[[0,[],0,\"ComponentType comes from an enumeration – and since Logger is the 1st value in the enum, it defaults to Logger if you don’t explicitly set the value. So while I had a test that was covering the entire method, it wasn’t testing all the combinations!\"]]],[1,\"p\",[[0,[],0,\"So I added a new test:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Now when I ran the tests, the 2nd test failed. Excellent! Now I’ve got a further test that will check for a bad piece of code.\"]]],[1,\"p\",[[0,[],0,\"To fix the bug, I had to add another enum value and of course, add in the missing SplitKey() call in the Type property getter:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now both tests are passing. Hooray!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I realize the red-green refactoring isn’t a new concept – but I wanted to show a real-life example of how you should be thinking about your dev and debugging. Even though the code itself had 100% code coverage, there were still bugs. But debugging with quality in mind means you can add tests that cover specific scenarios – and which will reduce the amount of buggy code going into production.\"]]],[1,\"p\",[[0,[],0,\"Happy dev’ing!\"]]]]}","published_at":1418331870000,"status":"published","published_by":1},{"id":"b15481b1-c8e3-4b34-8642-1dbe15fd5f7b","title":"DotNet Core, VS 2015, VSTS and Docker","slug":"dotnet-core-vs-2015-vsts-and-docker","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">DOCKER_OPTS=\\\"--tlsverify --tlscacert=/var/docker/ca.pem --tlscert=/var/docker/server-cert.pem --tlskey=/var/docker/server-key.pem -H=0.0.0.0:2376”</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">ExecStart</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">EnviromentFile</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">[Service]</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">EnvironmentFile=/etc/default/docker</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">ExecStart=/usr/bin/docker daemon $DOCKER_OPTS</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">sudo service restart docker</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Error response from daemon: client is newer than server (client API version: 1.24, server API version: 1.23)</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">.\\\\DockerTask.ps1 : The term '.\\\\DockerTask.ps1' is not recognized as the name of a cmdlet, function, script file, or operable program.</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Calibri\\\"> After lots of hacking, I finally found that there is a path issue somewhere. I opened up the Properties\\\\Docker.targets file and edited the &lt;DockerBuildCommand&gt;: I changed “.\\\\DockerTask.ps1” to the full path – c:\\\\projects\\\\docker\\\\TestApp\\\\src\\\\TestApp\\\\DockerTask.ps1. I did the same for the &lt;DockerCleanCommand&gt;. This won’t affect the build, but other developers who share this code will have to have the same path structure for this to work. Gah!</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">No machine name(s) specified and no “default” machines exist</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker-machine</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker-machine</font>\"}],[\"code\",{\"code\":\"if (![System.String]::IsNullOrWhiteSpace($Machine))\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker-machine</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/7a9f2047-bc76-4e56-80fc-b950129146fb.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b9037ccd-9949-4237-beda-39336a504480.png\\\" width=\\\"801\\\" height=\\\"197\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/ca2ef53b-d1e3-4f0f-96ca-89142aec60da.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/d3f7c0dd-ee31-4ff5-a8cb-04d4be906a74.png\\\" width=\\\"275\\\" height=\\\"129\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4fefa60a-529b-41ab-8aa7-0fbd31c0653c.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/c9a2fc1a-24ba-4693-807b-8ea518ee9493.png\\\" width=\\\"380\\\" height=\\\"228\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4549a874-8f7f-4252-90dc-65e39fb276a7.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/01a81113-2323-4fcb-84b8-20debbcc013b.png\\\" width=\\\"661\\\" height=\\\"368\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">The handle could not be duplicated during redirection of handle 1.</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Calibri\\\"> Turns out some over-eager developer had the following line in function Clean() in the DockerTask.ps1 file:</font>\"}],[\"code\",{\"code\":\"Invoke-Expression \\\"cmd /c $shellCommand `\\\"*&gt;&amp;1`\\\"\\\" | Out-Null\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">*&gt;&amp;1</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">2&gt;&amp;1</font>\"}],[\"code\",{\"code\":\"Invoke-Expression \\\"cmd /c $shellCommand `\\\"2&gt;&amp;1`\\\"\\\" | Out-Null\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/3f4f20cf-afdb-43d0-895d-8c573f15f77e.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/fe544ab7-0646-48eb-9642-590afa3459ef.png\\\" width=\\\"472\\\" height=\\\"351\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/3b91b44a-4f22-4594-ac36-96e8c8963d54.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4d3f9889-2874-4c15-aa0a-8de512c5ceaa.png\\\" width=\\\"483\\\" height=\\\"226\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4ae5f3d0-68a0-42fc-8ac4-a63dbc816412.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/2b404122-44da-4d58-838b-b59938581305.png\\\" width=\\\"426\\\" height=\\\"250\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/5690ee90-5e37-44b0-87fb-6248b332b210.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/52c53f1d-00dd-4f87-a719-c38ab50b65a1.png\\\" width=\\\"465\\\" height=\\\"165\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/3884730f-a3d2-44d7-a4fc-467febfb7b58.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/94c84c32-3cdf-4f95-bbc8-3f8a5464a9d2.png\\\" width=\\\"432\\\" height=\\\"209\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a778a8b0-b634-4140-88db-469fc9698c75.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/3486bd1a-2f63-4e47-8620-ab0f95d452e3.png\\\" width=\\\"480\\\" height=\\\"232\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c54f436a-20b1-4b8a-979f-aeeed9655aa4.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/5893c11a-ac07-4039-9250-03a8f9afa81a.png\\\" width=\\\"485\\\" height=\\\"174\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/ef6d06b7-8eb7-45f6-aa57-b6208b108a87.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/1750db0b-b7ce-440d-a034-09b9906b209a.png\\\" width=\\\"721\\\" height=\\\"401\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/02067b7f-4fdb-4c6e-a4c0-3a19ad4a5109.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/29c2096c-8c60-45cc-b1e1-bee3d3f7d1c6.png\\\" width=\\\"563\\\" height=\\\"151\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/13c12ad6-107b-4177-956e-df520a3ee6be.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/49e4d289-173f-4f9c-9f9c-9945812d8b72.png\\\" width=\\\"449\\\" height=\\\"290\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/6a397149-51c5-4786-96a4-72f8b5f44bef.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4de8a84d-045c-44f0-92ca-dbbcfb3d6804.png\\\" width=\\\"373\\\" height=\\\"261\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/927248b9-c84a-4dde-9dad-4e16b100c4d1.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/631f958a-f1ca-4355-86fb-8695dd8c558f.png\\\" width=\\\"561\\\" height=\\\"301\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.docker.com/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/docker-devops\"]],[\"a\",[\"href\",\"https://www.microsoft.com/net/core\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/0f5b2caa-ea00-41c8-b8a2-058c7da0b3e4\"]],[\"a\",[\"href\",\"https://docs.docker.com/docker-for-windows/\"]],[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/products/visual-studio-team-services-vs.aspx\"]],[\"em\"],[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/docs/release/examples/docker/aspnet-core10-docker\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/security/https/\"]],[\"a\",[\"href\",\"http://docker:5000\"]],[\"a\",[\"href\",\"http://aka.ms/dockertoolsforvsts\"]],[\"a\",[\"href\",\"https://hub.docker.com/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I unashamedly love \"],[0,[0],1,\"Docker\"],[0,[],0,\". Late last year I posted some thoughts I had on \"],[0,[1],1,\"Docker DevOps\"],[0,[],0,\". In this post I’m going to take a look at Docker DevOps using \"],[0,[2],1,\"DotNet Core 1.0.0\"],[0,[],0,\", \"],[0,[3],1,\"Docker Tools for Visual Studio\"],[0,[],0,\", \"],[0,[4],1,\"Docker for Windows\"],[0,[],0,\" and \"],[0,[5],1,\"VSTS\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Just before I continue – I’m getting tired of typing “DotNet Core 1.0.0” so for the rest of this post when I say “.NET Core” I mean “DotNet Core 1.0.0” (unless otherwise stated!).\"]]],[1,\"h3\",[[0,[],0,\"Highlights\"]]],[1,\"p\",[[0,[],0,\"For those of you that just want the highlights, here’s a quick summary:\"]]],[3,\"ul\",[[[0,[],0,\".NET Core does indeed run in a Docker container\"]],[[0,[],0,\"You can debug a .Net Core app running in a container from VS\"]],[[0,[],0,\"You can build and publish a DotNet Core app to a docker registry using VSTS Build\"]],[[0,[],0,\"You can run a Docker image from a registry using VSTS Release Management\"]],[[0,[],0,\"You can get frustrated by the lack of guidance and the amount of hacking required currently\"]]]],[1,\"p\",[[0,[],0,\"So what’s the point of this anyway? Well I wanted to know if I could create the following workflow:\"]]],[3,\"ul\",[[[0,[],0,\"Docker for Windows as a Docker host for local dev\"]],[[0,[],0,\"Visual Studio with the Docker Tools for VS for local debugging within a container\"]],[[0,[],0,\"VSTS for building (and publishing) a Docker image with an app\"]],[[0,[],0,\"VSTS for releasing (running) an image\"]]]],[1,\"p\",[[0,[],0,\"This is a pretty compelling workflow, and I was able to get it running \"],[0,[6],1,\"relatively\"],[0,[],0,\" easily. One of the biggest frustrations was the lack of documentation and the immaturity of some of the tooling.\"]]],[1,\"p\",[[0,[],0,\"Grab a cup of coffee (or tea or chai latte – or if it’s after hours, a good IPA) and I’ll take you on my journey!\"]]],[1,\"h2\",[[0,[],0,\"Docker Host in Azure\"]]],[1,\"p\",[[0,[],0,\"I started my journey from this post in article: \"],[0,[7],1,\"Deploy ASP.NET Core 1.0 apps to a Docker Container\"],[0,[],0,\" (aka the VSTS Docker article). While certainly helpful, there are some issue with this article. Firstly, it’s designed for ASP.NET Core 1.0.0-rc1-update1 and not the RTM release (1.0.0). This mainly had some implications for the Dockerfile, but wasn’t too big an issue. The bigger issue is that it’s a little ambiguous in places, and the build instructions were quite useless. We’ll get to that later.\"]]],[1,\"p\",[[0,[],0,\"After skimming the article, I decided to first stand up a Docker host in Azure and create the service connections that VSTS requires for performing Docker operations. Then, I figured, I’d be ready to start coding and I’ll have a host to deploy to.\"]]],[1,\"p\",[[0,[],0,\"Immediately I hit a strange limitation – the Docker image in Azure can only be created using “classic” and not “Resource Group” mode. I ended up deciding that wasn’t too big a deal, but it’s still frustrating that the official image isn’t on the latest tech within Azure.\"]]],[1,\"p\",[[0,[],0,\"The next challenge was getting Docker secured. I followed the VSTS Docker articles link to instructions on how to \"],[0,[8],1,\"protect the daemon socket\"],[0,[],0,\". I generated the ssh keys without too much fuss. However, I ran into issues ensuring that the Docker daemon starts with the keys! The article doesn’t tell you how to do that (it tells you how to start Docker manually), so I had to scratch around a bit. I found that you could set the daemon startup options by editing /etc/default/docker, so I opened it up and edited the DOCKER_OPTS to look like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Of course I copied the pem files to /var/docker. I then restarted the Docker service.\"]]],[1,\"p\",[[0,[],0,\"It didn’t work. After a long time of hacking, searching, sacrificing chickens and anything else I could think of to help, I discovered that the daemon ignores the /etc/default/docker file altogether! Perhaps it’s just the Azure VM and linux distro I’m on? Anyway, I had to edit the /etc/systemd/system/docker.service file. I changed the\"]]],[10,1],[1,\"p\",[[0,[],0,\"command and added an\"]]],[10,2],[1,\"p\",[[0,[],0,\"command in the\"]]],[10,3],[1,\"p\",[[0,[],0,\"section as follows:\"]]],[10,4],[10,5],[1,\"p\",[[0,[],0,\"Now when I restart the service (using\"]]],[10,6],[1,\"p\",[[0,[],0,\") the daemon starts correctly and is protected with the keys.\"]]],[1,\"p\",[[0,[],0,\"I could now run docker commands on the machine itself. However, I couldn’t run commands from my local machine (which is running Docker for Windows) because:\"]]],[10,7],[1,\"p\",[[0,[],0,\"I tried in vain to upgrade the Docker engine on the server, but could not for the life of me do it. The apt packages are on 1.23, and so eventually I gave up. I can run Docker commands by ssh-ing to the host machine if I really need to, so while irritating, it wasn’t a show-stopper.\"]]],[1,\"h2\",[[0,[],0,\".NET Core and Docker in VS\"]]],[1,\"p\",[[0,[],0,\"Now that I (finally) had a Docker host configured, I installed \"],[0,[3],1,\"Docker Tools for Visual Studio\"],[0,[],0,\" onto my Visual Studio 2015.3. I also installed the .NET Core 1.0 SDK. I then did a File->New->Project and created an ASP.NET project – you know, the boilerplate one. I then followed the instructions from the VSTS article and right-clicked the project and selected “Add->Docker support”. This created a DockerTask.ps1 file, a Dockerfile (and Dockerfile.debug) and some docker-compose yml files. Great! However, nothing worked straight away (argh!) so I had to start debugging the scripts.\"]]],[1,\"p\",[[0,[],0,\"I kept getting this error:\"]]],[10,8],[10,9],[1,\"p\",[[0,[],0,\"Now the command was being executed, but I was getting this error:\"]]],[10,10],[1,\"p\",[[0,[],0,\". I again opened the DockerTask.ps1 script. It’s checking for a machine name to invoke\"]]],[10,11],[1,\"p\",[[0,[],0,\"commands, but it’s only supposed to do this if the Docker machine name is specified. For Docker for Windows, you don’t have to use\"]]],[10,12],[1,\"p\",[[0,[],0,\", so the script makes provision for this by assuming Docker for Windows if the machine name is empty. At least, that’s what it’s supposed to do. For some reason, this line in the script is evaluating to true, even when $Machine was set to ‘’ (empty string):\"]]],[10,13],[1,\"p\",[[0,[],0,\"So I commented out the entire if block since I’ve got Docker for Windows and don’t need it to do and\"]]],[10,14],[1,\"p\",[[0,[],0,\"commands.\"]]],[1,\"p\",[[0,[],0,\"Now at least the build operation was working, and I could see VS creating an image in my local Docker for Windows:\"]]],[10,15],[1,\"p\",[[0,[],0,\"Next I tried debugging an app in the container. No dice. The issue seemed to be that the container couldn’t start on port 80. Looking at the Dockerfile and the DockerTask.ps1 files, I saw that the port is hard-coded to 80. So I changed the port to 5000 (making it a variable in the ps1 script and an ARG in my Dockerfile). Just remember that you have a Dockerfile.debug as well – and that the ports are hard-coded in the docker-compose.yml and docker-compose.debug.yml files too. The image name is also hardcoded all over the place to “username/appname”. I tried to change it, but ended up reverting back. This only affects local dev, so I don’t really care that much.\"]]],[1,\"p\",[[0,[],0,\"At this point I could get the container to run in Release, so I knew Docker was happy. However, I couldn’t debug. I was getting this error:\"]]],[10,16],[1,\"p\",[[0,[],0,\"Again a bit of googling led me to enable volume sharing in Docker for Windows (which is disabled by default). I clicked the moby in my task bar, opened the Docker settings and enabled volume sharing on my c drive:\"]]],[10,17],[1,\"p\",[[0,[],0,\"Debugging then actually worked – the script starts up a container (based on the image that gets created when you build) and attaches the remote debugger. Pretty sweet now that it’s working!\"]]],[10,18],[1,\"p\",[[0,[],0,\"In the above image you can see how I’m navigating to the About page (the url is \"],[0,[9],1,\"http://docker:5000\"],[0,[],0,\") and VS is spewing logging into the console showing the server (in the container) responding to the request).\"]]],[1,\"p\",[[0,[],0,\"One more issue – the clean command wasn’t working. I kept getting this error:\"]]],[10,19],[10,20],[10,21],[1,\"p\",[[0,[],0,\"I changed\"]]],[10,22],[1,\"p\",[[0,[],0,\"to\"]]],[10,23],[1,\"p\",[[0,[],0,\"like this:\"]]],[10,24],[1,\"p\",[[0,[],0,\"And now the clean was working great.\"]]],[1,\"p\",[[0,[],0,\"So I could get an ASP.NET Core 1.0 app working in VS in a container (with some work). Now for build and release automation in VSTS!\"]]],[1,\"h2\",[[0,[],0,\"Build and Release in VSTS\"]]],[1,\"p\",[[0,[],0,\"In order to execute Docker commands during build or release in VSTS, you need to install the \"],[0,[10],1,\"Docker extension from the marketplace\"],[0,[],0,\". Once you’ve installed it, you’ll get some new service endpoint types as well as a Docker task for builds and releases. You need two connections: one to a Docker registry (like \"],[0,[11],1,\"DockerHub\"],[0,[],0,\") and one for a Docker host. Images are built on the Docker host and published to the registry during a build. Then an image can be pulled from the registry and run on the host during a release. So I created a new private DockerHub repo (using the account that I created on DockerHub to get access to Docker for Windows). This info I used to create the Docker registry endpoint. Next I copied all the keys I created on my Azure Docker host and created a service endpoint for my Docker host. The trick here was the URL – initially I had “http://my-docker-host.cloudapp.net:2376” but that doesn’t work – it has to be “tcp://my-docker-host.cloudapp.net:2376”.\"]]],[1,\"p\",[[0,[],0,\"The cool thing about publishing to the repo is that you can have any number of hosts pull the image to run it!\"]]],[1,\"p\",[[0,[],0,\"Now I had the endpoints ready for build/deploy. I then added my solution to a Git repo and pushed to VSTS. Here’s the project structure:\"]]],[10,25],[1,\"p\",[[0,[],0,\"I then set up a build. In the VSTS Docker article, they suggest just two Docker tasks: the first with a “Build” action and the second with a “Push” action. However, I think this is meant to copy the source to the image and have the image do a dotnet restore – else how it work? However, I wanted the build to do the dotnet restore and publish (and test) and then just have the output bundled into the Docker image (as well as uploaded as a build drop). So I had to include two “run command” tasks and a publish build artifacts task. Here’s what my build definition ended up looking like:\"]]],[10,26],[1,\"p\",[[0,[],0,\"The first two commands are fairly easy – the trick is setting the working directory (to the folder containing the project) and the correct framework and runtimes for running inside a Docker container:\"]]],[10,27],[10,28],[1,\"p\",[[0,[],0,\"You’ll see that I output the published site to $(Build.ArtifactStagingDirectory)/site/app which is important for the later Docker commands.\"]]],[1,\"p\",[[0,[],0,\"I also created two variables (the values of which I got from the DockerTask.ps1 script) for this step:\"]]],[10,29],[1,\"p\",[[0,[],0,\"For building the Docker image, I specified the following arguments:\"]]],[10,30],[1,\"p\",[[0,[],0,\"I use the two service endpoints I created earlier and set the action to “Build an Image”. I then specify the path to the Dockerfile – initially I browsed to the location in the src folder, but I want the published app so I changed this to the path in the artifact staging directory (otherwise Docker complains that the Dockerfile isn’t within the context). I then specify a repo/tag name for the Image Name, and use the build number for the version. Finally, the context is the folder which contains the “app” folder – the Dockerfile needs to be in this location. This location is used as the root for any Dockerfile COPY commands.\"]]],[1,\"p\",[[0,[],0,\"Next step is publishing the image – I use the same endpoints, change the action to “Push an image” and specify the same repo/tag name:\"]]],[10,31],[1,\"p\",[[0,[],0,\"Now after running the build, I can see the image in my DockerHub repo (you can see how the build number and tag match):\"]]],[10,32],[1,\"p\",[[0,[],0,\"Now I could turn to the release. I have a single environment release with a single task:\"]]],[10,33],[1,\"p\",[[0,[],0,\"I named the ARG for the port in my Dockerfile APP_PORT, so I make sure it’s set to 5000 in the “Environment Variables” section. The example I followed had the HOST_PORT specified as well – I left that in, though I don’t know if it’s necessary. I linked the release to the build, so I can use the $(Build.BuildNumber) to specify which version (tag) of the container this release needs to pull.\"]]],[1,\"p\",[[0,[],0,\"Initially the release failed while attempting to download the drops. I wanted the drops to enable deployment of the build somewhere else (like Azure webapps or IIS), so this release doesn’t need them. I configured this environment to “Skip artifact download”:\"]]],[10,34],[1,\"p\",[[0,[],0,\"Lo and behold, the release worked after that! Unfortunately, I couldn’t browse to the site (connection refused). After a few moments of thinking, I realized that the Azure VM probably didn’t allow traffic on port 5000 – so I headed over to the portal and added an new endpoint (blegh – ARM network security groups are so much better):\"]]],[10,35],[1,\"p\",[[0,[],0,\"After that, I could browse to the ASP.NET Core 1.0 App that I developed in VS, debugged in Docker for Windows, source controlled in Git in VSTS, built and pushed in VSTS build and released in VSTS Release Management. Pretty sweet!\"]]],[10,36],[1,\"h3\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"The Docker workflow is compelling, and having it work (nearly) out the box for .NET Core is great. I think teams should consider investing into this workflow as soon as possible. I’ve said it before, and I’ll say it again – containers are the way of the future! Don’t get left behind – start learning Docker today and skill up for the next DevOps wave – especially if you’re embarking on .NET Core dev!\"]]],[1,\"p\",[[0,[],0,\"Happy Dockering!\"]]]]}","published_at":1469047950000,"status":"published","published_by":1},{"id":"87511efc-04f5-428b-a7aa-87d82b8f6a83","title":"Easy Config Management when Deploying Azure Web Apps from VSTS","slug":"easy-config-management-when-deploying-azure-web-apps-from-vsts","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">/p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true /p:PackageLocation=\\\"$(build.artifactStagingDirectory)\\\"</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/657d0222-729e-447c-a46a-122b0b24ab9e.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/15034800-5f57-4cdf-995d-b8439e785b68.png\\\" width=\\\"653\\\" height=\\\"218\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/1af3ab91-6d0f-4f71-9b47-d12a5165d104.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/1f640afd-820c-4eeb-b378-8a215c75b7b2.png\\\" width=\\\"631\\\" height=\\\"174\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/fbf32097-75bf-4130-95d1-db285fe0e931.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/25157cd4-966d-4254-8b99-ab2932c2e60b.png\\\" width=\\\"583\\\" height=\\\"397\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/b824632d-0cc6-406f-8545-f14d8fc5b737.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/831d1f07-feb3-4a1b-93e4-fade36c84874.png\\\" width=\\\"516\\\" height=\\\"303\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://bit.ly/cacbuildtasks\"]],[\"a\",[\"href\",\"http://bit.ly/e2ewadep\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-tasks/blob/master/Tasks/AzureRmWebAppDeployment/README.md\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"A good DevOps pipeline should utilize the principle of build once, deploy many times. In fact, I’d go so far as to say it’s essential for a good DevOps pipeline. That means that you have to have a way to manage your configuration in such a way that the package coming out of the build process is tokenized somehow so that when you release to different environments you can inject environment-specific values. Easier said that done – until now.\"]]],[1,\"h2\",[[0,[],0,\"Doing it the Right but Hard Way\"]]],[1,\"p\",[[0,[],0,\"Currently I’ve recommended that you use WebDeploy to do this. You define a publish profile to handle connection string and a parameters.xml file to handle any other config you want to tokenize during build. This produces a WebDeploy zip file along with a (now tokenized) SetParameters.xml file. Then you use the ReplaceTokens task from my \"],[0,[0],1,\"VSTS build/release task pack extension\"],[0,[],0,\" and inject the environment values into the SetParameters.xml file before invoking WebDeploy. This works, but it’s complicated. You can read a \"],[0,[1],1,\"full end to end walkthrough in this post\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Doing it the Easy Way\"]]],[1,\"p\",[[0,[],0,\"A recent release to the \"],[0,[2],1,\"Azure Web App deploy task\"],[0,[],0,\" in VSTS has just dramatically simplified the process! No need for parameters.xml or publish profiles at all.\"]]],[1,\"p\",[[0,[],0,\"Make sure your build is producing a WebDeploy zip file. You can read my end to end post on how to add the build arguments to the VS Build task – but now you don’t have to specify a publish profile. You also don’t need a parameters.xml in the solution. The resulting zip file will deploy (by default) with whatever values you have in the web.config at build time.\"]]],[1,\"p\",[[0,[],0,\"Here’s what I recommend:\"]]],[10,0],[1,\"p\",[[0,[],0,\"You can now just paste that into the build task:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can see the args (in the new build UI). This tells VS to create the WebDeploy zip and put it into the artifact staging directory. The Publish Artifact Drop task uploads anything that it’s the artifact staging directory (again, by default) – which at the time it runs should be the WebDeploy files.\"]]],[1,\"h2\",[[0,[],0,\"The Release\"]]],[1,\"p\",[[0,[],0,\"Here’s where the magic comes in: drop in an Azure App Service Deploy task. Set it’s version to 3.*(preview). You’ll see a new section called “File Transforms & Variable Substitution Options”. Just enable the “XML Override substitution”.\"]]],[10,2],[1,\"p\",[[0,[],0,\"That’s it! Except for defining the values we want to use for the said substitution. To do this, open the web.config and look at your app setting keys or connection string names. Create a variable that matches the name of the setting and enter a value. In my example, I have Azure B2C so I need a key called “ida:Tenant” so I just created a variable with that name and set the value for the DEV environment. I did the same for the other web.config variables:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now you can run your release!\"]]],[1,\"h2\",[[0,[],0,\"Checking the Web.Config Using Kudu\"]]],[1,\"p\",[[0,[],0,\"Once the release had completed, I wanted to check if the value had been set. I opened up the web app in the Azure portal, but there were no app settings defined there. I suppose that makes sense – the substitutions are made onto the web.config itself. So I just opened the Kudu console for the web app and cat’ed the web.config by typing “cat Web.config”. I could see that the environment values had been injected!\"]]],[10,4],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"It’s finally become easy to manage web configs using the VSTS Azure Web App Deploy task. No more publish profiles, parameters.xml files, SetParameters.xml files or token replacement. It’s refreshingly clean and simple. Good job VSTS team!\"]]],[1,\"p\",[[0,[],0,\"I did note that there is also the possibility of injecting environment-specific values into a json file – so if you have .NET CORE apps, you can easily inject values at deploy time.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1489816294000,"status":"published","published_by":1},{"id":"bc9e66d6-7521-4bd6-ba5b-a023b207414a","title":"Enable Custom IntelliTrace Web Events with a Right-Click","slug":"enable-custom-intellitrace-web-events-with-a-right-click","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-dvLBpOnAXds/UWemJ36qb5I/AAAAAAAAArM/uvPWeP3hZEQ/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-tP5bj2cs4-k/UWemLeFP79I/AAAAAAAAArU/7zJG4RSFRoM/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"311\\\" height=\\\"283\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Windows Registry Editor Version 5.00</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">[HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\VisualStudio\\\\11.0_Config\\\\TraceDebugger]<br>\\\"IntelliTraceEventsEnabled\\\"=dword:00000001</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-CENUNMpO-cE/UWemMlMqYXI/AAAAAAAAArc/WyaWXJoAdLk/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-DcRv07DyD5s/UWemNh-msOI/AAAAAAAAArk/cwTHy1vpNm0/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"443\\\" height=\\\"116\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-EiEhnhRSQhU/UWemPGNY9iI/AAAAAAAAArs/6f6hZsPLM-g/s1600-h/image%25255B12%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-aKHozhN87bw/UWemRKDkOYI/AAAAAAAAAr0/f2mO0X2b_Co/image_thumb%25255B6%25255D.png?imgmax=800\\\" width=\\\"365\\\" height=\\\"340\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-DhL5tFzpOrg/UWemR5PRDoI/AAAAAAAAAr8/Nd1IckmzTtk/s1600-h/image%25255B26%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-U-40H7ZwnqI/UWemTmhaNlI/AAAAAAAAAsE/7NMY9idTF4Q/image_thumb%25255B14%25255D.png?imgmax=800\\\" width=\\\"494\\\" height=\\\"204\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-DxpfSF2FsRc/UWemUrhzS5I/AAAAAAAAAsM/IP7f3a3NdK0/s1600-h/image%25255B30%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-MAt8013SHKc/UWemWK4iY-I/AAAAAAAAAsU/qpFiVWhICdU/image_thumb%25255B16%25255D.png?imgmax=800\\\" width=\\\"445\\\" height=\\\"248\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-DjF5GKXphvk/UWemXeipbvI/AAAAAAAAAsc/FpGjwQJ20uA/s1600-h/image%25255B18%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-C7wqpOBQ_ho/UWemY_oC4kI/AAAAAAAAAsk/T8OIUaZcO_Y/image_thumb%25255B10%25255D.png?imgmax=800\\\" width=\\\"317\\\" height=\\\"349\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-t-R1fkao-sY/UWemaIzjMNI/AAAAAAAAAss/3NR52N_L2GE/s1600-h/image%25255B22%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-LP0JJZS-vMI/UWemcJkbMmI/AAAAAAAAAs0/lHKQKKfeltk/image_thumb%25255B12%25255D.png?imgmax=800\\\" width=\\\"406\\\" height=\\\"326\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/intellitrace-tips-and-tricks-basics.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2013/05/13/guest-post-intellitrace-tips-and-tricks-the-basics-part-1-colin-dembovsky.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/intellitrace-tips-and-tricks.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2013/05/13/guest-post-intellitrace-tips-and-tricks-intellitrace-everywhere-part-2-colin-dembovsky.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/04/enable-custom-intellitrace-web-events.html\"]],[\"strong\"],[\"a\",[\"href\",\"http://www.teched.co.za/\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/bharry/archive/2012/08/13/announcing-git-integration-with-tfs.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/bharry/archive/2013/04/04/vs-tfs-2012-2-update-2-released-today.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/vstudio/dd264915.aspx\"]],[\"a\",[\"href\",\"http://continuouslyintegrating.blogspot.com/\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/vstudio/hh398365.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series Links:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1\"],[0,[],0,\": The Basics (this post) – also \"],[0,[1],1,\"guest posted\"],[0,[],0,\" on MSDevDiv SA Blog\"]],[[0,[2],1,\"Part 2\"],[0,[],0,\": IntelliTrace Everywhere – also \"],[0,[3],1,\"guest posted\"],[0,[],0,\" on MSDevDiv SA Blog\"]],[[0,[4],1,\"Part 3\"],[0,[],0,\": Enable Custom IntelliTrace Events with a Right-Click\"]]]],[1,\"p\",[[0,[5],1,\"Note: This is an unsupported feature!\"],[0,[],0,\" Use at your own risk – though to be honest I can’t see what the risk really is. Just know that this is not supported by Microsoft.\"]]],[1,\"h2\",[[0,[],0,\"Preamble (TL;DR – skip to next section for the Good Stuff)\"]]],[1,\"p\",[[0,[],0,\"I’m going to be presenting 3 deep dives at \"],[0,[6],1,\"TechEd Africa\"],[0,[],0,\" next week:\"]]],[3,\"ul\",[[[0,[],0,\"Version Control (\"],[0,[7],1,\"including Git\"],[0,[],0,\")\"]],[[0,[8],1,\"Agile Planning Tools\"],[0,[],0,\" and Customizations\"]],[[0,[9],1,\"IntelliTrace\"]]]],[1,\"p\",[[0,[],0,\"While I was preparing for my IntelliTrace session, I was watching \"],[0,[10],1,\"Larry Guger\"],[0,[],0,\" (IntelliTrace PM) do a presentation when \"],[0,[11],1,\"IntelliTrace-in-Production\"],[0,[],0,\" had just launched. Right at the end of his talk, he did something which boggled my mind – he right-clicked a method and right there in the context menu was “Insert IntelliTrace Event”. He did this for a couple of methods, and then exported these events to a folder. When he ran the IntelliTrace collector for IIS using PowerShell, the custom events were present in the log file.\"]]],[10,0],[1,\"p\",[[0,[],0,\"This was amazing because I know how tedious it is to create custom IntelliTrace events. I quickly fired up my VS just to see if I could do it – and couldn’t find the menu option.\"]]],[1,\"p\",[[0,[],0,\"I then mailed Larry and after a short email conversation and some scratching around the IntelliTrace dll’s with Reflector, I was able to figure out that if you add a registry entry, you “unlock” this feature.\"]]],[1,\"p\",[[0,[],0,\"This is a “partially complete” feature in VS – here is the big limitation:\"]]],[1,\"p\",[[0,[5],1,\"This only works with IntelliTrace collection for IIS applications via PowerShell\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"The Good Stuff\"]]],[1,\"p\",[[0,[],0,\"So I’ll cut to the chase: here’s the registry key:\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"Just copy and paste this into a file (intelli.reg or something) and double-click it. Restart VS.\"]]],[1,\"p\",[[0,[],0,\"Now open up a Web Application, find a method, right click, select “Insert IntelliTrace Event”. Repeat for a couple of other methods. You’ll see a glyph in the gutter indicating that you have an event for that method.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now go to Debug->IntelliTrace->Export IntelliTrace Events. Save this file to a folder somewhere.\"]]],[10,4],[1,\"p\",[[0,[],0,\"You’ll see that the file has an .iFragment extension – this is an IntelliTrace config fragment. If you open up the file, you’ll see it has created Category, Module and Diagnostic sections – these are the same sections you’d have to manually create if you wanted some custom IntelliTrace events.\"]]],[10,5],[1,\"p\",[[0,[5],1,\"Aside:\"],[0,[],0,\" You can see the “AutomaticDataQuery” element in the  tag. This allows you to see the in/out arguments of the method in the locals window when you debug the log file later… No messing around with argument positions and stuff…\"]]],[10,6],[1,\"h2\",[[0,[],0,\"Using iFragments\"]]],[1,\"p\",[[0,[],0,\"Now that you have a custom iFragment, you need to go to the server that you want to collect IntelliTrace events from. Go to the folder where you extracted the collector and open (or create) a folder called \"],[0,[5],1,\"CustomEvents\"],[0,[],0,\". This is where you drop your iFragments.\"]]],[1,\"p\",[[0,[],0,\"Now fire up your collector (using the default collection plan). Collect a log. Open it in VS. Start debugging.\"]]],[1,\"p\",[[0,[],0,\"The first thing to note is that you have a custom category in the categories list of the IntelliTrace events window:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Secondly, you’ll see the events by the same glyph that you saw when you added the events in VS:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Happy logging!\"]]]]}","published_at":1365779700000,"status":"published","published_by":1},{"id":"fda7db7d-7f62-4153-855a-87027f3c7cfd","title":"Enable SAFe Features in Existing Team Projects After Upgrading to TFS 2015","slug":"enable-safe-features-in-existing-team-projects-after-upgrading-to-tfs-2015","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"&lt;#\\n.SYNOPSIS\\n\\nAuthor: Colin Dembovsky (http://colinsalmcorner.com)\\nUpdates 2013 Templates to 2015 base templates, including addition of Epic Backlog and Area Value Field.\\n\\n\\n.DESCRIPTION\\n\\nAdds SAFe support to the base templates. This involves adding the Epic work item (along with its backlog and color settings) as well as adding 'Value Area' field to Features and Requirements (or PBIs or User Stories).\\n\\nThis isn't fully tested, so there may be issues depending on what customizations of the base templates you have already made. The script attempts to add in values, so it should work with your existing customizations.\\n\\nTo execute this script, first download the Agile, Scrum or CMMI template from the Process Template Manager in Team Explorer. You need the Epic.xml file for this script.\\n\\n.PARAMETER tpcUri\\n\\nThe URI to the Team Project Collection - defaults to 'http://localhost:8080/tfs/defaultcollection'\\n\\n.PARAMETER project\\n\\nThe name of the Team Project to ugprade\\n\\n.PARAMETER baseTemplate\\n\\nThe name of the base template. Must be Agile, Scrum or CMMI\\n\\n.PARAMETER pathToEpic\\n\\nThe path to the WITD xml file for the Epic work item\\n\\n.PARAMETER layoutGroupToAddValueAreaControlTo\\n\\nThe name of the control group to add the Value Area field to in the FORM - defaults to 'Classification' (Agile), 'Details' (SCRUM) and '' (CMMI). Leave this as $null unless you've customized your form layout.\\n\\n.PARAMETER pathToWitAdmin\\n\\nThe path to witadmin.exe. Defaults to 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\witadmin.exe'\\n\\n.EXAMPLE\\n\\nUpgrade-Template -project FabrikamFiber -baseTemplate Agile -pathToEpic '.\\\\Agile\\\\WorkItem Tracking\\\\TypeDefinitions\\\\Epic.xml'\\n\\n#&gt;\\n\\nparam(\\n    [string]$tpcUri = \\\"http://localhost:8080/tfs/defaultcollection\\\",\\n\\n    [Parameter(Mandatory=$true)]\\n    [string]$project,\\n\\n    [Parameter(Mandatory=$true)]\\n    [ValidateSet(\\\"Agile\\\", \\\"Scrum\\\", \\\"CMMI\\\")]\\n    [string]$baseTemplate,\\n\\n    [Parameter(Mandatory=$true)]\\n    [string]$pathToEpic,\\n\\n    [string]$layoutGroupToAddValueAreaControlTo = $null,\\n\\n    [string]$pathToWitAdmin = 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\witadmin.exe'\\n)\\n\\nif (-not (Test-Path $pathToEpic)) {\\n    Write-Error \\\"Epic WITD not found at $pathToEpic\\\"\\n    exit 1\\n}\\n\\nif ((Get-Alias -Name witadmin -ErrorAction SilentlyContinue) -eq $null) {\\n    New-Alias witadmin -Value $pathToWitAdmin\\n}\\n\\n$valueAreadFieldXml = '\\n&lt;FIELD name=\\\"Value Area\\\" refname=\\\"Microsoft.VSTS.Common.ValueArea\\\" type=\\\"String\\\"&gt;\\n    &lt;REQUIRED /&gt;\\n    &lt;ALLOWEDVALUES&gt;\\n        &lt;LISTITEM value=\\\"Architectural\\\" /&gt;\\n        &lt;LISTITEM value=\\\"Business\\\" /&gt;\\n    &lt;/ALLOWEDVALUES&gt;\\n    &lt;DEFAULT from=\\\"value\\\" value=\\\"Business\\\" /&gt;\\n    &lt;HELPTEXT&gt;Business = delivers value to a user or another system; Architectural = work to support other stories or components&lt;/HELPTEXT&gt;\\n&lt;/FIELD&gt;'\\n$valueAreaFieldFormXml = '&lt;Control FieldName=\\\"Microsoft.VSTS.Common.ValueArea\\\" Type=\\\"FieldControl\\\" Label=\\\"Value area\\\" LabelPosition=\\\"Left\\\" /&gt;'\\n\\n$epicCategoryXml = '\\n&lt;CATEGORY name=\\\"Epic Category\\\" refname=\\\"Microsoft.EpicCategory\\\"&gt;\\n  &lt;DEFAULTWORKITEMTYPE name=\\\"Epic\\\" /&gt;\\n&lt;/CATEGORY&gt;'\\n\\n$epicBacklogXml = '\\n    &lt;PortfolioBacklog category=\\\"Microsoft.EpicCategory\\\" pluralName=\\\"Epics\\\" singularName=\\\"Epic\\\" workItemCountLimit=\\\"1000\\\"&gt;\\n      &lt;States&gt;\\n        &lt;State value=\\\"New\\\" type=\\\"Proposed\\\" /&gt;\\n        &lt;State value=\\\"Active\\\" type=\\\"InProgress\\\" /&gt;\\n        &lt;State value=\\\"Resolved\\\" type=\\\"InProgress\\\" /&gt;\\n        &lt;State value=\\\"Closed\\\" type=\\\"Complete\\\" /&gt;\\n      &lt;/States&gt;\\n      &lt;Columns&gt;\\n        &lt;Column refname=\\\"System.WorkItemType\\\" width=\\\"100\\\" /&gt;\\n        &lt;Column refname=\\\"System.Title\\\" width=\\\"400\\\" /&gt;\\n        &lt;Column refname=\\\"System.State\\\" width=\\\"100\\\" /&gt;\\n        &lt;Column refname=\\\"Microsoft.VSTS.Scheduling.Effort\\\" width=\\\"50\\\" /&gt;\\n        &lt;Column refname=\\\"Microsoft.VSTS.Common.BusinessValue\\\" width=\\\"50\\\" /&gt;\\n        &lt;Column refname=\\\"Microsoft.VSTS.Common.ValueArea\\\" width=\\\"100\\\" /&gt;\\n        &lt;Column refname=\\\"System.Tags\\\" width=\\\"200\\\" /&gt;\\n      &lt;/Columns&gt;\\n      &lt;AddPanel&gt;\\n        &lt;Fields&gt;\\n          &lt;Field refname=\\\"System.Title\\\" /&gt;\\n        &lt;/Fields&gt;\\n      &lt;/AddPanel&gt;\\n    &lt;/PortfolioBacklog&gt;'\\n$epicColorXml = '&lt;WorkItemColor primary=\\\"FFFF7B00\\\" secondary=\\\"FFFFD7B5\\\" name=\\\"Epic\\\" /&gt;'\\n\\n#####################################################################\\nfunction Add-Fragment(\\n    [System.Xml.XmlNode]$node,\\n    [string]$xml\\n) {\\n    $newNode = $node.OwnerDocument.ImportNode(([xml]$xml).DocumentElement, $true)\\n    [void]$node.AppendChild($newNode)\\n}\\n\\nfunction Add-ValueAreaField(\\n    [string]$filePath,\\n    [string]$controlGroup\\n) {\\n    $xml = [xml](gc $filePath)\\n    # check if the field already exists\\n    if (($valueAreaField = $xml.WITD.WORKITEMTYPE.FIELDS.ChildNodes | ? { $_.refname -eq \\\"Microsoft.VSTS.Common.ValueArea\\\" }) -ne $null) {\\n        Write-Host \\\"Work item already has Value Area field\\\" -ForegroundColor Yellow\\n    } else {\\n        # add field to FIELDS\\n        Add-Fragment -node $xml.WITD.WORKITEMTYPE.FIELDS -xml $valueAreadFieldXml\\n\\n        # add field to FORM\\n        # find the \\\"Classification\\\" Group\\n        $classificationGroup = (Select-Xml -Xml $xml -XPath \\\"//Layout//Group[@Label='$layoutGroupToAddValueAreaControlTo']\\\").Node\\n        Add-Fragment -node $classificationGroup.Column -xml $valueAreaFieldFormXml\\n\\n        # upload definition\\n        $xml.Save((gi $filePath).FullName)\\n        witadmin importwitd /collection:$tpcUri /p:$project /f:$filePath\\n    }\\n}\\n#####################################################################\\n\\n$defaultControlGroup = \\\"Classification\\\"\\nswitch ($baseTemplate) {\\n    \\\"Agile\\\" { $wit = \\\"User Story\\\" }\\n    \\\"Scrum\\\" { $wit = \\\"Product Backlog Item\\\"; $defaultControlGroup = \\\"Details\\\" }\\n    \\\"CMMI\\\"  { $wit = \\\"Requirement\\\" }\\n}\\nif ($layoutGroupToAddValueAreaControlTo -ne $null) {\\n    $defaultControlGroup = $layoutGroupToAddValueAreaControlTo\\n}\\n\\nWrite-Host \\\"Exporting requirement work item type $wit\\\" -ForegroundColor Cyan\\nwitadmin exportwitd /collection:$tpcUri /p:$project /n:$wit /f:\\\"RequirementItem.xml\\\"\\n\\nWrite-Host \\\"Adding 'Value Area' field to $wit\\\" -ForegroundColor Cyan\\nAdd-ValueAreaField -filePath \\\".\\\\RequirementItem.xml\\\" -controlGroup $defaultControlGroup\\n\\nWrite-Host \\\"Exporting work item type Feature\\\" -ForegroundColor Cyan\\nwitadmin exportwitd /collection:$tpcUri /p:$project /n:Feature /f:\\\"Feature.xml\\\"\\n\\nWrite-Host \\\"Adding 'Value Area' field to Feature\\\" -ForegroundColor Cyan\\nAdd-ValueAreaField -filePath \\\".\\\\Feature.xml\\\" -controlGroup $defaultControlGroup\\n\\nif (((witadmin listwitd /p:$project /collection:$tpcUri) | ? { $_ -eq \\\"Epic\\\" }).Count -eq 1) {\\n    Write-Host \\\"Process Template already contains an Epic work item type\\\" -ForegroundColor Yellow\\n} else {\\n    Write-Host \\\"Adding Epic\\\" -ForegroundColor Cyan\\n    witadmin importwitd /collection:$tpcUri /p:$project /f:$pathToEpic\\n}\\n\\nwitadmin exportcategories /collection:$tpcUri /p:$project /f:\\\"categories.xml\\\"\\n$catXml = [xml](gc \\\"categories.xml\\\")\\nif (($catXml.CATEGORIES.ChildNodes | ? { $_.name -eq \\\"Epic Category\\\" }) -ne $null) {\\n    Write-Host \\\"Epic category already exists\\\" -ForegroundColor Yellow\\n} else {\\n    Write-Host \\\"Updating categories\\\" -ForegroundColor Cyan\\n    Add-Fragment -node $catXml.CATEGORIES -xml $epicCategoryXml\\n    $catXml.Save((gi \\\".\\\\categories.xml\\\").FullName)\\n    witadmin importcategories /collection:$tpcUri /p:$project /f:\\\"categories.xml\\\"\\n\\n    Write-Host \\\"Updating ProcessConfig\\\" -ForegroundColor Cyan\\n    witadmin exportprocessconfig /collection:$tpcUri /p:$project /f:\\\"processConfig.xml\\\"\\n    $procXml = [xml](gc \\\"processConfig.xml\\\")\\n\\n    Add-Fragment -node $procXml.ProjectProcessConfiguration.PortfolioBacklogs -xml $epicBacklogXml\\n    Add-Fragment -node $procXml.ProjectProcessConfiguration.WorkItemColors -xml $epicColorXml\\n\\n    $featureCat = $procXml.ProjectProcessConfiguration.PortfolioBacklogs.PortfolioBacklog | ? { $_.category -eq \\\"Microsoft.FeatureCategory\\\" }\\n    $parentAttrib = $featureCat.OwnerDocument.CreateAttribute(\\\"parent\\\")\\n    $parentAttrib.Value = \\\"Microsoft.EpicCategory\\\"\\n    $featureCat.Attributes.Append($parentAttrib)\\n\\n    $procXml.Save((gi \\\".\\\\processConfig.xml\\\").FullName)\\n    witadmin importprocessconfig /collection:$tpcUri /p:$project /f:\\\"processConfig.xml\\\"\\n}\\n\\nWrite-Host \\\"Done!\\\" -ForegroundColor Green\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<font size=\\\"3\\\" face=\\\"Cordia New\\\">Get-Help .\\\\Upgrade-TemplateTo2015.ps1</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bb4f7202-9bc1-4f81-a9ea-edf90a783768.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/76dbf0dc-5910-4ed4-a2d2-f5b39dbe0656.png\\\" width=\\\"347\\\" height=\\\"238\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/21cc8c7d-d422-4af5-87f0-acfb586e93b5.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8326d637-7260-4617-9c10-c8568f793fb4.png\\\" width=\\\"302\\\" height=\\\"277\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b55b60e2-7e74-4e3c-8edb-5dde3793208a.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6f88f50f-4c71-4e68-974a-b359bcf91a10.png\\\" width=\\\"413\\\" height=\\\"145\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://1drv.ms/1T9Ph0q\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"TFS 2015 has almost reached RTM! If you upgrade to CTP2, you’ll see a ton of new features, not least of which are significant backlog and board improvements, the identity control, Team Project rename, expanded features for Basic users, the new Build Engine, PRs and policies for Git repos and more. Because of the schema changes required for Team Project rename, this update can take a while. If you have large databases, you may want to run the “pre-upgrade” utility that will allow you to prep your server while it’s still online and decrease the time required to do the upgrade (which will need to be done offline).\"]]],[1,\"h2\",[[0,[],0,\"SAFe Support\"]]],[1,\"p\",[[0,[],0,\"The three out of the box templates have been renamed to simply Scrum, Agile and CMMI. Along with the name change, there is now “built in” support for SAFe. This means if you create a new TFS 2015 team project, you’ll have 3 backlogs – Epic, Feature and “Requirement” (where Requirement will be Requirement, User Story or PBI for CMMI, Agile and Scrum respectively). In Team Settings, team can opt into any of the 3 backlogs. Also, Epics, Features and “Requirements” now have an additional “Value Area” field which can be Business or Architectural, allowing you to track Business vs Architectural work.\"]]],[1,\"h2\",[[0,[],0,\"Where are my Epics?\"]]],[1,\"p\",[[0,[],0,\"After upgrading my TFS to 2015, I noticed that I didn’t have Epics. I remember when upgrading from 2012 to 2013, when you browsed to the Backlog a message popped up saying, “Some features are not available” and a wizard walked you through enabling the “backlog feature”, adding in missing work items and configuring the process template settings. I was expecting the same behavior when upgrading to TFS 2015 – but that didn’t happen. I pinged the TFS product team and they told me that, “Epics are not really a new ‘feature’ per se – just a new backlog level, so the ‘upgrade’ ability was not built in.” If you’re on VSO, your template did get upgraded, so you won’t have a problem – however, for on-premises Team Projects you have to apply the changes manually.\"]]],[1,\"h3\",[[0,[],0,\"Doing it Manually\"]]],[1,\"p\",[[0,[],0,\"Here are the steps for enabling SAFe to your existing TFS 2013 Agile, Scrum or CMMI templates:\"]]],[3,\"ol\",[[[0,[],0,\"Add the Epic work item type\"]],[[0,[],0,\"Add the “Value Area” field to Features and “Requirements”\"]],[[0,[],0,\"Add the “Value Area” field to the Feature and “Requirement” form\"]],[[0,[],0,\"Add the Epic category\"]],[[0,[],0,\"Add the Epic Product Backlog\"]],[[0,[],0,\"Set the Feature Product Backlog parent to Epic Backlog\"]],[[0,[],0,\"Set the work item color for Epics\"]]]],[1,\"p\",[[0,[],0,\"It’s a whole lot of “witadmin” and XML editing – never fun. Fortunately for you, I’ve created a script that will do it for you.\"]]],[1,\"h3\",[[0,[],0,\"Isn’t there a script for that?\"]]],[1,\"p\",[[0,[],0,\"Here’s the script – but you can download it from \"],[0,[0],1,\"here\"],[0,[],0,\".\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Running the Script\"]]],[1,\"p\",[[0,[],0,\"To run the script, just make sure you’re a Team Project administrator and log in to a machine that has witadmin.exe on it. Then open Team Explorer, connect to your server, and click Settings. Then click “Process Template Manager” and download the new template (Agile, Scrum or CMMI) to a folder somewhere. You really only need the Epic work item WITD. Make a note of where the Epic.xml file ends up.\"]]],[1,\"p\",[[0,[],0,\"Then you’re ready to run the script. You’ll need to supply:\"]]],[3,\"ul\",[[[0,[],0,\"(Optional) The TPC Uri (defaults is http://localhost:8080/tfs/defaultcollection)\"]],[[0,[],0,\"The Team Project name\"]],[[0,[],0,\"The path to the Epic.xml file\"]],[[0,[],0,\"The name of the base template – either Agile, Scrum or CMMI\"]],[[0,[],0,\"(Optional) The path to witadmin.exe (defaults to C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\witadmin.exe)\"]],[[0,[],0,\"(Optional) The name of the group you want to add the “Value Area” field to on the form – default is “Classification”\"]]]],[1,\"p\",[[0,[],0,\"You can run\"]]],[10,1],[1,\"p\",[[0,[],0,\"to get help and examples.\"]]],[1,\"p\",[[0,[],0,\"Bear in mind that this script is a “best-effort” – make sure you test it in a test environment before going gung-ho on your production server!\"]]],[1,\"h2\",[[0,[],0,\"Results\"]]],[1,\"p\",[[0,[],0,\"After running the script. you’ll be able to create Epic work items:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You’ll be able to opt in/out of the Epic backlog in the Team Settings page:\"]]],[10,3],[1,\"p\",[[0,[],0,\"You’ll see “Value Area” on your Features and “Requirements”:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Happy upgrading!\"]]]]}","published_at":1437148211000,"status":"published","published_by":1},{"id":"c00f0ff0-2629-4a06-8e88-b2763448ab04","title":"Enabling JavaScript Code Coverage Link in Builds","slug":"enabling-javascript-code-coverage-link-in-builds","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"Param(\\n  [string]$testResultsDir = $env:TF_BUILD_TESTRESULTSDIRECTORY,\\n  [string]$dropLocation = $env:TF_BUILD_DROPLOCATION,\\n  [string]$tpcUri = $env:TF_BUILD_COLLECTIONURI,\\n  [string]$buildUri = $env:TF_BUILD_BUILDURI\\n)\\n\\n$coverageFileName = \\\"\\\\_Chutzpah.coverage.html\\\"\\n$jsScriptResultsFile = $testResultsDir + $coverageFileName\\nif (Test-Path($jsScriptResultsFile)) {\\n    try {\\n        Write-Host \\\"Copying Chutzpah coverage files\\\"\\n        copy $jsScriptResultsFile $dropLocation\\n\\n        # add the link into the build summary\\n        Write-Host \\\"Loading TFS assemblies\\\"\\n        [Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Client')\\n        [Reflection.Assembly]::LoadWithPartialName('Microsoft.TeamFoundation.Build.Client')\\n    \\n        Write-Host \\\"Getting build object\\\"\\n        $tpc = [Microsoft.TeamFoundation.Client.TfsTeamProjectCollectionFactory]::GetTeamProjectCollection($tpcUri)\\n        $buildService = $tpc.GetService([Microsoft.TeamFoundation.Build.Client.IBuildServer])\\n        $build = $buildService.GetBuild($buildUri)\\n\\n        Write-Host \\\"Writing Chutzpah coverage link to build summary\\\"\\n        $message = \\\"Javascript testing was detected. Open [coverage results]($dropLocation\\\\$coverageFileName)\\\"\\n        [Microsoft.TeamFoundation.Build.Client.InformationNodeConverters]::AddCustomSummaryInformation($build.Information, $message, \\\"ConfigurationSummary\\\", \\\"Javascript Coverage\\\", 200)\\n        $build.Information.Save();\\n\\n        # all is well with the world\\n        Write-Host \\\"Success!\\\"\\n        exit 0\\n    }\\n    catch {\\n        Write-Error $_\\n        exit 1\\n    }\\n} else {\\n    # let the build know there were no coverage files\\n    Write-Warning \\\"No Chutzpah coverage file detected\\\"\\n    exit 0\\n}\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a4f94785-d8fd-4e7b-af54-c648bc1c77a6.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b236a53c-1ed2-4625-b4b9-340b571d9299.png\\\" width=\\\"413\\\" height=\\\"484\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6ad64dfc-ef5a-4c5b-b343-f0a25e2a5071.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c9497e2e-6641-4866-86ad-7bf08f41ba7e.png\\\" width=\\\"644\\\" height=\\\"201\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/99bcacbb-0bc3-4cac-a6ac-980b849166a4.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/94d99bff-5c69-4239-a145-6795dd3916e4.png\\\" width=\\\"453\\\" height=\\\"354\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e710bde6-a4bc-492b-a1df-d287ef0328b7.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/033ce68c-1531-47b8-8241-cc884562de5f.png\\\" width=\\\"644\\\" height=\\\"296\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/unit-testing-javascript-in-vs-2012\"]],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe\"]],[\"a\",[\"href\",\"http://www.geekswithblogs.net/jakob/Default.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.teamfoundation.build.client.informationnodeconverters.addcustomsummaryinformation.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/hh850448.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In a \"],[0,[0],1,\"previous post\"],[0,[],0,\" I wrote about how to do JavaScript unit testing in VS 2012. The same procedure applies to VS 2013 – but the \"],[0,[1],1,\"Chutzpah test adapter\"],[0,[],0,\" now allows you to run code coverage too. At the end of my post I link to another blog entry about how to enable the tests to run during team builds.\"]]],[1,\"p\",[[0,[],0,\"I recently added some tests to a VS 2013 solution I was working on and was pleased to see that when you “Analyze Code Coverage for all Tests” in the Test Explorer, VS pops open a nicely formatted html page that shows you your JavaScript coverage. I wanted to have that file available in my build results too. Looking at the test results folder of the local VS test run, I saw that Chutzpah created an html file called “_Chutzpah.coverage.html”. I wanted that script to be copied to the drop folder of the build and create a link in the build summary that you could click to open it.\"]]],[1,\"h2\",[[0,[],0,\"Post-Test Script\"]]],[1,\"p\",[[0,[],0,\"Fortunately you can do this without even having to customize the build template – as long as you’re using the TfvcTemplate.12.xaml template – the default build template that ships with TFS 2013. This build has some really useful script hooks – and there’s one for running a post-test script. I knew I could easily copy the Chutpah result file to the drop folder – no problem. But how do you add to the build summary report from a script that’s running “outside” the workflow? If you customize the workflow you can use the WriteCustomSummaryInformation activity, but I wanted to do this without modifying the template.\"]]],[1,\"p\",[[0,[],0,\"After mailing the ChampsList, \"],[0,[2],1,\"Jakob Ehn\"],[0,[],0,\" pointed me in the right direction – I needed to use the \"],[0,[3],1,\"InformationNodeCoverters.AddCustomSummaryInformation\"],[0,[],0,\" method. Once I had that, the rest of the PowerShell script was almost trivial. I did hit one snag – I need the Team Project Collection URI for the script to work, but for some reason the value of the TF_BUILD_COLLECTIONURI \"],[0,[4],1,\"build environment variable\"],[0,[],0,\" was empty. Updating my build agent to VS 2013 Update 2 resolved this issue. Here’s the script:\"]]],[10,0],[1,\"p\",[[0,[],0,\"I saved this to my build scripts folder under source control and checked it in.\"]]],[1,\"p\",[[0,[],0,\"Opening up the build definition, I had to create a second build run to run the JavaScript tests – here’s the settings I used:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Note how I’ve enabled Code Coverage in the options dropdown.\"]]],[1,\"p\",[[0,[],0,\"I added this folder to the source mappings for my build and then called the script in the post-test settings of the build:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Now when I run my build, I get a link to the JavaScript coverage file:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Clicking on the “coverage results” link opens the results page:\"]]],[10,4],[1,\"p\",[[0,[],0,\"As a next project, I want to see if I can incorporate the coverage results into the build warehouse so that there’s metrics not only on .NET coverage over time, but also for JavaScript tests.\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1400100183000,"status":"published","published_by":1},{"id":"3e2e295b-080f-41a9-ad93-6c4116932d08","title":"End to End Walkthrough: Deploying Web Applications Using Team Build and Release Management","slug":"end-to-end-walkthrough-deploying-web-applications-using-team-build-and-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/774fea15-bff2-4a5c-9ee4-3ec8ee2ab4e1.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/0696e9ca-6e0f-4595-a7f5-08ddaa9ed66b.png\\\" width=\\\"749\\\" height=\\\"395\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/b1cffea2-7866-4dcf-9d14-f45b2ada68e2.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/57b55985-d6c4-41fe-b93f-4e5408368d74.png\\\" width=\\\"733\\\" height=\\\"435\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/78397225-1cbf-453c-a940-e9982a93a85b.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/eceddac6-78f8-4ce8-9dfe-e183f9c91b29.png\\\" width=\\\"298\\\" height=\\\"237\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/f23edb73-ed6e-4aa5-bfa6-ad4b25b6896a.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8ae35ddc-e25b-4c4a-befa-010ca01bd112.png\\\" width=\\\"298\\\" height=\\\"237\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a9ba6f7c-f0d2-411e-952f-74026144eaf0.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/1e67b4a8-0cb0-438b-bf41-81263ebe7de4.png\\\" width=\\\"300\\\" height=\\\"309\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/028a55c0-5785-4849-959d-8a690e1441d7.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/1ff23295-0800-4a82-8c74-b6c8f7ef5c59.png\\\" width=\\\"303\\\" height=\\\"134\\\"></a>\"}],[\"code\",{\"code\":\"&lt;appSettings&gt;\\n  ...\\n  &lt;add key=\\\"Environment\\\" value=\\\"debug!\\\" /&gt;\\n&lt;/appSettings&gt;\\n\",\"language\":\"xml; highlight\"}],[\"code\",{\"code\":\"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\" ?&gt;\\n&lt;parameters&gt;\\n  &lt;parameter name=\\\"Environment\\\" description=\\\"doesn't matter\\\" defaultvalue=\\\"__Environment__\\\" tags=\\\"\\\"&gt;\\n    &lt;parameterentry kind=\\\"XmlFile\\\" scope=\\\"\\\\\\\\web.config$\\\" match=\\\"/configuration/appSettings/add[@key='Environment']/@value\\\"&gt;\\n    &lt;/parameterentry&gt;\\n  &lt;/parameter&gt;\\n&lt;/parameters&gt;\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/40c8a86e-f910-4b91-80b7-ae417d834f89.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/13b93fe8-2aa0-4963-93fe-8d8a32cc131c.png\\\" width=\\\"216\\\" height=\\\"333\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/43e2a952-000a-4581-9ea3-e2df5058807e.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/dc8c4a62-ccb8-4944-8946-6ab51549f722.png\\\" width=\\\"532\\\" height=\\\"125\\\"></a>\"}],[\"code\",{\"code\":\"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?&gt;\\n&lt;parameters&gt;\\n  &lt;setParameter name=\\\"IIS Web Application Name\\\" value=\\\"Default Web Site/WebDeployMe_deploy\\\" /&gt;\\n  &lt;setParameter name=\\\"Environment\\\" value=\\\"__Environment__\\\" /&gt;\\n  &lt;setParameter name=\\\"DefaultConnection-Web.config Connection String\\\" value=\\\"__AppDbContextStr__\\\" /&gt;\\n  &lt;setParameter name=\\\"SomeConnection-Web.config Connection String\\\" value=\\\"Server=__DbServer__;Initial Catalog=__DbName__;User Name=__DbUser__;Password=__DbPassword__\\\" /&gt;\\n&lt;/parameters&gt;\\n\",\"language\":\"xml; highlight\"}],[\"code\",{\"code\":\"&lt;connectionStrings&gt;\\n  &lt;add name=\\\"DefaultConnection\\\" connectionString=\\\"$(ReplacableToken_DefaultConnection-Web.config Connection String_0)\\\" providerName=\\\"System.Data.SqlClient\\\" /&gt;\\n  &lt;add name=\\\"SomeConnection\\\" connectionString=\\\"$(ReplacableToken_SomeConnection-Web.config Connection String_0)\\\" providerName=\\\"System.Data.SqlClient\\\" /&gt;\\n&lt;/connectionStrings&gt;\\n&lt;appSettings&gt;\\n  ...\\n  &lt;add key=\\\"Environment\\\" value=\\\"debug!\\\" /&gt;\\n&lt;/appSettings&gt;\\n\",\"language\":\"xml; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/3fdf8cd8-facf-4e27-ab99-0ed18975350f.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/852b438e-1036-48b6-9960-dee8d366289c.png\\\" width=\\\"435\\\" height=\\\"196\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/2fda7c25-9194-4f17-8d4e-de8ebd7369fd.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/6cae9fce-241a-4162-8d15-1807ac5d2666.png\\\" width=\\\"611\\\" height=\\\"153\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a83bd59e-2b0c-4405-a9e8-7b357aef64ac.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/e487be58-b7bf-4be6-93c3-8ad769ace1ec.png\\\" width=\\\"553\\\" height=\\\"217\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/f3ddc364-ce7f-4502-a3a3-549b554aa40a.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/0eb528d2-d7a7-41df-84b2-4bf259c5dae1.png\\\" width=\\\"649\\\" height=\\\"211\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/2c310cdf-b57c-477b-b871-fd42589b5ec3.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8f7c1569-46bd-47d4-85f3-cfbdffa71aca.png\\\" width=\\\"782\\\" height=\\\"247\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/0b1d1624-05f5-46ed-b1dd-174bd43bee8d.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b4d102bb-e130-4885-9dd1-53e7005bd392.png\\\" width=\\\"780\\\" height=\\\"295\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4c08facb-f44a-4a55-bc84-d5498355d7de.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a3bd502f-3644-464d-b60d-764e0468e420.png\\\" width=\\\"783\\\" height=\\\"491\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/69d3456e-12fc-4133-bd4e-2bb9be8c67fd.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/9069ba71-d0bc-4e94-8b99-8e6ea9196f89.png\\\" width=\\\"784\\\" height=\\\"332\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/84b88615-45bd-46d0-9fff-7be7d606c2d6.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/955909c6-ebe8-4b89-8ee2-6cf5e04ffcf8.png\\\" width=\\\"427\\\" height=\\\"323\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/00a30998-705a-4572-bc12-5c0223e33058.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/5f6d3bba-6e6a-4c3e-b6fd-465bcde179be.png\\\" width=\\\"555\\\" height=\\\"213\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c0042ad8-6884-4e88-a58a-44cc995bef5b.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8a049eba-a5b4-421d-8811-414610eafdf2.png\\\" width=\\\"518\\\" height=\\\"310\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c3826514-e9cd-4e4e-ab60-e86d057ca282.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/8f8b4569-4f02-4d7c-9012-77b018862fe8.png\\\" width=\\\"614\\\" height=\\\"233\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/86f31379-edfc-47e9-a7e8-6bb67791d5ab.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/d7f9617f-7dae-43d3-a99c-184761542e68.png\\\" width=\\\"612\\\" height=\\\"300\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/config-per-environment-vs-tokenization-in-release-management\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/webdeploy-configs-and-web-release-management\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=colinsalmcorner.colinsalmcorner-buildtasks\"]],[\"em\"],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-absolutely-need-to-unit-test\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=ms-vscs-rm.iiswebapp\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/PartsUnlimited/blob/master/env/Templates/Website.json\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-tasks/pull/1748\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/PartsUnlimited/tree/master/docs/HOL-Continuous_Deployment\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve posted previously about deploying web applications using Team Build and Release Management (see \"],[0,[0],1,\"Config Per Environment vs Tokenization in Release Management\"],[0,[],0,\" and \"],[0,[1],1,\"WebDeploy, Configs and Web Release Management\"],[0,[],0,\"). However, reviewing those posts recently at customers I’ve been working with, I’ve realized that these posts are a little outdated, you need pieces of both to form a full picture and the scripts that I wrote for those posts are now encapsulated in Tasks in my \"],[0,[2],1,\"marketplace extension\"],[0,[],0,\". So in this post I’m going to do a complete end-to-end walkthrough of deploying web applications using Team Build and Release Management. I’ll be including handling configs – arguably the hardest part of the whole process.\"]]],[1,\"h2\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"Let’s start with an overview of the process. I like to think of three distinct “areas” – source control, build and release. Conceptually you have tokenized config in source control (more on how to do this coming up). Then you have a build that takes in the source control and produces a WebDeploy package – \"],[0,[3],1,\"a single tokenized package that is potentially deployable to multiple environments\"],[0,[],0,\". The build should not have to know about anything environment specific (such as the correct connection string for Staging or Production, for example). Then release takes in the package and (conceptually) performs two steps: inject environment values for the tokens, and then deploy using WebDeploy. Here’s a graphic (which I’ll refer to as the Flow Diagram) of the process:\"]]],[10,0],[1,\"p\",[[0,[],0,\"I’ve got some details in this diagram that we’ll cover shortly, so don’t worry about the details for now. The point is to clearly separate responsibilities (especially for build and release): build compiles source code, runs tests and performs static code analysis and so on. It shouldn’t have to know anything about environments – the output of the build is a single package with “holes” for environment values (tokens). Release will take in this single package and deploy it to each environment, plugging environment values into the holes as it goes. This guarantees that the bits you test in Dev and Staging are the same bits that get deployed to Prod!\"]]],[1,\"h2\",[[0,[],0,\"Deep Dive: Configuration\"]]],[1,\"p\",[[0,[],0,\"Let’s get down into the weeds of configuration. If you’re going to produce a single package from build, then how should you handle config? Typically you have (at least) a connection string that you want to be different for each environment. Beyond that you probably have appSettings as well. If the build shouldn’t know about these values when it’s creating the package, then how do you manage config? Here are a some options:\"]]],[3,\"ol\",[[[0,[],0,\"Create a web.config for each environment in source control\"]]]],[3,\"ul\",[[[0,[],0,\"Pros: All your configs are in source control in their entirety\"]],[[0,[],0,\"Cons: Lots of duplications – and you have to copy the correct config to the correct environment; requires a re-build to change config values in release management\"]]]],[3,\"ol\",[[[0,[],0,\"Create a config transform for each environment\"]]]],[3,\"ul\",[[[0,[],0,\"Pros: Less duplication, and you have all the environment values in source control\"]],[[0,[],0,\"Cons: Requires a project (or solution) config per environment, which can lead to config bloat; requires that you create a package per environment during build; requires a re-build to change config values in release management\"]]]],[3,\"ol\",[[[0,[],0,\"Tokenize using a single transform and parameters.xml\"]]]],[3,\"ul\",[[[0,[],0,\"Pros: No duplication; enables a single package that can be deployed to multiple environments; no rebuild required to change config values in release management\"]],[[0,[],0,\"Cons: Environment values aren’t in source control (though they’re in Release Management); learning curve\"]]]],[1,\"p\",[[0,[],0,\"Furthermore, if you’re targeting Azure, you can use the same techniques as targeting IIS, or you can use Azure Resource Manager (ARM) templates to manage your configuration. This offloads the config management to the template and you assume that the target Azure Web App is correctly configured at the time you run WebDeploy.\"]]],[1,\"p\",[[0,[],0,\"Here’s a decision tree to make this a bit easier to digest:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Let’s walk through it:\"]]],[3,\"ul\",[[[0,[],0,\"If you’re deploying to Azure, and using ARM templates, just make sure that you configure the settings correctly in the template (I won’t cover how to do this in this post)\"]],[[0,[],0,\"If you’re deploying to IIS (or you’re deploying to Azure and don’t have ARM templates or just want to manage config in the same manner as you would for IIS), you should create a single publish profile using right-click->Publish (on the web application) called “Release”. This should target the release configuration and you should tokenize the connection strings in the wizard (details coming up)\"]],[[0,[],0,\"Next, if you have appSettings, you’ll have to create a parameters.xml file (details coming up)\"]],[[0,[],0,\"Commit to source control\"]]]],[1,\"p\",[[0,[],0,\"For the remainder of this post I’m going to assume that you’re deploying to IIS (or to Azure and handling config outside of an ARM template).\"]]],[1,\"h3\",[[0,[],0,\"Creating a Publish Profile\"]]],[1,\"p\",[[0,[],0,\"So what is this publish profile and why do you need it? The publish profile enables you to:\"]]],[3,\"ul\",[[[0,[],0,\"provide a single transform (via the Web.release.config) that makes your config release-ready (removing the debug compilation property, for example)\"]],[[0,[],0,\"tokenize the connection strings\"]]]],[1,\"p\",[[0,[],0,\"To create the profile, right-click your web application project and select “Publish…”. Then do the following:\"]]],[3,\"ul\",[[[0,[],0,\"Select “Custom” to create a new custom profile. Name this “Release” (you can name this anything, but you’ll need to remember the name for the build later)\"]]]],[10,2],[3,\"ul\",[[[0,[],0,\"On the Connection page, change the Publish method to “Web Deploy Package”. Type anything you want for the filename and leave the site name empty. Click Next.\"]]]],[10,3],[3,\"ul\",[[[0,[],0,\"On the Settings page, select the configuration you want to compile. Typically this is Release – remember that the name of the configuration here is how the build will know which transform to apply. If you set this to Release, it will apply Web.Release.config – if you set it to Debug it will apply Web.Debug.Release. Typically you want to specify Release here since you’re aiming to get this site into Prod (otherwise why are you coding at all?) and you probably don’t want debug configuration in Prod!\"]],[[0,[],0,\"You’ll see a textbox for each connection string you have in your Web.config. You can either put a single token in or a tokenized connection string. In the example below, I’ve used a single token (“__AppDbContextStr__”) for the one connection string and a tokenized string (“Server=__DbServer__;Initial Catalog=__DbName__;User Name=__DbUser__;Password=__DbPassword__”) for the other (just so you can see the difference). I’m using double underscore pre- and post-fix for the tokens:\"]]]],[10,4],[3,\"ul\",[[[0,[],0,\"Now click “Close” (don’t hit publish). When prompted to save the profile, select yes. This creates a Release.pubxml file in the Properties folder (the name of the file is the name of the profile you selected earlier):\"]]]],[10,5],[1,\"h3\",[[0,[],0,\"Creating a parameters.xml File\"]]],[1,\"p\",[[0,[],0,\"The publish profile takes care of the connection strings – but you will have noticed that it doesn’t ask for values for appSettings (or any other configuration) anywhere. In order to tokenize anything in your web.config other than connection strings, you’ll need to create a parameters.xml file (yes, it has to be called that) in the root of your web application project. When the build runs, it will use this file to expose properties for you to tokenize (it doesn’t actually transform the config at build time).\"]]],[1,\"p\",[[0,[],0,\"Here’s a concrete example: in my web.config, I have the following snippet:\"]]],[10,6],[1,\"p\",[[0,[],0,\"There’s an appSetting key called “Environment” that has the value “debug!”. When I run or debug out of Visual Studio, this is the value that will be used. If I want this value to change on each environment I target for deployment, I need to add a parameters.xml file to the root of my web application with the following xml:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Lines 3-6 are repeated for each parameter I want to configure. Let’s take a deeper look:\"]]],[3,\"ul\",[[[0,[],0,\"parameter name (line 3) – by convention it should be the name of the setting you’re tokenizing\"]],[[0,[],0,\"parameter description (line 3) – totally immaterial for this process, but you can use it if you need to. Same with tags.\"]],[[0,[],0,\"parameter defaultvalue (line 3) – this is the token you want injected – notice the double underscore again. Note that this can be a single token or a tokenized string (like the connection strings above)\"]],[[0,[],0,\"parameterentry match (line 4) – this is the xpath to the bit of config you want to replace. In this case, the xpath says in the “configuration” element, find the “appSetting” element, then find the “add” element with the key property = ‘Environment’ and replace the value parameter with the defaultvalue.\"]]]],[1,\"p\",[[0,[],0,\"Here you can see the parameters.xml file in my project:\"]]],[10,8],[1,\"p\",[[0,[],0,\"To test your transform, right-click and publish the project using the publish profile (for this you may want to specify a proper path for the Filename in the Connection page of the profile). After a successful publish, you’ll see 5 files. The important files are the zip file (where the bits are kept – this is all the binary and content files, no source files) and the SetParameters.xml file:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Opening the SetParameters.xml file, you’ll see the following:\"]]],[10,10],[1,\"p\",[[0,[],0,\"You’ll see the tokens for the appSetting (Environment, line 4) and the connection strings (lines 5 and 6). Note how the tokens live in the SetParameters.xml file, not in the web.config file! In fact, if you dive into the zip file and view the web.config file, you’ll see this:\"]]],[10,11],[1,\"p\",[[0,[],0,\"You can see that there are placeholders for the connection strings, but the appSetting is unchanged from what you have in your web.config! As long as your connection strings have placeholders and your appSettings are in the SetParameters.xml file, you’re good to go – don’t worry, WebDeploy will still inject the correct values for your appSettings at deploy time (using the xpath you supplied in the parameters.xml file).\"]]],[1,\"h2\",[[0,[],0,\"Deep Dive: Build\"]]],[1,\"p\",[[0,[],0,\"You’re now ready to create the build definition. There are some additional build tasks which may be relevant – such as creating dacpacs from SQL Server Data Tools (SSDT) projects to manage database schema changes – that are beyond the scope of this post. As for the web application itself, I like to have builds do the following:\"]]],[3,\"ul\",[[[0,[],0,\"Version assemblies to match the build number (optional, but recommended)\"]],[[0,[],0,\"Run unit tests, code analysis and other build verification tasks\"]],[[0,[],0,\"Create the WebDeploy package\"]]]],[1,\"p\",[[0,[],0,\"To version the assemblies, you can use the VersionAssemblies task from my \"],[0,[2],1,\"build tasks extension in the marketplace\"],[0,[],0,\". You’ll need the ReplaceTokens task for the release later, so just install the extension even if you’re not versioning. To show the minimum setup required to get the release working, I’m skipping unit tests and code analysis – but this is only for brevity. I highly recommend that \"],[0,[4],1,\"unit testing\"],[0,[],0,\" and code analysis become part of every build you have.\"]]],[1,\"p\",[[0,[],0,\"Once you’ve created a build definition:\"]]],[3,\"ul\",[[[0,[],0,\"Click on the General tab and change the build number format to 1.0.0$(rev:.r). This makes the first build have the number 1.0.0.1, the second build 1.0.0.2 etc.\"]],[[0,[],0,\"Add a VersionAssemblies task as the first task. Set the Source Path to the folder that contains the projects you want to version (typically the root folder). Leave the rest defaulted.\"]]]],[10,12],[3,\"ul\",[[[0,[],0,\"Leave the NuGet restore task as-is (you may need to edit the solution filter if you have multiple solutions in the repo)\"]],[[0,[],0,\"On the VS Build task, edit the MSBuild Arguments parameter to be /p:DeployOnBuild=true /p:PublishProfile=Release /p:PackageLocation=$(build.artifactstagingdirectory)\"]],[[0,[],0,\"This tells MSBuild to publish the site using the profile called Release (or whatever name you used for the publish profile you created) and place the package in the build artifact staging directory\"]]]],[10,13],[3,\"ul\",[[[0,[],0,\"Now you should put in all your code analysis and test tasks – I’m omitting them for brevity\"]],[[0,[],0,\"The final task should be to publish the artifact staging directory, which at this time contains the WebDeploy package for your site\"]]]],[10,14],[1,\"p\",[[0,[],0,\"Run the build. When complete, the build drop should contain the site zip and SetParameters.xml file (as well as some other files):\"]]],[10,15],[1,\"p\",[[0,[],0,\"You now have a build that is potentially deployable to multiple environments.\"]]],[1,\"h2\",[[0,[],0,\"Deep Dive: Release\"]]],[1,\"p\",[[0,[],0,\"In order for the release to work correctly, you’ll need to install some extensions from the Marketplace. If you’re targeting IIS, you need to install the \"],[0,[5],1,\"IIS Web App Deployment Using WinRM\"],[0,[],0,\" Extension. For both IIS and Azure deployments, you’ll need the ReplaceTokens task from \"],[0,[2],1,\"my custom build tasks extension\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"There are a couple of ways you can push the WebDeploy package to IIS:\"]]],[3,\"ul\",[[[0,[],0,\"Use the IIS Web App Deployment using WinRM task. This is fairly easy to use, \"],[0,[3],1,\"but requires that you copy the zip and SetParameters files to the target server deploying\"],[0,[],0,\".\"]],[[0,[],0,\"Use the cmd file that gets generated with the zip and SetParameters files to deploy remotely. This requires you to know the cmd parameters and to have the WebDeploy remote agent running on the target server.\"]]]],[1,\"p\",[[0,[],0,\"I recommend the IIS task generally – unless for some or other reason you don’t want to open up WinRM.\"]]],[1,\"p\",[[0,[],0,\"So there’s some configuration required on the target IIS server:\"]]],[3,\"ul\",[[[0,[],0,\"Install WebDeploy\"]],[[0,[],0,\"Install WebDeploy Remote Agent – for using the cmd. Note: if you install via Web Platform Installer you’ll need to go to Programs and Features and Modify the existing install, since the remote agent isn’t configured when installing WebDeploy via WPI\"]],[[0,[],0,\"Configure WinRM – for the IIS task. You can run “winrm quickconfig” to get the service started. If you need to deploy using certificates, then you’ll have to configure that too (I won’t cover that here)\"]],[[0,[],0,\"Firewall – remember to open ports for WinRM (5895 or 5986 for default WinRM HTTP or HTTPS respectively) or 8172 for the WebDeploy remote agent (again, this is the default port)\"]],[[0,[],0,\"Create a service account that has permissions to copy files and “do stuff” in IIS – I usually recommend that this user account be a local admin on the target server\"]]]],[1,\"p\",[[0,[],0,\"Once you’ve done that, you can create the Release Definition. Create a new release and specify the build as the primary artifact source. For this example I’m using the IIS task to deploy (and create the site in IIS – this is optional). Here are the tasks you’ll need and their configs:\"]]],[3,\"ul\",[[[0,[],0,\"Replace Tokens Task\"]],[[0,[],0,\"Source Path – the path to the folder that contains the SetParameters.xml file within the drop\"]],[[0,[],0,\"Target File Pattern – set to *.SetParameters.xml\"]]]],[10,16],[3,\"ul\",[[[0,[],0,\"Windows Machine File Copy Task\"]],[[0,[],0,\"Copy the drop folder (containing the SetParameters.xml and website zip files) to a temp folder on the target server. Use the credentials of the service account you created earlier. I recommend using variables for all the settings.\"]]]],[10,17],[3,\"ul\",[[[0,[],0,\"(Optional) WinRM – IIS Web App Management Task\"]],[[0,[],0,\"Use this task to create (or update) the site in IIS, including the physical path, host name, ports and app pool. If you have an existing site that you don’t want to mess with, then skip this task.\"]]]],[10,18],[3,\"ul\",[[[0,[],0,\"WinRM – IIS Web App Deployment Task\"]],[[0,[],0,\"This task takes the (local) path of the site zip file and the SetParameters.xml file and deploys to the target IIS site.\"]]]],[10,19],[3,\"ul\",[[[0,[],0,\"You can supply extra WebDeploy args if you like – there are some other interesting switches under the MSDeploy Additional Options section.\"]]]],[1,\"p\",[[0,[],0,\"Finally, open the Environment variables and supply name/value pairs for the values you want injected. In the example I’ve been using, I’ve got a token for Environment and then I have a tokenized connection string with tokens for server, database, user and password. These are the variables that the Replace Tokens task uses to inject the real environment-specific values into the SetParameters file (in place of the tokens). When WebDeploy runs, it transforms the web.config in the zip file with the values that are in the SetParameters.xml file. Here you can see the variables:\"]]],[10,20],[1,\"p\",[[0,[],0,\"You’ll notice that I also created variables for the Webserver name and admin credentials that the IIS and Copy Files tasks use.\"]]],[1,\"p\",[[0,[],0,\"You can of course do other things in the release – like run integration tests or UI tests. Again for brevity I’m skipping those tasks. Also remember to make the agent queue for the environment one that has an agent that can reach the target IIS server for that environment. For example I have an agent queue called “webdeploy” with an agent that can reach my IIS server:\"]]],[10,21],[1,\"p\",[[0,[],0,\"I’m now ready to run the deployment. After creating a release, I can see that the tasks completed successfully! Of course the web.config is correct on the target server too.\"]]],[10,22],[1,\"h2\",[[0,[],0,\"Deploying to Azure\"]]],[1,\"p\",[[0,[],0,\"As I’ve noted previously if you’re deploying to Azure, you can put all the configuration into the ARM template (see an example \"],[0,[6],1,\"here\"],[0,[],0,\" – note how the connection strings and Application Insights appSettings are configured on the web application resource). That means you don’t need the publish profile or parameters.xml file. You’ll follow exactly the same process for the build (just don’t specify the PublishProfile argument). The release is a bit easier too – you first deploy the resource group using the Azure Deployment: Create or Update Resource Group task like so:\"]]],[10,23],[1,\"p\",[[0,[],0,\"You can see how I override the template parameters – that’s how you “inject” environment specific values.\"]]],[1,\"p\",[[0,[],0,\"Then you use a Deploy AzureRM Web App task (no need to copy files anywhere) to deploy the web app like so:\"]]],[10,24],[1,\"p\",[[0,[],0,\"I specify the Azure Subscription – this is an Azure ARM service endpoint that I’ve preconfigured – and then the website name and optionally the deployment slot. Here I am deploying to the Dev slot – there are a couple extensions in the marketplace that allow you to swap slots (usually after you’ve smoke-tested the non-prod slot to warm it up and ensure it’s working correctly). This allows you to have zero downtime. The important bit here is the Package or Folder argument – this is where you’ll specify the path to the zip file.\"]]],[1,\"p\",[[0,[],0,\"Of course if you don’t have the configuration in an ARM template, then you can just skip the ARM deployment task and run the Deploy AzureRM Web App task. There is a parameter called SetParameters file (my \"],[0,[7],1,\"contribution\"],[0,[],0,\" to this task!) that allows you to specify the SetParameters file. You’ll need to do a Replace Tokens task prior to this to make sure that environment specific values are injected.\"]]],[1,\"p\",[[0,[],0,\"For a complete walkthrough of deploying a Web App to Azure with an ARM template, look at this \"],[0,[8],1,\"hands-on-lab\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Once you understand the pieces involved in building, packaging and deploying web applications, you can fairly easily manage configuration without duplicating yourself – including connection strings, appSettings and any other config – using a publish profile and a parameters.xml file. Then using marketplace extensions, you can build, version, test and package the site. Finally, in Release Management you can inject environment specific values for your tokens and WebDeploy to IIS or to Azure.\"]]],[1,\"p\",[[0,[],0,\"Happy deploying!\"]]]]}","published_at":1477103539000,"status":"published","published_by":1},{"id":"51f7f99f-78a2-4844-9aa9-340f86b3483d","title":"Enterprise GitHub","slug":"enterprise-github","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://news.microsoft.com/2018/06/04/microsoft-to-acquire-github-for-7-5-billion/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/devops/repos/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/devops/boards/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/devops/test-plans/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/devops/pipelines/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/devops/artifacts/\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/features/actions\"]],[\"a\",[\"href\",\"https://www.techrepublic.com/article/microsoft-may-be-the-worlds-largest-open-source-contributor-but-developers-dont-yet-care/\"]],[\"a\",[\"href\",\"https://libgit2.org/\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/modernizing-source-control---migrating-to-git\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/repos/tfvc/add-check-policies?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/boards/queries/wiql-syntax?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/project/search/overview?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/report/powerbi/create-quick-report-odataq?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/report/powerbi/what-is-analytics?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/process/stages?view=azure-devops&tabs=yaml\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/artifacts/get-started-nuget?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/artifacts/get-started-npm?view=azure-devops&tabs=windows\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/artifacts/get-started-maven?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/artifacts/quickstarts/python-packages?view=azure-devops\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/artifacts/quickstarts/universal-packages?view=azure-devops&tabs=azuredevops\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Since \"],[0,[0],1,\"Microsoft acquired GitHub\"],[0,[],0,\", and the anti-Microsoft folks had calmed down, there have been a number of interesting developments in the GitHub ecosystem. If you’ve ever read one of my blogs or attended any events that I’ve spoken at, you’ll know that I am a raving Azure DevOps fan. I do, however, also have several \"],[0,[1],1,\"repos on GitHub\"],[0,[],0,\". As a DevOpsologist (someone who is in a constant state of learning about DevOps) I haven’t ever recommended GitHub for Enterprises – but the lines of functionality are starting to blur between GitHub and Azure DevOps. So which should you use, and when?\"]]],[1,\"p\",[[0,[],0,\"One note before we launch in: Azure DevOps is the name of the suite of functionality for the Microsoft DevOps platform. There are a number of “verticals” within the suite, which you can mix and match according to your needs. \"],[0,[2],1,\"Azure Repos\"],[0,[],0,\" is the source control feature, \"],[0,[3],1,\"Azure Boards\"],[0,[],0,\" the project management feature, \"],[0,[4],1,\"Azure Test Plans\"],[0,[],0,\" the manual test management feature, \"],[0,[5],1,\"Azure Pipelines\"],[0,[],0,\" the Continuous Integration/Continuous Deployment (CI/CD) feature and \"],[0,[6],1,\"Azure Artifacts\"],[0,[],0,\", the package management feature.\"]]],[1,\"h2\",[[0,[],0,\"Blurring the Line\"]]],[1,\"p\",[[0,[],0,\"Up until a couple years ago, the line between GitHub and Azure DevOps was fairly simple to me: if you’re doing open-source development, use GitHub. If you’re doing Enterprise development, use Azure DevOps. The primary reasons were that GitHub was mainly just source control with some basic issue tracking, and you got unlimited \"],[0,[7],1,\"public\"],[0,[],0,\" repos (at least, private repos were a lot more expensive). Azure DevOps (or Visual Studio Team Services, VSTS, as it used to be called), on the other hand, offered Project Management, CI/CD, Test Management and Package Management in addition to unlimited \"],[0,[7],1,\"private\"],[0,[],0,\" repos. However, now you can now create private repos inexpensively in GitHub and you can create public repos in Azure DevOps. And last week, GitHub announced an update to \"],[0,[8],1,\"GitHub Actions\"],[0,[],0,\" that enables first-class CI, which conceivably will expand to CD fairly soon. While you can’t do manual Test Management using GitHub, you get a far better “InnerSource” experience in GitHub than you do in Azure DevOps.\"]]],[1,\"h2\",[[0,[],0,\"This Shouldn’t Be A Surprise\"]]],[1,\"p\",[[0,[],0,\"This isn’t entirely apples-to-apples, because (to oversimplify a bit) Azure DevOps is a DevOps platform, while GitHub is primarily a source control platform. At least, that’s been my experience up until recently. GitHub has some good, basic project management capabilities that work fantastically for open source development, but is a little simplistic for Enterprise development. As the industry shifts more and more to automated testing over manual testing, fewer teams have a need to manage manual testing. While you can publish releases on GitHub repos, Azure Artifacts arguably offers a more feature-rich service for package management. Also, while Azure DevOps, when it was still Team Foundation Server (TFS), used to be a “better together” suite, where you were probably better off doing everything in TFS, Azure DevOps is embracing the fact that developers have diverse toolsets, languages, platforms and target platforms. You can now very easily have source code in GitHub, Project Management in Jira, CI/CD in Azure Pipelines and have good visibility and traceability end-to-end.\"]]],[1,\"p\",[[0,[],0,\"We shouldn’t be surprised by the blur between GitHub and Azure DevOps. After all, Microsoft owns both now. And I think acquiring GitHub was an astute move by the tech giant – because, \"],[0,[7],1,\"if you win the developer, you’re likely to win the organization\"],[0,[],0,\". The perception of a “big bad Microsoft” is rapidly changing. Even before the GitHub acquisition, Microsoft employees were the \"],[0,[9],1,\"top contributors\"],[0,[],0,\" to open source projects in GitHub. So not only is Microsoft embracing open source more and more, but they purchased the premier open source platform in the world!\"]]],[1,\"p\",[[0,[],0,\"The question then becomes: Where is Microsoft focusing? Are you better off in GitHub or in Azure DevOps, or some Frankenstein mix of the two? Will Azure Repos continue to evolve, or will Microsoft “Silverlight” Azure Repos? If I could gaze into a crystal ball, I’d predict that in 3 – 5 years, most organizations will have source code in GitHub and utilize Azure DevOps for Project Management, Pipelines and Package Management. Disclaimer: this is pure conjecture on my part!\"]]],[1,\"h2\",[[0,[],0,\"Azure DevOps and GitHub Head to Head\"]]],[1,\"p\",[[0,[],0,\"The blurred lines between GitHub and Azure DevOps should be cause for celebration, not consternation. It just means that we have more options! And options are good, if you consider them carefully and don’t make hype-based decisions. So let’s compare Azure DevOps and GitHub head to head in the realms of Source Control, Project Management, CI/CD and Package Management.\"]]],[1,\"h3\",[[0,[],0,\"Source Control\"]]],[1,\"p\",[[0,[],0,\"There’s not much difference as far as source control management goes between Azure Repos and GitHub, if you’re talking about Git. Fortunately, when the Azure DevOps team decided to add distributed version control to Azure DevOps, they just added Git. Not some funky Microsoft version of Git (though they contribute actively to Git and specifically to \"],[0,[10],1,\"libgit2\"],[0,[],0,\", the core Git libraries). So if you have a Git repo, you can add a GitHub remote, or an Azure DevOps remote, or both. Both are just Git remote repositories. However, if you want centralized source control (\"],[0,[11],1,\"don’t do this any more\"],[0,[],0,\") then you have to go with Azure DevOps. I would argue that the Pull Request experience is slightly better in Azure DevOps, but not by much. Both platforms allow you to protect branches using policies, so not much difference there either. Both platforms have WebHooks that you can use to trigger custom actions off events. Both have APIs for interaction. GitHub Enterprise has pre-receive hooks that can validate code before it is actually imported into the repo. Azure DevOps has a similar mechanism for centralized version control with \"],[0,[12],1,\"Check-In policies\"],[0,[],0,\", but these do not work for Git repos. We’ll call this one a tie.\"]]],[1,\"h3\",[[0,[],0,\"Project Management\"]]],[1,\"p\",[[0,[],0,\"Azure Boards has a better Enterprise Project Management story. With GitHub you get Issues and Pull Requests (PRs) as the base “work item” types. You can add Task Lists to Issues, but the overall forms and flows of Issues and PRs is basic. Azure Boards work items can be a lot more complex, but offer much more customization opportunities. You can also do portfolio management more effectively in Azure Boards, since you can create work item hierarchies. GitHub does have the notion of Milestones and Projects, but again the functionality is fairly basic and probably too simplistic for Enterprises. While you can create basic filters for work items in GitHub, Azure DevOps has an advanced \"],[0,[13],1,\"Work Item Query Language\"],[0,[],0,\" and \"],[0,[14],1,\"elastic search\"],[0,[],0,\". Both platforms allow you to tag (or label) work items. Azure Boards also lets you create widgets and dashboards and even has an \"],[0,[15],1,\"OData feed\"],[0,[],0,\" and an \"],[0,[16],1,\"Analytics Service\"],[0,[],0,\" so that you can create reports (say from PowerBI) over your work items. Of course you could use neither system for Project Management, you could use Jira, integrating Jira tickets easily to both Azure Repos (and Pipelines) or GitHub.\"]]],[1,\"p\",[[0,[],0,\"In terms of Enterprise project management, I’d have to give this one to Azure Boards.\"]]],[1,\"h3\",[[0,[],0,\"CI/CD\"]]],[1,\"p\",[[0,[],0,\"GitHub introduced GitHub actions about a year ago. Hundreds of Actions were created by the community, validating the demand for actions triggered off events on a repo. But it seemed that doing any sort of Enterprise-scale CI with Actions was a challenge. Last week, a new and improved version of GitHub Actions was announced, and now CI is baked into GitHub through GitHub actions. I expect that we’ll see a huge surge in adoption of this CI tool and platforms like CircleCI and other cloud-CI systems may battle to compete. The feature isn’t GA yet, so we’ll see. It’s also suspiciously close to the YML format used by Azure Pipelines and supports Windows, Mac and Linux, just like Azure Pipelines…\"]]],[1,\"p\",[[0,[],0,\"The story doesn’t quite end there – if you want CI for GitHub repos, you now have a choice of GitHub Actions or Azure Pipelines, since Azure Pipelines has native support for GitHub repos. If you have repos outside of GitHub, you can’t use Actions.\"]]],[1,\"p\",[[0,[],0,\"I’d have to give this one to Azure Pipelines, at least for now. Azure Pipelines does include Release Management (for CD) or \"],[0,[17],1,\"multi-stage YML\"],[0,[],0,\" files. I am sure we’ll see similar support soon for Actions, but for now while you can do CI pretty easily in GitHub Actions, CD is going to be a challenge. At this point in time, I’ll give this one to Azure Pipelines.\"]]],[1,\"h3\",[[0,[],0,\"Package Management\"]]],[1,\"p\",[[0,[],0,\"GitHub releases allow you to tag a repo and publish that version of the repo as a release. You can also upload binaries that are the versioned packages for that release. Azure Artifacts allows you to create feeds that can be consumed – you can access a feed using \"],[0,[18],1,\"NuGet\"],[0,[],0,\", \"],[0,[19],1,\"npm\"],[0,[],0,\", \"],[0,[20],1,\"Maven\"],[0,[],0,\", \"],[0,[21],1,\"Python packages\"],[0,[],0,\" or \"],[0,[22],1,\"Universal Packages\"],[0,[],0,\" (which is a feed of arbitrary files – think NuGet for anything). Feeds are usually better than releases since tools like NuGet or npm know how to connect to feeds. Again, in terms of Enterprise package management, this one goes to Azure Artifacts.\"]]],[1,\"h2\",[[0,[],0,\"Just Tell Me Which One To Use Already!\"]]],[1,\"p\",[[0,[],0,\"So the final score is 3.5 to 0.5 for Azure DevOps. But that’s not a full reflection of the situation, so don’t start porting to Azure DevOps just yet. Remember, options are great if you consider them \"],[0,[7],1,\"carefully\"],[0,[],0,\". And they’re not mutually exclusive either. So here is what I think are the key considerations:\"]]],[1,\"h3\",[[0,[],0,\"Unit Of Management: Code or Work Items?\"]]],[1,\"p\",[[0,[],0,\"Do you track your work by looking at work item tracking, reporting and rolling up across your portfolio? Then you probably need to look at Azure Boards, since GitHub Issues are not going to handle complex Enterprise-level portfolio management. However, if your teams operate a bit more independently and you track work by looking at changes to repos, GitHub may be a better fit. Don’t forget that you can still keep source code in GitHub and use Azure Boards for project management!\"]]],[1,\"h3\",[[0,[],0,\"Single Pane of Glass or Bring Your Own Tools?\"]]],[1,\"p\",[[0,[],0,\"I’ve seen Enterprises that are trying to standardize tooling and processes across teams. In this case, since Azure DevOps is a complete end to end platform, you’re probably better off using Azure DevOps. If you prefer a smorgasbord of tools, you could go either way. Even if you are managing work with Jira, building with TeamCity and deploying with Octopus Deploy, Azure DevOps could still tie these tools together to serve as a “backbone” giving you a single pane of glass.\"]]],[1,\"h3\",[[0,[],0,\"Manual Test Management or Centralized Source Control?\"]]],[1,\"p\",[[0,[],0,\"If you need a tool for Manual Test Management, then Azure DevOps is for you. However, you could easily keep source code in GitHub and still use Azure Test Management to manage manual tests. And if you need centralized source control for some reason, then your only option is Azure Repos using Team Foundation Version Control (TFVC).\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"As you can see, there’s a lot of overlap between GitHub and Azure DevOps. Here’s my final prediction – we’ll see more innovation in the source control space in GitHub than we will in Azure Repos. Once again, the disclaimer is that this is my observation, and is in no way based on any official communication from either GitHub or Azure DevOps. I do think that a very viable option for Enterprises in the next few years will be to manage source code in GitHub and use Azure DevOps for CI/CD, Project Management and Package Management. This gives you the best of both worlds – you’re on (arguably) the best source control system – GitHub – and you get Enterprise-grade features for project, build and package management. As GitHub Actions evolves, perhaps CI/CD can be moved over to GitHub too. Yes, it’s still fuzzy even after thinking through all these options!\"]]],[1,\"p\",[[0,[],0,\"In short, think clearly about which platform (or combination of platforms) is going to best suit your Enterprise’s culture. Remember, DevOps is as much about people (culture) as is is about tooling.\"]]],[1,\"p\",[[0,[],0,\"Happy DevOps!\"]]]]}","published_at":1565765651000,"status":"published","published_by":1},{"id":"ad4f8327-ba16-436f-8873-51ca2f31d09e","title":"Error Handling Poor Man’s RunScript in 2012 Builds","slug":"error-handling-poor-mans-runscript-in-2012-builds","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"Param(\\n  [string]$pathToSearch = $env:TF_BUILD_SOURCESDIRECTORY,\\n  [string]$buildNumber = $env:TF_BUILD_BUILDNUMBER,\\n  [string]$searchFilter = \\\"AssemblyInfo.*\\\",\\n  [regex]$pattern = \\\"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\"\\n)\\n\\nif ($buildNumber -match $pattern -ne $true) {\\n    Write-Error \\\"Could not extract a version from [$buildNumber] using pattern [$pattern]\\\"\\n    exit 1\\n} else {\\n    try {\\n        $extractedBuildNumber = $Matches[0]\\n        Write-Host \\\"Using version $extractedBuildNumber\\\"\\n\\n        gci -Path $pathToSearch -Filter $searchFilter -Recurse | %{\\n            Write-Host \\\"  -&gt; Changing $($_.FullName)\\\" \\n        \\n            # remove the read-only bit on the file\\n            sp $_.FullName IsReadOnly $false\\n\\n            # run the regex replace\\n            (gc $_.FullName) | % { $_ -replace $pattern, $extractedBuildNumber } | sc $_.FullName\\n        }\\n\\n        Write-Host \\\"Done!\\\"\\n        exit 0\\n    } catch {\\n        Write-Error $_\\n        exit 1\\n    }\\n}\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-qNekJ3Hour8/UuC6fEQ-p7I/AAAAAAAABNM/M4gYUr9O4x4/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-4tYWfCn9V_I/UuC6f6pcTPI/AAAAAAAABNU/_SeZD3uAPiM/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"353\\\" height=\\\"222\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-MTQdOTgLRDg/UuC6gsK3_II/AAAAAAAABNc/qC-XTk2fIWg/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-ozo4fmKIliU/UuC6hZtLoEI/AAAAAAAABNk/qT4IrldRB30/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"370\\\" height=\\\"206\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/--a1xaEwoCgI/UuC6iVATnlI/AAAAAAAABNs/-ixIhLWldus/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-Q89AqukHG8E/UuC6jJdpQlI/AAAAAAAABN0/vvy9_ZMnmF8/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"479\\\" height=\\\"214\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2014/01/build-script-hooks-for-tfs-2012-builds.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Yesterday I posted about \"],[0,[0],1,\"how to create script hooks in a 2012 build template\"],[0,[],0,\". My colleague Tyler Doerksen commented and pointed out that there was no error handling in my solution.\"]]],[1,\"h2\",[[0,[],0,\"Returning Error Codes from PowerShell\"]]],[1,\"p\",[[0,[],0,\"I knew that if I could get the script to return an error code, it would be simple to add an If activity to check it. The trick is to use “exit 0” for success and “exit 1” for failures. I also changed any error messages from Write-Host to Write-Error so that they go to errOutput and not stdOutput. Here’s the updated “UpdateVersion” script:\"]]],[10,0],[1,\"p\",[[1,[],0,0],[1,[],0,1]]],[1,\"h2\",[[0,[],0,\"Error Handling in the Build\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"Go back to the InvokeProcess activity that calls your script. Go to its parent activity (usually a sequence) and add a variable Int32 called “scriptResult”. On the InvokeProcess, set the result property to “scriptResult”.\"]]],[1,\"p\",[[1,[],0,3]]],[10,1],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"Now you just need to add an If activity below the InvokeProcess that has condition “scriptResult <> 0” and add a Throw in the “Then”. I’m just throwing an Exception with an error message.\"]]],[1,\"p\",[[1,[],0,5]]],[10,2],[1,\"p\",[[0,[],0,\"Here’s the output if the script fails:\"],[1,[],0,6]]],[10,3],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1390495500000,"status":"published","published_by":1},{"id":"e1fbddf0-ba96-4486-bcd5-1a9a6534a4e6","title":"Excel Sheet Showing Parent Items Whose Child Items Are All Closed","slug":"excel-sheet-showing-parent-items-whose-child-items-are-all-closed","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-kBlKYGe6tes/UUxBP4M2IHI/AAAAAAAAAqE/N4-ZHd3b9Qg/s1600-h/image3.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-uu-IUV_rr7s/UUxBRLnsXTI/AAAAAAAAAqM/ZzFU12rNTu4/image_thumb1.png?imgmax=800\\\" width=\\\"473\\\" height=\\\"274\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-EqsAQKRXMf4/UUxBSHdfsaI/AAAAAAAAAqU/rAEWmkmX1Zk/s1600-h/image7.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-GEoCShfkdZo/UUxBTdMHRaI/AAAAAAAAAqc/WsKBJ17MoUE/image_thumb3.png?imgmax=800\\\" width=\\\"638\\\" height=\\\"110\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-CfgYi0KSXnc/UUxBUmO7NGI/AAAAAAAAAqk/eCTA7gWpaOA/s1600-h/image11.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-EK1RTzozD8c/UUxBVi3HG7I/AAAAAAAAAqs/al0OWs_EZno/image_thumb5.png?imgmax=800\\\" width=\\\"584\\\" height=\\\"113\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-4lnO85G6O5g/UUxBWz7sBSI/AAAAAAAAAq0/mL71ifeUQ6Q/s1600-h/image15.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-0_1tZa_2QHE/UUxBYDhzufI/AAAAAAAAAq8/MeghQlIniRk/image_thumb7.png?imgmax=800\\\" width=\\\"528\\\" height=\\\"124\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://sdrv.ms/16MQpQy\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve often had the question from my customers – “I’ve got a bunch of Requirements. Some of them are Active, but all their Child Tasks are Closed. Can TFS automatically close the Parent items? Or can I at least query these Requirements out?”\"]]],[1,\"p\",[[0,[],0,\"You can do this with a server component that will listen to Work Item change events, query the work items and transition the states. Either you’ll download some code someone else wrote or you’ll write it yourself. As for a query, well unfortunately, you can’t do this sort of query in WIQL (to the best of my knowledge). While being really powerful, WIQL does lack things like aggregations and queries that rely on history (like show me all bugs that have been reactivated more than once). You can do these sorts of queries using the TFS API, but then you have to maintain an app and code – it gets messy.\"]]],[1,\"p\",[[0,[],0,\"So I’ve done some Excel “juggling” and enabled you to add some formulas to a work item query that will highlight Parents that need to be closed because all their child Tasks are closed. Unfortunately I can’t give you a simple way of implementing it because Excel doesn’t support copying formulas out of the “special” tables that the TFS Plugin creates for Work Item Queries.\"]]],[1,\"p\",[[0,[],0,\"Here’s the end product, showing an “Action” column that highlight Parent items whose Child Items are all closed, or Child Items that are active whose Parent items are closed:\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Set up a Tree Query with ID, Work Item Type and State\"]]],[1,\"p\",[[0,[],0,\"The first thing you need to do is set up the query that returns the tree of work items. This can be any query that you want, as long as you return the ID, Work Item Type and State columns. The logic I’ll show you requires at least these 3 fields.\"]]],[1,\"p\",[[0,[],0,\"Now open Excel and connect to that query to pull in all your Work Items.\"]]],[1,\"h2\",[[0,[],0,\"The Settings Sheet\"]]],[1,\"p\",[[0,[],0,\"Now add a new Sheet in the workbook called “Settings”. Add the following data (you can open my \"],[0,[0],1,\"sample sheet\"],[0,[],0,\" and copy/paste the values in if you want):\"]]],[10,1],[1,\"p\",[[0,[],0,\"You’ll have to set the data in the columns appropriately:\"]]],[3,\"ul\",[[[0,[],0,\"Parent WIT\"]],[[0,[],0,\"The Work Item types that can be parents (usually the same work item types that you have in your Requirements Category)\"]],[[0,[],0,\"Child WIT\"]],[[0,[],0,\"The child work item type (usually the same work item types that you have in your Task category)\"]],[[0,[],0,\"Parent Active States\"]],[[0,[],0,\"States of the Parent work items that you consider to be “Active” states\"]],[[0,[],0,\"Child Closed States\"]],[[0,[],0,\"States of the Child work items that you consider to be “Closed” states\"]],[[0,[],0,\"Column Count\"]],[[0,[],0,\"When you pull in a Tree Query, Excel creates a column per level in the tree. This figure could therefore change is another level in the tree appears – so just count the columns from your original query and enter the count here.\"]],[[0,[],0,\"ID Column Index\"]],[[0,[],0,\"The 1-based column index of the ID column\"]]]],[1,\"h2\",[[0,[],0,\"Adding in the Formulas\"]]],[1,\"p\",[[0,[],0,\"Now go back to your query sheet. Add the following column headers to the right of your query columns (don’t worry, you’ll hide most of these anyway):\"]]],[3,\"ul\",[[[0,[],0,\"IsParent\"]],[[0,[],0,\"ChildIndex\"]],[[0,[],0,\"ParentID\"]],[[0,[],0,\"ParentActive\"]],[[0,[],0,\"ChildActive\"]],[[0,[],0,\"ParentIDMunge\"]],[[0,[],0,\"IDMunge\"]],[[0,[],0,\"Action\"]]]],[1,\"p\",[[0,[],0,\"Your spreadsheet should look as follows:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Now insert the following formulas into the columns (the 1st one goes into IsParent, the 2nd into ChildIndex and so on):\"]]],[1,\"p\",[[0,[],0,\"=IF(COUNTIF(Setup!$A$2:$A$10,[@[Work Item Type]]), \\\"P\\\", IF([@[Title 1]]=\\\"\\\",\\\"C\\\", \\\"P\\\"))\"]]],[1,\"p\",[[0,[],0,\"=IF(H3=\\\"C\\\", IF(H2=\\\"P\\\", 1, I2+1), \\\"\\\")\"]]],[1,\"p\",[[0,[],0,\"=IF([@IsParent]=\\\"P\\\", \\\"\\\", INDEX([#All],ROW()-[@ChildIndex]-1,Setup!$F$2))\"]]],[1,\"p\",[[0,[],0,\"=IF([@IsParent]=\\\"P\\\",IF(COUNTIF(Setup!$C$2:$C$10,[@State]), \\\"Y\\\", \\\"N\\\"), INDEX([#All],ROW()-[@ChildIndex]-1,4+Setup!$E$2))\"]]],[1,\"p\",[[0,[],0,\"=IF([@IsParent]=\\\"C\\\",IF(COUNTIF(Setup!$D$2:$D$10,[@State]), \\\"N\\\", \\\"Y\\\"), \\\"\\\")\"]]],[1,\"p\",[[0,[],0,\"=IF([@IsParent]=\\\"P\\\", [@ID]&(IF([@ParentActive]=\\\"Y\\\", \\\"N\\\", \\\"Y\\\")), [@ParentId]&[@ChildActive])\"]]],[1,\"p\",[[0,[],0,\"=IF([@IsParent]=\\\"P\\\", [@ID]&[@ParentActive], [@ParentId]&[@ChildActive])\"]]],[1,\"p\",[[0,[],0,\"=IF(AND(COUNTIF([ParentIDMunge], [@ParentIDMunge])>1, COUNTIF([IDMunge], [@IDMunge])=1), IF([@IsParent]=\\\"P\\\",IF([@ParentActive]=\\\"Y\\\", \\\"Close Me (all children closed)\\\", \\\"\\\"), \\\"\\\"), IF(AND([@ChildActive]=\\\"Y\\\", [@ParentActive]=\\\"N\\\"), \\\"Close Me (Parent is closed)\\\",\\\"\\\"))\"]]],[1,\"p\",[[0,[],0,\"Your spreadsheet should look something like this:\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Clean Up\"]]],[1,\"p\",[[0,[],0,\"Finally, to clean up a bit, select the columns from IsParent to IDMunge and hide them, leaving only the Action column visible. You can of course filter this column to not show blanks and so on.\"]]],[1,\"p\",[[0,[],0,\"Perhaps there’s better ways of doing this – the primary difficulty in these formulas was the fact that you’re not only querying the row you’re in, but also the row’s “parent row” (if it has one). If you can see better ways of doing this, then please add a comment!\"]]],[1,\"p\",[[0,[],0,\"Here’s my \"],[0,[0],1,\"sample sheet\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Happy querying!\"]]]]}","published_at":1363984320000,"status":"published","published_by":1},{"id":"021cadfa-cd44-495e-a030-473ad226f00f","title":"Executing JMeter Tests in an Azure Pipeline","slug":"executing-jmeter-tests-in-an-azure-pipeline","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/383e1f2b-79bb-470a-95b7-3c5ad81e9a4e.png\\\" target=\\\"_blank\\\"><img width=\\\"572\\\" height=\\\"166\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4359d3c5-a86f-4417-96bf-4479c18de9ce.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9d59db6f-02f7-4e70-9e06-acd413a44420.png\\\" target=\\\"_blank\\\"><img width=\\\"576\\\" height=\\\"306\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/dfe7f36d-bb44-4c5a-b841-d2d57b8d3b21.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">jmeter -n -t CartTest.jmx -l results.jtl -Jhost=cdpartsun2-dev.azurewebsites.net –j jmeter.log –e –o reports</font>\"}],[\"code\",{\"code\":\"#!/bin/bash\\n#\\n# Run JMeter Docker image with options\\n\\nNAME=\\\"jmetertest\\\"\\nIMAGE=\\\"justb4/jmeter:latest\\\"\\nROOTPATH=$1\\n\\necho \\\"$ROOTPATH\\\"\\n# Finally run\\ndocker stop $NAME &gt; /dev/null 2&gt;&amp;1\\ndocker rm $NAME &gt; /dev/null 2&gt;&amp;1\\ndocker run --name $NAME -i -v $ROOTPATH:/test -w /test $IMAGE ${@:2}\\n\",\"language\":\"bash;\"}],[\"code\",{\"code\":\"#!/bin/bash\\n#\\nrootPath=$1\\ntestFile=$2\\nhost=$3\\n\\necho \\\"Root path: $rootPath\\\"\\necho \\\"Test file: $testFile\\\"\\necho \\\"Host: $host\\\"\\n\\nT_DIR=.\\n\\n# Reporting dir: start fresh\\nR_DIR=$T_DIR/report\\nrm -rf $R_DIR &gt; /dev/null 2&gt;&amp;1\\nmkdir -p $R_DIR\\n\\nrm -f $T_DIR/test-plan.jtl $T_DIR/jmeter.log  &gt; /dev/null 2&gt;&amp;1\\n\\n./run.sh $rootPath -Dlog_level.jmeter=DEBUG \\\\\\n\\t-Jhost=$host \\\\\\n\\t-n -t /test/$testFile -l $T_DIR/test-plan.jtl -j $T_DIR/jmeter.log \\\\\\n\\t-e -o $R_DIR\\n\\necho \\\"==== jmeter.log ====\\\"\\ncat $T_DIR/jmeter.log\\n\\necho \\\"==== Raw Test Report ====\\\"\\ncat $T_DIR/test-plan.jtl\\n\\necho \\\"==== HTML Test Report ====\\\"\\necho \\\"See HTML test report in $R_DIR/index.html\\\"\\n\",\"language\":\"bash;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">./test.sh $PWD CartTest.jmx cdpartsun2-dev.azurewebsites.net</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">./test.sh 'C:\\\\repos\\\\10m\\\\partsunlimited\\\\jmeter' CartTest.jmx cdpartsun2-dev.azurewebsites.net</font>\"}],[\"code\",{\"code\":\"variables:\\n  host: cdpartsun2-dev.azurewebsites.net\\n\\njobs:\\n- job: jmeter\\n  pool:\\n    vmImage: ubuntu-latest\\n  displayName: Run JMeter tests\\n  steps:\\n  - task: Bash@3\\n    displayName: Execute JMeter tests\\n    inputs:\\n      targetType: filePath\\n      filePath: 'jmeter/test.sh'\\n      arguments: '$PWD CartTest.jmx $(host)'\\n      workingDirectory: jmeter\\n      failOnStderr: true\\n  - task: PublishPipelineArtifact@1\\n    displayName: Publish JMeter Report\\n    inputs:\\n      targetPath: jmeter/report\\n      artifact: jmeter\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8d34e502-8ec6-40cb-a6bc-9749ecc61b6b.png\\\" target=\\\"_blank\\\"><img width=\\\"601\\\" height=\\\"373\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9a0be48f-24f9-49b0-b18b-88d91491d76f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">git update-index --chmod=+x test.sh</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">git update-index --chmod=+x run.sh</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3f644203-2dc7-403c-bb3e-a0bd4a4a47fb.png\\\" target=\\\"_blank\\\"><img width=\\\"643\\\" height=\\\"241\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/15731b4e-5e66-4b24-bc2a-98077b02b689.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f20ace7b-cab9-4c7c-b98e-77e64d6967aa.png\\\" target=\\\"_blank\\\"><img width=\\\"648\\\" height=\\\"359\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b9d81a3e-d21e-429f-b612-27893b42205c.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/test/load-test/getting-started-with-performance-testing?view=azure-devops\"]],[\"a\",[\"href\",\"http://jmeter.apache.org/\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/appinsights-analytics-in-the-real-world\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/testing-in-production-routing-traffic-during-a-release\"]],[\"a\",[\"href\",\"https://www.blazemeter.com/\"]],[\"a\",[\"href\",\"https://guide.blazemeter.com/hc/en-us/articles/360004191957-Testing-via-Azure-DevOps-Pipeline-Testing-via-Azure-DevOps-Pipeline\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/java/labs/?view=azure-devops\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/jmeter-azpipeline\"]],[\"a\",[\"href\",\"https://github.com/justb4/docker-jmeter\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Microsoft have deprecated Load Testing in Visual Studio. Along with this, they have also \"],[0,[0],1,\"deprecated the cloud load testing\"],[0,[],0,\" capability in Azure/Azure DevOps. On the official \"],[0,[0],1,\"alternatives document\"],[0,[],0,\", several alternative load testing tools and platforms are mentioned, including \"],[0,[1],1,\"JMeter\"],[0,[],0,\". What is not clear from this page is how exactly you’re supposed to integrate JMeter into your pipelines.\"]]],[1,\"p\",[[0,[],0,\"I have a demo that shows how you can use Application Insights to provide \"],[0,[2],1,\"business telemetry\"],[0,[],0,\". In the demo, I update a website (PartsUnlimited) and then use traffic routing to \"],[0,[3],1,\"route 20% of traffic to a canary slot\"],[0,[],0,\". To simulate traffic, I run a cloud load test. Unfortunately, I won’t be able to use that for much longer since the cloud load test functionality will end of life soon! I set about figuring out how to run this test using JMeter.\"]]],[1,\"p\",[[0,[],0,\"JMeter tests can be run on a platform called \"],[0,[4],1,\"BlazeMeter\"],[0,[],0,\". BlazeMeter has \"],[0,[5],1,\"integration with Azure DevOps\"],[0,[],0,\". However, I wanted to see if I could get a solution that didn’t require a subscription service.\"]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"JMeter is a Java-based application. I’m not a big fan of Java – even though I authored a lot of the \"],[0,[6],1,\"Java Hands on Labs for Azure DevOps\"],[0,[],0,\"! I had to install the JRE on my machine in order to open the JMeter GUI so that I could author my test. However, I didn’t want to have to install Java or JMeter on the build agent – so of course I looked to Docker. And it turns out that you can run JMeter tests in a Docker container pretty easily!\"]]],[1,\"p\",[[0,[],0,\"To summarize the solution:\"]]],[3,\"ol\",[[[0,[],0,\"Create your JMeter test plans (and supporting files like CSV files) and put them into a folder in your repo\"]],[[0,[],0,\"Create a “run.sh” file that launches the Docker image and runs the tests\"]],[[0,[],0,\"Create a “test.sh” file for each JMeter test plan – this just calls “run.sh” passing in the test plan and any parameters\"]],[[0,[],0,\"Publish the “reports” directory for post-run analysis\"]]]],[1,\"p\",[[0,[],0,\"I’ve created a \"],[0,[7],1,\"GitHub repo\"],[0,[],0,\" that has the source code for this example.\"]]],[1,\"h2\",[[0,[],0,\"Test Plan Considerations\"]]],[1,\"p\",[[0,[],0,\"I won’t go over recording and creating a JMeter test in this post – I assume that you have a JMeter test ready to go. I do, however, want to discuss parameters and data files.\"]]],[1,\"p\",[[0,[],0,\"It’s common to have some parameters for your test plans. For example, you may want to run the same test plan against multiple sites – DEV or STAGING for example. In this case you can specify a parameter called “host” that you can specify when you run the test. To access parameters in JMeter, you have to use the parameter function, “__P”. JMeter distinguishes between parameters and variables, so you can have both a variable and a parameter of the same name.\"]]],[1,\"p\",[[0,[],0,\"In the figure below, I have a test plan called CartTest.jmx where I specify a User Defined Variable (UDV) called “host”. I use the parameter function to read the parameter value if it exists, or default to “cdpartsun2-prod.azurewebsites.net” if the parameter does not exist:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The value of the host UDV is “${__P(host,cdpartsun2-prod.azurewebsites.net)}”. Of course you can use the __P function wherever you need it – not just for UDVs.\"]]],[1,\"p\",[[0,[],0,\"In my test plan, I also have a CSV for test data. I set the path of this file as a relative path to the JMX file:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now that I have the test plan and supporting data files, I am ready to script test execution. Before we get to running the test in a container, let’s see how I can run the test from the command line. I simply execute this command from within the folder containing the JMX file:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"-n tells JMeter to run in non-GUI mode\"]],[[0,[],0,\"-t specifies the path to the test plan\"]],[[0,[],0,\"-l specifies the path to output results to\"]],[[0,[],0,\"-J<name>=<value> is how I pass in parameters; there may be multiple of these\"]],[[0,[],0,\"-j specifies the path to the log file\"]],[[0,[],0,\"-e specifies that JMeter should produce a report\"]],[[0,[],0,\"-o specifies the report folder location\"]]]],[1,\"p\",[[0,[],0,\"We now have all the pieces to script this into a pipeline! Let’s encapsulate some of this logic into two scripts: “run.sh” which will launch a Docker container and execute a test plan, and “test.sh” that is a wrapper for executing the CartTest.jmx file.\"]]],[1,\"h3\",[[0,[],0,\"run.sh\"]]],[1,\"p\",[[0,[],0,\"I based this script off this \"],[0,[8],1,\"GitHub repo\"],[0,[],0,\" by Just van den Broecke.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"The NAME variable is the name of the container instance\"]],[[0,[],0,\"The IMAGE is the container image to launch – in this case “justb4/jmeter:latest” – this container includes Java and JMeter, as well as an entrypoint that launches a JMeter test\"]],[[0,[],0,\"ROOTPATH is the first arg to the script and is the path that contains the JMeter test plan and data files\"]],[[0,[],0,\"The script stops any running instance of the container, and then deletes it\"]],[[0,[],0,\"The final line of the script runs a new instance of the container, mapping a volume from “ROOTPATH” on the host machine to a folder in the container called “/test” and then passes in remaining parameters (skipping ROOTPATH) as arguments to the entrypoint of the script. These are the JMeter test arguments.\"]]]],[1,\"h3\",[[0,[],0,\"test.sh\"]]],[1,\"p\",[[0,[],0,\"Now we have a generic way to launch the container, map the files and run the tests. Let’s wrap this call into a script for executing the CartTest.jmx test plan:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 3-5: We need 3 args: the rootPath on the host containing the test plan, the name of the test plan (the jmx file) and a host parameter, which is specific to this test plan\"]],[[0,[],0,\"Line 9: set the T_DIR to the current directory\"]],[[0,[],0,\"Lines 14-16: Create a report directory, cleaning it if it exists already\"]],[[0,[],0,\"Line 18: Clear previous result files\"]],[[0,[],0,\"Lines 20-23: Call run.sh, passing in the rootPath and all the other JMeter args and parameters we need to invoke the test\"]],[[0,[],0,\"Lines 22-29: Echo the location of the log, raw report and HTML reports\"]]]],[1,\"p\",[[0,[],0,\"As long as we have Docker, we can run the script and we don’t need to install Java or JMeter!\"]]],[1,\"p\",[[0,[],0,\"We can execute the test from bash like so:\"]]],[10,5],[1,\"h3\",[[0,[],0,\"WSL Gotcha\"]]],[1,\"p\",[[0,[],0,\"One caveat for Windows Subsystem for Linux (WSL): $PWD will not work for the volume mapping. This is because Docker for Windows is running on Windows, while the WSL paths are mounted in the Linux subsystem. In my case, the folder in WSL is “/mnt/c/repos/10m/partsunlimited/jmeter”, while the folder in Windows is “c:\\\\repos\\\\10m\\\\partsunlimited\\\\jmeter”. It took me a while to figure this out – the volume mapping works, but the volume is always empty. To work around this, just pass in the Windows path instead:\"]]],[10,6],[1,\"h2\",[[0,[],0,\"Executing from a Pipeline\"]]],[1,\"p\",[[0,[],0,\"We’ve done most of the hard work – now we can put the script into a pipeline. We need to execute the test script with the correct arguments and upload the test results and we’re done! Here’s the pipeline:\"]]],[10,7],[1,\"p\",[[0,[],0,\"This is very simple – and we don’t even have to worry about installing Java or JMeter – the only prerequisite we have is that the agent is able to run Docker containers! The first step executes the test.sh script, passing in the arguments just like we did from in the console. The second task publishes the report folder so that we can analyze the run.\"]]],[1,\"p\",[[0,[],0,\"Here’s a snippet of the log while the test is executing: we can see the download of the Docker image and the boot up – now we just wait for the test to complete.\"]]],[10,8],[1,\"h3\",[[0,[],0,\"Executable Attributes\"]]],[1,\"p\",[[0,[],0,\"One quick note: initially when I committed the scripts to the repo, they didn’t have the executable attribute set – this caused the build to fail because the scripts were not executable. To set the executable attribute, I ran the following command in the folder containing the sh files:\"]]],[10,9],[10,10],[1,\"p\",[[0,[],0,\"Once the build completes, we can download the report file and analyze the test run:\"]]],[10,11],[10,12],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Once you have a JMeter test, it’s fairly simple to run the it in a Docker container as part of your build (or release) process. Of course this doesn’t test load from multiple locations and is limited to the amount of threads the agent can spin up, but for quick performance metrics it’s a clean and easy way to execute load tests. Add to that the powerful GUI authoring capabilities of JMeter and you have a good performance testing platform.\"]]],[1,\"p\",[[0,[],0,\"Happy load testing!\"]]]]}","published_at":1581686431000,"status":"published","published_by":1},{"id":"109d3c9c-bcae-42b6-b03e-cd47bffecde3","title":"Extending Hybrid Lab Workflow Virtual Hosts","slug":"extending-hybrid-lab-workflow-virtual-hosts","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"namespace HybridLab.Virtual.Interfaces<br>{<br>    internal class VirtualHostContainer<br>    {<br>        [ImportMany(typeof(IVirtualHost))]<br>        private IEnumerable<ivirtualhost> VirtualHosts;<br><br>        internal VirtualHostContainer()<br>        {<br>            var path = Path.GetDirectoryName(Assembly.GetExecutingAssembly().Location);<br>            var dlls = new DirectoryInfo(path).GetFileSystemInfos(\\\"*.dll\\\");<br>            var catalog = new AggregateCatalog();<br>            foreach (var dll in dlls.Where(d =&gt; !(d.Name.StartsWith(\\\"System\\\") || d.Name.StartsWith(\\\"Microsoft\\\"))))<br>            {<br>                try<br>                {<br>                    var asm = Assembly.LoadFrom(dll.FullName);<br>                    var assemblyCatalog = new AssemblyCatalog(asm);<br>                    if (assemblyCatalog.Parts.Count() &gt; 0)<br>                    {<br>                        catalog.Catalogs.Add(assemblyCatalog);<br>                    }<br>                }<br>                catch (Exception)<br>                {<br>                }<br>            }<br>            new CompositionContainer(catalog).SatisfyImportsOnce(this);<br>        }<br><br>        internal List<string> GetHostTypes()<br>        {<br>            return (from h in VirtualHosts<br>                    select h.HostType).ToList();<br>        }<br><br>        internal IVirtualHost GetHost(string hostType, string hostName, string domain = \\\"\\\", string userName = \\\"\\\", string password = \\\"\\\")<br>        {<br>            var host = VirtualHosts.Single(h =&gt; h.HostType == hostType);<br>            host.Connect(hostName, domain, userName, password);<br>            return host;<br>        }<br>    }<br>}<br></string></ivirtualhost>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"namespace HybridLab.Virtual.Interfaces<br>{<br>    public sealed class VirtualHostCatalog<br>    {<br>        public static List<string> GetHostTypes()<br>        {<br>            return new VirtualHostContainer().GetHostTypes();<br>        }<br><br>        public static IVirtualHost GetHost(string hostType, string hostName, string domain = \\\"\\\", string userName = \\\"\\\", string password = \\\"\\\")<br>        {<br>            return new VirtualHostContainer().GetHost(hostType, hostName, domain, userName, password);<br>        }<br>    }<br>}<br><br></string>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"namespace HybridLab.Virtual.Interfaces<br>{<br>    public interface IVirtualHost<br>    {<br>        string HostName { get; }<br>        string Domain { get; }<br>        string UserName { get; }<br>        string Password { get; }<br>        string HostType { get; }<br>        <br>        void Connect(string hostName, string domain, string userName, string password);<br><br>        List<ivirtualmachine> GetVMs();<br>    }<br>}<br></ivirtualmachine>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"namespace HybridLab.Virtual.Interfaces<br>{<br>    public abstract class HostBase : IVirtualHost<br>    {<br>        public string HostName { get; protected set; }<br>        public string Domain   { get; protected set; }<br>        public string UserName { get; protected set; }<br>        public string Password { get; protected set; }<br><br>        public virtual string HostType { get; protected set; }<br><br>        public virtual void Connect(string hostName, string domain, string userName, string password)<br>        {<br>            HostName = hostName;<br>            Domain = domain;<br>            UserName = userName;<br>            Password = password;<br>        }<br><br>        public virtual List<ivirtualmachine> GetVMs()<br>        {<br>            throw new NotImplementedException();<br>        }<br>    }<br>}<br></ivirtualmachine>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"namespace HybridLab.Virtual.Interfaces<br>{<br>    public interface IVirtualMachine<br>    {<br>        void ApplySnapshot(string snapshotName);<br><br>        void CreateSnapshot(string snapshotPrefix);<br><br>        IVirtualHost Host { get; }<br><br>        string Name { get; }<br><br>        string DnsName { get; }<br><br>        List<string> Snapshots { get; }<br>    }<br>}<br></string>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"namespace HybridLab.Virtual.TestHost<br>{<br>    [Export(typeof(IVirtualHost))]<br>    public class TestHost : HostBase<br>    {<br>        public override string HostType<br>        {<br>            get<br>            {<br>                return \\\"Test\\\";<br>            }<br>        }<br>    }<br>}<br>\",\"language\":\"csharp; ruler\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/10/hybrid-lab-workflow-standard-lab.html\"]],[\"a\",[\"href\",\"http://hybridlabworkflow.codeplex.com/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/11/developing-hybrid-lab-workflow.html\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd460648.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In an \"],[0,[0],1,\"earlier post\"],[0,[],0,\" I talked about my \"],[0,[1],1,\"Hybrid Lab Workflow\"],[0,[],0,\" – this workflow allows you to do a Build-Deploy-Test workflow against a TFS 2012 Standard Environment, and as long as the environment is composed of VMs and you’re able to connect to the VM Host, then you can apply pre-deployment snapshots and take post-deployment snapshots. I also \"],[0,[2],1,\"blogged\"],[0,[],0,\" about the “nastiness” of PsKill and PsExec for getting the Lab into a workable state after snapshots were applied. In this post I’ll talk about how you could use exactly the same workflow for another Virtual Host – say VMWare or something else.\"]]],[1,\"h2\",[[0,[],0,\"The Magic – MEF\"]]],[1,\"p\",[[0,[],0,\"When I first thought of this workflow, I immediately thought that I could make the Virtual machine and Virtual Hosts interfaces in the workflow, so that you could hook into any virtualization platform that you want. Lab Management itself if oblivious to the virtualization platform you use (well, in Standard Environments anyway) since it treats the machines as if they are physical (not through the Virtual Host, which is what happens in SCVMM Environments).\"]]],[1,\"p\",[[0,[],0,\"So I created an IVirtualHost interface and an IVirtualMachine interface. I then created an enumeration of VirtualHostTypes and a VirtualHostFactory that could instantiate an IVirtualHost concrete class based on the enumeration you fed in. I soon realised that if you wanted to add a Host Type, you’d need to recompile the factory and update the workflow – it would be a bit messy. I ideally wanted something a little more “dynamic”. So I thought of the \"],[0,[3],1,\"Managed Extensibility Framework (MEF)\"],[0,[],0,\" that is now baked into .NET. It’s designed exactly for this sort of problem.\"]]],[1,\"p\",[[0,[],0,\"I created a VirtualHostContainer that could dynamically load assemblies and find IVirtualHost implementations. Then I created a static class that uses the container to enumerate the HostTypes available and get you one when you need it.\"]]],[1,\"h2\",[[0,[],0,\"VirtualHost Container\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Above is the container code. The VirtualHosts property is attributed with an [ImportMany] attribute. The MEF framework will inject any classes that are attributed with the matching [Export] attribute into this property. This is done on line 28 in the call to SatisfyImportsOnce().\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"Once that property is populated, the GetHostTypes() and GetHost() methods are trivial. What’s a little trickier is dynamically loading the assemblies that may (or may not) contain IVirtualHost implementations. For that I just get the current assembly’s location and then try to load each dll (that doesn’t start with Microsoft or System). I create a new AssemblyCatalog from the assembly, and if there are any MEF exports (parts) in the assembly, I add the catalog to an AggregateCatalog. Once I’ve got all the AssemblyCatalogs into the AggregateCatalog, I create a CompositionContainer and tell it to make any hook ups using the SatisfyImportsOnce call on the VirtualHostContainer itself.\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"h2\",[[0,[],0,\"Virtual Host Catalog\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"I’ve wrapped this class into a static class that you can call to get VirtualHosts. Here it is:\"]]],[10,1],[1,\"p\",[[1,[],0,4]]],[1,\"h2\",[[0,[],0,\"Implementing IVirtualHost and IVirtualMachine\"]]],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"These interfaces are really simple (like all good interfaces). When I was implementing them for HyperV (on Windows 8) I realized that you can have a simple interface and a complicated implementation. I suppose that’s the power of interfaces – the interface consumer doesn’t have to care.\"]]],[10,2],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"The IVirtualHost has a couple of properties – the host type is used dynamically to discover host types. The rest are connection properties – the host name and admin credentials to the host machine. The Connect() method gets called in the VirtualHostContainer, so you don’t actually ever need to call it yourself. The final method is the method to get the list of VMs on the host.\"]]],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"I’ve also supplied an abstract HostBase class to get you started:\"]]],[10,3],[1,\"p\",[[1,[],0,8]]],[1,\"p\",[[0,[],0,\"The IVirtualMachine interface is also fairly simple:\"]]],[10,4],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"There’s a reference to the VMs IVirtualHost, a name, a property that returns all the snapshots and the DnsName of the guest OS. Then there are ApplySnapshot() and CreateSnapshot() for doing those operations.\"]]],[1,\"p\",[[1,[],0,10]]],[1,\"p\",[[0,[],0,\"Once you’ve implemented an IVirtualHost and IVirtualMachine, simply add the MEF Export attribute onto your IVirtualHost implementation and drop the assembly into source control along with the interface and other assemblies for the Hybrid Lab Workflow. You won’t need to change the workflow at all.\"]]],[1,\"p\",[[1,[],0,11]]],[1,\"p\",[[0,[],0,\"Here’s an example TestHost that I implemented for testing the VirtualHostCatalog class:\"]]],[10,5],[1,\"p\",[[1,[],0,12]]],[1,\"p\",[[0,[],0,\"Note the [Export] attribute decorating the class. Of course you’ll need some more code for a real host implementation!\"]]],[1,\"p\",[[1,[],0,13]]],[1,\"p\",[[0,[],0,\"That’s all there is to it. If you’re going to be implementing a host and need some help, let me know! Also, I can add it to the \"],[0,[1],1,\"Hybrid Lab Workflow project on Codeplex\"],[0,[],0,\".\"]]],[1,\"p\",[[1,[],0,14]]],[1,\"p\",[[0,[],0,\"Happy implementing!\"]]]]}","published_at":1351807680000,"status":"published","published_by":1},{"id":"2ed5e073-5c57-4d78-a1da-13de1e03f60d","title":"Failing Builds based on Code Coverage","slug":"failing-builds-based-on-code-coverage","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-uFYwr-eopiI/URUS_LxqDYI/AAAAAAAAAnU/zxwdE1TfSqA/s1600-h/image%25255B2%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-nSLjQI64yqU/URUTAn7hq-I/AAAAAAAAAnc/ApNLrZoXdlo/image_thumb.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"203\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-ydfIeHUXePk/URUTBhLTkGI/AAAAAAAAAnk/eIEBsALcjiY/s1600-h/image%25255B6%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-pPjB5feSLkA/URUTEF5clcI/AAAAAAAAAns/D71dF3t2yok/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"334\\\" height=\\\"320\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-ajVXY2oML4U/URUTFWhpdlI/AAAAAAAAAn0/14bzM6-jTh0/s1600-h/image%25255B10%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-jfWeLc0BmvA/URUTIP_I1QI/AAAAAAAAAoA/CQQpNfXpOB4/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"270\\\" height=\\\"361\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Timespan.FromSeconds(arg)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-xtYLqShGV-A/URUTJNV5wuI/AAAAAAAAAoI/7f8eS6KBBU8/s1600-h/image%25255B30%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-YeCIihP6MAM/URUTKlUyRbI/AAAAAAAAAoQ/2UuYhVr6ek0/image_thumb%25255B14%25255D.png?imgmax=800\\\" width=\\\"433\\\" height=\\\"147\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-34TlW0CDXYw/URUTL_MsdzI/AAAAAAAAAoY/_00-xufRTq8/s1600-h/image%25255B34%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-TZ6t_0yDqh0/URUTNQab3lI/AAAAAAAAAog/ReNMBCE2zPw/image_thumb%25255B16%25255D.png?imgmax=800\\\" width=\\\"255\\\" height=\\\"372\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-o2UxodX4KM4/URUTOtdjo0I/AAAAAAAAAoo/ybzXtjAjo-o/s1600-h/image%25255B26%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-Xs3LDlV2Bsk/URUTTg3K0rI/AAAAAAAAAow/ba5PAMSuyy0/image_thumb%25255B12%25255D.png?imgmax=800\\\" width=\\\"374\\\" height=\\\"320\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2013/02/custom-build-task-include-merged.html\"]],[\"strong\"],[\"em\"],[\"a\",[\"href\",\"http://sdrv.ms/XsMf8h\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"The logical next step after you start unit testing your code is to analyse code coverage. You can do this easily in TFS by enabling Code Coverage in the test settings of the default build template. But what about failing builds (or checkins) based on low code coverage?\"]]],[1,\"p\",[[0,[],0,\"Out of the box, there’s nothing that can do that. You could write a checkin policy that inspects code coverage, but you’d have to make sure it’s deployed to all VS instances. Or, you could implement the logic in a build, and then make the build a rolling gated checkin build. That way if the build fails for low coverage, the checkin is discarded.\"]]],[1,\"h2\",[[0,[],0,\"Modifying the Default Template\"]]],[1,\"p\",[[0,[],0,\"I’ve created a CodeActivity for your toolbox that will inspect the test coverage and return the coverage percentage. You can then easily implement some logic from there.\"]]],[1,\"p\",[[0,[],0,\"To get started, you’ll have to import the Custom assembly (link at the bottom of this post). Follow steps 1 to 3 of my \"],[0,[0],1,\"previous custom build post\"],[0,[],0,\" to get going.\"]]],[1,\"p\",[[0,[],0,\"Now you can add the ColinsALMCorner.CustomBuildTasks in to the imports of your workflow:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Click on the Arguments tab and add an Int32 in argument called “CoverageFailureIfBelow”. Set the metadata as follows:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now scroll down in the workflow until \"],[0,[1],1,\"after\"],[0,[],0,\" the testing section, between “If CompilationStatus = Uknown” and “If TestingStatus = Uknown”. Add a Sequence between the two If activities called “Check Coverage”.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Click on Variables at the bottom of the workflow window and create an Int32 variable (scoped to Check Coverage, called coverageTotal). You can double-click the sequence to “zoom in”.\"]]],[1,\"p\",[[0,[],0,\"The “GetCoverageTotal” activity relies on the test attachments that the test engine uploads to TFS when the tests are completed (most notably the Code Coverage attachment). Since this is done asynchronously, I introduced a Delay activity. Add a new Argument to the workflow (if you don’t want to hard-code the delay) and set its default to 10 seconds.\"]]],[1,\"p\",[[0,[],0,\"Set the Timeout of the Delay activity to\"]]],[10,3],[1,\"p\",[[0,[],0,\"where \"],[0,[2],1,\"arg\"],[0,[],0,\" is the name of your timeout argument.\"]]],[1,\"p\",[[0,[],0,\"Drop a “GetCoverageTotal” activity (from the custom assembly) into the sequence (NOTE: You may have to import the assembly into the Toolbox if you’ve never done that before). Set the properties as follows:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Once that’s done, you now have the coverage total, so you can do whatever you need to. Here’s what I did:\"]]],[10,5],[1,\"p\",[[0,[],0,\"I put an If that evaluates if the total coverage is too low or not. If the coverage is too low, I set the BuildDetail.TestStatus to TestStatus.Failed. I also set the overall build status to PartiallySucceeded. I then have a WriteBuildWarning (“Failing build because coverage is too low”) or WriteBuildMessage (“Coverage is acceptable”) depending on the result.\"]]],[1,\"p\",[[0,[],0,\"Now I just queue up a build, and voila – I can now fail it for low coverage (or in this case, set the result to Partially Successful).\"]]],[10,6],[1,\"p\",[[0,[],0,\"You can download the dll and the DefaultCoverageTemplate.11.1.xaml from my \"],[0,[3],1,\"skydrive\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Happy coverage!\"]]]]}","published_at":1360371660000,"status":"published","published_by":1},{"id":"9617735a-1580-4696-9609-4dd423f72e4d","title":"Fix: InRelease Demo “Hangs” on Keller’s 2013 Preview VM","slug":"fix-inrelease-demo-hangs-on-kellers-2013-preview-vm","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-MQluE4CaTfg/Uhsnt8OdVYI/AAAAAAAABBc/vTroxm1Vr5o/s1600-h/clip_image001%25255B4%25255D.png\\\"><img title=\\\"clip_image001\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"clip_image001\\\" src=\\\"http://lh6.ggpht.com/-2WD-HsVs950/UhsnvKWec4I/AAAAAAAABBk/1XABKuHiVGo/clip_image001_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"418\\\" height=\\\"264\\\"></a>\"}]],\"markups\":[[\"u\"],[\"a\",[\"href\",\"http://blogs.msdn.com/b/briankel/\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/briankel/archive/2013/08/02/visual-studio-2013-application-lifecycle-management-virtual-machine-and-hands-on-labs-demo-scripts.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/briankel/archive/2011/09/16/visual-studio-11-application-lifecycle-management-virtual-machine-and-hands-on-labs-demo-scripts.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/bharry/archive/2013/06/03/visual-studio-2013.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/bharry/archive/2013/07/10/inrelease-acquisition-is-complete.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/virtual_pc_guy/archive/2008/01/09/using-hyper-v-with-a-wireless-network-adapter.aspx\"]],[\"a\",[\"href\",\"http://sstjean.blogspot.com/2008/09/hyper-v-give-your-virtual-machines.html\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"Update 2012-09-04:\"],[0,[],0,\" Brian Keller posted a fix that seems to work for this problem (so you can run the InRelease build without connecting to a physical external network).\"]]],[1,\"p\",[[0,[],0,\"I love \"],[0,[1],1,\"Brian Keller\"],[0,[],0,\"’s VMs. I use them as the starting point for my TFS demos (which I’ve probably done hundreds of now). Brian’s latest \"],[0,[2],1,\"VM with TFS 2013\"],[0,[],0,\" is not quite what the \"],[0,[3],1,\"2012 one\"],[0,[],0,\" was (no reporting services or cube, no MS Project and a host of other things missing) but it’s enough to get started with.\"]]],[1,\"p\",[[0,[],0,\"One of the big \"],[0,[4],1,\"new features\"],[0,[],0,\" of TFS 2013 is Release Management (with the \"],[0,[5],1,\"acquisition of InRelease\"],[0,[],0,\"). An Brian provides a hands-on-lab that lets you kick the tires a bit. The lab starts with you making a small modification to the FabrikamFiber website, and then using a TFS build to trigger a deployment in InRelease. I dutifully made my change and queued the build – but the build hangs, and never completes.\"]]],[10,0],[1,\"p\",[[0,[],0,\"I can run other builds successfully, and if I turn off “Trigger Deployment” in the build, the build completes. That led me to conclude that InRelease was hanging for some reason. Neither the event logs, nor the InRelease logs had any useful info.\"]]],[1,\"h2\",[[0,[],0,\"The Solution: Connect the VM to an External Network\"]]],[1,\"p\",[[0,[],0,\"I mailed Brian and he gave me a few things to try – the one that ended up working was connecting the VM to an external network. From that point on, the demo works as scripted. (Read \"],[0,[6],1,\"this post\"],[0,[],0,\" to see how to connect your HyperV VM to your WiFi, and \"],[0,[7],1,\"this one\"],[0,[],0,\" about connecting your VM to a 3G dongle).\"]]],[1,\"p\",[[0,[],0,\"I’m not sure why you need an external network for this – there seems to be some challenges in the networking logic of InRelease. This is just a preview release, so hopefully this gets fixed before release.\"]]],[1,\"p\",[[0,[],0,\"Anyway, if you’ve been having this issue while working the lab, then hopefully you can now continue to check it out.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1377543720000,"status":"published","published_by":1},{"id":"6e265334-6093-4854-9cfb-4a0940406960","title":"Fix: Release Management “Service Unavailable 503”","slug":"fix-release-management-service-unavailable-503","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">appcmd.exe set config -section:system.webServer/globalModules /[name='SPNativeRequestModule'].preCondition:integratedMode,bitness64</font>\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"At a customer we installed Release Management for their TFS 2013 TFS Server. The server component installation went really smoothly – however, it was only when we installed the Client that we realized that the Release Management service was not right – we kept getting a 503 Service Unavailable error. I opened IIS and could see that the Release Management application pool was stopped. I started the app pool, but it immediately shut down. We checked the event log and saw a few obscure error messages about NullReferenceExceptions – nothing particularly helpful.\"]]],[1,\"h2\",[[0,[],0,\"The Problem: SharePoint\"]]],[1,\"p\",[[0,[],0,\"It turns out that the problem was (surprise, surprise) SharePoint. The server we were using (for a small team) was a single server TFS – data and application tier on the same machine, as well as SSRS, SSAS and SharePoint. Unfortunately, SharePoint doesn’t play well with others – specifically with other 32-bit web applications. SharePoint’s install adds a global ISAPI module, but for some reason doesn’t add a bitness filter – so if you have a 32-bit web application (like Release Management) the 64-bit SharePoint module gets loaded anyway, and this causes the 32-bit application pool to crash.\"]]],[1,\"h2\",[[0,[],0,\"The Fix\"]]],[3,\"ol\",[[[0,[],0,\"Log on to your TFS Server and open a command prompt as administrator.\"]],[[0,[],0,\"cd to \\\\windows\\\\system32\\\\inetsrv\"]],[[0,[],0,\"Now enter:\"]]]],[10,0],[1,\"p\",[[0,[],0,\"Now restart your Release Management application pool, and you’ll be good to go.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1391792460000,"status":"published","published_by":1},{"id":"2fc67b76-82b9-4f5b-a296-4e3673c4fb45","title":"Fix: Release Management WebDeploy Deployment Fails: Access Denied","slug":"fix-release-management-webdeploy-deployment-fails-access-denied","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">Info: Adding sitemanifest (sitemanifest).<br>Info: Creating application (Default Web Site/MyWebsite)<br>Error: An error occurred when reading the IIS Configuration File 'MACHINE/REDIRECTION'. The identity performing the operation was 'DOMAIN\\\\tfsservice'.<br>Error: Filename: \\\\\\\\?\\\\C:\\\\Windows\\\\system32\\\\inetsrv\\\\config\\\\redirection.config<br>Error: Cannot read configuration file due to insufficient permissions</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-SXLi5YiFJJ8/UvSWgtofeyI/AAAAAAAABOo/m7-YRNPhdvg/s1600-h/image%25255B2%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-SMFFZcKdfi0/UvSWhMDKxUI/AAAAAAAABOw/d86F02LLIaE/image_thumb.png?imgmax=800\\\" width=\\\"185\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-hx6-XXpFqHI/UvSWhj12lvI/AAAAAAAABO4/eIvZ_GGn0BU/s1600-h/image%25255B5%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-QNJhFwsmd9M/UvSWiNpdQdI/AAAAAAAABPA/X--C0nBBLEo/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"184\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-2LshUZF4El4/UvSWi59mevI/AAAAAAAABPI/pZdmwSbM9TU/s1600-h/image%25255B8%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-XrHxyKXxteU/UvSWjSVE48I/AAAAAAAABPQ/phXqt3yXlV4/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"185\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/11/webdeploy-and-release-management.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"If you’re using WebDeploy and Release Management (\"],[0,[0],1,\"as you should\"],[0,[],0,\" to release Web Applications) you may hit the following error:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Seems that the WebDeploy command can’t access some files in c:\\\\Windows\\\\system32\\\\inetsrv. It may be the irmsdeploy.exe MSDeploy wrapper that I’m using for doing WebDeploy in Release Management (see my \"],[0,[0],1,\"post about how to do this\"],[0,[],0,\"), since logging into the machine and running the webdeploy.cmd file manually works just fine.\"]]],[1,\"h2\",[[0,[],0,\"The Resolution\"]]],[1,\"p\",[[0,[],0,\"You have to add permissions for the release management agent identity to the folder, but this is a folder who’s owner identity is TrustedInstaller – meaning you have to change the owner to yourself first.\"]]],[3,\"ul\",[[[0,[],0,\"Right click the insetsrv folder in c:\\\\windows\\\\system32 and select Properties.\"]],[[0,[],0,\"Click on the Security tab and click the “Advanced” button:\"]]]],[10,1],[3,\"ul\",[[[0,[],0,\"Click on the owner tab and then on the Edit button:\"]]]],[10,2],[3,\"ul\",[[[0,[],0,\"Select yourself (I logged in as TfsSetup which is in the local admin group on this machine), check “Replace owner on subcontainers and objects” checkbox and click “OK”:\"]]]],[10,3],[3,\"ul\",[[[0,[],0,\"Close all the dialogs and then right-click the inetsrv folder again and click Properties. Now you can allow read access to the Release Management agent identity to this folder.\"]]]],[1,\"p\",[[0,[],0,\"Once you’ve changed the permissions, you will need to reboot the machine. After the reboot, the WebDeploy through Release Management should work without a hitch.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1391797020000,"status":"published","published_by":1},{"id":"01cae2ee-780a-45d6-8929-e111983abdca","title":"Fix: You Open TfcvTemplate.12.xaml and Don’t See Any Parameters","slug":"fix-you-open-tfcvtemplate12xaml-and-dont-see-any-parameters","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-xpu4oH2llnU/UjMt5Ek83oI/AAAAAAAABFA/zBq_rr_Uj0Q/s1600-h/clip_image001%25255B4%25255D.jpg\\\"><img title=\\\"clip_image001\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"clip_image001\\\" src=\\\"http://lh4.ggpht.com/-wZkhv8yNy_w/UjMt6Hy0ZpI/AAAAAAAABFI/a6TAabG9fXI/clip_image001_thumb%25255B1%25255D.jpg?imgmax=800\\\" width=\\\"578\\\" height=\\\"236\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">C:\\\\Program Files\\\\Microsoft Team Foundation Server 12.0\\\\Tools\\\\Newtonsoft.Json.dll</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 12.0\\\\Common7\\\\IDE\\\\PrivateAssemblies\\\\</font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://nakedalm.com/whats-new-in-visual-studio-2013-rc-with-team-foundation-server/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I upgraded my demo environment from 2013 Preview to 2013 RC. Everything looked good until I got to the builds. I had configured a couple of default builds – the 2013 default template is actually stored in the TFS database (not in source control like the old Default xaml files) unless you actually download it for customizing.\"]]],[1,\"p\",[[0,[],0,\"However, when I opened the build definition, the parameters section of the Process tab was empty:\"]]],[10,0],[1,\"p\",[[0,[],0,\"All the other tabs worked just fine, and all the other (XAML from source control) templates worked just fine too.\"]]],[1,\"p\",[[0,[],0,\"I quickly mailed the Champs List, and got some great assistance from Jason Prickett of the product team. I attached a debugger to VS and opened the template, and I got some “could not load assembly” errors for Newtonsoft.Json.dll.\"]]],[1,\"p\",[[0,[],0,\"Jason then told me the solution was simple: copy the dll. So I copied\"]]],[10,1],[1,\"p\",[[0,[],0,\"to\"]]],[10,2],[1,\"p\",[[0,[],0,\"and that resolved the problem.\"]]],[1,\"p\",[[0,[],0,\"Now I can create, edit and run builds again. And I’m loving the \"],[0,[0],1,\"new RC features\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1379118180000,"status":"published","published_by":1},{"id":"0d165ae8-5cfa-4e67-9c34-52a962925b4a","title":"Frequent Status Updates–What They Really Mean","slug":"frequent-status-updateswhat-they-really-mean","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://amzn.com/1935401009\"]],[\"a\",[\"href\",\"http://en.wikipedia.org/wiki/Observer_effect_(physics)\"]],[\"u\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Are you (as a developer) inundated with frequent status updates? Requests like: “How far are you?” “What did you do today?” “Where are we?” Or are you a project manager that requests frequent status updates? Then this post is for you.\"]]],[1,\"p\",[[0,[],0,\"Let’s start by defining \"],[0,[0],1,\"frequent\"],[0,[],0,\" – I think this is going to be different for different teams, and will vary with the Application Lifecycle Management (ALM) maturity within the team. I would go so far as to say that one status update request a week is too frequent. Certainly one a day is far too frequent.\"]]],[1,\"p\",[[0,[],0,\"Frequent Status Updates reveal some symptoms of poor process:\"]]],[3,\"ul\",[[[0,[],0,\"Measuring the Wrong Things\"]],[[0,[],0,\"Large Batch Size\"]],[[0,[],0,\"Lack of Transparency\"]],[[0,[],0,\"Lack of Trust\"]],[[0,[],0,\"Unpredictable Meeting Times\"]]]],[1,\"h2\",[[0,[],0,\"Measuring the Wrong Things\"]]],[1,\"p\",[[0,[],0,\"In his book \"],[0,[0,1],2,\"The Principles of Product Development Flow\"],[0,[],0,\", Donald Reinersten frequently discusses the impact of measuring “proxy variables”. Proxy variables are quantified measures that are \"],[0,[0],1,\"substitutes for real economic measures\"],[0,[],0,\". For example, most developers measure and try really hard to reduce cycle time, but if you ask them how much it would cost to delay a release by a week, they would have no idea. Instead of measuring the economic impact of decisions (the real variable), they’re measuring cycle time (a proxy variable).\"]]],[1,\"p\",[[0,[],0,\"Similarly, frequent status updates are attempts to measure productivity or value-added. However, “being busy” is simply a proxy variable for delivering value. \"],[0,[2],1,\"The Observer Effect\"],[0,[],0,\" shows us that the very act of measuring a system alters the system. Status updates usually require a fair amount of effort (especially if you don’t have a proper work management tool) so every time you ask for status updates, you slow the team.\"]]],[1,\"p\",[[0,[],0,\"This gets even worse when the time to gather the status report is long – this can mean that by the time you get the report, the report is out of date. That’s not helpful or valuable to anyone.\"]]],[1,\"p\",[[0,[],0,\"Another problem that the frequent updates could indicate is measuring success by conformance to the plan. Again this is just a proxy to \"],[0,[0],1,\"did the team deliver value\"],[0,[],0,\". Product Development is volatile and unpredictable at times – trying to manage that by eliminating change is usually a bad idea. Responding rapidly to and managing change is far more valuable. If a project manager is always checking on the team to make sure they’re conforming to a plan, then perhaps that indicates that the plan is incorrect?\"]]],[1,\"h2\",[[0,[],0,\"Large Batch Size\"]]],[1,\"p\",[[0,[],0,\"If you get “value” at the end of a batch, then it stands to reason that the larger your batch is, the longer you’ll wait to get value. This can cause a knee-jerk reaction: treatment of a symptom (long spaces between feedback), rather than treatment of a cause (too large batch sizes). In other words, if you have to request feedback to see where you are, then perhaps you should shrink your batch size so that you get “feedback” (delivery of value) quicker. Shorter batches negate the need to status updates since they end quickly.\"]]],[1,\"p\",[[0,[],0,\"There are a myriad of other benefits to smaller batch sizes (including improving cycle times) that I won’t go into here. However, smaller batch sizes allow the end of the batch to provide the feedback, rather than having to ask for status mid-batch. This removes the “cost” of gathering the status report during the execution of a batch.\"]]],[1,\"h2\",[[0,[],0,\"Lack of Transparency\"]]],[1,\"p\",[[0,[],0,\"Frequent status updates can highlight the fact that there’s very little transparency into what work is in the pipeline. If you don’t have an effective tool to manage work that anyone can query or report from, then you’ll see a lot of “how far are you” requests. I’d even go so far as to say if you don’t have a tool to \"],[0,[0],1,\"visualize\"],[0,[],0,\" your pipeline, you’re still going to get a lot of status update requests.\"]]],[1,\"h2\",[[0,[],0,\"Lack of Trust\"]]],[1,\"p\",[[0,[],0,\"Often this is an extension of Lack of Transparency. If there is a lack of trust between the project manager and the team, then often a symptom of this lack of trust is frequent status updates. The project manager doesn’t trust the team to deliver value, so they continually “police” the team to make sure that the team is working according to the plan. In high-trust environments, there is a lack of these frequent “how far are you” requests.\"]]],[1,\"h2\",[[0,[],0,\"Unpredictable Meeting Times\"]]],[1,\"p\",[[0,[],0,\"This may seem basic, but it’s surprisingly common. If you have a status update meeting “when some milestone is reached”, and there is some slippage, then you need to re-organise the meeting again. If, however, you have a fixed status meeting date and time (like a daily stand-up, perhaps?) then you never have to re-organise meetings. You know exactly when the status is going to be communicated.\"]]],[1,\"h1\",[[0,[],0,\"How to Counter Frequent Status Updates\"]]],[1,\"p\",[[0,[3],1,\"Measure the Right Things\"]]],[1,\"p\",[[0,[],0,\"A \"],[0,[0],1,\"busy\"],[0,[],0,\" developer is not necessarily a \"],[0,[0],1,\"productive\"],[0,[],0,\" developer. Conversely, having spare capacity does not mean that work won’t be done on time. We need to move beyond the archaic and out-dated models of Product Development Management and embrace new paradigms. This means that we need to start measuring the right things – such as cycle times, batch sizes and value delivered. In \"],[0,[0,1],2,\"The Principles of Product Development Flow\"],[0,[],0,\", Reinersten starts off by bringing us back to basics: \"],[0,[0],1,\"economics\"],[0,[],0,\". If you can’t express your measurement in financial terms, then it’s probably not worth measuring. To refer back to the cycle time argument, you can only focus on reducing cycle time if you actually know how much the cost of delay is. Start measuring the right things, and you’ll see your “how far are you?” requests dry up (much to everyone’s relief).\"]]],[1,\"p\",[[0,[3],1,\"Reduce Batch Size\"]]],[1,\"p\",[[0,[],0,\"Working smaller queues has many benefits – one of these is that if the queue is short enough, it finishes before you need an update. This eliminates the need for a status update altogether.\"]]],[1,\"p\",[[0,[3],1,\"Surface (and Visualize) Work In Progress\"]]],[1,\"p\",[[0,[],0,\"Having a way to visualize your work in progress is critical – whether you use a whiteboard with stickys or have some sort of electronic system, make sure you can visualize your work in progress, especially your queues. This will not only allow anyone to see what’s going on, but to start making decisions without having to call everyone together.\"]]],[1,\"p\",[[0,[3],1,\"Improve Trust\"]]],[1,\"p\",[[0,[],0,\"Trust is earned, and that’s certainly the case in Product Development. But remember, it’s a two-way street – business needs to earn ITs trust, and the other way around too. This is a “soft-skill”, and while it’s the hardest to get right, in my opinion it’s the one with the highest reward level. High trust environments foster creativity, productivity and sustainability, so work on making your environment one of high trust.\"]]],[1,\"p\",[[0,[3],1,\"Schedule Predictable Meetings\"]]],[1,\"p\",[[0,[],0,\"Just do it – make a regular, time-boxed meeting (like a daily stand-up). You’ll never have to guess if the meeting is on or not, you always know it’s on. Time-box it to keep it snappy and meaningful so that people value them. You’ll start finding that this meeting gives you pretty much everything you need to know, so you won’t need to ask people where they are.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"If you’re a victim (or perpetrator!) of frequent status updates, you may want to assess your ALM process to do a health check. Frequent status updates indicate bad health and you should change your processes accordingly.\"]]]]}","published_at":1377034740000,"status":"published","published_by":1},{"id":"81c2acfc-4bb9-41d6-8d0d-ea487121b3b6","title":"GenericAutomationPeer – Helping the Coded UI Framework Find Your Custom Controls","slug":"genericautomationpeer--helping-the-coded-ui-framework-find-your-custom-controls","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-DcFLf0Uxges/TrKVV0HYxyI/AAAAAAAAAUY/aXxzDeswSXY/s1600-h/clip_image0013.jpg\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"clip_image001\\\" border=\\\"0\\\" alt=\\\"clip_image001\\\" src=\\\"http://lh6.ggpht.com/-o4LLkVVhXOo/TrKVXdg2dTI/AAAAAAAAAUg/5RPUOBZq-p4/clip_image001_thumb.jpg?imgmax=800\\\" width=\\\"244\\\" height=\\\"218\\\"></a>\"}],[\"code\",{\"code\":\"  public class GenericAutomationPeer : UIElementAutomationPeer {\\n    public GenericAutomationPeer(UIElement owner) : base(owner) { }\\n    \\n    protected override List<automationpeer> GetChildrenCore() {\\n      var list = base.GetChildrenCore();\\n      list.AddRange(GetChildPeers(Owner));\\n      return list;\\n    }\\n    \\n    private List<automationpeer> GetChildPeers(UIElement element) {\\n      var list = new List<automationpeer>();\\n      for (int i = 0; i &lt; VisualTreeHelper.GetChildrenCount(element); i++) {\\n        var child = VisualTreeHelper.GetChild(element, i) as UIElement;\\n        if (child != null) {\\n          var childPeer = UIElementAutomationPeer.CreatePeerForElement(child);\\n          if (childPeer != null) {\\n            list.Add(childPeer);\\n          } else { \\n            list.AddRange(GetChildPeers(child));\\n          }\\n        }\\n      }\\n      return list;\\n      }\\n    }\\n  </automationpeer></automationpeer></automationpeer>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"public class CustomTabControl : TabControl{    protected override AutomationPeer OnCreateAutomationPeer()    {        return new GenericAutomationPeer(this);    }}\",\"language\":\"xml; ruler\"}]],\"markups\":[[\"a\",[\"href\",\"http://compositewpf.codeplex.com/\"]],[\"strong\"],[\"u\"],[\"em\"],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/cc165614.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/ms727247.aspx\"]],[\"a\",[\"href\",\"http://cuite.codeplex.com/\"]],[\"a\",[\"href\",\"http://social.msdn.microsoft.com/forums/en-US/wpf/thread/fa8eb86f-5001-4af6-adb3-ceb0799a0cf3\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Sometimes you’ll write an WPF application that has some sort of “dynamic” way of loading portions of the UI (think: \"],[0,[0],1,\"Prism\"],[0,[],0,\"). Sometimes entire frameworks are too much, so you’d prefer something a bit simpler – like, say, a TabControl with a data template. Bind the ItemsSource of your TabControl to an ObservableCollection (where T is some model) and you’ve got a “dynamic” interface.\"]]],[1,\"p\",[[0,[],0,\"So you’ve written your killer dynamic interface app – and, since you’re a good developer, you try to add some coded UI tests.\"]]],[1,\"p\",[[0,[1,2],2,\"Problem:\"],[0,[],0,\" The UI Test framework doesn’t “see” any of the controls that were loaded dynamically! What gives?\"]]],[10,0],[1,\"p\",[[0,[3],1,\"The figure above shows the “deepest” level that the UI Framework can see – none of the child controls (the comboboxes or table) exist as far as the coded UI Test framework is concerned.\"]]],[1,\"p\",[[0,[],0,\"What’s happening here is that the \"],[0,[4],1,\"AutomationPeer\"],[0,[],0,\" of the TabControl doesn’t know anything about the controls within it, since they’re loaded dynamically. You have to help your little TabControl a bit. You have to let the Automation framework know about all the little controls that you load. But what if each tab loads a completely different UserControl? This sounds like a lot of work…\"]]],[1,\"h2\",[[0,[],0,\"The Solution: GenericAutomationPeer\"]]],[1,\"p\",[[0,[],0,\"Fortunately, you can walk the child controls and just get them to give their “default” AutomationPeers to you (most primitive WPF controls – like TextBoxes, ComboBoxes, Buttons and so on – have built in AutomationPeers). So we need 2 things: a way to walk the child controls and a hook into the Automation Framework.\"]]],[10,1],[1,\"p\",[[0,[],0,\">p>This class inherits from UIElementAutomationPeer, so you just need to override the GetChildrenCore() method. Inside that, just use the VisualTreeHelper to walk the child controls. For each child control, see if it has an AutomationPeer by calling the static method UIElementAutomationPeer.CreatePeerForElement(). If it has an element, add it to the list of AutomationPeers. If it doesn’t, then recursively call to see if it’s children have AutomationPeers.\"]]],[1,\"p\",[[0,[],0,\"So we’ve got our GenericAutomationPeer: now we just need a hook in to use it. In this example, the “lowest” control visible to the Automation Framework was the TabControl – so that’s where we’ll get our hook in.\"]]],[10,2],[1,\"p\",[[0,[],0,\"We create a CustomTabControl that inherits from TabControl and simply overrides its OnCreateAutomationPeer. Inside the override, simple new up a GenericAutomationPeer, and you're done. Don’t forget to change the XAML from TabControl to local:CustomTableControl (where local is your imported namespace).\"]]],[1,\"h2\",[[0,[],0,\"Other Tools for your Toolbox\"]]],[3,\"ul\",[[[0,[5],1,\"UISpy\"],[0,[],0,\" – this lets you inspect exactly what the Automation Framework can “see”\"]],[[0,[6],1,\"CUITE\"],[0,[],0,\" – Coded UI Test Enhancements on Codeplex – useful coded UI Extension library\"]]]],[1,\"p\",[[0,[],0,\"Happy (coded UI) testing!\"]]],[1,\"p\",[[0,[1],1,\"Update:\"],[0,[],0,\" This \"],[0,[7],1,\"post\"],[0,[],0,\" also had a solution specifically for TabControls that avoids having to implement AutomationPeers entirely.\"]]]]}","published_at":1320358860000,"status":"published","published_by":1},{"id":"39fd604e-0498-4f7b-985c-8edd63f07dde","title":"Getting Results from Backlog Overview Report in TFS 2013 Preview","slug":"getting-results-from-backlog-overview-report-in-tfs-2013-preview","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-d0_Lac10QwE/Uh2ykKy9RVI/AAAAAAAABB4/OLKW0qAGAB0/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-JxBhrUErPx8/Uh2yk24YcII/AAAAAAAABCA/PTwdcR-hfgM/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"358\\\" height=\\\"171\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-J-SvW_Qv_qk/Uh2ylvXlbSI/AAAAAAAABCI/0Por7daq1nI/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-kM5yH2KlNKY/Uh2ymekwNZI/AAAAAAAABCQ/0vBuP4XwRTQ/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"397\\\" height=\\\"142\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-h_RuPpvEW4g/Uh2ymwcKigI/AAAAAAAABCY/u0-EYlPLFMc/s1600-h/image%25255B14%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-BYcRDlkO2J8/Uh2ynh6szEI/AAAAAAAABCg/JT-2VVE7nVg/image_thumb%25255B6%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"226\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-Ub9fsKHsWTI/Uh2yoq_lvmI/AAAAAAAABCo/WxcCDYwpACA/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-L3SYRnD1Jaw/Uh2ypmJP56I/AAAAAAAABCw/wP1h6EsXPj0/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"363\\\" height=\\\"270\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-2zhM0DW9DuM/Uh2yqTSx9yI/AAAAAAAABC4/qy_Mob-nc9k/s1600-h/image%25255B18%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-2SNZg-v38rE/Uh2yrSt17LI/AAAAAAAABC8/XpGu_axd7Lw/image_thumb%25255B8%25255D.png?imgmax=800\\\" width=\\\"383\\\" height=\\\"255\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://server/reports\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"One of my favourite reports in TFS is the Backlog Overview (Scrum) or User Story Overview (Agile). So after installing and playing with TFS 2013 Preview, I went to see what the report looks like.\"]]],[1,\"p\",[[0,[],0,\"What I found wasn’t pretty: though I could verify that there was data in the warehouse, the report stubbornly refused to show any data.\"]]],[10,0],[1,\"p\",[[0,[],0,\"I thought that something was broken with my warehouse, so I dug into the rdl file and into the warehouse database. I could see data, but one of the queries wasn’t returning any data. It turns out the query is looking for the “root” level of “deliverables”. This defaults to the set (PBI, User Story, Requirement) which would work perfectly in the out-of-the-box 2012 templates. But the 2013 templates now root “higher up” in Features. So you have to add Feature to the list. Here’s how to do it:\"]]],[1,\"p\",[[0,[],0,\"1. Browse to the reports folder root (usually this is \"],[0,[0],1,\"http://server/reports\"],[0,[],0,\") where server is the name of your TFS box. Now navigate through TfsReports to the collection and team project folder where your “Backlog Overview” report is:\"]]],[10,1],[1,\"p\",[[0,[],0,\"2. Hover your mouse over the “Backlog Overview” report and click the arrow to expand the menu. Select “Manage”.\"]]],[10,2],[1,\"p\",[[0,[],0,\"3. Click on the Parameters tab on the left and find the parameter called “DeliverableCategory”. Add “Feature” to the list. \"],[0,[1],1,\"Don’t forget to scroll down and press the “Apply” button!\"]]],[10,3],[1,\"p\",[[0,[],0,\"Voila! You now have data when you browse to the report. The PBIs are grouped under their respective Features, which is a nice bonus.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Happy reporting!\"]]]]}","published_at":1377710340000,"status":"published","published_by":1},{"id":"31a9b33f-0671-4edf-85a9-65fa82638079","title":"Gulp – Workaround for Handling VS Solution Configuration","slug":"gulp--workaround-for-handling-vs-solution-configuration","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"// set a variable telling us if we're building in release\\nvar isRelease = true;\\nif (process.env.NODE_ENV &amp;&amp; process.env.NODE_ENV !== 'Release') {\\n    isRelease = false;\\n}\\n\",\"language\":\"js; toolbar\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/52d9d0d7-57d9-4c1e-bd63-87e51fe08c0b.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ffdc4f2a-2252-4913-a7d0-ad319441e07d.png\\\" width=\\\"212\\\" height=\\\"148\\\"></a>\"}],[\"image\",{\"src\":\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7708bf5c-ba1a-48b0-add0-2442734925a8.png\",\"alt\":\"image\",\"title\":\"image\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b6e9a620-1c68-4d7b-a65a-705f3ec21f79.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4ed9d16f-af16-4f65-9e4c-1c884c49472b.png\\\" width=\\\"461\\\" height=\\\"156\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.typescriptlang.org/\"]],[\"a\",[\"href\",\"http://vswebessentials.com/\"]],[\"a\",[\"href\",\"http://nancyfx.org/\"]],[\"a\",[\"href\",\"http://gulpjs.com/\"]],[\"a\",[\"href\",\"http://www.hanselman.com/blog/IntroducingGulpGruntBowerAndNpmSupportForVisualStudio.aspx\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708\"]],[\"a\",[\"href\",\"http://madskristensen.net/\"]],[\"a\",[\"href\",\"https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708/view/Discussions/2\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"We’ve got some \"],[0,[0],1,\"TypeScript\"],[0,[],0,\" models for our web frontend. If you’re doing any enterprise JavaScript development, then TypeScript is a must. It’s much more maintainable and even gives you some compile-time checking.\"]]],[1,\"p\",[[0,[],0,\"Even though TypeScript is a subset of JavaScript, you still need to “compile” the TypeScript into regular JavaScript for your files to be used in a web application. Visual Studio 2013 does this out of the box (you may need to install TypeScript if you’ve never done so). However, what if you want to concat the compiled scripts to reduce requests from the site? Or minify them?\"]]],[1,\"h2\",[[0,[],0,\"WebEssentials\"]]],[1,\"p\",[[0,[1],1,\"WebEssentials\"],[0,[],0,\" provides functionality like bundling and minification within Visual Studio. The MVC bundling features let you bundle your client side scripts “server side” – so the server takes a bundle file containing a list of files to bundle as well as some settings (like minification) and produces the script file “on-the-fly”. However, if you’re using some over framework, such as \"],[0,[2],1,\"Nancy\"],[0,[],0,\" – you’re out of luck. Well, not entirely.\"]]],[1,\"h2\",[[0,[],0,\"Gulp\"]]],[1,\"p\",[[0,[],0,\"What if you could have a pipeline (think: build engine) that could do some tasks such as concatenation and minification (and other tasks) on client-side scripts? Turns out there is a tool for that – it’s called \"],[0,[3],1,\"Gulp\"],[0,[],0,\" (there are several other tools in this space too). I’m not going to cover Gulp in much detail in this post – you can look to \"],[0,[4],1,\"Scott Hanselman’s excellent intro post\"],[0,[],0,\". There are also some excellent tools for Visual Studio that support Gulp – notably \"],[0,[5],1,\"Task Runner Explorer Extension\"],[0,[],0,\" (soon to be baked into VS 2015).\"]]],[1,\"h2\",[[0,[],0,\"Configurations for Gulp\"]]],[1,\"p\",[[0,[],0,\"After a bit of learning curve, we finally got our Gulp file into a good place – we were able to compile TypeScript to JavaScript, concat the files preserving ordering for dependencies, minify, include source maps and output to the correct directories. We even got this process kicked off as part of our TFS Team build for our web application.\"]]],[1,\"p\",[[0,[],0,\"However, I did run into a hitch – configurations. It’s easy enough to specify a configuration (or environment) for Gulp using the NODE_ENV setting from the command line. Just set the value in the CLI you’re using (so “set NODE_ENV Release” for DOS prompt, and “$env:NODE_ENV = ‘Release’” for PowerShell) and invoke gulp. However, it seems that configurations are not yet supported within Visual Studio. I wanted to minify only for Release configurations – and I found there was no obvious way to do this.\"]]],[1,\"p\",[[0,[],0,\"I even managed to find a reply to a question on the Task Runner Explorer on VS Gallery where \"],[0,[6],1,\"Mads Krisensen\"],[0,[],0,\" states there is no configuration support for the extension yet – he says it’s coming though (see \"],[0,[7],1,\"here\"],[0,[],0,\" – look for the question titled “Passing build configuration into Gulp”).\"]]],[1,\"p\",[[0,[],0,\"The good news is I managed to find a passable workaround.\"]]],[1,\"h2\",[[0,[],0,\"The Workaround\"]]],[1,\"p\",[[0,[],0,\"In my gulp file I have the following lines:\"]]],[10,0],[1,\"p\",[[0,[],0,\"This is supposed to grab the value of the NODE_ENV from the environment for me. However, running within the Task Runner Explorer quickly showed me that it was not able to read this value from anywhere.\"]]],[1,\"p\",[[0,[],0,\"At the top of the Gulp file, there is a /// comment that allows you to bind VS events to the Gulp file – so if you want a task executed before a build, you can set BeforeBuild=’default’ inside the ///. At first I tried to set the environment using “set ENV_NODE $(Configuration)” in the pre-build event for the project, but no dice.\"]]],[1,\"p\",[[0,[],0,\"Here’s the workaround:\"]]],[3,\"ul\",[[[0,[],0,\"Remove the BeforeBuild binding from the Gulp file (i.e. when you build your solution, the Gulp is not triggered). You can see your bindings in the Task Runner Explorer – you want to make sure “Before Build” and “After Build” are both showing 0:\"]]]],[10,1],[3,\"ul\",[[[0,[],0,\"Add the following into the Pre-Build event on your Web project Build Events tab:\"]],[[0,[],0,\"set NODE_ENV=$(ConfigurationName)\"]],[[0,[],0,\"gulp\"]]]],[10,2],[1,\"p\",[[0,[],0,\"That’s it! Now you can just change your configuration from “Debug” to “Release” in the configuration dropdown in VS and when you build, Gulp will find the correct environment setting. Here you can see I set the config to Debug, the build executes in Debug and Gulp is correctly reading the configuration setting:\"]]],[10,3],[1,\"h3\",[[0,[],0,\"Caveats\"]]],[1,\"p\",[[0,[],0,\"There are always some – in this case only two that I can think of:\"]]],[3,\"ol\",[[[0,[],0,\"You won’t see the Gulp output in the Task Runner Explorer Window on builds – however, if you use the Runner to invoke tasks yourself, you’ll still see the output. The Gulp output will now appear in the Build output console when you build.\"]],[[0,[],0,\"If you’ve set a Watch task (to trigger Gulp when you change a TypeScript file, for example) it won’t read the environment setting. For me it’s not a big deal since build is invoked prior to debugging from VS anyway. Also, for our build process I default the value to “release” just in case.\"]]]],[1,\"p\",[[0,[],0,\"Happy gulping!\"]]]]}","published_at":1421436654000,"status":"published","published_by":1},{"id":"7dad79a2-0571-4f29-84f2-2225f0350985","title":"Hybrid Lab Workflow: Standard Lab Environment with Snapshots","slug":"hybrid-lab-workflow-standard-lab-environment-with-snapshots","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-SgigVcjWFb4/UJF5Ldaxc1I/AAAAAAAAAdo/mRcIP4V5dNo/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-ran9DR0lnC4/UJF5Ncgm8CI/AAAAAAAAAdw/VRD1MaqAzA8/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"147\\\" height=\\\"335\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-WRgOpghk5Uc/UJF5PRMMuNI/AAAAAAAAAd4/hkzZqqBvHVg/s1600-h/image%25255B55%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-nxJGo0vtCo4/UJF5RH7lkEI/AAAAAAAAAeA/pqWjqE1z5BM/image_thumb%25255B28%25255D.png?imgmax=800\\\" width=\\\"265\\\" height=\\\"286\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-5_9NYYbEUs8/UJF5T9eOBgI/AAAAAAAAAeI/DxJdxwV4gv4/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-VRkMsR5BBlY/UJF5Vz8Wj1I/AAAAAAAAAeQ/HYASsv3YT2Q/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"391\\\" height=\\\"367\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-2qVJ9X4xANk/UJF5XhBXYQI/AAAAAAAAAeY/unWJmQvSSVM/s1600-h/image%25255B20%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-V1crAWF2tJM/UJF5ZHOPGaI/AAAAAAAAAeg/8G1PTWimFNE/image_thumb%25255B9%25255D.png?imgmax=800\\\" width=\\\"220\\\" height=\\\"338\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-OcwSDzyAIUA/UJF5baQnpEI/AAAAAAAAAeo/gTfRLjjKt-A/s1600-h/image%25255B25%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-JgIkbgZAdbI/UJF5dD0UBKI/AAAAAAAAAew/bzjl227XSOo/image_thumb%25255B12%25255D.png?imgmax=800\\\" width=\\\"429\\\" height=\\\"234\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-dFyDuWeKj4U/UJF5ewvJp9I/AAAAAAAAAe4/CAc_nbkbmto/s1600-h/image%25255B29%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-RhadNL6jmbE/UJF5gAvkBfI/AAAAAAAAAfA/vFziFxJ8mIY/image_thumb%25255B14%25255D.png?imgmax=800\\\" width=\\\"367\\\" height=\\\"151\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-UBX1_1wsKto/UJF5hcKmOOI/AAAAAAAAAfI/AbrArnoYpj8/s1600-h/image%25255B34%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-Z_PVpIocYAc/UJF5iqbgqyI/AAAAAAAAAfQ/jcc_RVFEJns/image_thumb%25255B17%25255D.png?imgmax=800\\\" width=\\\"277\\\" height=\\\"248\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-7UfH6pD_Kb8/UJF5jiprjiI/AAAAAAAAAfY/1vPcR0K845w/s1600-h/image%25255B38%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-rWPfoi6YjSQ/UJF5k5rZOvI/AAAAAAAAAfg/yyQ_n7Ll3R8/image_thumb%25255B19%25255D.png?imgmax=800\\\" width=\\\"392\\\" height=\\\"229\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-u5ZnUp_VMDI/UJF5l12wE_I/AAAAAAAAAfo/KHEygPUuA38/s1600-h/image%25255B42%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-qEynVsScyMs/UJF5nM3oKVI/AAAAAAAAAfw/w15-nDy20qE/image_thumb%25255B21%25255D.png?imgmax=800\\\" width=\\\"384\\\" height=\\\"202\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-qW60sTNeKUE/UJF5ocfQ0BI/AAAAAAAAAf4/BX1Y0bKsWFs/s1600-h/image%25255B46%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-AM-1EhL1QVA/UJF5qMfXKrI/AAAAAAAAAgA/lpoTKKY7Gas/image_thumb%25255B23%25255D.png?imgmax=800\\\" width=\\\"415\\\" height=\\\"350\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-CMIRw7wOgCU/UJF5rvnpxzI/AAAAAAAAAgI/f_cioxXtU-Y/s1600-h/image%25255B50%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-Ns9aBl0cC8I/UJF5tmsFK2I/AAAAAAAAAgQ/6cGfuKo_avM/image_thumb%25255B25%25255D.png?imgmax=800\\\" width=\\\"410\\\" height=\\\"300\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/cc136992(v=vs.85).aspx\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/02/build-deploy-test-workflow-for-physical.html\"]],[\"a\",[\"href\",\"http://hybridlabworkflow.codeplex.com/\"]],[\"strong\"],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/11/developing-hybrid-lab-workflow.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Arguably one of the best features of TFS is Lab Management. I loved Lab Management in TFS 2010, even though it was a real pain to set up. There were a lot of moving parts and setup was tricky, but once you got SCVMM configured the rest was magic.\"]]],[1,\"p\",[[0,[],0,\"TFS 2012 improved Lab Management in three very significant ways:\"]]],[3,\"ol\",[[[0,[],0,\"If you don’t have SCVMM, you can still benefit from Lab Management by using Standard Environments without having to configure anything – just install and configure (or upgrade to) TFS and you can immediately start creating Standard Environments.\"]],[[0,[],0,\"The Lab, Build and Test Agents were consolidated into one agent that Lab Management pushes to the test machines, so you don’t have to set up any agents manually.\"]],[[0,[],0,\"You can now run the Build-Deploy-Test workflow against Standard Environments (without snapshots) and not just against Virtual Environments (what are now called SCVMM environments in TFS 2012).\"]]]],[1,\"p\",[[0,[],0,\"Lab Management now becomes not only powerful, but much easier.\"]]],[1,\"h2\",[[0,[],0,\"Standard Environments\"]]],[1,\"p\",[[0,[],0,\"Standard Environments allow you to configure Labs using either physical machines or “non SCVMM” virtual machines. These could be VMWare or HyperV without SCVMM. However, one of the things I miss about the old Virtual Environment (now called SCVMM Environments) is the ability to snapshot the test machines. That way you can create a “clean” snapshot that has your test machines configured before any deployments. When the Lab Workflow kicks in, you can just restore to this clean point so that any current deployments or environment changes get reversed.\"]]],[1,\"p\",[[0,[],0,\"You can also create another snapshot after deployment (but before testing) so that you can re-test after any automated tests without having to redeploy or manually reset the environment to the “post-deploy-pre-test” state. For example, if you have a test that adds a Customer record, you can only run it once, since the 2nd time you run the test you’re likely to get “Duplicate” errors.\"]]],[1,\"p\",[[0,[],0,\"In my own day-to-day work, I use HyperV on Windows 8 and so I have to create Standard Environments (since SCVMM doesn’t support Client HyperV hosts). But I often longed for the ability to be able to restore to a clean snapshot and take a post-deployment snapshot in the Lab workflow – I am using VMs after all!\"]]],[1,\"h2\",[[0,[],0,\"Enter Hybrid Lab Workflow\"]]],[1,\"p\",[[0,[],0,\"So I started tinkering around with controlling HyperV from C# and found that you could \"],[0,[0],1,\"do it via WMI\"],[0,[],0,\". Once I was able to enumerate VMs and their snapshots, create and apply snapshots, I was ready to take the experience I had from \"],[0,[1],1,\"creating a TFS 2010 physical environment Build-Deploy-Test workflow\"],[0,[],0,\" and incorporate the snapshot bits and Standard Environments. The Hybrid Lab workflow was born!\"]]],[1,\"p\",[[0,[],0,\"Here’s the “simplified” workflow state diagram:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The blue blocks are what you get with the out-of-the-box workflow for Standard Environments. The orange blocks are what you get if you have an SCVMM environment, and what I added in to get the Hybrid Lab workflow.\"]]],[1,\"h2\",[[0,[],0,\"Disclaimer\"]]],[1,\"p\",[[0,[],0,\"I’ve only tested this on HyperV Client (HyperV on Windows 8). So here is my “get out of jail free” image:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Also note that I may sporadically tinker around with this code a little, but don’t expect large scale support. The source code is on \"],[0,[2],1,\"hybridlabworkflow.codeplex.com\"],[0,[],0,\", so if you really want to change things, go ahead!\"]]],[1,\"h2\",[[0,[],0,\"Setting up the Hybrid Lab Workflow\"]]],[1,\"p\",[[0,[],0,\"To set up the Hybrid Lab Workflow, download the latest binaries from \"],[0,[2],1,\"hybridlabworkflow.codeplex.com\"],[0,[],0,\". You should have a few dll’s, PsKill.exe and PsExec.exe and the HybidLabTemplate.xaml. Here are the steps to follow for installation:\"]]],[3,\"ol\",[[[0,[],0,\"Copy the HybridLabTemplate.xaml to the BuildProcessTemplates folder of your Team Project (where the DefaultTemplate, UpgradeTemplate and LabDefaultTemplate.xaml files live).\"]],[[0,[],0,\"Copy all the other files to a folder under source control.\"]],[[0,[],0,\"Check in the files (the XAML and the dll’s and exe’s).\"]]]],[10,2],[3,\"ol\",[[[0,[],0,\"Configure your build controller to point to the folder that contains the Hybrid Lab Workflow binaries (the folder from step 2). You will want to restart Visual Studio at this point so that the wizard can launch properly.\"]]]],[10,3],[3,\"ol\",[[[0,[3],1,\"IMPORTANT\"],[0,[],0,\": Log into the build machine as the build service and run both PsExec.exe and PsKill.exe. You’ll see a EULA popup appear – this only happens on the machine for the login once, so you only need to do this once. If you don’t do this, the Hybrid Lab Workflow will hang. I’ll explain in my \"],[0,[4],1,\"next post\"],[0,[],0,\" why the PsExec and PsKill are necessary at all.\"]]]],[1,\"h2\",[[0,[],0,\"Prerequisites for creating A Hybrid Lab Workflow Build\"]]],[1,\"p\",[[0,[],0,\"Now you’re ready to create a build. The same “pre workflow” steps for the out-of-the-box workflow apply here, as well as a couple of Hybrid Lab Workflow specific ones:\"]]],[3,\"ul\",[[[0,[],0,\"You need to create a standard environment for the workflow out of VMs. The VMs all need to be in HyperV and on the same host. (I’ve made the Lab Workflow extensible for other virtualization platforms like VMWare, but I’ve only implemented HyperV so far – more in another post).\"]],[[0,[],0,\"For a “clean” snapshot, snapshot the VMs AFTER the lab setup and configuration has complete (i.e. when the Environment is in the Ready state in MTM)\"]],[[0,[],0,\"You’ll need to have a build that produces bins that you want to deploy to the environment (this is called the Code build)\"]],[[0,[],0,\"You’ll need to know how to deploy (either have your commands ready or include scripts into the Code build\"]],[[0,[],0,\"If you want to run automated tests, you’ll have to have created them, linked them to test cases in a test plan and created automated test settings\"]],[[0,[],0,\"You’ll need to have an admin username and password for the VM HyperV host machine\"]]]],[1,\"h2\",[[0,[],0,\"Creating the Build\"]]],[1,\"p\",[[0,[],0,\"Click on “New Build Definition” in the Builds pane of the Team Explorer in VS 2012. Name your workflow on the General tab. On the “Build Defaults” tab set the Staging Location to “This build does not copy output files to a drop location”. Then click on the Process tab.\"]]],[1,\"p\",[[0,[],0,\"Change the process template to HybridLabTemplate.xaml (if it’s not in the drop down, then click New, then “Select an existing XAML file” and browse to the source control location of the HybridLabTemplate.xaml).\"]]],[10,4],[1,\"p\",[[0,[],0,\"Next you’ll want to click the ellipses that appear next to “Click here to edit details” of the Lab Process Settings parameter. This will launch the Hybrid Lab Workflow Wizard.\"]]],[1,\"p\",[[0,[],0,\"The Welcome page reminds you of some of the prerequisites for the workflow. Click Next.\"]]],[1,\"h2\",[[0,[],0,\"The Environment Page\"]]],[1,\"p\",[[0,[],0,\"Here you’ll select your environment from the dropdown. You’ll also see the “Restore Snapshots” checkbox. Select this if you want to restore to the “clean” state. If you do, you’ll have to specify which machines and which snapshots you want to apply.\"]]],[1,\"p\",[[0,[],0,\"Initially the Snapshot Information will be empty and you’ll have to connect to your virtual machine host. Click the “…” button next to the “Virtual Host Machine” textbox.\"]]],[10,5],[1,\"p\",[[0,[],0,\"This will open the “Connect to Virtual Host” dialog. Select a host type (at present you’ll only see HyperV). If you’re running this on the host machine, enter “localhost” for the host name and leave the rest of the boxes empty (and make sure you’re running VS as administrator). If the host is another machine, enter the host name, username, domain and password to connect to the host machine. This account must be an administrator on the host machine. Click OK.\"]]],[10,6],[1,\"p\",[[0,[],0,\"If all is well, you’ll see the VMs from the host that are in the Lab Environment populated in the grid. Select which snapshot you want to apply (remember this is pre-deployment) for each machine. Leave the snapshot on <> if you don’t want to apply a snapshot on that machine.\"]]],[1,\"p\",[[0,[],0,\"The final bit of information you’ll need here applies only if you plan to run coded UI tests and one of your lab machines is configured to run interactively. Enter the password for the interactive agent account (I’ll explain why you need this in a later post) – this is the same password that you used when you configured the Lab in MTM.\"]]],[10,7],[1,\"h2\",[[0,[],0,\"The Rest of the Wizard\"]]],[1,\"p\",[[0,[],0,\"The “Configure Build” and “Configure Testing” pages of the wizard work exactly the same as in the Lab Default template. However, there is one more bit of info in the “Deployment Scripts” tab: the “Take a post deployment snapshot” setting. Select this if you want one, and optionally enter a prefix for the snapshot name. Note that all the VMs in the lab will get the same snapshot name when the workflow runs.\"]]],[10,8],[1,\"p\",[[0,[],0,\"Click Finish to close the wizard, and you’re ready to rip.\"]]],[1,\"h2\",[[0,[],0,\"Sample Output\"]]],[1,\"p\",[[0,[],0,\"Here’s a screenshot of a successful run, as well as a look at the VMs in my HyperV. You’ll see that the PostDeployment snapshot name is in the build report.\"]]],[10,9],[10,10],[1,\"p\",[[0,[],0,\"Happy (Hybrid) Lab Workflow-ing!\"]]]]}","published_at":1351743540000,"status":"published","published_by":1},{"id":"36081bf4-c602-47ae-beea-9dd2533c659e","title":"Imaginet (Notion Solutions) wins Microsoft Partner of the Year in the Application Lifecycle Management Category","slug":"imaginet-(notion-solutions)-wins-microsoft-partner-of-the-year-in-the-application-lifecycle-management-category","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://www.notionsolutions.com\"]],[\"a\",[\"href\",\"http://www.imaginets.com/\"]],[\"a\",[\"href\",\"http://bit.ly/microsoftpartner\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"Notion Solutions\"],[0,[],0,\" is part of \"],[0,[1],1,\"Imaginet\"],[0,[],0,\", and last week Microsoft announced that Imaginet had won the Microsoft Partner of the Year in the inaugural Application Lifecycle Management category.\"]]],[1,\"p\",[[0,[],0,\"We were selected from over 3000 other worldwide entrants in various categories. The award honours partners who have exhibited excellence in providing services that increase the speed of deployment or implementation of Microsoft Visual Studio.\"]]],[1,\"p\",[[0,[],0,\"Hear our CEO, Rod Giesbrecht, \"],[0,[2],1,\"talk about\"],[0,[],0,\" what this award means to Imaginet.\"]]]]}","published_at":1308928140000,"status":"published","published_by":1},{"id":"ddead0a8-9793-4305-aaa9-2f1ee2abb58c","title":"Imaginet Timesheet: Time Tracking for TFS and Visual Studio Online","slug":"imaginet-timesheet-time-tracking-for-tfs-and-visual-studio-online","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://www.imaginet.com/imaginettimesheet\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"We’ve been working on a rewrite of our Timetracking tool (formerly Notion Timesheet) and it’s going live today – \"],[0,[0],1,\"Imaginet Timesheet\"],[0,[],0,\"! Timesheet lets you log time against TFS work items using a web interface. The web site can be installed on any IIS server (if you want to host it on-premises) or even onto Windows Azure Web Sites (WAWS) if you have a public-facing TFS or are using Visual Studio Online. Once you’ve installed it, just log in, select a date-range (week) and a query and start logging time.\"]]],[1,\"p\",[[0,[],0,\"It’s free for up to 5 users so you can try it out to see if it works for you and your organization. There are some report samples out-the-box and you can also easily create your own reports using PowerPivot.\"]]],[1,\"p\",[[0,[],0,\"We used Entity Framework, MVC, Bootstrap and Knockout to make the site. Most of our JavaScript is typed using TypeScript. Of course we have unit tests (.NET and JavaScript) and a build that builds the installer (Wix) package. It was a fun project to work on and I think we’ve turned out a great product. Download your copy today!\"]]],[1,\"p\",[[0,[],0,\"Here’s the overview video:\"]]],[1,\"p\",[[0,[],0,\"Happy timetracking!\"]]]]}","published_at":1401417186000,"status":"published","published_by":1},{"id":"227c45a4-594d-4647-9a02-f470d43b25f7","title":"Implement an Azure DevOps Release Gate to ServiceNow","slug":"implement-an-azure-devops-release-gate-to-servicenow","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">https://&lt;instance&gt;.servicenow.com/api/now/table/change_request?sysparm_query=number=&lt;number&gt;&amp;sysparm_fields=state&amp;sysparm_display_value=true</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">{\\\"result\\\":[{\\\"state\\\":\\\"Implement\\\"}]}</font>\"}],[\"code\",{\"code\":\"\\\"inputs\\\": [\\n  {\\n    \\\"name\\\": \\\"connectedServiceName\\\",\\n    \\\"type\\\": \\\"connectedService:ServiceNow\\\",\\n    \\\"label\\\": \\\"Service Now endpoint\\\",\\n    \\\"required\\\": true,\\n    \\\"helpMarkDown\\\": \\\"Service Now endpoint connection.\\\"\\n  },\\n  {\\n    \\\"name\\\": \\\"crNumber\\\",\\n    \\\"type\\\": \\\"string\\\",\\n    \\\"label\\\": \\\"Change Request number\\\",\\n    \\\"defaultValue\\\": \\\"\\\",\\n    \\\"required\\\": true,\\n    \\\"helpMarkDown\\\": \\\"Change Request number to check.\\\"\\n  },\\n  {\\n    \\\"name\\\": \\\"validState\\\",\\n    \\\"type\\\": \\\"string\\\",\\n    \\\"label\\\": \\\"State\\\",\\n    \\\"defaultValue\\\": \\\"Implement\\\",\\n    \\\"helpMarkDown\\\": \\\"State that the CR should be in to pass the gate.\\\",\\n    \\\"required\\\": true\\n  }\\n]\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"\\\"execution\\\": {\\n  \\\"HttpRequest\\\": {\\n    \\\"Execute\\\": {\\n      \\\"EndpointId\\\": \\\"$(connectedServiceName)\\\",\\n      \\\"EndpointUrl\\\": \\\"$(endpoint.url)/api/now/table/change_request?sysparm_query=number=$(crNumber)&amp;sysparm_fields=state&amp;sysparm_display_value=true\\\",\\n      \\\"Method\\\": \\\"GET\\\",\\n      \\\"Body\\\": \\\"\\\",\\n      \\\"Headers\\\": \\\"{\\\\\\\"Content-Type\\\\\\\":\\\\\\\"application/json\\\\\\\"}\\\",\\n      \\\"WaitForCompletion\\\": \\\"false\\\",\\n      \\\"Expression\\\": \\\"eq(jsonpath('$.result[0].state')[0], '$(validState)')\\\"\\n    }\\n  }\\n}\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"{\\n  \\\"id\\\": \\\"colinsalmcorner-snow-endpoint-type\\\",\\n  \\\"type\\\": \\\"ms.vss-endpoint.service-endpoint-type\\\",\\n  \\\"targets\\\": [\\n    \\\"ms.vss-endpoint.endpoint-types\\\"\\n  ],\\n  \\\"properties\\\": {\\n    \\\"name\\\": \\\"ServiceNow\\\",\\n    \\\"displayName\\\": \\\"Service Now\\\",\\n    \\\"helpMarkDown\\\": \\\"Create an authenticated endpoint to a Service Now instance.\\\",\\n    \\\"url\\\": {\\n      \\\"displayName\\\": \\\"Service Now URL\\\",\\n         \\\"description\\\": \\\"The Service Now instance Url, e.g. `https://instance.service-now.com`.\\\"\\n    },\\n    \\\"authenticationSchemes\\\": [\\n    {\\n      \\\"type\\\": \\\"ms.vss-endpoint.endpoint-auth-scheme-basic\\\",\\n      \\\"inputDescriptors\\\": [\\n        {\\n          \\\"id\\\": \\\"username\\\",\\n          \\\"name\\\": \\\"Username\\\",\\n          \\\"description\\\": \\\"Username\\\",\\n          \\\"inputMode\\\": \\\"textbox\\\",\\n          \\\"isConfidential\\\": false,\\n          \\\"validation\\\": {\\n            \\\"isRequired\\\": true,\\n            \\\"dataType\\\": \\\"string\\\",\\n            \\\"maxLength\\\": 300\\n          }\\n        },\\n        {\\n          \\\"id\\\": \\\"password\\\",\\n          \\\"name\\\": \\\"Password\\\",\\n          \\\"description\\\": \\\"Password for the user account.\\\",\\n          \\\"inputMode\\\": \\\"passwordbox\\\",\\n          \\\"isConfidential\\\": true,\\n          \\\"validation\\\": {\\n            \\\"isRequired\\\": true,\\n            \\\"dataType\\\": \\\"string\\\",\\n            \\\"maxLength\\\": 300\\n          }\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">ms.vss-endpoint.endpoint-auth-scheme-basic</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/772b0be7-080e-439d-95cb-72e002288330.png\\\" target=\\\"_blank\\\"><img width=\\\"482\\\" height=\\\"78\\\" title=\\\"SNAGHTMLaca5f5b\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"SNAGHTMLaca5f5b\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b2a33460-f6e7-448b-b85f-862ac2c19f86.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/900b68e0-511c-46bf-81e4-9f52d23df052.png\\\" target=\\\"_blank\\\"><img width=\\\"439\\\" height=\\\"362\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/03fc601c-eebd-435d-a80f-5bb40ae20032.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/930b95a8-7435-4353-8e27-2b13152ac289.png\\\" target=\\\"_blank\\\"><img width=\\\"446\\\" height=\\\"418\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ea57f2f3-432b-4cab-9a3f-462598289d4f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$(ChangeRequestNumber)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/dd675a14-0c80-47de-a015-96dcc14527c1.png\\\" target=\\\"_blank\\\"><img width=\\\"426\\\" height=\\\"304\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e542cf32-4c7a-48fa-a0c8-e50b9d1edb9f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0a3ba383-88c6-41fa-bf64-9c1a8b2b60f1.png\\\" target=\\\"_blank\\\"><img width=\\\"435\\\" height=\\\"140\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/673645cc-d97d-499e-a210-a8d815201d42.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/pipelines/release/approvals/gates?view=vsts\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-service-now-extensions\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=ms-devlabs.vsts-developer-tools-build-tasks\"]],[\"a\",[\"href\",\"https://jessehouwing.net/\"]],[\"a\",[\"href\",\"https://bit.ly/cacbuildtasks\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-service-now-extensions/blob/master/Tasks/SnowChangeRequestGate/task.json\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=colinsalmcorner.colinsalmcorner-snow-extensions\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I'm currently doing some work with a customer that is integrating between ServiceNow and Azure DevOps (the artist formerly known as VSTS). I quickly spun up a development ServiceNow instance to play around a bit. One of the use-cases I could foresee was a release gate that only allows a release to continue if a Change Request (CR) is in the Implement state. So I had to do some investigation: I know there are a few out-of-the-box Azure DevOps \"],[0,[0],1,\"release gates\"],[0,[],0,\", including a REST API call - but I knew that you could also create a custom gate. I decided to see if I could create the gate without expecting the release author having to know the REST API call to ServiceNow or how to parse the JSON response!\"]]],[1,\"p\",[[0,[],0,\"Follow along to see the whole process - or just grab the code in the \"],[0,[1],1,\"Github repo\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Finding the ServiceNow REST API\"]]],[1,\"p\",[[0,[],0,\"Part One of my quest was to figure out the REST API call to make to ServiceNow. The ServiceNow documentation is ok - perhaps if you understand ServiceNow concepts (and I don't have deep experience with them) then they're fine. But I quickly felt like I was getting lost in the weeds. Add to that many, many versions of the product - which all seem to have different APIs. After a couple hours I did discover that the ServiceNow instance has a REST API explorer - but I'm almost glad I didn't start there as you do need some knowledge of the product in order to really use the explorer effectively. For example, I was able to query the state of the CR if I had its internal sys_id, but I didn't expect the user to have that. I wanted to get the state of the CR by its number - and how to do that wasn't obvious from the REST API explorer.\"]]],[1,\"p\",[[0,[],0,\"Anyway, I was able to find the REST API to query the state of a Change Request:\"]]],[10,0],[1,\"p\",[[0,[],0,\"A couple notes on the query strings:\"]]],[3,\"ul\",[[[0,[],0,\"sysparm_query lets me specify that I want to query the change_request table for the expression \\\"number=<number>\\\", which lets me get the CR via its number instead of its sys_id\"]],[[0,[],0,\"sysparm_fields lets me specify which fields I want returned - in this case, just the state field\"]],[[0,[],0,\"sysparm_value=true expands the enums from ints to strings, so I get the \\\"display value\\\" of the state instead of the state ID\"]]]],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"The next problem is authentication - turns out if you have a username and password for your ServiceNow instance, you can include a standard auth header using BasicAuth (this is over HTTPS, so that's ok). I tested this with curl and was able to get a response that looks something like this:\"]]],[10,1],[1,\"h2\",[[0,[],0,\"Creating a Custom Release Gate Extension\"]]],[1,\"p\",[[0,[],0,\"Now that I know the REST API call to ServiceNow, I turned to how to Part Two of my quest: create a custom Release Gate extension. Fortunately, I had Microsoft DevLabs' great \"],[0,[2],1,\"Azure DevOps Extension extension\"],[0,[],0,\" as a reference (this was originally from \"],[0,[3],1,\"Jesse Houwing\"],[0,[],0,\") - and I use this all the time to package and publish my own \"],[0,[4],1,\"Azure DevOps Build and Release extension pack\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"It turns out that the release gate \\\"task\\\" itself is pretty simple, since the entire task is just a JSON file which specifies its UI and the expression to evaluate on the response packet. The full file is \"],[0,[5],1,\"here\"],[0,[],0,\" but let's examine the two most important parts of this task: the \\\"inputs\\\" element and the \\\"execution\\\" element. First the inputs:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"connectedServiceName is of type \\\"connectedService:ServiceNow\\\". This is the endpoint used to call the REST API and should handle authentication.\"]],[[0,[],0,\"crNumber is a string and is the CR number we're going to search on\"]],[[0,[],0,\"validState is a string and is the state the CR should be in to pass the gate\"]]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"Given those inputs, we can look at the execute element:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"The execution is an HttpRequest\"]],[[0,[],0,\"Endpoint is set to the connectedService input\"]],[[0,[],0,\"EndpointUrl is the full URL to use to hit the REST API\"]],[[0,[],0,\"The REST method is a GET\"]],[[0,[],0,\"The body is empty\"]],[[0,[],0,\"We're adding a Content-Type header of \\\"application/json\\\" - notice that we don't need to specify auth headers since the Endpoint will take care of that for us\"]],[[0,[],0,\"The expression to evaluate is checking that the state field of the first result is set to the value of the validState variable\"]]]],[1,\"p\",[[0,[],0,\"And that's it! Let's take a look at the connected service endpoint, which is defined in the extension manifest (not in the task definition):\"]]],[10,4],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 2-6: specify that this contribution is of type Service Endpoint\"]],[[0,[],0,\"Line 8: name of the endpoint type - this is referenced by the gate in the endpoint input\"]],[[0,[],0,\"Lines 9-10: description and help text\"]],[[0,[],0,\"Line 11-14: specify a URL input for this endpoint\"]],[[0,[],0,\"The rest: specify the authentication scheme for the endpoint\"]]]],[1,\"p\",[[0,[],0,\"By default the\"]]],[10,5],[1,\"p\",[[0,[],0,\"authentication scheme adds an Authorization header to any request made to the URL of the service endpoint. The value of the header is a base64 encoded munge of user:password. It's great that you don't have to mess with this yourself!\"]]],[1,\"h2\",[[0,[],0,\"Putting It All Together\"]]],[1,\"p\",[[0,[],0,\"Now we have the service endpoint and the gate, we're ready to publish and install the extension! The readme.md in the repo has some detail on this if you want to try your own (or make changes to the code from mine), or you can just install the \"],[0,[6],1,\"extension that I've published\"],[0,[],0,\" if you want to use the gate as-is. If you do publish it yourself, you'll need to change the publisher and the GUIDs before you publish.\"]]],[1,\"p\",[[0,[],0,\"For the release to work, you'll need to make the CR a variable. I did this by adding the variable and making it settable at queue time:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Now when I queue the release, I have to add the CR. Of course you could imagine a release being queued off from an automated process, and that can pass the CR as part of the body of the REST API call to queue the release. For now, I'm entering it manually:\"]]],[10,7],[1,\"p\",[[0,[],0,\"So how do we specify the gate? Edit the release and click on the pre- or post-approval icon for the environment and open the Gates section. Click the + to add a new gate and select the \\\"Change Request Status\\\" gate. We can then configure the endpoint, the CR number and the State we want to pass on:\"]]],[10,8],[1,\"p\",[[0,[],0,\"To create an endpoint, just click on \\\"+ New\\\" next to the Service Now endpoint drop-down - this will open a new tab to the Service Endpoints page where you can add a new ServiceNow endpoint.\"]]],[1,\"p\",[[0,[],0,\"Note how we set the Change Request number to the variable\"]]],[10,9],[1,\"p\",[[0,[],0,\". That way this field is dynamic.\"]]],[1,\"p\",[[0,[],0,\"Finally, set the \\\"Evaluation options\\\" to configure the frequency, timeout and other gate settings:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Once the release runs, we can see the Gate invocations and results:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Note that the Gate has to pass twice in a row before it's successful and moves the pipeline on.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Creating release gates as extensions is not too hard once you have some of the bits in place. And it's a far better authoring experience than the out of the box REST API call - which leaves you trying to mess with auth headers and parsing JSON responses. If you want to get release authors to really fully utilize the power of gates, do them a solid and wrap the gate in an extension!\"]]],[1,\"p\",[[0,[],0,\"Happy gating!\"]]]]}","published_at":1540005893000,"status":"published","published_by":1},{"id":"f6ff63db-be8b-4b3b-818e-88c69c2cfadc","title":"Improvements to Code Review Checkin Policy","slug":"improvements-to-code-review-checkin-policy","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-b8GGm8WncxY/UZpCUC-wWHI/AAAAAAAAAuk/rcUzkAhkcTM/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-r87J-QroDqY/UZpCWES-vmI/AAAAAAAAAus/E0SRc-OZ3_U/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"266\\\" height=\\\"227\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/c476b708-77a8-4065-b9d0-919ab688f078\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2012/12/custom-code-review-checkin-policy.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Late last year I uploaded my first VS Gallery contribution – \"],[0,[0],1,\"Colin’s ALM Corner Checkin Policies\"],[0,[],0,\". One of the policies in this pack is a Code Review Checkin Policy. I blogged about it in \"],[0,[1],1,\"this post\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"One of the pieces of feedback I got about this policy was that this is counter the “checkin early, checkin often” mantra of most development shops. Some suggestions were to only apply this policy to “junior” team members, allowing “senior” members to checkin without requiring a review. I decided however to approach this from a source control perspective as opposed to a group membership perspective.\"]]],[1,\"p\",[[0,[],0,\"The policy now allows you to configure which source control paths it must fire on – so if you checkin files in the “mapping”, a code review is required, otherwise no code review is necessary. This means you can make the DEV branch (or your equivalent) review-free, while enforcing reviews on MAIN or LIVE branches, according to your practices. This way you get the benefit of “checkin early, checkin often” without the hassle of reviewing every single change. When you merge a bunch of changes through to MAIN or LIVE, you can then enforce Code Review (you would perform your code review on the merge-set).\"]]],[1,\"p\",[[0,[],0,\"Here’s what the Policy Configuration settings now look like:\"]]],[10,0],[1,\"p\",[[0,[],0,\"You can see a list of “Paths to apply policy to”. Just add your source folders here and you get finer-grained control over when a Code Review is required.\"]]],[1,\"p\",[[0,[],0,\"Happy reviewing!\"]]]]}","published_at":1369096380000,"status":"published","published_by":1},{"id":"114625bd-a2e9-471a-990e-b8c0544724b3","title":"Install and Configure SQL Server using PowerShell DSC","slug":"install-and-configure-sql-server-using-powershell-dsc","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b2b82361-a674-42ca-a2c3-f221f092fe92.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8eb3ba64-e66c-496f-8cdc-56ff047923ed.png\\\" width=\\\"644\\\" height=\\\"483\\\"></a>\"}],[\"code\",{\"code\":\"Configuration SQLInstall\\n{\\n    param (\\n        [Parameter(Mandatory=$true)]\\n        [ValidateNotNullOrEmpty()]\\n        [String]\\n        $PackagePath,\\n\\n        [Parameter(Mandatory=$true)]\\n        [ValidateNotNullOrEmpty()]\\n        [String]\\n        $WinSources\\n    )\\n\\n    Node $AllNodes.where{ $_.Role.Contains(\\\"SqlServer\\\") }.NodeName\\n    {\\n        Log ParamLog\\n        {\\n            Message = \\\"Running SQLInstall. PackagePath = $PackagePath\\\"\\n        }\\n\\n        WindowsFeature NetFramework35Core\\n        {\\n            Name = \\\"NET-Framework-Core\\\"\\n            Ensure = \\\"Present\\\"\\n            Source = $WinSources\\n        }\\n\\n        WindowsFeature NetFramework45Core\\n        {\\n            Name = \\\"NET-Framework-45-Core\\\"\\n            Ensure = \\\"Present\\\"\\n            Source = $WinSources\\n        }\\n\\n        # copy the sqlserver iso\\n        File SQLServerIso\\n        {\\n            SourcePath = \\\"$PackagePath\\\\en_sql_server_2012_developer_edition_x86_x64_dvd_813280.iso\\\"\\n            DestinationPath = \\\"c:\\\\temp\\\\SQLServer.iso\\\"\\n            Type = \\\"File\\\"\\n            Ensure = \\\"Present\\\"\\n        }\\n\\n        # copy the ini file to the temp folder\\n        File SQLServerIniFile\\n        {\\n            SourcePath = \\\"$PackagePath\\\\ConfigurationFile.ini\\\"\\n            DestinationPath = \\\"c:\\\\temp\\\"\\n            Type = \\\"File\\\"\\n            Ensure = \\\"Present\\\"\\n            DependsOn = \\\"[File]SQLServerIso\\\"\\n        }\\n\\n        #\\n        # Install SqlServer using ini file\\n        #\\n        Script InstallSQLServer\\n        {\\n            GetScript = \\n            {\\n                $sqlInstances = gwmi win32_service -computerName localhost | ? { $_.Name -match \\\"mssql*\\\" -and $_.PathName -match \\\"sqlservr.exe\\\" } | % { $_.Caption }\\n                $res = $sqlInstances -ne $null -and $sqlInstances -gt 0\\n                $vals = @{ \\n                    Installed = $res; \\n                    InstanceCount = $sqlInstances.count \\n                }\\n                $vals\\n            }\\n            SetScript = \\n            {\\n                # mount the iso\\n                $setupDriveLetter = (Mount-DiskImage -ImagePath c:\\\\temp\\\\SQLServer.iso -PassThru | Get-Volume).DriveLetter + \\\":\\\"\\n                if ($setupDriveLetter -eq $null) {\\n                    throw \\\"Could not mount SQL install iso\\\"\\n                }\\n                Write-Verbose \\\"Drive letter for iso is: $setupDriveLetter\\\"\\n                \\n                # run the installer using the ini file\\n                $cmd = \\\"$setupDriveLetter\\\\Setup.exe /ConfigurationFile=c:\\\\temp\\\\ConfigurationFile.ini /SQLSVCPASSWORD=P2ssw0rd /AGTSVCPASSWORD=P2ssw0rd /SAPWD=P2ssw0rd\\\"\\n                Write-Verbose \\\"Running SQL Install - check %programfiles%\\\\Microsoft SQL Server\\\\120\\\\Setup Bootstrap\\\\Log\\\\ for logs...\\\"\\n                Invoke-Expression $cmd | Write-Verbose\\n            }\\n            TestScript =\\n            {\\n                $sqlInstances = gwmi win32_service -computerName localhost | ? { $_.Name -match \\\"mssql*\\\" -and $_.PathName -match \\\"sqlservr.exe\\\" } | % { $_.Caption }\\n                $res = $sqlInstances -ne $null -and $sqlInstances -gt 0\\n                if ($res) {\\n                    Write-Verbose \\\"SQL Server is already installed\\\"\\n                } else {\\n                    Write-Verbose \\\"SQL Server is not installed\\\"\\n                }\\n                $res\\n            }\\n        }\\n    }\\n}\\n\\n# command for RM\\n#SQLInstall -ConfigurationData $configData -PackagePath \\\"\\\\\\\\rmserver\\\\Assets\\\" -WinSources \\\"d:\\\\sources\\\\sxs\\\"\\n\\n# test from command line\\nSQLInstall -ConfigurationData configData.psd1 -PackagePath \\\"\\\\\\\\rmserver\\\\Assets\\\" -WinSources \\\"d:\\\\sources\\\\sxs\\\"\\nStart-DscConfiguration -Path .\\\\SQLInstall -Verbose -Wait -Force\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"#$configData = @{\\n@{\\n    AllNodes = @(\\n        @{\\n            NodeName = \\\"*\\\"\\n            PSDscAllowPlainTextPassword = $true\\n         },\\n\\n        @{\\n            NodeName = \\\"fabfiberserver\\\"\\n            Role = \\\"WebServer,SqlServer\\\"\\n         }\\n    );\\n}\\n\\n# Note: different 1st line for RM or command line invocation\\n# use $configData = @{ for RM\\n# use @{ for running from command line\\n\",\"language\":\"ps;\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/powershell-dsc-remotely-configuring-a-node-to-rebootnodeifneeded\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/using-powershell-dsc-in-release-management-the-hidden-manual\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/more-dsc-release-management-goodness-readying-a-webserver-for-deployment\"]],[\"a\",[\"href\",\"http://1drv.ms/1n6C0J9\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd239405.aspx\"]],[\"a\",[\"href\",\"http://www.mssqltips.com/sqlservertip/2511/standardize-sql-server-installations-with-configuration-files/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’m well into my journey of discovering the capabilities of PowerShell DSC and Release Management’s DSC feature (See my previous posts: \"],[0,[0],1,\"PowerShell DSC: Configuring a Remote Node to “Reboot If Needed”\"],[0,[],0,\", \"],[0,[1],1,\"Using PowerShell DSC in Release Management: The Hidden Manual\"],[0,[],0,\" and \"],[0,[2],1,\"More DSC Release Management Goodness: Readying a Webserver for Deployment\"],[0,[],0,\"). I’ve managed to work out how to use Release Management to run DSC scripts on nodes. Now I am trying to construct a couple of scripts that I can use to deploy applications to servers – including, of course, configuring the servers – using DSC. (All scripts for this post are available for download \"],[0,[3],1,\"here\"],[0,[],0,\").\"]]],[1,\"h2\",[[0,[],0,\"SQL Server Installation\"]]],[1,\"p\",[[0,[],0,\"To install SQL Server via a script, there are two prerequisites: the SQL install sources and a silent (or unattended) installation command.\"]]],[1,\"p\",[[0,[],0,\"Fortunately the SQL server installer takes care of the install command – you run the install wizard manually, specifying your installation options as you go. On the last page, just before clicking “Install”, you’ll see a path to the ini conifguration file. I saved the configuration file and cancelled the install. Then I opened the config file and tweaked it slightly (see \"],[0,[4],1,\"this post\"],[0,[],0,\" and \"],[0,[5],1,\"this post\"],[0,[],0,\" on some tweaking ideas)– till I could run the installer from the command line (using the /configurationFile switch). That takes care of the install command itself.\"]]],[10,0],[1,\"p\",[[0,[],0,\"There are many ways to make the SQL installation sources available to the target node. I chose to copy the ISO to the node (using the File DSC resource) from a network share, and then use a Script resource to mount the iso. Once it’s mounted, I can run the setup command using the ini file.\"]]],[1,\"p\",[[0,[],0,\"SQL Server requires .NET 3.5 to be installed on the target node, so I’ve added that into the script using the WindowsFeature resource. Here’s the final script:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here’s some analysis:\"]]],[3,\"ul\",[[[0,[],0,\"(Line 7 / 12) The config takes in 2 parameters: $PackagePath (location of SQL ISO and config ini file) and $WinSources (Path to windows sources).\"]],[[0,[],0,\"(Line 15) I changed my config data so that I can specify a comma-separated list of roles (since a node might be a SQLServer and a WebServer) so I’ve made the comparer a “contains” rather than an equals (as I’ve had in my previous scripts) – see the config script below.\"]],[[0,[],0,\"(Line 22 / 29) Configure .NET 3.5 and .NET 4.5 Windows features, using the $WinSources path if the sources are required\"]],[[0,[],0,\"(Line 37) Copy the SQL iso to the target node from the $PackagePath folder\"]],[[0,[],0,\"(Line 46) Copy the ini file to the target node from the $PackagePath folder\"]],[[0,[],0,\"(Line 58) Begins the Script to install SQL server\"]],[[0,[],0,\"The Get-Script does a check to see if there is a SQL server service running. If there is, it returns the SQL instance count for the machine.\"]],[[0,[],0,\"The Set-Script mounts the iso, saving the drive letter to a variable. Then I invoke the setup script (passing in the config file and required passwords) writing the output to Write-Verbose, which will appear on the DSC invoking machine as the script executes.\"]],[[0,[],0,\"The Test-Script does the same basic “is there a SQL server service running” check. If there is, skip the install – else run the install. Of course this could be refined to ensure each and every component is installed, but I didn’t want to get that granular.\"]],[[0,[],0,\"The last couple of lines of the script show the command for Release Management (commented out) as well as the command to run the script manually from a PowerShell prompt.\"]]]],[1,\"p\",[[0,[],0,\"Here’s my DSC config script:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can download the above scripts (and my SQL configuration ini file for reference) \"],[0,[3],1,\"here\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"What’s Next\"]]],[1,\"p\",[[0,[],0,\"After running this script, I have a server with SQL Server installed and configured according to my preferences (which are contained in the ini file). From here, I can run restores or dacpac deployments and so on. Of course this is going to be executed from within Release Management as part of the release pipeline.\"]]],[1,\"p\",[[0,[],0,\"Next up will be the full WebServer DSC script – and then we’ll be ready to tackle the actual application deployment, since we’ll have servers ready to host our applications.\"]]],[1,\"p\",[[0,[],0,\"Until then, happy releasing!\"]]]]}","published_at":1405460113000,"status":"published","published_by":1},{"id":"85753968-4f09-4e02-bcd3-501fcdab3278","title":"Integrating TFS and Project Server – Two Way Manual Sync","slug":"integrating-tfs-and-project-server--two-way-manual-sync","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-1U07pdzT8So/U0ZoEDsILRI/AAAAAAAABQA/2wR2fSttZGA/image%25255B4%25255D.png?imgmax=800\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-RxAf9MlfQZc/U0ZoYu3aGdI/AAAAAAAABQI/FWdajgisrU8/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"388\\\" height=\\\"137\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-kYUvjCANm-Q/U0ZogSgS4wI/AAAAAAAABQQ/ztg49Jb4-J4/s1600-h/image%25255B8%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-v6-6p7k9vdI/U0Zoy9Gzw0I/AAAAAAAABQY/n3nnHwbAcmo/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"302\\\" height=\\\"113\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-Xq6q9B_jy1g/U0ZpVrK1sxI/AAAAAAAABQg/mCMekBn4vgE/s1600-h/image%25255B12%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-RdTNXoOuiMM/U0ZpawIdX_I/AAAAAAAABQo/tF5-8bA1k0M/image_thumb%25255B6%25255D.png?imgmax=800\\\" width=\\\"350\\\" height=\\\"93\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-cNpEfJv1BeY/U0ZpprvKNPI/AAAAAAAABQw/Bb_IuqGfppY/s1600-h/image%25255B16%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-BHD9yxoxJM4/U0ZpyPl3BdI/AAAAAAAABQ4/wx1vkMzy1QA/image_thumb%25255B8%25255D.png?imgmax=800\\\" width=\\\"349\\\" height=\\\"108\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-YFplXjsmygM/U0ZqbY8PEvI/AAAAAAAABRA/1KXsqReAjKQ/s1600-h/image%25255B20%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-71hMv3PfjjU/U0Zqce9dqjI/AAAAAAAABRI/konInnYntdc/image_thumb%25255B10%25255D.png?imgmax=800\\\" width=\\\"311\\\" height=\\\"184\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-snsqKnV-rbU/U0ZqdQQCvBI/AAAAAAAABRQ/WmymmFuD6sY/s1600-h/image%25255B26%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-tH3HO9xMQJk/U0ZqeM_xRDI/AAAAAAAABRY/Qeh4-xVGssY/image_thumb%25255B14%25255D.png?imgmax=800\\\" width=\\\"398\\\" height=\\\"83\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-2HjkNrHVrJI/U0ZqfNPxn4I/AAAAAAAABRg/Vas5fjPcAdA/s1600-h/image%25255B30%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; border-left: 0px; display: inline\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-qgqXZ_rsFjE/U0ZqgB8694I/AAAAAAAABRo/1Fyu40l_p50/image_thumb%25255B16%25255D.png?imgmax=800\\\" width=\\\"498\\\" height=\\\"126\\\"></a>\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/gg455680.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/gg593279.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2011/07/tfs-and-project-server-integration-tips.html\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/07/adding-custom-team-field-to-ms-project.html\"]],[\"a\",[\"href\",\"http://blog.imaginet.com/coming-early-this-summer-imaginet-timesheet-for-tfs-2013-and-visual-studio-online/\"]],[\"a\",[\"href\",\"http://www.amazon.com/Principles-Product-Development-Flow-Generation-ebook/dp/B007TKU0O0/ref=sr_1_1?ie=UTF8&qid=1397122619&sr=8-1\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I often do road-shows showing off TFS and VS to customers around South Africa. Usually I’m doing this with Ahmed Salijee, the Developer Platform Specialist (DPS) for Developer Tools in Microsoft South Africa. Ahmed is an amazing speaker (we’ve co-presented regularly) and is great at helping customers at a strategic level – and, as he likes to say, for his sins, he gets to help customers with their licensing queries!\"]]],[1,\"p\",[[0,[],0,\"Ahmed and I agree on most aspects of Application Lifecycle Management (ALM) using TFS – but one of the places we disagree on is the integration of TFS and Project Server.\"]]],[1,\"h2\",[[0,[],0,\"Philosophy: Why You Shouldn’t Be Using Project Server in the First Place\"]]],[1,\"p\",[[0,[],0,\"If you don’t care to wax philosophical about Project Server, then skip this section. However, I think it’s important to step back and think about what you’re getting into if you’re planning on using Project Server for tracking software development projects.\"]]],[1,\"p\",[[0,[],0,\"Project Plans are made for environments where “change is bad”. Think about what Project Managers do – they create a plan, perhaps entering in requirements, breaking those down into tasks with estimates that they then farm out to team members. Then they do some curious things: they set milestones and they baseline the project. Let’s examine what milestones and baselining mean.\"]]],[1,\"p\",[[0,[],0,\"Milestones are points along the way that Project Managers use to answer a simple question: are we conforming to the plan? Baselines communicate an idea: “What we’ve planned is what we value – we should not deviate from this plan. If we do, then it’ll Be Bad”. Project Plans work well when tasks are exactly predictable and repeatable. Unfortunately, that’s not the case in software development. There are many reasons why this is the case – requirements are language-based, and as such are subject to misinterpretations. Coding is an abstract art – and as such it’s extremely difficult to estimate how long a task will take with any accuracy. Even if you get the code bits right, there are always unforeseen issues in integration of components, deployments, testing and so on. This is why people lie about progress – who wants to deviate from the baseline when the baseline is the Ultimate Good? We’d rather bend the truth about how far we are so that, on paper at least, we look good.\"]]],[1,\"p\",[[0,[],0,\"That’s why Agile has come to the form. The basic philosophy of any Agile technique is \"],[0,[0],1,\"embrace the change\"],[0,[],0,\". If we know things are going to change, then why not embrace the change? Let’s shorten cycle times so that we can get more rapid feedback – that way we minimize the risk of doing the wrong thing for too long. Let’s focus on measuring what value we deliver to business, rather than tracking \"],[0,[0],1,\"conformance to a plan\"],[0,[],0,\". Let’s make it concrete: imagine two teams. Team A sticks to the Project Plan closely, and after months of work deliver a system that is average (statistics show that most waterfall-based projects don’t even reach delivery, so we’re being optimistic about Team A’s delivery). Team B deviates widely from their Project Plan, deliver small components frequently and at the end of the project have exceeded expectations. Which team is “better”? The one that stuck to the Plan or the one that Delivered Value? And yet, if we really care about delivering value, why do we beat on teams to Stick To The Plan?\"]]],[1,\"h2\",[[0,[],0,\"TFS and Project Server Integration: Good and Bad\"]]],[1,\"p\",[[0,[],0,\"So you’re insisting on integrating with Project Server anyway. Fair enough. Let’s examine the \"],[0,[1],1,\"TFS to Project Server integration\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"The good: avoid double entry. That’s about it. The integration allows Work Items added in TFS to be synced over to Project Server (or vice-versa). The integration simply means you don’t have to add (or update progress) in two places.\"]]],[1,\"p\",[[0,[],0,\"Perhaps I could add that once you have tasks in a project plan you can do resource leveling and all the other “voodoo” that Project is capable of, but frankly I think that that’s a waste of effort. Let’s imagine you’ve spent a couple of days doing all the leveling. After 3 days, Bob gets sick and is off for 2 days. So you re-level everything, carefully watching as your project drifts from the baseline. Next thing Joe comes and tells you he forgot that one of the changes he’s making will require an extra 2 days of refactoring. You adjust again, sweating a little as your plan deviates further from the baseline. You call up Frank and tell him he’ll have to work the weekend so that you can catch up. Every time something happens, you’re frantically re-adjusting your project plan. At the end, the actual is so far off the baseline, you wonder why you bothered in the first place.\"]]],[1,\"p\",[[0,[],0,\"If you train your teams to self-organize, you can track progress of delivered value (which is a much better thing to measure than “conformance to a plan”), rather than dictate who should be doing what when. As changes come in, you embrace the change and adjust course, smiling because change is now a positive thing – not a shame.\"]]],[1,\"p\",[[0,[],0,\"If you’re still insisting on integrating TFS and Project Server, there are some caveats that you’ll need to know about before embarking on the integration:\"]]],[3,\"ol\",[[[0,[],0,\"Installing the Integration will modify your process templates. The integration adds a bunch of fields and a Project Server form to your work items.\"]],[[0,[],0,\"You need to configure which PS Projects can be linked to which TFS Team Projects. A PS Project can only be linked to 1 TFS Team Project. If you have a lot of PS Projects (or a lot of TFS Team Projects, or lots of both) you’re going to end up with a lot of integration admin.\"]],[[0,[],0,\"You need to configure which work items are synced across – at TFS Team Project level. The integration requires you to tell the connector which Work Items it needs to sync. Again, if you have large amounts of PS or TFS Team Projects, you’re looking at a lot of admin.\"]],[[0,[],0,\"Updating tasks in TFS does not fill in the Timesheet in Project Server. TFS has no knowledge of \"],[0,[0],1,\"when\"],[0,[],0,\" work is done – only that work has been done. That means that if you’re going to want to do billing from Project Server, your team members are going to end up filling in Timesheets in Project Server. Updating Timesheets in Project Server does sync actuals and remaining work for work items though.\"]],[[0,[],0,\"The connector is notoriously hard to debug. If the connector has errors, it can be really hard to track them down.\"]],[[0,[],0,\"If you have change approvals enabled on Project Server, a project manager can reject changes made to a plan. Imagine a team member updates a work item, which causes the connector to send the change to Project Server. The project manager then rejects the changes. At this point, the sync engine turns off sync for this work item, and the only way to know is to open the work item and take a look. The history has an entry stating the reject reason, and in order to re-sync this work item going forward, you have to re-enable the sync for this one work item.\"]],[[0,[],0,\"You cannot assign multiple resources to a Task in the Project Server Project Plan. TFS only allows one resource to be Assigned To a Work Item at any one time – which means that if you’re used to multiple resources on the same Task in Project Server, you’re going to have to split the tasks.\"]]]],[1,\"h2\",[[0,[],0,\"Two Way Manual Sync\"]]],[1,\"p\",[[0,[],0,\"So since the integration is so hard (and fragile), perhaps you can consider this alternative: two way manual sync. \"],[0,[2],1,\"This page\"],[0,[],0,\" explains in detail the differences between syncing to MS Project versus syncing to Project Server – what I propose here is a “middle ground” that give you best of both worlds without all the pesky configuration required for the integration extension.\"]]],[1,\"p\",[[0,[],0,\"Here are the steps to get going:\"]]],[3,\"ul\",[[[0,[],0,\"On Project Web Access (PWA), create a new enterprise project.\"]],[[0,[],0,\"Open the Project and press “Build Team” and add the resources that will form part of this project.\"]],[[0,[],0,\"Open MS Project and connect to the new Enterprise Project. Check out the plan for editing.\"]],[[0,[],0,\"On the Team Tab, click “Get Work Items” and select a query for the work items you want to bring into the plan.\"]]]],[10,0],[3,\"ul\",[[[0,[],0,\"(Tip: I normally work in the Iteration Backlog and then hit the “Create Query” button to create the iteration backlog query)\"]]]],[10,1],[3,\"ul\",[[[0,[],0,\"Select the work items from the query and click Add.\"]],[[0,[],0,\"Now you can work with the Tasks in Project – leveling resources etc. etc. You can also set a baseline if you want to.\"]],[[0,[],0,\"(Tip: Establish predecessor relationships. Then select all the rows by clicking the row ID – the leftmost column –  of the 1st task and then shift-clicking the ID of the last task. Then right click and select “Auto Schedule”. This creates the initial Gantt for you).\"]]]],[10,2],[3,\"ul\",[[[0,[],0,\"Once you’re done, hit Publish in the Team tab to save your changes back to TFS. In this screenshot, I added a new Task at the bottom of the Project Plan and hit Publish. This then brought back the TFS Work Item id (48).\"]]]],[10,3],[3,\"ul\",[[[0,[],0,\"Now you need to publish the changes to Project Server. Hit File and then click the Publish button.\"]]]],[10,4],[3,\"ul\",[[[0,[],0,\"This can take a few seconds, so make sure you watch the status bar to see that the publish succeeded.\"]]]],[10,5],[3,\"ul\",[[[0,[],0,\"When you close the project plan, make sure you check it in.\"]]]],[1,\"p\",[[0,[],0,\"Now imagine that the TFS team is updating their tasks. To pull those updates in, let’s open the Project Plan again:\"]]],[3,\"ul\",[[[0,[],0,\"Go to the Team Tab and press “Refresh”. (Notice in this screenshot that the task actuals / remaining have been updated).\"]]]],[10,6],[3,\"ul\",[[[0,[],0,\"One gotcha: If new tasks were added in TFS, you’ll have to schedule them to see the time estimates (see Task 49 in the above screenshot). It’s a good idea to always hit “auto-schedule” on new (or all) tasks to get new tasks into the Gantt correctly.\"]],[[0,[],0,\"Now you need to publish to Project Server. Again, go to File and click the Publish button.\"]]]],[1,\"p\",[[0,[],0,\"Finally, consider when new Tasks are added to the Plan in Project Server by another user.\"]]],[3,\"ul\",[[[0,[],0,\"Open the Project Plan from Project Server\"]],[[0,[],0,\"You’ll immediately see the new tasks\"]],[[0,[],0,\"Add the Work Item Type column and map the new Tasks to work item types. You can also bring in the Area Path and Iteration Path columns.\"]],[[0,[],0,\"Now go to the Team Tab and hit Publish.\"]]]],[1,\"h2\",[[0,[],0,\"Other Useful Stuff\"]]],[1,\"p\",[[0,[],0,\"I wrote a series of posts about integrating TFS and Project server – you can find the fist post \"],[0,[3],1,\"here\"],[0,[],0,\". Also, you can customize the field mappings between TFS and MS Project (not Server) using the guide in \"],[0,[4],1,\"this post\"],[0,[],0,\". Also, if you’re looking for a Timesheet solution and don’t want to use Project Server, then look out for \"],[0,[5],1,\"Imaginet’s soon-to-be-released new Timesheet product\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"I also can’t recommend highly enough Donald G. Reinertsen’s \"],[0,[6],1,\"The Principles of Product Development Flow\"],[0,[],0,\" – it’ll revolutionize the way you think about delivering value to business.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Philosophically I think software development teams should stay away from Project Server (or Project Plans) entirely – focus more effort on measuring delivered value to business than conformance to a plan. However, this change is cultural (and needs to be pervasive) so I know that there are teams that are still going to have to integrate to Project Server. Before you embark on the long and painful process of using the server integration, consider using my two way manual sync to see how it works.\"]]],[1,\"p\",[[0,[],0,\"Happy Project Tracking!\"]]]]}","published_at":1397156100000,"status":"published","published_by":1},{"id":"6cfecdf7-7faf-4532-aeff-28b425326e56","title":"IntelliTrace Tips and Tricks: IntelliTrace Everywhere","slug":"intellitrace-tips-and-tricks-intellitrace-everywhere","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-e59bhWvHapQ/UcBUDNdtF4I/AAAAAAAAA7U/GxHP9U18nXQ/s1600-h/image3.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-NyIjIpNHEeU/UcBUEzKM0xI/AAAAAAAAA7c/GakVNgEfig8/image_thumb1.png?imgmax=800\\\" width=\\\"408\\\" height=\\\"240\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-V1o2CHUMfBM/UcBUF-6jEwI/AAAAAAAAA7k/WgOXVh1MTrU/s1600-h/image5.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-yVR_FFUbXEU/UcBUHp9xD7I/AAAAAAAAA7s/xAfbEXcRiWc/image_thumb2.png?imgmax=800\\\" width=\\\"429\\\" height=\\\"276\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-ObB1izF-F-Y/UcBUIrOlUPI/AAAAAAAAA70/4oQfPrEGU4k/s1600-h/image9.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-HE_KFIql3x8/UcBUKsbf7AI/AAAAAAAAA78/ZP_-4PXrA_Q/image_thumb4.png?imgmax=800\\\" width=\\\"425\\\" height=\\\"262\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\"><font size=\\\"2\\\">Start-IntelliTraceCollection –ApplicationPool <strong>AppPoolName</strong> –CollectionPlan <strong>plan.xml</strong> –OutputPath <strong>logPath</strong></font></font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">IntelliTraceSC.exe help launch</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">IntelliTraceSC.exe launch /cp:<strong>plan.xml</strong> /f:<strong>pathToLogFile</strong> <strong>application</strong></font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/intellitrace-tips-and-tricks-basics.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2013/05/13/guest-post-intellitrace-tips-and-tricks-the-basics-part-1-colin-dembovsky.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/intellitrace-tips-and-tricks.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2013/05/13/guest-post-intellitrace-tips-and-tricks-intellitrace-everywhere-part-2-colin-dembovsky.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/04/enable-custom-intellitrace-web-events.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/msaffer/archive/2011/02/23/using-intellitrace-with-services.aspx\"]],[\"a\",[\"href\",\"http://www.microsoft.com/en-za/download/details.aspx?id=30665\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series Links:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1\"],[0,[],0,\": The Basics (this post) – also \"],[0,[1],1,\"guest posted\"],[0,[],0,\" on MSDevDiv SA Blog\"]],[[0,[2],1,\"Part 2\"],[0,[],0,\": IntelliTrace Everywhere – also \"],[0,[3],1,\"guest posted\"],[0,[],0,\" on MSDevDiv SA Blog\"]],[[0,[4],1,\"Part 3\"],[0,[],0,\": Enable Custom IntelliTrace Events with a Right-Click\"]]]],[1,\"p\",[[0,[],0,\"In my previous post I showed you how to enable IntelliTrace for debugging – F5 IntelliTrace. That’s all well and good, but what about getting IntelliTrace logs from your test environments? Or from production?\"]]],[1,\"p\",[[0,[],0,\"Here’s where you can use IntelliTrace:\"]]],[3,\"ul\",[[[0,[],0,\".NET 2.0 and above managed code (note: the collector requires .NET 3.5, so you’ll need that on the target server even if you app is in .NET 2.0)\"]],[[0,[],0,\"Enable the IntelliTrace diagnostic adapter in Test Manager to collect logs during test runs\"]],[[0,[],0,\"IIS: Use PowerShell to attach to an application pool and collect logs\"]],[[0,[],0,\"Desktop Apps: Use IntelliTraceSC.exe to launch an app and collect logs\"]],[[0,[],0,\"Windows Services: This one is tough, but possible. Involves some registry tweaking. \"],[0,[5],1,\"Read more here\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"Here’s where you can’t collect IntelliTrace:\"]]],[3,\"ul\",[[[0,[],0,\"Silverlight applications\"]],[[0,[],0,\"Windows Phone applications\"]],[[0,[],0,\".NET 1 applications\"]],[[0,[],0,\"Native code applications\"]]]],[1,\"h2\",[[0,[],0,\"The IntelliTrace Standalone Collector\"]]],[1,\"p\",[[0,[],0,\"You can get this from the Visual Studio installation folder, or you can \"],[0,[6],1,\"download it here\"],[0,[],0,\". I recommend downloading it, since this will be the latest and greatest collector available. The page has a link to instructions about how to extract the cab file. Once you’ve expanded the cab, you’ll have the PowerShell module as well as the IntelliTraceSC.exe for collecting application data.\"]]],[1,\"h2\",[[0,[],0,\"Symbols\"]]],[1,\"p\",[[0,[],0,\"Before we look at how to collect logs, let’s talk about symbols. In order to open up code from the iTrace logs, you’re going to need to supply symbols. Ever seen a pdb file when you compile your apps? The pdbs map source code to compiled code. But of course no self-respecting developer ever deploys pdbs, right? So if you’re not deploying your pdbs (and you shouldn’t be) then where do you put them? You get the build to publish them to a shared folder, or Symbol Server. (If you don’t use Team Build, you can simply keep your pdbs somewhere – when you open an iTrace file, you can provide the location of your pdbs).\"]]],[1,\"p\",[[0,[],0,\"Here’s an image showing where in the DefaultTemplate you can set the symbols location:\"]]],[10,0],[1,\"p\",[[0,[],0,\"One more thing – you’ll need to set this location in the Debugging options of VS in order for it to look there for the symbols. In VS, go to the Tools->Options dialog. Then expand the Debugging->Symbols section. There are some buttons in the top right of the dialog – click the “New Location” icon (between the yellow warning icon and the ‘x’ button) and type in the same directory that you used in the Build Process settings (above).\"]]],[10,1],[1,\"p\",[[0,[],0,\"You’ll also need to open the “General” tab under Debugging and set the following checkboxes:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Now you’re ready to open the logs – let’s see how you can collect them.\"]]],[1,\"h2\",[[0,[],0,\"Collect IntelliTrace from Web Applications\"]]],[1,\"p\",[[0,[],0,\"If you’re developing and deploying web applications, you get a lot of love from IntelliTrace. Here are the steps you’ll need to follow to start logging:\"]]],[3,\"ol\",[[[0,[],0,\"Download the IntelliTrace collector and expand it.\"]],[[0,[],0,\"Find the name (and identity) of the application pool that your web app is running under.\"]],[[0,[],0,\"Create a log folder. Make sure the app pool identity has write access to this folder.\"]],[[0,[],0,\"Open a PowerShell prompt. Go to the IntelliTrace folder. Run “Import-Module Microsoft.VisualStudio.IntellITrace.PowerShell.dll”\"]],[[0,[],0,\"To list the commands, type “Get-Command *IntelliTrace*”. This will list the 5 IntelliTrace cmdlets.\"]],[[0,[],0,\"To start logging, type\"]]]],[10,3],[1,\"p\",[[0,[],0,\"where\"]]],[3,\"ul\",[[[0,[],0,\"AppPoolName is the name of the app pool your application is running under\"]],[[0,[],0,\"plan.xml is the collection plan (more on this in the next paragraph)\"]],[[0,[],0,\"logPath is the path you want the log files dropped into\"]]]],[1,\"p\",[[0,[],0,\"The plan.xml is the settings file for what IntelliTrace events you want to collect (and if you’re running in Events Only or Events and Call Information mode). You’ll see 2 xml files in the IntelliTrace folder that ship with IntelliTrace – the lightweight collection_plan.ASP.NET.default.xml and the diagnostic collection_plan.ASP.NET.trace.xml. I recommend starting with the default plan (Events Only) and if you get stuck then dial it up to the trace plan (Events and Call Information). In the next post, I’ll show you how to customize the collection to get fine-grained control over the events.\"]]],[1,\"p\",[[0,[],0,\"Be aware that when you run Start-IntelliTraceCollection, IntelliTrace will attach itself to the app pool, but part of that will require a recycle.\"]]],[1,\"p\",[[0,[],0,\"Don’t leave this on too long – especially if you’re using the trace plan. One the collector is running, you can use the following commands when you want to grab the log and open it:\"]]],[3,\"ul\",[[[0,[],0,\"Stop-IntelliTraceCollection – which stops the collector entirely\"]],[[0,[],0,\"Checkpoint-IntelliTraceCollection – which unlocks the log file and starts logging to a new log file\"]]]],[1,\"p\",[[0,[],0,\"Use checkpoint when you want to open the log and leave the collector running (when the collector is running the file is locked by the collection process, so you won’t be able to open it).\"]]],[1,\"h2\",[[0,[],0,\"Collect IntelliTrace from Desktop Applications\"]]],[1,\"p\",[[0,[],0,\"Collecting IntelliTrace from Desktop apps gets some love – not as much as the IIS scenario. Instead of launching your application directly, go to the IntelliTrace collector folder and run the following command:\"]]],[10,4],[1,\"p\",[[0,[],0,\"to see the help on how to launch. Here’s the basic launch command:\"]]],[10,5],[1,\"p\",[[0,[],0,\"where\"]]],[3,\"ul\",[[[0,[],0,\"plan.xml is the collection plan – use the same ones as the web collection command – don’t worry that it’s called ASP.NET – it’ll work for most default scenarios\"]],[[0,[],0,\"pathToLogFile is the full path and filename of the log file\"]],[[0,[],0,\"application is the path to the application you want to collect the log from\"]]]],[1,\"p\",[[0,[],0,\"As soon as you exit your application, the logging completes and you’ll have your log file.\"]]],[1,\"p\",[[0,[],0,\"Happy logging!\"]]]]}","published_at":1371591300000,"status":"published","published_by":1},{"id":"18d024a9-c3ba-492d-b014-5acee1ff7215","title":"IntelliTrace Tips and Tricks: The Basics","slug":"intellitrace-tips-and-tricks-the-basics","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-CYt5QEthRTc/UcBTHGhxisI/AAAAAAAAA4E/7jIrJmB1AUg/s1600-h/image3.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/--zXqZig5Sf8/UcBTIkP0YzI/AAAAAAAAA4M/1v74jQp6eCA/image_thumb11.png?imgmax=800\\\" width=\\\"377\\\" height=\\\"264\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-PwYyK9Zowug/UcBTKOpZYnI/AAAAAAAAA4U/RsA_8d-Xb-8/s1600-h/image7.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-EGHkvfBc-uA/UcBTLe54PAI/AAAAAAAAA4c/lC_QonAhUms/image_thumb31.png?imgmax=800\\\" width=\\\"396\\\" height=\\\"224\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-68iYP0exLcU/UcBTMcXJACI/AAAAAAAAA4k/yEgRe86RtBE/s1600-h/image11.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-8pLWIk-38mI/UcBTN6AE2jI/AAAAAAAAA4s/L_T2kYa3rDE/image_thumb5.png?imgmax=800\\\" width=\\\"199\\\" height=\\\"320\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-A-3Uofqd3y0/UcBTO2c5P4I/AAAAAAAAA40/grT2xnpLDOU/s1600-h/image14.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-1qVinYo0zzY/UcBTQKZrq0I/AAAAAAAAA48/ZPUovcn1Fsw/image_thumb6.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"208\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-DaLsuX2tB3I/UcBTRZ1I3aI/AAAAAAAAA5E/qMOfr1Ot6oA/s1600-h/image18.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-a_Qc-RhXtRM/UcBTTLKR2eI/AAAAAAAAA5M/nsEyxmInQOQ/image_thumb8.png?imgmax=800\\\" width=\\\"340\\\" height=\\\"162\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-Kfln_SXOdlc/UcBTUNbxSII/AAAAAAAAA5U/KJ8Jc6oBOsw/s1600-h/image41.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-jBH7H6YzVHI/UcBTVrNDPSI/AAAAAAAAA5c/0J_w722KdPU/image_thumb111.png?imgmax=800\\\" width=\\\"361\\\" height=\\\"162\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-lero369BGZo/UcBTXPpGgzI/AAAAAAAAA5k/mBIc2JBDELw/s1600-h/image8.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-3yu_4eE8pLw/UcBTYVFG5UI/AAAAAAAAA5s/mcRV54X6YtU/image_thumb3.png?imgmax=800\\\" width=\\\"406\\\" height=\\\"202\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-AHFOiZK2Tgo/UcBTZSq8ToI/AAAAAAAAA50/nUiREd8Zdpo/s1600-h/image12.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-Y6Maui4gC_c/UcBTaidU2kI/AAAAAAAAA58/Gy7rtaP-3Sg/image_thumb51.png?imgmax=800\\\" width=\\\"444\\\" height=\\\"252\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-CkpyfLc3cYE/UcBTbg5zUJI/AAAAAAAAA6E/kPFhUlPrTuc/s1600-h/image13.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-NtpzfhfOtrE/UcBTdEzj4TI/AAAAAAAAA6M/wKWY2GC2UCE/image_thumb6%25255B1%25255D.png?imgmax=800\\\" width=\\\"304\\\" height=\\\"224\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-aWhGA-AlR_E/UcBTe3efEwI/AAAAAAAAA6U/QNxzunCeLvU/s1600-h/image17.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-5m_cz6gQRyg/UcBTgegBALI/AAAAAAAAA6c/MXjyjFuQ5sg/image_thumb8%25255B1%25255D.png?imgmax=800\\\" width=\\\"266\\\" height=\\\"295\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-PmAywdKi4t4/UcBThc9D6zI/AAAAAAAAA6k/Ll5EfRwKTBA/s1600-h/image411.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/--HZ_0Wa_Au8/UcBTjbnE8qI/AAAAAAAAA6s/WG4zkUOlERA/image_thumb1.png?imgmax=800\\\" width=\\\"484\\\" height=\\\"352\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-NSCwg7KR6Xg/UcBTkvgC48I/AAAAAAAAA60/ccOa7YzVC38/s1600-h/image18%25255B1%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-ThnqArcP5nY/UcBTnP8xqoI/AAAAAAAAA68/yRVA9c_eUx8/image_thumb9.png?imgmax=800\\\" width=\\\"327\\\" height=\\\"112\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-sA8MQhwTELo/UcBTqCo87bI/AAAAAAAAA7E/RumDaU3mTvg/s1600-h/image9.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-RwFRQkuZIiU/UcBTsImCP0I/AAAAAAAAA7M/EII8TwZlhNQ/image_thumb4.png?imgmax=800\\\" width=\\\"367\\\" height=\\\"99\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/intellitrace-tips-and-tricks-basics.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2013/05/13/guest-post-intellitrace-tips-and-tricks-the-basics-part-1-colin-dembovsky.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/intellitrace-tips-and-tricks.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2013/05/13/guest-post-intellitrace-tips-and-tricks-intellitrace-everywhere-part-2-colin-dembovsky.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/04/enable-custom-intellitrace-web-events.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Series Links:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1\"],[0,[],0,\": The Basics (this post) – also \"],[0,[1],1,\"guest posted\"],[0,[],0,\" on MSDevDiv SA Blog\"]],[[0,[2],1,\"Part 2\"],[0,[],0,\": IntelliTrace Everywhere – also \"],[0,[3],1,\"guest posted\"],[0,[],0,\" on MSDevDiv SA Blog\"]],[[0,[4],1,\"Part 3\"],[0,[],0,\": Enable Custom IntelliTrace Events with a Right-Click\"]]]],[1,\"p\",[[0,[],0,\"This brief series of blog posts will show you how to get the most out of IntelliTrace – a historical debugger that allows you to record (and replay) program execution.\"]]],[1,\"h2\",[[0,[],0,\"Using IntelliTrace\"]]],[1,\"p\",[[0,[],0,\"You can use IntelliTrace in the following scenarios:\"]]],[3,\"ol\",[[[0,[],0,\"During debugging in VS using F5\"]],[[0,[],0,\"In Test Manager (enabled as a Data Diagnostic Adapter)\"]],[[0,[],0,\"Anywhere (using the standalone collector)\"]]]],[1,\"h2\",[[0,[],0,\"Understanding IntelliTrace “Modes”\"]]],[1,\"p\",[[0,[],0,\"Out of the box, IntelliTrace has 2 broad modes – “Events Only” and “Events and Call Information”. Events Only is much more lightweight, and allows you to record “events” that occur when your program is running. These “events” are “interesting occurrences” – like ASP.NET calls or ADO.NET calls – or exceptions. The product team tried to record events which would help you understand how your code works so that you can understand long “cause and effect” chains.\"]]],[1,\"p\",[[0,[],0,\"Events and Call Information turns the dial up a lot – it collects not only all of the events from the Events Only mode, but also method calls (including in and out arguments). There are some limits placed on what data is collected – otherwise the logs would become even more enormous than they are using the default settings.\"]]],[1,\"h2\",[[0,[],0,\"F5 IntelliTrace\"]]],[1,\"p\",[[0,[],0,\"Use this when you’re coding. It means you can debug and move back and forwards in your debug session without having to stop your application and add breakpoints. To turn it on, go to Tools->Options and go to IntelliTrace settings. Here you’ll see the 2 modes:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Once you’ve turned it on (in this case I’ve got the Events and Call Information mode on) you can start debugging. Click around your app, and then click the “Break All” link in VS in the IntelliTrace window:\"]]],[10,1],[1,\"p\",[[0,[],0,\"(Here I am debugging my Calculator application – click “Break All” to go to the IntelliTrace log).\"]]],[1,\"p\",[[0,[],0,\"When you look at the log, you’ll initially see all the events that occurred. In this case, button clicks are “Gesture” events, so that’s what I see:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Let’s say that I remember something weird happened when I clicked “=” when multiplying 2 numbers – well, I don’t have to restart the application, I can simply click on the “=” gesture after the “*” button click (in the above log, I’ve clicked 3 x 6 =). So I can click on that event and “rewind” to that point.\"]]],[10,3],[1,\"p\",[[0,[],0,\"What I can now do is click “Calls View” to start walking through the log from that point.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Double-click on the call to btnEqual_Click to “zoom” to that point of the execution. You’ll see the IntelliTrace glyphs in the gutter of the source window:\"]]],[10,5],[1,\"p\",[[0,[],0,\"If you mouse-over the icons, you’ll get a tooltip. The functions (in the order they appear) are:\"]]],[3,\"ul\",[[[0,[],0,\"return to calling method\"]],[[0,[],0,\"step back\"]],[[0,[],0,\"step into\"]],[[0,[],0,\"step over\"]],[[0,[],0,\"return to Live Debugging\"]]]],[1,\"p\",[[0,[],0,\"(Just a handy tip: If you press the bottom button “return to Live Debugging” remember that this now puts you at the current debugger point – you’ll need to press F5 again if you want to start running the application again.)\"]]],[1,\"p\",[[0,[],0,\"If you advance using F11, the current pointer will advance. Pressing twice from the previous image gets me to the point where the program enters the switch statement:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Notice that I don’t need to guess which case statement was selected – the log records exactly what the program did. I’ve also pinned the mouse-overs for val1 and val2 – you can see their values. Not all values are collected, but since these are primitives and are being passed into (or out of) a method, IntelliTrace dutifully collects their values.\"]]],[1,\"p\",[[0,[],0,\"I’ll press F11 again to step into the Multiply() method, and then F11 again to advance one more event:\"]]],[10,7],[1,\"p\",[[0,[],0,\"That puts me onto the closing curly brace of the Multiply method. If I look in the Autos window, I’ll see that the method in parameters (val1 and val2) were 3 and 6 respectively, and that the return value was 9.\"]]],[1,\"p\",[[0,[],0,\"Voila – with 3 or 4 clicks we found the bug in our program. And we didn’t even have to set a breakpoint!\"]]],[1,\"h2\",[[0,[],0,\"Filtering Events\"]]],[1,\"p\",[[0,[],0,\"As you use IntelliTrace, you’ll start seeing large amount of events – to make the logs easier to navigate, you can use the category, thread and search box at the top of the events window to filter the events.\"]]],[10,8],[1,\"p\",[[0,[],0,\"For example, expanding the dropdown with “All Categories” in it I can filter just exceptions:\"]]],[10,9],[1,\"h2\",[[0,[],0,\"Search For This Line…\"]]],[1,\"p\",[[0,[],0,\"Another useful tip is the “Search For this line” or “Search For This Method”. You suspect that a method was hit sometime during your debug session – but the log is quite long. No problem – right click a line (or method) and select “Search For This Line / Method in IntelliTrace”.\"]]],[10,10],[1,\"p\",[[0,[],0,\"Once you do, you’ll see the search results in a bar at the top of the current code window – press the arrows to go to the previous or next instance of that line or method in the log file:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Here I’ll click on the arrow icon directly after the work “Multiply” to go to the first call to this method in the log, and I can start “debugging” from there.\"]]],[10,12],[1,\"p\",[[0,[],0,\"In the next post, I’ll show you how to run IntelliTrace anywhere and everywhere.\"]]],[1,\"p\",[[0,[],0,\"Happy debugging!\"]]]]}","published_at":1371591180000,"status":"published","published_by":1},{"id":"f3e022ef-88c7-4982-ae2d-1358ded549a9","title":"ISubscriber: Getting the TFS Url for Client Operations","slug":"isubscriber-getting-the-tfs-url-for-client-operations","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"private Uri GetTFSUri(TeamFoundationRequestContext requestContext)<br>{<br>    var locationService = requestContext.GetService<teamfoundationlocationservice>();<br>    return new Uri(locationService.ServerAccessMapping.AccessPoint + \\\"/\\\" + requestContext.ServiceHost.Name);<br>}<br><br>private WorkItemStore GetWorkItemStore(TeamFoundationRequestContext requestContext)<br>{<br>    var uri = GetTFSUri(requestContext);<br>    var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(uri);<br>    return tpc.GetService<workitemstore>();<br>}<br></workitemstore></teamfoundationlocationservice>\",\"language\":\"xml; ruler\"}]],\"markups\":[[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/magazine/cc507647.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.teamfoundation.framework.server.isubscriber.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.teamfoundation.framework.server.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.teamfoundation.framework.server.teamfoundationrequestcontext.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.teamfoundation.workitemtracking.client.workitemstore.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/ms252473.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Responding to TFS events can be done in (at least) 2 ways: create a SOAP webservice and register with \"],[0,[0],1,\"bissubscribe\"],[0,[],0,\" (this works in a “client” fashion) or implement the \"],[0,[1],1,\"ISubscriber interface\"],[0,[],0,\" (in the \"],[0,[2],1,\"Microsoft.TeamFoundation.Framework.Server\"],[0,[],0,\" namespace).\"]]],[1,\"p\",[[0,[],0,\"The advantage to the ISubscriber interface implementation is that the plugin can be installed on the TFS server and can also be used to allow or disallow a change (such as a check-in policy). One disadvantage of ISubscribers is that you only get access to a \"],[0,[3],1,\"TeamFoundationRequestContext\"],[0,[],0,\" object, not a TfsTeamProjectCollection object. This can limit what operations you can perform.\"]]],[1,\"p\",[[0,[],0,\"I was working with an ISubscriber and wanted to update a Global List, so I needed a reference to the \"],[0,[4],1,\"WorkItemStore\"],[0,[],0,\" object. Unfortunately, there was no obvious way to do this from the TeamFoundationRequestContext. I could have hard-coded a TFS url into a config file, but this felt like a cop out to me. So I dug around a little bit more and came up with a solution: the TeamFoundationLocationService (see \"],[0,[5],1,\"this article\"],[0,[],0,\" about TFS services).\"]]],[1,\"h2\",[[0,[],0,\"The TeamFoundationLocationService\"]]],[1,\"p\",[[0,[],0,\"You can easily get the TeamFoundationLocationService from the TeamFoundationRequestContext. You can then query the location service to get the url of the TFS server and collection. Once you have that, you can then instantiate a TfsTeamProjectCollection object and use that to get the WorkItemStore. Here’s the code:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Happy subscribing!\"]]]]}","published_at":1325221380000,"status":"published","published_by":1},{"id":"b819b2d7-b26f-40a4-b0ab-4516909e8548","title":"I’ve been made an MVP","slug":"ive-been-made-an-mvp","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-z-0Yt16n_10/To13Uhd0TII/AAAAAAAAATk/V6NtSYXZVBg/s1600-h/mvp_wallpaper_800x600%25255B3%25255D.jpg\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"mvp_wallpaper_800x600\\\" border=\\\"0\\\" alt=\\\"mvp_wallpaper_800x600\\\" src=\\\"http://lh3.ggpht.com/-8Cs-IaHAKYE/To13VqjDZqI/AAAAAAAAATo/JtFsA7O8flc/mvp_wallpaper_800x600_thumb%25255B1%25255D.jpg?imgmax=800\\\" width=\\\"348\\\" height=\\\"273\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.teched.co.za\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This week I got an email congratulating me on becoming an MVP for Application Lifecycle Management. This is a huge honour! Thanks to all involved in this – including the community!\"]]],[1,\"p\",[[0,[],0,\"I thought the stats on where MVPs are globally was interesting – here’s an image showing the distribution geographically.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Don’t forget to catch me at \"],[0,[0],1,\"TechEd Africa\"],[0,[],0,\"!\"]]]]}","published_at":1317925800000,"status":"published","published_by":1},{"id":"48fe61fa-9328-4d5c-8f32-e2156fce53e2","title":"JSPM, NPM, Gulp and WebDeploy in a TeamBuild","slug":"jspm-npm-gulp-and-webdeploy-in-a-teambuild","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"&lt;!-- Include the jspm_packages folder when packaging in webdeploy since they are not included in the project --&gt;\\n&lt;PropertyGroup&gt;\\n  &lt;CopyAllFilesToSingleFolderForPackageDependsOn&gt;\\n    CustomCollectFiles;\\n    $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n  &lt;/CopyAllFilesToSingleFolderForPackageDependsOn&gt;\\n\\n  &lt;CopyAllFilesToSingleFolderForMsdeployDependsOn&gt;\\n    CustomCollectFiles;\\n    $(CopyAllFilesToSingleFolderForPackageDependsOn);\\n  &lt;/CopyAllFilesToSingleFolderForMsdeployDependsOn&gt;\\n&lt;/PropertyGroup&gt;\\n\\n&lt;Target Name=\\\"CustomCollectFiles\\\"&gt;\\n  &lt;ItemGroup&gt;\\n    &lt;_CustomFiles Include=\\\".\\\\jspm_packages\\\\**\\\\*\\\"&gt;\\n      &lt;DestinationRelativePath&gt;%(RecursiveDir)%(Filename)%(Extension)&lt;/DestinationRelativePath&gt;\\n    &lt;/_CustomFiles&gt;\\n    &lt;FilesForPackagingFromProject Include=\\\"%(_CustomFiles.Identity)\\\"&gt;\\n      &lt;DestinationRelativePath&gt;jspm_packages\\\\%(RecursiveDir)%(Filename)%(Extension)&lt;/DestinationRelativePath&gt;\\n    &lt;/FilesForPackagingFromProject&gt;\\n  &lt;/ItemGroup&gt;\\n&lt;/Target&gt;\\n\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"param(\\n    [string]$sourcesDirectory = $env:TF_BUILD_SOURCESDIRECTORY\\n)\\n\\n$webDirectory = $sourcesDirectory + \\\"\\\\src\\\\MyWebProject\\\"\\nPush-Location\\n\\n# Set location to MyWebProject folder\\nSet-Location $webDirectory\\n\\n# refresh the packages required by gulp (listed in the package.json file)\\n$res = npm install 2&gt;&amp;1\\n$errs = ($res | ? { $_.gettype().Name -eq \\\"ErrorRecord\\\" -and $_.Exception.Message.ToLower().Contains(\\\"err\\\") })\\nif ($errs.Count -gt 0) {\\n    $errs | % { Write-Error $_ }\\n    exit 1\\n} else {\\n    Write-Host \\\"Successfully ran 'npm install'\\\"\\n}\\n\\n# refresh the packages required by jspm (listed in the jspm section of package.json file)\\n$res = jspm install -y 2&gt;&amp;1\\n$errs = ($res | ? { $_.gettype().Name -eq \\\"ErrorRecord\\\" -and $_.Exception.Message.ToLower().Contains(\\\"err\\\") })\\nif ($errs.Count -gt 0) {\\n    $errs | % { Write-Error $_ }\\n    exit 1\\n} else {\\n    Write-Host \\\"Successfully ran 'jspm install -y'\\\"\\n}\\n\\n# explicitly set the configuration and invoke gulp\\n$env:NODE_ENV = 'Release'\\nnode_modules\\\\.bin\\\\gulp.cmd build\\n\\nPop-Location\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"Param(\\n  [string]$srcDir = $env:TF_BUILD_SOURCESDIRECTORY\\n)\\n\\n# forcefully remove left over node module folders\\n# necessary because the folder depth means paths end up being &gt; 260 chars\\n# run it twice since it sometimes complains about the dir not being empty\\n# supress errors\\n$x = cmd /c \\\"rd $srcDir\\\\src\\\\MyWebProject\\\\node_modules /s /q\\\" 2&gt;&amp;1\\n$x = cmd /c \\\"rd $srcDir\\\\src\\\\MyWebProject\\\\node_modules /s /q\\\" 2&gt;&amp;1\\n\",\"language\":\"ps;\"}]],\"markups\":[[\"a\",[\"href\",\"http://aurelia.io\"]],[\"a\",[\"href\",\"https://twitter.com/eisenbergeffect\"]],[\"a\",[\"href\",\"https://www.npmjs.com/\"]],[\"a\",[\"href\",\"http://jspm.io/\"]],[\"a\",[\"href\",\"http://www.hanselman.com/blog/WebDeploymentMadeAwesomeIfYoureUsingXCopyYoureDoingItWrong.aspx\"]],[\"a\",[\"href\",\"http://azure.microsoft.com/en-us/services/websites/\"]],[\"a\",[\"href\",\"http://sedodream.com\"]],[\"a\",[\"href\",\"http://sedodream.com/2010/03/10/WebDeploymentToolIncludingOtherFiles.aspx\"]],[\"a\",[\"href\",\"http://www.asp.net/mvc/overview/deployment/visual-studio-web-deployment/deploying-extra-files\"]],[\"u\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve been coding a web project using \"],[0,[0],1,\"Aurelia\"],[0,[],0,\" for the last couple of weeks (more posts about what I’m actually doing to follow soon!). Aurelia is an amazing SPA framework invented by Rob Eisenberg (\"],[0,[1],1,\"@EisenbergEffect\"],[0,[],0,\").\"]]],[1,\"h2\",[[0,[],0,\"JSPM\"]]],[1,\"p\",[[0,[],0,\"Aurelia utilizes \"],[0,[2],1,\"npm (Node Package Manager)\"],[0,[],0,\" as well as the relatively new \"],[0,[3],1,\"jspm\"],[0,[],0,\" – which is like npm for “browser package management”. In fact Rob and his Aurelia team are working very closely with the jspm team in order to add in functionality that will improve how Aurelia is bundled and packaged – but I digress.\"]]],[1,\"p\",[[0,[],0,\"To utilize npm and jspm, you need to specify the dependencies that you have on any npm/jspm packages in a packages.json file. Then you can run “npm install” and “jspm install” and the package managers spring into action pulling down all your dependencies. This works great while you’re developing – but can be a bit strange when you’re deploying with \"],[0,[4],1,\"WebDeploy\"],[0,[],0,\" (and you should be!)\"]]],[1,\"p\",[[0,[],0,\"WebDeploy (out of the box) only packages files that are included in your project. This is what you want for any of your source (or content) files. But you really don’t want to include dependencies in your project (or in source control for that matter) since the package managers are going to refresh the dependencies during the build anyway. That’s the whole point of using Package Managers in the first place! The problem is that when you package your website, none of the dependencies will be included in the package (since they’re not included in the VS project).\"]]],[1,\"p\",[[0,[],0,\"There are a couple solutions to this problem:\"]]],[3,\"ol\",[[[0,[],0,\"You could execute the package manager install commands after you’ve deployed your site via WebDeploy. However, if you’re deploying to \"],[0,[5],1,\"WAWS\"],[0,[],0,\" (or don’t have access to running scripts on the server where your site is hosted) you won’t be able to – and you are going to end up with missing dependencies.\"]],[[0,[],0,\"You could include the packages folder in your project. The problem with this is that if you upgrade a package, you’ll end up having to exclude the old package (and its dependencies) and include the new package (and any of its dependencies). You lose the value of using the Package Manager in the first place.\"]],[[0,[],0,\"Customize WebDeploy to include the packages folder when creating the deployment package. Now we’re talking!\"]]]],[1,\"h2\",[[0,[],0,\"Including Package Folders in WebDeploy\"]]],[1,\"p\",[[0,[],0,\"Of course as I considered this problem I was not happy with either running the Package Manager commands on my hosting servers (in the case of WAWS this isn’t even possible) or including the package files in my project. I then searched out \"],[0,[6],1,\"Sayed Ibrahim Hashimi’s site\"],[0,[],0,\" to see what guidance he could offer (he’s a build guru!). I found an old post that explained how to include \"],[0,[7],1,\"“extra folders” in web deployment\"],[0,[],0,\" – however, that didn’t quite work for me. I had to apply the slightly more up-to-date property group specified in \"],[0,[8],1,\"this post\"],[0,[],0,\". Sayed had a property group for <CopyAllFilesToSingleFolderFor\"],[0,[9],1,\"Package\"],[0,[],0,\"DependsOn> but you need the same property group for <CopyAllFilesToSingleFolderFor\"],[0,[9],1,\"Msdeploy\"],[0,[],0,\"DependsOn>.\"]]],[1,\"p\",[[0,[],0,\"My final customized target to include the jspm package folder in WebDeploy actions is as follows (you can add this to the very bottom of your web project file, just before the closing </Project> tag):\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now when I package my site, I get all the jspm packages included.\"]]],[1,\"h2\",[[0,[],0,\"TeamBuild with Gulp, NPM, JSPM and WebDeploy\"]]],[1,\"p\",[[0,[],0,\"The next challenge is getting this all to work on a TeamBuild. Let’s quickly look at what you need to do manually to get a project like this to compile:\"]]],[3,\"ol\",[[[0,[],0,\"Pull the sources from source control\"]],[[0,[],0,\"Run “npm install” to install the node pacakges\"]],[[0,[],0,\"Run “jspm install –y” to install the jspm packages\"]],[[0,[],0,\"(Optionally) Run gulp – in our case this is required since we’re using TypeScript. We’ve got gulp set up to transpile our TypeScript source into js, do minification etc.\"]],[[0,[],0,\"Build in VS – for our WebAPI backend\"]],[[0,[],0,\"Publish using WebDeploy (could just be targeting a deployment package rather than pushing to a server)\"]]]],[1,\"p\",[[0,[],0,\"Fortunately, once you’ve installed npm and jspm and gulp globally (using –g) you can create a simple PowerShell script to do steps 2 – 4. The out of the box build template does the rest for you. Here’s my Gulp.ps1 script, which I specify in the “Pre-build script path” property of my TeamBuild Process:\"]]],[10,1],[1,\"p\",[[0,[],0,\"One last challenge – one of the folders (a lodash folder) ends up having a path > 260 characters. TeamBuild can’t remove this folder before doing a pull of the sources, so I had to modify the build template in order to execute a “CleanNodeDirs” command (I implemented this as an optional “pre-pull” script). However, this is a chicken-and-egg problem – if the pull fails because of old folders, then you can’t get the script to execute to clean the folders before the pull… So the logic I wrap the “pre-pull” invocation in an If activity that first checks if the “pre-pull” script exists. If it does, execute it, otherwise carry on.\"]]],[1,\"p\",[[0,[],0,\"The logic for this is as follows:\"]]],[3,\"ol\",[[[0,[],0,\"On a clean build (say a first build) the pre-pull script does not exist\"]],[[0,[],0,\"When the build checks for the pre-pull script, it’s not there – the build continues\"]],[[0,[],0,\"The build executes jspm, and the offending lodash folder is created\"]],[[0,[],0,\"The next build initializes, and detects that the pre-pull script exists\"]],[[0,[],0,\"The pre-pull script removes the offending folders\"]],[[0,[],0,\"The pull and the remainder of the build can now continue\"]]]],[1,\"p\",[[0,[],0,\"Unfortunately straight PowerShell couldn’t delete the folder (since the path is > 260 chars). I resorted to invoking cmd. I repeat it twice since the first time it complains that the folder isn’t empty – running the 2nd time completes the delete. Here’s the script:\"]]],[10,2],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Getting NPM, JSPM, Gulp, WebDeploy and TeamBuild to play nicely is not a trivial exercise. Perhaps vNext builds will make this all easier – I’ve yet to play with it. For now, we’re happy with our current process.\"]]],[1,\"p\",[[0,[],0,\"Any build/deploy automation can be tricky to set up initially – especially if you’ve got as many moving parts as we have in our solution. However, the effort pays off, since you’ll be executing the build/deploy cycle many hundreds of times over the lifetime of an agile project – each time you can deploy from a single button-press is a win!\"]]],[1,\"p\",[[0,[],0,\"Happy packaging!\"]]]]}","published_at":1426611627000,"status":"published","published_by":1},{"id":"f59859e8-da3e-4194-981d-69468f3b0d4f","title":"Lab Management: Configuring Workgroup Lab Machines to a TFS on a Domain","slug":"lab-management-configuring-workgroup-lab-machines-to-a-tfs-on-a-domain","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-SzO2qJAk-SQ/UD5xPLbf7fI/AAAAAAAAAcQ/IGrqrMfjMIw/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-gmh1XpU_oFI/UD5xQXmOpNI/AAAAAAAAAcY/ffq6SCMAxWI/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"337\\\" height=\\\"377\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-yubYbEQn_Qg/UD5xTD_T3PI/AAAAAAAAAcg/NdidJf1S7f0/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-DK9IN-dO-2o/UD5xURu7WvI/AAAAAAAAAco/cxajQ_5kB4M/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"383\\\" height=\\\"288\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-wVITFbHQ5UA/UD5xVfRNSRI/AAAAAAAAAcw/YHYHpUmD25g/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-atbN0Klxga4/UD5xW00wd2I/AAAAAAAAAc4/pGV9rIE4l_A/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"413\\\" height=\\\"191\\\"></a>\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve installed Windows 8 on my laptop, enabled HyperV and upgraded my TFS from 2010 to 2012. Since Lab Management 2012 introduced Standard Environments, I can use my HyperV machines in lab environments right out of Windows 8. However, while I was configuring my lab, I ran into a problem. My laptop is on my work domain, and my lab machines (running on HyperV) are not joined to the domain – they’re in a workgroup.\"]]],[1,\"p\",[[0,[],0,\"Having the TFS on a domain and the lab machines in a workgroup can cause the following symptoms:\"]]],[3,\"ol\",[[[0,[],0,\"When adding the lab machine to an environment, TFS cannot push the test agent to the lab machine. The environment stalls in the “Preparing to install agent”.\"]],[[0,[],0,\"If you install the test agent on the lab machine manually, you cannot connect to the controller on the TFS machine. The Test Agent Config Tool complains that the test controller is not accessible or the service is not running, even though you’ve verified that it is.\"]]]],[1,\"p\",[[0,[],0,\"Here are a couple of things you can check – once I had done all of these, the environment came online without any issues.\"]]],[1,\"h2\",[[0,[],0,\"Shadow Accounts\"]]],[1,\"p\",[[0,[],0,\"Since you’re dealing with a workgroup, you’re going to need to create “shadow” accounts. These are local accounts on each machine that have the same name and password. I created a local account on my TFS machine (note: this is not a domain account – it’s a local account) and did the same on the lab machines, using the same password each time. Then make sure you add the accounts into the local administrators group.\"]]],[1,\"p\",[[0,[],0,\"When you configure the Test Controller on the TFS machine, use “.\\\\username” for the logon account and the “lab service” account, where username is the name of the shadow account you created. In the figure below, you’ll see how I’ve done that using my shadow account, “labservice”.\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Network Settings on the Lab Machine\"]]],[1,\"p\",[[0,[],0,\"Since the Lab machine is on a workgroup, you have to make sure you can successfully communicate with the TFS machine which is on the domain. The first thing you’ll need to do is make sure that you enable File and Printer sharing on the Lab machine. If you right-click the network icon in the Taskbar, select “Open Network and Sharing Center”. Then click the link on the left, “Change advanced sharing settings”. Check that you have File and Printer sharing turned on for your “current” network profile (in the case of my lab machines, since they’re on HyperV, it’s the network connected to my virtual network).\"]]],[10,1],[1,\"p\",[[0,[],0,\"You’ll also probably need to add an entry to the hosts file on the lab machine. The trick here is to add an entry using the FQDN of the TFS machine, not simply the machine name. On your TFS machine, open a command prompt and ping yourself: if your machine’s name is “mytfs” then type “ping mytfs”. This should show you the FQDN of the TFS machine – something like “mytfs.domain.local” or something. That’s the name you’ll need for the hosts entry on the lab machine. Go to the lab machine and open c:\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts and add an entry for the TFS machine, using the form IP  FQDN. Here is an example from one of my lab machines:\"]]],[10,2],[1,\"p\",[[0,[],0,\"That should be it – you can now repair your environment, and you should be good to go!\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1346301900000,"status":"published","published_by":1},{"id":"0262e1e1-27e5-49e5-bd69-d1213bffdb7a","title":"Lab management: The Dreaded “Unknown Error: 0x8033811e”","slug":"lab-management-the-dreaded-unknown-error-0x8033811e","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I have been setting up a TFS for demos and web training. Since TFS installation is now really easy, I got it up quickly and configured Lab Management. Everything was plain sailing until I tried to deploy a stored environment. The SCVMM job failed and provided this rather unhelpful message (where host.com is my host server):\"]]],[1,\"p\",[[0,[0],1,\"A Hardware Management error has occurred trying to contact server host.com (Unknown error 0x8033811e). Check that WinRM is installed and running on the server host.com.\"],[0,[],0,\"\"]]],[1,\"p\",[[0,[],0,\"Well I did check WinRM – no problems there. So I started to search for other solutions – found one about ip listening ports and a whole bunch of other nothing. After sifting through some obscure sites, I managed to narrow the solution to one of two things:\"]]],[3,\"ol\",[[[0,[],0,\"BITS port conflicts\"]],[[0,[],0,\"Delegation issues\"]]]],[1,\"h2\",[[0,[],0,\"Changing the BITS Port\"]]],[1,\"p\",[[0,[],0,\"By default, BITS uses port 443 to transfer files – so if you have anything using SSL or firewalls blocking the port, you’ll have issues with BITS. I ignored this at first since the SCVMM job seemed to get past copying the VHD over BITS. Anyway, this turned out to be the solution to my problem. Here’s how you fix it:\"]]],[3,\"ol\",[[[0,[],0,\"Open the registry of the VMM server machine\"]],[[0,[],0,\"Find HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Microsoft\\\\Microsoft System Center Virtual Machine Manager Server\\\\Settings\"]],[[0,[],0,\"Edit the BITSTcpPort to something other than 443 (I used 8050)\"]],[[0,[],0,\"Restart the “Virtual Machine Manager” service\"]]]],[1,\"h2\",[[0,[],0,\"Fixing Delegation Issues\"]]],[1,\"p\",[[0,[],0,\"The other possibility is delegation issues. Follow these steps:\"]]],[3,\"ol\",[[[0,[],0,\"Go to the Domain Controller and open “Active Directory Users and Computers”\"]],[[0,[],0,\"In the Computers node, find the host machine and open its properties\"]],[[0,[],0,\"Go to the Delegation tab and check “Trust this computer for delegation to specified services only”\"]],[[0,[],0,\"Select “Use any authentication protocol”\"]],[[0,[],0,\"In the list of services, click “Add” and find the VMM server computer\"]],[[0,[],0,\"When you click OK, a list of services is shown – find the “cifs” service, select it and click ok\"]]]],[1,\"p\",[[0,[],0,\"Happy labbing!\"]]]]}","published_at":1302802920000,"status":"published","published_by":1},{"id":"09c777fe-b4bb-4073-b800-56c903764699","title":"Lab Management: Use Database Backups During Deployment for Repeatable Automated Tests","slug":"lab-management-use-database-backups-during-deployment-for-repeatable-automated-tests","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-qV6WpXmPpRo/TftecOdQt5I/AAAAAAAAAQ0/daxSWiL18So/s1600-h/image%25255B6%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-SR3qj05dN4w/Tftec_9ydBI/AAAAAAAAAQ4/3pY5gilG5aE/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"207\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">cmd /c sqlcmd -S .\\\\SqlExpress -Q \\\"RESTORE DATABASE Demo FROM Disk='c:\\\\data\\\\Lab DB.bak' WITH MOVE 'DataLogicalName' TO 'c:\\\\data\\\\Lab DB.mdf', MOVE 'LogLogicalName' TO 'c:\\\\data\\\\Lab DB.ldf'\\\"</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-8qeXFB6Jeew/TftedTCeRSI/AAAAAAAAAQ8/iFeSMAXtRlc/s1600-h/image%25255B9%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-cncp3DtmnD8/TfteeSMoFXI/AAAAAAAAARA/RxUOv7S1tgw/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"103\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-LG-NcoKoKa8/TftefPEiXVI/AAAAAAAAARE/ZomMBSE4gbQ/s1600-h/image%25255B12%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; margin: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-alUXc0p651E/Tftef-LuRUI/AAAAAAAAARI/JlzqfxT61UM/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"123\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/aa833404.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/aa833267.aspx\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I was at a customer recently who have a small test database – around 120MB in size. I had encouraged them to use the \"],[0,[0],1,\"DB Professional tools in VS 2010\"],[0,[],0,\" to track their schema – that way they could deploy the database (as well as the latest schema) and some test data (through INSERT statements or even better, \"],[0,[1],1,\"data generation plans\"],[0,[],0,\"). However, their developers didn’t have the time or skills (yet) to do this.\"]]],[1,\"p\",[[0,[],0,\"So we initially created the lab with a dependency on a database outside the Lab environment – an \"],[0,[2],1,\"external\"],[0,[],0,\" database if you will. However, we were using Network Isolation and had numerous instances of the Lab environments – so contention on the external database was a risk.\"]]],[10,0],[1,\"p\",[[0,[2],1,\"The Lab with the External DB Dependency\"]]],[1,\"p\",[[0,[],0,\"I suggested that we install SQLExpress within the Lab environment (since we didn’t have a database VM in the environment, we just installed it on the Client VM where the tests are running). We could have restored a backup of the external database into the Lab as part of the “Clean” snapshot (allowing the Lab to start with a known database), but this would mean any changes to the database schema or lookup data would require someone re-create the snapshot.\"]]],[1,\"p\",[[0,[],0,\"So we came up with another solution: we created a shared folder on the network and placed a backup of the database into this folder. Then in the deployment section of the Lab Workflow, we added a script to copy the backup to a folder on the Lab machine (in this case, c:\\\\data). The last step is to use sqlcmd to restore the database as follows (paths and dbnames depend on your environment, of course):\"]]],[10,1],[1,\"p\",[[0,[],0,\"where DataLogicalName and LogLogicalName are the logical names of the data and log files within the backup.\"]]],[1,\"p\",[[0,[],0,\"This approach allows you to “deploy” schema changes by refreshing the backup on the shared location. If you add a new table or something that the code requires, simply backup the new database to the shared location and you’re done.\"]]],[10,2],[1,\"p\",[[0,[2],1,\"The Lab at the “Clean” Snapshot\"]]],[10,3],[1,\"p\",[[0,[2],1,\"The Lab after deployment\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1308351720000,"status":"published","published_by":1},{"id":"4de3daec-0f0f-4b17-9ddb-44635915665e","title":"Lessons about DevOps from 3D Printing","slug":"lessons-about-devops-from-3d-printing","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8feab4fd-e6bb-48e4-a8c6-ea80d50779ba.jpg\\\" target=\\\"_blank\\\"><img width=\\\"381\\\" height=\\\"287\\\" title=\\\"IMG_20190120_001801\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"IMG_20190120_001801\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c6ecad64-eefb-40d6-819e-327fc336a254.jpg\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0e51842d-b2b4-42b6-9274-0fa0b369361f.jpg\\\" target=\\\"_blank\\\"><img width=\\\"235\\\" height=\\\"312\\\" title=\\\"IMG_20190203_205203_Bokeh\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"IMG_20190203_205203_Bokeh\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b6a00ae0-e341-41d4-b811-b91a25fbec20.jpg\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/879660a1-da7c-42e5-b2d3-42085ba893a9.jpg\\\" target=\\\"_blank\\\"><img width=\\\"358\\\" height=\\\"269\\\" title=\\\"IMG_20190123_233331\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"IMG_20190123_233331\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1201059e-78b5-4ecd-8c5a-172c21c2a3cc.jpg\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.gallupstrengthscenter.com/home/en-us/strengthsfinder\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=odDZMYr1di8\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Polylactic_acid\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Slicer_(3D_printing)\"]],[\"em\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Fused_filament_fabrication\"]],[\"a\",[\"href\",\"https://www.plaidhatgames.com/games/dead-of-winter\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=hUL-g3MUvJY\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"It's no surprise that I'm passionate about DevOps. I think that has to do with my personality - my top five \"],[0,[0],1,\"StrengthsFinder\"],[0,[],0,\" strengths are Strategic, Ideation, Learner, Activator, Achiever. I love the combination of people and tools that DevOps brings together. Being deeply technical and also fascinated by how people interact means I'm built for DevOps consulting. Add to that my love of learning, and I'm in my perfect job, since if there's one thing I've learned about DevOps - it's that I'll never learn everything there is to learn about it!\"]]],[10,0],[1,\"p\",[[0,[],0,\"I recently got a 3D printer (an \"],[0,[1],1,\"Ender3\"],[0,[],0,\") for my birthday - I was actually looking for some Raspberry Pi projects to do with my kids when I saw a post about a guy who created some programmable LED Christmas lights. He'd printed himself a case for his Raspberry Pi and I wondered, \\\"How much does a 3D printer go for nowadays?\\\" I was pleasantly surprised to learn that you can get a pretty decent printer for around $200. So I got one for my birthday - and it's been a ton of fun learning how to model, then slice models, then tweak the printer to make it bend to my will. But watching an abstract model turn into physical reality before your eyes is fantastic!\"]]],[1,\"p\",[[0,[],0,\"While I was learning how to 3D print, I realized there are a lot of parallels between 3D printing and DevOps. Some may accuse me of \\\"when you have a hammer, everything looks like a nail\\\" and I suspect they're partly right. But that doesn't mean you can't learn about one discipline from studying another! Remember, Lean and Agile have roots in auto manufacturing!\"]]],[10,1],[1,\"p\",[[0,[],0,\"So here are some thoughts about DevOps that I got from 3D Printing.\"]]],[1,\"h3\",[[0,[],0,\"You Have To Just Jump In At Some Stage\"]]],[1,\"p\",[[0,[],0,\"I ordered my printer just before Christmas (my birthday is in January) but I had about 3 weeks between ordering and receiving my printer. I watched a ton of videos, read as much as I could, learned a couple of modeling and slicing programs - but no matter how much I read, I had to just start printing! There's something about learning while you go that's fun (and sometimes frightening) about both 3D printing and DevOps. Don't get into \\\"analysis paralysis\\\" - start somewhere and you'll be surprised how quickly you can move. Having a partner who can help you decide on some \\\"low-lying fruit\\\" will help you start faster - but don't be afraid to start somewhere. Also, no matter how much theoretical knowledge you have, you're going to have to start implementing and then adjust along the way.\"]]],[1,\"h3\",[[0,[],0,\"Understanding Fundamentals Improves Your Success\"]]],[1,\"p\",[[0,[],0,\"While it was frustrating to wait so long for my printer to arrive, I am ultimately grateful because I learned a lot of fundamentals. Before ordering the printer I had no idea what \"],[0,[2],1,\"PLA filament\"],[0,[],0,\" was or what \"],[0,[3],1,\"slicers\"],[0,[],0,\" were. But learning the fundamentals of how printers actually work has helped me troubleshoot and improved my success.\"]]],[1,\"p\",[[0,[],0,\"Learning about DevOps can help you on your journey - some teams implement automation and call it DevOps. This shows that they don't fully understand what DevOps is about - and understanding the higher-level goals and history of DevOps can help you on your journey. Don't just jump onto the buzz words - understand \"],[0,[4],1,\"why\"],[0,[],0,\" they make a difference. Understanding fundamentals will help you improve your success.\"]]],[1,\"h3\",[[0,[],0,\"Small Changes Can Have Radical Impact\"]]],[1,\"p\",[[0,[],0,\"Because \"],[0,[5],1,\"Fused Deposition Modeling\"],[0,[],0,\" (FDM) printing is additive, the 1st layer is critical. This layer needs to bond correctly to the build bed, otherwise the print is doomed. Adjusting the bed to make it level and the correct height from the nozzle is a fiddly task - and small changes can make a huge difference.\"]]],[1,\"p\",[[0,[],0,\"In DevOps, sometimes small changes make a big difference. Be mindful of how you make changes in your teams, processes or tools. Making too many changes at once will prevent you from determining which changes are working and which are not. Also small changes let your team get some wins and build momentum.\"]]],[1,\"h3\",[[0,[],0,\"Just When You Think Everything Is Perfect, Something Fails\"]]],[1,\"p\",[[0,[],0,\"One of the challenges with printing is extrusion - the amount of plastic that is fed into the hot-end as the printer works. Too little and you get holes and missing layers, too much and you get blobs and stringing. The printer firmware has a multiplier for the extruder - if you program it to extrude 100mm of filament, it should extrude 100mm of filament! However, the stepper motor isn't perfectly calibrated, so you have to tweak the multiplier to get the correct extrusion. I had gone through the process of setting the extruder multiplier and was happy with the prints I was getting. I wanted to install some upgrades and wanted to print a baseline print for comparison - but the baseline print was terrible! There was clear under-extrusion - which I wasn't expecting since I hadn't touched the extruder settings. Eventually I had to recalibrate the extruder multiplier again.\"]]],[1,\"p\",[[0,[],0,\"In DevOps, you never \\\"arrive\\\". DevOps is a journey, and things can sometimes just blow up when you least expect. Remember that DevOps is more than just tooling and automation - people are a critical component of DevOps. And people change, new people come in or leave - and these changes can affect your culture - and therefore your DevOps. Keeping your eyes open, ensuring that you're following the vision and making sure everyone is still with you is key to success.\"]]],[1,\"h3\",[[0,[],0,\"Fast Feedback Is Critical\"]]],[10,2],[1,\"p\",[[0,[],0,\"Some prints can take a while - the Yoda print I made for my son took just over 7 hours. I watched closely (especially in the beginning) to make sure the first couple layers worked correctly - fortunately I got a good couple layers early on and the print turned out great. I have also done some other prints where the first couple layers didn't bond to the build surface correctly, and I aborted before wasting time (and filament). Getting feedback quickly was critical - fortunately I got to see the layers as they ran, so I got immediate feedback.\"]]],[1,\"p\",[[0,[],0,\"Getting feedback quickly is one of the primary goals of DevOps - reducing cycle times ensures that you can iterate rapidly and adjust rapidly. You may have heard the expression \\\"Fail fast\\\". Rather get feedback after 2 weeks and adjust than go off for 3 months building the wrong thing. Whatever you do and however far you are on the DevOps journey, make sure that you get rapid feedback - both for the software you're building and for your DevOps processes (how you're building) so that you can adjust quickly and often.\"]]],[1,\"h3\",[[0,[],0,\"You Can Use a Printer To Improve a Printer\"]]],[1,\"p\",[[0,[],0,\"It's almost a rite of passage - when you get your printer, your first dozen prints are upgrades for your printer! Every print you make for your printer improves your printer so that you keep getting better prints.\"]]],[1,\"p\",[[0,[],0,\"In DevOps, you can apply principles for good software delivery to the process itself. How about \\\"evidence gathered in production\\\"? There's no place like production, so that's where you want to get usage and performance metrics from. Similarly, you want to measure your team in their native habitat to see how they're doing. Reducing cycle times and getting feedback fast improves your software, but are you applying the same principles to your processes? Try something, evaluate, and abort quickly if it's not working.\"]]],[1,\"p\",[[0,[],0,\"At some stage, however, you need to print something other than parts for your printer. If all I did was print parts, then the parts become pointless since I bought the printer to enable me to print prototypes or art or whatever, not just printer parts. Don't navel gaze too much into your DevOps processes and remember that the ultimate goal is to deliver value to your customers!\"]]],[1,\"h3\",[[0,[],0,\"Sometimes You Have To Cut Your Losses And Try Again\"]]],[1,\"p\",[[0,[],0,\"I love board games - and we have a good collection of them. I wanted to print some inserts for sorting the cards and components to \"],[0,[6],1,\"Dead of Winter\"],[0,[],0,\" - and the prints kept failing. I got a bit frustrated and stopped printing (for now) until I've got some confidence back.\"]]],[1,\"p\",[[0,[],0,\"In DevOps, you can try things and if they fail, you may have to cut your losses and start again. Perhaps you need to try again - perhaps you need to try something different. Don't give up because something didn't work like you expected it to. Regroup, recoup and try again!\"]]],[1,\"h3\",[[0,[],0,\"Your Printer Is Unique\"]]],[1,\"p\",[[0,[],0,\"I'd seen a few experts recommend \"],[0,[7],1,\"TL smoothers\"],[0,[],0,\" to improve prints. The stepper motors on a printer are driven by voltage differentials, and the TL smoother is a set of diodes that prevent a voltage \\\"flutter\\\" when the voltage dips towards 0 - which smooths the movement of the stepper motor. So I decided I want to get some. I got a baseline print, installed the smoothers and repeated the print. Absolutely no difference. To be fair, the smoothers only cost $12 so it's not a big deal. It turns out that my Ender3 probably has better components than some earlier models, so the smoothers weren't necessary, even though experts had recommended them.\"]]],[1,\"p\",[[0,[],0,\"With DevOps, you're far better off being pragmatic about how and when to apply changes in your processes, tools and people. Don't just implement blindly - understand how DevOps practices will affect your team and how best to implement them for your team. Just because another organization or team was successful with some practice, doesn't mean you have to do it in the same way (or at all). Each team, environment and organization is different, so your DevOps could look different from that in other orgs. As Dori says, \\\"Just keep swimming!\\\"\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"There's a ton to learn about DevOps from 3D printing. As with anything, we're all on a journey. But sometimes we have to remember to step back and remember how far we've come - and why we're on the journey in the first place! Hopefully this reflection is positive for you.\"]]],[1,\"p\",[[0,[],0,\"Happy printing!\"]]]]}","published_at":1549501880000,"status":"published","published_by":1},{"id":"dad66e80-4957-4928-9691-f0ba19618869","title":"Load Balancing Application Tiers on VMWare","slug":"load-balancing-application-tiers-on-vmware","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-AA0f_VbeXZU/Tklw_zLpkQI/AAAAAAAAASc/tagzTWnSsaE/s1600-h/image%25255B3%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-8ngYatToOv8/TklxBX35RqI/AAAAAAAAASg/P6HJLM-rBAA/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"392\\\" height=\\\"377\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">arp [ip] [cluster multicast mac] ARPA</font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://technet.microsoft.com/en-us/library/bb742455.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Most posts about load balancing TFS Application Tiers using \"],[0,[0],1,\"NLB\"],[0,[],0,\" use either physical servers or Hyper-V virtual servers. So you would think that you can do the same using VMWare for the Application Tiers, right?\"]]],[1,\"p\",[[0,[],0,\"Wrong. NLB doesn’t play nicely with VMWare out-of-the-box – actually, it’s the virtual switches that are problematic. The details have to do with port-flooding and RARP packets when you configure NLB in unicast mode – the details are not that important, but the solution is. There are two solutions to the problem: configure the VMWare virtual switch and set up NLB in unicast mode, or set up NLB in multicast mode.\"]]],[1,\"p\",[[0,[],0,\"At a customer that I had to work at recently, we couldn’t tinker with the virtual switch because there were other servers connected to it, so we had to take the multicast route. To do this, you need to set up a static ARP route once you’ve set up the NLB.\"]]],[1,\"p\",[[0,[],0,\"When setting up the NLB, make a note of the MAC address that the cluster is assigned – you’ll need both the IP address and the MAC address of the cluster to set up the static ARP entry.\"]]],[10,0],[1,\"p\",[[0,[],0,\"On your router, configure the static ARP entry:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Once that’s done, you can configure all the nodes in the NLB cluster with an application tier only install. Configure the friendly name of the TFS url to the “Full Internet Name” from the above dialog (of course you’ll need an A record that points the friendly name to the IP address of the cluster).\"]]],[1,\"p\",[[0,[],0,\"Once that’s done, you’re good to go!\"]]]]}","published_at":1313468280000,"status":"published","published_by":1},{"id":"a214994f-998d-4b17-9eda-32ad40b62847","title":"Load Balancing DotNet Core Docker Containers with nginx","slug":"load-balancing-dotnet-core-docker-containers-with-nginx","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"namespace NginXService.Controllers\\n{\\n    public class HomeController : Controller\\n    {\\n        // GET: /&lt;controller&gt;/\\n        public IActionResult Index()\\n        {\\n            // platform agnostic call\\n            ViewData[\\\"Hostname\\\"] = Environment.GetEnvironmentVariable(\\\"COMPUTERNAME\\\") ??\\n                Environment.GetEnvironmentVariable(\\\"HOSTNAME\\\");\\n\\n            return View();\\n        }\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"@{\\n    &lt;h1&gt;Hello World!&lt;/h1&gt;\\n    &lt;br/&gt;\\n\\n    &lt;h3&gt;Info&lt;/h3&gt;\\n    &lt;p&gt;&lt;b&gt;HostName:&lt;/b&gt; @ViewData[\\\"Hostname\\\"]&lt;/p&gt;\\n    &lt;p&gt;&lt;b&gt;Time:&lt;/b&gt; @string.Format(\\\"{0:yyyy-MM-dd HH:mm:ss}\\\", DateTime.Now)&lt;/p&gt;\\n}\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">app.UseMvc()</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Configure()</font>\"}],[\"code\",{\"code\":\"app.UseMvc(routes =&gt;\\n{\\n    routes.MapRoute(\\n        name: \\\"default\\\",\\n        template: \\\"{controller=Home}/{action=Index}/{id?}\\\");\\n});\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"FROM microsoft/dotnet:1.0.0-core\\n\\n# Set the Working Directory\\nWORKDIR /app\\n\\n# Configure the listening port\\nARG APP_PORT=5000\\nENV ASPNETCORE_URLS http://*:$APP_PORT\\nEXPOSE $APP_PORT\\n\\n# Copy the app\\nCOPY . /app\\n\\n# Start the app\\nENTRYPOINT dotnet NginXService.dll\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker pull nginx</font>\"}],[\"code\",{\"code\":\"worker_processes 1;\\n\\nevents { worker_connections 1024; }\\n\\nhttp {\\n\\n    sendfile on;\\n\\n    # List of application servers\\n    upstream app_servers {\\n\\n        server app1:5000;\\n        server app2:5000;\\n        server app3:5000;\\n\\n    }\\n\\n    # Configuration for the server\\n    server {\\n\\n        # Running port\\n        listen [::]:5100;\\n        listen 5100;\\n\\n        # Proxying the connections\\n        location / {\\n\\n            proxy_pass         http://app_servers;\\n            proxy_redirect     off;\\n            proxy_set_header   Host $host;\\n            proxy_set_header   X-Real-IP $remote_addr;\\n            proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;\\n            proxy_set_header   X-Forwarded-Host $server_name;\\n\\n        }\\n    }\\n}\\n\\n\",\"language\":\"\"}],[\"code\",{\"code\":\"version: '2'\\n\\nservices:\\n  app1:\\n    image: colin/nginxservice:latest\\n  app2:\\n    image: colin/nginxservice:latest\\n  app3:\\n    image: colin/nginxservice:latest\\n\\n  nginx:\\n    image: nginx\\n    links:\\n     - app1:app1\\n     - app2:app2\\n     - app3:app3\\n    ports:\\n     - \\\"5100:5100\\\"\\n    volumes:\\n     - ./nginx.conf:/etc/nginx/nginx.conf\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker-compose up</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/dc796082-cfca-4a0a-95ed-956160f52b5b.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/70a176b1-99a8-45d5-8946-ead54c69b254.png\\\" width=\\\"455\\\" height=\\\"254\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker build</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">build</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker build</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/b9385ace-8038-4e6c-88e1-de26aa9a939a.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/7b82771f-1ffd-474d-9d28-b4e9f3a6fef3.png\\\" width=\\\"235\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/2566950d-3709-4fcf-a654-41fceda99e2e.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/c8494cea-7c5b-4589-b7f9-b80981e095f8.png\\\" width=\\\"222\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/d55d4238-0095-4c19-b213-70c60b4a1a12.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/d2128d96-b76c-4634-9cbc-4447fb7b749b.png\\\" width=\\\"241\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/785447ca-9765-44d4-a299-f7d1932ecb87.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/454927e0-2492-4a6f-a21f-fce3580dac99.png\\\" width=\\\"456\\\" height=\\\"254\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://redis.io/\"]],[\"a\",[\"href\",\"https://hub.docker.com/_/redis/\"]],[\"a\",[\"href\",\"http://www.haproxy.org/\"]],[\"a\",[\"href\",\"https://www.nginx.com/\"]],[\"a\",[\"href\",\"https://hub.docker.com/_/haproxy/\"]],[\"a\",[\"href\",\"https://hub.docker.com/_/nginx/\"]],[\"a\",[\"href\",\"https://docs.docker.com/compose/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Yes, I’ve been playing with Docker again – no big surprise there. This time I decided to take a look at scaling an application that’s in a Docker container. Scaling and load balancing are concepts you have to get your head around in a microservices architecture!\"]]],[1,\"p\",[[0,[],0,\"Another consideration when load balancing is of course shared memory. \"],[0,[0],1,\"Redis\"],[0,[],0,\" is a popular mechanism for that (and since we’re talking Docker I should mention that there’s a \"],[0,[1],1,\"Docker image for Redis\"],[0,[],0,\") – but for this POC I decided to keep the code very simple so that I could see what happens on the networking layer. So I created a very simple .NET Core ASP.NET Web API project and added a single MVC page that could show me the name of the host machine. I then looked at a couple of load balancing options and started hacking until I could successfully (and easily) load balance three Docker container instances of the service.\"]]],[1,\"h2\",[[0,[],0,\"The Code\"]]],[1,\"p\",[[0,[],0,\"The code is stupid simple – for this POC I’m interested in configuring the load balancer more than anything, so that’s ok. Here’s the controller that we’ll be hitting:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Getting the hostname is a bit tricky for a cross-platform app, since *nix systems and windows use different environment variables to store the hostname. Hence the ?? code.\"]]],[1,\"p\",[[0,[],0,\"Here’s the View:\"]]],[10,1],[1,\"p\",[[0,[],0,\"I had to change the Startup file to add the MVC route. I just changed the\"]]],[10,2],[1,\"p\",[[0,[],0,\"line in the\"]]],[10,3],[1,\"p\",[[0,[],0,\"method to this:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Finally, here’s the Dockerfile for the container that will be hosting the site:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Pretty simple so far.\"]]],[1,\"h2\",[[0,[],0,\"Proxy Wars: HAProxy vs nginx\"]]],[1,\"p\",[[0,[],0,\"After doing some research it seemed to me that the serious contenders for load balancing Docker containers boiled down to \"],[0,[2],1,\"HAProxy\"],[0,[],0,\" and \"],[0,[3],1,\"nginx\"],[0,[],0,\" (with corresponding Docker images \"],[0,[4],1,\"here\"],[0,[],0,\" and \"],[0,[5],1,\"here\"],[0,[],0,\"). In the end I decided to go with nginx for two reasons: firstly, nginx can be used as a reverse proxy, but it can also serve static content, while HAProxy is just a proxy. Secondly, the nginx website is a lot cooler – seemed to me that nginx was more modern than HAProxy (#justsaying). There’s probably as much religious debate about which is better as there is about git rebase vs git merge. Anyway, I picked nginx.\"]]],[1,\"h2\",[[0,[],0,\"Configuring nginx\"]]],[1,\"p\",[[0,[],0,\"I quickly pulled the image for nginx (\"]]],[10,6],[1,\"p\",[[0,[],0,\") and then set about figuring out how to configure it to load balance three other containers. I used a Docker volume to keep the config outside the container – that way I could tweak the config without having to rebuild the image. Also, since I was hoping to spin up numerous containers, I turned to \"],[0,[6],1,\"docker-compose\"],[0,[],0,\". Let’s first look at the nginx configuration:\"]]],[10,7],[1,\"p\",[[0,[],0,\"This is really a bare-bones config for nginx. You can do a lot in the config. This config does a round-robin load balancing, but you can also configure least_connected, provide weighting for each server and more. For the POC, there are a couple of important bits:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 10-16: this is the list of servers that nginx is going to be load balancing. I’ve used aliases (app1, app2 and app3, all on port 5000) which we’ll configure through docker-compose shortly.\"]],[[0,[],0,\"Lines 22-23: the nginx server itself will listen on port 5100.\"]],[[0,[],0,\"Line 26, 28: we’re passing all traffic on to the configured servers.\"]]]],[1,\"p\",[[0,[],0,\"I’ve saved this config to a file called nginx.conf and put it into the same folder as the Dockerfile.\"]]],[1,\"h2\",[[0,[],0,\"Configuring the Cluster\"]]],[1,\"p\",[[0,[],0,\"To configure the whole cluster (nginx plus three instances of the app container) I use the following docker-compose yml file:\"]]],[10,8],[1,\"p\",[[0,[],0,\"That’s 20 lines of code to configure a cluster – pretty sweet! Let’s take a quick look at the file:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 4-9: Spin up three containers using the image containing the app (that I built separately, since I couldn’t figure out how to build and use the same image multiple times in a docker-compose file).\"]],[[0,[],0,\"Line 12: Spin up a container based on the stock nginx image.\"]],[[0,[],0,\"Lines 13-16: Here’s the interesting bit: we tell docker to create links between the nginx container and the other containers, aliasing them with the same names. Docker creates internal networking (so it’s not exposed publically) between the containers. This is very cool – the nginx container can reference app1, app2 and app3 (as we did in the nginx config file) and docker takes care of figuring out the IP addresses on the internal network.\"]],[[0,[],0,\"Line 18: map port 5100 on the nginx container to an exposed port 5100 on the host (remember we configured nginx to listen on the internal 5100 port).\"]],[[0,[],0,\"Line 20: map the nginx.conf file on the host to /etc/nginx/nginx.conf within the container.\"]]]],[1,\"p\",[[0,[],0,\"Now we can simply run\"]]],[10,9],[1,\"p\",[[0,[],0,\"to run the cluster!\"]]],[10,10],[1,\"p\",[[0,[],0,\"You can see how docker-compose pulls the logs into a single stream and even color-codes them!\"]]],[1,\"p\",[[0,[],0,\"The one thing I couldn’t figure out was how to do a\"]]],[10,11],[1,\"p\",[[0,[],0,\"on an image and use that image in another container within the docker-compose file. I could just have three\"]]],[10,12],[1,\"p\",[[0,[],0,\"directives, but that felt a bit strange to me since I wanted to supply build args for the image. So I ended up doing the\"]]],[10,13],[1,\"p\",[[0,[],0,\"to create the app image and then just using the image in the docker-compose file.\"]]],[1,\"p\",[[0,[],0,\"Let’s hit the index page and then refresh a couple times:\"]]],[10,14],[10,15],[10,16],[1,\"p\",[[0,[],0,\"You can see in the site (the hostname) as well as in the logs how the containers are round-robining:\"]]],[10,17],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Load balancing containers with nginx is fairly easy to accomplish. Of course the app servers don’t need to be running .NET apps – nginx doesn’t really care, since it’s just directing traffic. However, I was pleased that I could get this working so painlessly.\"]]],[1,\"p\",[[0,[],0,\"Happy load balancing!\"]]]]}","published_at":1470492913000,"status":"published","published_by":1},{"id":"53a1a627-983e-461c-8211-4dc12ba21745","title":"Managing Config for .NET Core Web App Deployments with Tokenizer and ReplaceTokens Tasks","slug":"managing-config-for-net-core-web-app-deployments-with-tokenizer-and-replacetokens-tasks","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"{\\n  \\\"ConnectionStrings\\\": {\\n    \\\"DefaultConnection\\\": \\\"Server=(localdb)\\\\\\\\mssqllocaldb;Database=aspnet-WebApplication1-26e8893e-d7c0-4fc6-8aab-29b59971d622;Trusted_Connection=True;MultipleActiveResultSets=true\\\"\\n  },\\n  \\\"Tricky\\\": {\\n    \\\"Gollum\\\": \\\"Smeagol\\\",\\n    \\\"Hobbit\\\": \\\"Frodo\\\"\\n  },\\n  \\\"Logging\\\": {\\n    \\\"IncludeScopes\\\": false,\\n    \\\"LogLevel\\\": {\\n      \\\"Default\\\": \\\"Debug\\\",\\n      \\\"System\\\": \\\"Information\\\",\\n      \\\"Microsoft\\\": \\\"Information\\\"\\n    }\\n  }\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"{\\n  \\\"ConnectionStrings\\\": {\\n    \\\"DefaultConnection\\\": \\\"__ConnectionStrings.DefaultConnection__\\\"\\n  },\\n  \\\"Tricky\\\": {\\n    \\\"Gollum\\\": \\\"__Tricky.Gollum__\\\",\\n    \\\"Hobbit\\\": \\\"__Tricky.Hobbit__\\\"\\n  },\\n  \\\"Logging\\\": {\\n    \\\"IncludeScopes\\\": false,\\n    \\\"LogLevel\\\": {\\n      \\\"Default\\\": \\\"__Logging.LogLevel.Default__\\\",\\n      \\\"System\\\": \\\"Information\\\",\\n      \\\"Microsoft\\\": \\\"Information\\\"\\n    }\\n  }\\n}\\n\",\"language\":\"js; highlight\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/91122559-5470-46fa-b8f5-e9c464d5cb7f.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/7b982804-8515-4c8d-a458-aa844cc2f3e5.png\\\" width=\\\"406\\\" height=\\\"356\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a5a4f06d-e255-4521-ad9e-5a5b46666842.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/5bfe206d-bff6-4adb-9dea-d89fb3c655e3.png\\\" width=\\\"724\\\" height=\\\"187\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/f39f1990-af9b-45c5-ac5c-30784ab19dab.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/283a9350-102d-4f3f-b3ce-ea1c9a6722e9.png\\\" width=\\\"545\\\" height=\\\"234\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4b94b9a1-f720-4169-b63f-7fa3fdbf3e86.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b241102d-fe0a-4e24-a4c6-3411e22cc4d4.png\\\" width=\\\"506\\\" height=\\\"234\\\"></a>\"}],[\"code\",{\"code\":\"param(\\n  $sourceFile,\\n  $targetPath)\\n\\nExpand-Archive -Path $sourceFile -DestinationPath $targetPath -Force\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/961fd32e-87b7-4a81-9d49-590d1e5833d5.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a17eccde-040d-422e-8254-5c21663d9785.png\\\" width=\\\"522\\\" height=\\\"218\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/be93b375-e5e0-4cde-ace9-41b6520e62ee.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/1208a734-c236-472c-832c-6d2991e0f027.png\\\" width=\\\"522\\\" height=\\\"371\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/9a92ca4f-f091-49cb-bbe0-8419047cbecc.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a5505b40-51b5-4c22-82bb-fd5706e59728.png\\\" width=\\\"552\\\" height=\\\"383\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/end-to-end-walkthrough-deploying-web-applications-using-team-build-and-release-management\"]],[\"em\"],[\"a\",[\"href\",\"https://scottaddie.com/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/ReplaceTokens\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=colinsalmcorner.colinsalmcorner-buildtasks\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/Tokenizer\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=Trackyon.trackyonadvantage\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Last week I posted an \"],[0,[0],1,\"end-to-end walkthrough\"],[0,[],0,\" about how to build and deploy web apps using Team Build and Release Management – including config management. The post certainly helps you if you’re on the .NET 4.x Framework – but what about deploying .NET Core apps?\"]]],[1,\"h2\",[[0,[],0,\"The Build Once Principle\"]]],[1,\"p\",[[0,[],0,\"If you’ve ever read any of my blogs you’ll know I’m a proponent of the “build once” principle. That is, your build should be taking source code and (after testing and code analysis etc.) producing a \"],[0,[1],1,\"single package\"],[0,[],0,\" that can be deployed to multiple environments. The biggest challenge with a “build once” approach is that it’s non-trivial to manage configuration. If you’re building a single package, how do you deploy it to multiple environments when the configuration is different on those environments? I present a solution in my walkthrough – use a publish profile and a parameters.xml file to tokenize the configuration file during build. Then replace the tokens with environment values at deploy time. I show you how to do that starting with the required source changes, how the build works and finally how to craft your release definition for token replacements and deployment.\"]]],[1,\"h2\",[[0,[],0,\"AppSettings.json\"]]],[1,\"p\",[[0,[],0,\"However, .NET Core apps are a different kettle of fish. There is no web.config file (by default). If you File->New Project and create a .NET Core web app, you’ll get an appsettings.json file. This is the “new” web.config if you will. If you then go to the .NET Core documentation, you’ll see that you can create multiple configuration files using “magic” names like appsettings.dev.json and appsettings.prod.json (these are loaded up during Startup.cs). I understand the appeal of this approach, but to me it feels like having multiple web.config files which you replace at deployment time (like web.dev.config and web.prod.config). I’m not even talking about config transforms – just full config files that you keep in source control and (conceptually) overwrite during deployment. So you’re duplicating code – which is bad juju.\"]]],[1,\"p\",[[0,[],0,\"I got to thinking about how to handle configuration for .NET Core apps, and after mulling it over and having a good breakfast chat fellow MVP \"],[0,[2],1,\"Scott Addie\"],[0,[],0,\", I thought about tokenizing the appsettings.json file. If I could figure out a clean way to tokenize the file at build time, then I could use my existing \"],[0,[3],1,\"ReplaceTokens\"],[0,[],0,\" task (part of my \"],[0,[4],1,\"marketplace extension\"],[0,[],0,\") during deploy time to fill in environment specific values. Unfortunately there’s no config transform for JSON files, so I decided to create a \"],[0,[5],1,\"Tokenizer\"],[0,[],0,\" task that could read in a JSON file and then auto-replace values with tokens (based on the object hierarchy).\"]]],[1,\"h2\",[[0,[],0,\"Tokenizer Task\"]]],[1,\"p\",[[0,[],0,\"To see this in action, I created a new .NET Core Web App in Visual Studio. I then added a custom config section. I ended up with an appsettings.json file that looks as follows:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Looking at this config, I can see that I might want to change the ConnectionStrings.DefaultConnection as well as the Tricky.Gollum and Tricky.Hobbit settings (yes, I’m reading the Lord of the Rings – I’ve read it about once a year since I was 11). I may want to change Logging.LogLevel.Default too.\"]]],[1,\"p\",[[0,[],0,\"Since the file is JSON, I figured I could create a task that reads the file in and then walks the object hierarchy, replacing values with tokens as it goes. But I realized that you may not want to replace every value in the file, so the task would have to take an explicit include (for only replacing certain values) or exclude list (for replacing all but certain values).\"]]],[1,\"p\",[[0,[],0,\"I wanted the appsettings file to look like this once the tokenization had completed:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can see the tokens on the highlighted lines.\"]]],[1,\"p\",[[0,[],0,\"After coding for a while on the plane (#RoadWarrior) I was able to create a task for tokenizing a JSON file (perhaps in the future I’ll make more file types available – or I’ll get some Pull Requests!). Having recently added unit tests for my Node tasks, I was able to bang this task out rather quickly.\"]]],[1,\"h2\",[[0,[],0,\"The Build Definition\"]]],[1,\"p\",[[0,[],0,\"With my shiny new Tokenize task, I was ready to see if I could get the app built and deployed. Here’s what my build definition looks like:\"]]],[10,2],[1,\"p\",[[0,[],0,\"The build tasks perform the following operations:\"]]],[3,\"ol\",[[[0,[],0,\"Run dotnet with argument “restore” (restores the package dependencies)\"]],[[0,[],0,\"Tokenize the appsettings.json file\"]],[[0,[],0,\"At this point I should have Test, Code Annalysis etc. – I’ve omitted these quality tasks for brevity\"]],[[0,[],0,\"Run dotnet with arguments “publish src/CoreWebDeployMe --configuration $(BuildConfiguration) --output $(Build.ArtifactStagingDirectory)/Temp” (I’m publishing the folder that contains my .NET Core web app with the BuildConfiguration and placing the output in the Build.ArtifactStagingDirectory/Temp folder)\"]],[[0,[],0,\"Zip the published folder (the zip task comes from \"],[0,[6],1,\"this extension\"],[0,[],0,\")\"]],[[0,[],0,\"Remove the temp folder from the staging directory (since all the files I need are now in the zip)\"]],[[0,[],0,\"Upload the zip as a build drop\"]]]],[1,\"p\",[[0,[],0,\"The Tokenize task is configured as follows:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Let’s look at the arguments:\"]]],[3,\"ul\",[[[0,[],0,\"Source Path – the path containing the file(s) I want to tokenize\"]],[[0,[],0,\"File Pattern – the mini-match pattern for the file(s) within the Source Path I want to tokenize\"]],[[0,[],0,\"Tokenize Type – I only have json for now\"]],[[0,[],0,\"IncludeFields – the list of properties in the json file that I want the Tokenizer to tokenize\"]],[[0,[],0,\"ExcludeFields – I could have used a list of properties I wanted to exclude from tokenization here instead of using the Include Fields property\"]]]],[1,\"p\",[[0,[],0,\"Once the build completes, I now have a potentially deployable .NET Core web application with a tokenized appsettings file. I could have skipped the zip task and just uploaded the site unzipped, but uploading lots of little files takes longer than uploading a single larger file. Also, I was thinking about the deployment – downloading a single larger file (I guessed) was going to be faster than downloading a bunch of smaller files.\"]]],[1,\"h2\",[[0,[],0,\"The Release\"]]],[1,\"p\",[[0,[],0,\"I was expecting to have to unzip the zip file, replace the tokens in the appsettings.json file and then re-zip the file before invoking WebDeploy to push the zip file to Azure. However, the AzureRM WebDeploy task recently got updated, and I noticed that what used to be “Package File” was now “Package File or Folder”. So the release turned out to be really simple:\"]]],[3,\"ol\",[[[0,[],0,\"Unzip the zip file to a temp folder using an inline PowerShell script (why is there no complementary Unzip task from the Trackyon extension?)\"]],[[0,[],0,\"Run ReplaceTokens on the appsettings.json file in the temp folder\"]],[[0,[],0,\"Run AzureRM WebDeploy using the temp folder as the source folder\"]]]],[10,4],[1,\"p\",[[0,[],0,\"Here’s how I configured the PowerShell task:\"]]],[10,5],[1,\"p\",[[0,[],0,\"The script takes in the sourceFile (the zip file) as well as the target path (which I set to a temp folder in the drop folder):\"]]],[10,6],[1,\"p\",[[0,[],0,\"My first attempt deployed the site – but the ReplaceTokens task didn’t replace any tokens. After digging a little I figured out why – the default regex pattern – __(\\\\w+)__ – doesn’t work when the token name have periods in them. So I just updated the regex to __(\\\\w+[\\\\.\\\\w+]*)__ (which reads “find double underscore, followed by a word, followed by a period and word repeated 0 or more times, ending with double underscore”.\"]]],[10,7],[1,\"p\",[[0,[],0,\"That got me closer – one more change I had to make was replacing the period with underscore in the variable names on the environment:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Once the ReplaceTokens task was working, the Deploy task was child’s play:\"]]],[10,9],[1,\"p\",[[0,[],0,\"I just made sure that the “Package or Folder” was set to the temp path where I unzipped the zip file in the first task. Of course at this point the appsettings.json now contains real environment-specific values instead of tokens, so WebDeploy can go and do its thing.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"It is possible to apply the Build Once principle to .NET Core web applications, with a little help from my friends Tokenizer and ReplaceTokens in the build and release respectively. I think this approach is fairly clean – you get to avoid duplication in source code, build a single package and deploy to multiple environments. Of course my experimentation is available to your for free from the tasks in my \"],[0,[4],1,\"marketplace extension\"],[0,[],0,\"! Sound off in the comments if you think this is useful (or horrible)…\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1477477220000,"status":"published","published_by":1},{"id":"9d796ced-3e31-42d7-a2a5-1d5914f1979a","title":"Managing Credentials and Secrets in VSTS Release Management","slug":"managing-credentials-and-secrets-in-vsts-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e34bd373-bcc6-4049-a58c-8ec88c10a454.png\\\" target=\\\"_blank\\\"><img width=\\\"307\\\" height=\\\"254\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3e2a7993-9152-4c88-b60e-5447179db634.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/18eeb213-bcec-456c-b3d0-7e8de9957d68.png\\\" target=\\\"_blank\\\"><img width=\\\"303\\\" height=\\\"107\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/81c1ba4e-e21a-438e-b4bb-150ce1d58cf2.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/library/dd465318(v=vs.100).aspx\"]],[\"strong\"],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/pipelines/tasks/deploy/iis-web-app-deployment-on-machine-group?view=vsts\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/pipelines/tasks/deploy/azure-rm-web-app-deployment?view=vsts\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/pipelines/tasks/transforms-variable-substitution?view=vsts#xml-variable-substitution\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/easy-config-management-when-deploying-azure-web-apps-from-vsts\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/key-vault/\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/pipelines/library/variable-groups?view=vsts\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Releases almost always require some kind of credentials - from service credentials to database usernames and passwords. There are a number of ways to manage credentials in VSTS release management. In this post I'll look at a couple of common techniques. For brevity, I'm going to refer to \"],[0,[0],1,\"secrets\"],[0,[],0,\" as a proxy for secrets and credentials.\"]]],[1,\"h2\",[[0,[],0,\"Don't Store Secrets in Source Control\"]]],[1,\"p\",[[0,[],0,\"One bad practice you want to steer away from is storing secrets in source control. A lot of teams I work with have their build process create multiple environment-specific packages, using tools like \"],[0,[1],1,\"config transforms\"],[0,[],0,\". I like to get teams to think of build and release as two separate (but linked) processes:\"]]],[1,\"p\",[[0,[2],0,\"Process\"],[0,[],0,\"Input\"],[0,[],0,\"Process\"],[0,[],1,\"Output\"],[0,[],0,\"BuildSource CodeCompile, unit test, packageTokenized build packagesReleaseBuild artifacts, config source codeInfrastructure deployment/config, approvals, integration/functional tests, app deploymentsDeployed application\"]]],[1,\"p\",[[0,[],0,\"The point is that the build should be \"],[0,[0],1,\"totally environment agnostic\"],[0,[],0,\". Good unit tests use mocking or fakes, so they shouldn't need environment-specific information. That means that they need to create packages that are, as I like to call them, swiss cheese - they need to have holes or tokens that can have environment-specific values injected at deployment time. You don't need tokens if your deployment process is capable of doing variable substitution - like the \"],[0,[3],1,\"IIS Deployment on Machine Group\"],[0,[],0,\" task or \"],[0,[4],1,\"Azure App Service Deployment\"],[0,[],0,\" task that can both do \"],[0,[5],1,\"inline variable replacement\"],[0,[],0,\" (see my \"],[0,[6],1,\"earlier post\"],[0,[],0,\" on how to do this - and this also now applies to the IIS Deployment on Machine Group task).\"]]],[1,\"h2\",[[0,[],0,\"Centralized vs Decentralized Secret Management\"]]],[1,\"p\",[[0,[],0,\"I see two broad categories of secret management: centralized and decentralized. Centralized secret management has the advantage of specifying/updating the secret once, even if it's used in many places - but has the disadvantage of being managed by a small subset of users (admins typically). This can also be an advantage, but can be a bottleneck. Decentralized secret management usually ends up in duplicated secrets (so updating a password leaves you hunting for every occurrence of that password) but removes the bottleneck of centralized management. Choosing a method will depend on your culture, auditing requirements and management overhead.\"]]],[1,\"h3\",[[0,[],0,\"Decentralized Secret Management\"]]],[1,\"p\",[[0,[],0,\"Decentralized secret management is the easiest to consider, and there's really only one way to do it: in your release definition, define your secrets as variables that are locked and you're done. If you need to use the same secret in multiple definitions, you just create the same variable. Of course if you change the value, you have to change it in each release that uses it. But you don't have to log a ticket or wait for anyone to change the value for you - if it changes, you update it in place for each release and you're done.\"]]],[1,\"h3\",[[0,[],0,\"Centralized Secret Management\"]]],[1,\"p\",[[0,[],0,\"There are three types of centralized secret management: \"],[0,[7],1,\"Azure KeyVault\"],[0,[],0,\", \"],[0,[8],1,\"Variable Groups\"],[0,[],0,\" and Custom Key Vault. Let's consider each method.\"]]],[1,\"p\",[[0,[],0,\"The KeyVault and Variable Group methods both define a Variable Group - but if you use KeyVault, you manage the values in KeyVault rather than in the Variable Group itself. Otherwise they are exactly the same.\"]]],[1,\"p\",[[0,[],0,\"Go to the VSTS release hub and click on Library to see variable groups. Create a new Variable Group and give it a name. If this is a \\\"plain\\\" Variable Group, define all your secrets and their values - don't forget to padlock the values that you want to hide. If you're using KeyVault, first define a Service Endpoint in the Services hub for authenticating to the KeyVault. Then come back and link the Variable Group to the KeyVault and specify which Secrets are synchronized.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now when you run define a release, you link the Variable Group (optionally scoping it) and voila - you have a centralized place to manage secrets, either directly in the Variable Group or via KeyVault.\"]]],[10,1],[1,\"p\",[[0,[],0,\"The variable group can be linked to many releases, so you only ever have to manage the values in one place, irrespective of how many releases reference them. To use the values, just use $(SecretName) in your tasks.\"]]],[1,\"p\",[[0,[],0,\"The last method is Custom Key Vault. I worked with a customer a few months back that used some sort of third-party on-premises key vault. Fortunately this vault had a REST API and we were able to create a custom task that fetched secrets from this third-party key vault. If you do this, you need to remember to add in a custom task to get the values, but this was an elegant solution for my customer since they already had an internal key vault.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"There are a number of ways to manage secrets and credentials in VSTS/TFS. The most robust is to use Azure KeyVault, but if you don't have or don't want one you can use Variable Groups in-line. Whatever method you choose, just make sure you don't store any secrets in source control!\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1531881415000,"status":"published","published_by":1},{"id":"c903c414-40a9-432f-b4d3-edfb544d425c","title":"Matching Binary Version to Build Number Version in TFS 2013 Builds","slug":"matching-binary-version-to-build-number-version-in-tfs-2013-builds","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"Param(\\n  [string]$pathToSearch = $env:TF_BUILD_SOURCESDIRECTORY,\\n  [string]$buildNumber = $env:TF_BUILD_BUILDNUMBER,\\n  [string]$searchFilter = \\\"AssemblyInfo.*\\\",\\n  [regex]$pattern = \\\"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\"\\n)\\n\\ntry\\n{\\n    if ($buildNumber -match $pattern -ne $true) {\\n        Write-Host \\\"Could not extract a version from [$buildNumber] using pattern [$pattern]\\\"\\n        exit 1\\n    } else {\\n        $extractedBuildNumber = $Matches[0]\\n        Write-Host \\\"Using version $extractedBuildNumber\\\"\\n\\n        gci -Path $pathToSearch -Filter $searchFilter -Recurse | %{\\n            Write-Host \\\"  -&gt; Changing $($_.FullName)\\\" \\n        \\n            # remove the read-only bit on the file\\n            sp $_.FullName IsReadOnly $false\\n\\n            # run the regex replace\\n            (gc $_.FullName) | % { $_ -replace $pattern, $extractedBuildNumber } | sc $_.FullName\\n        }\\n\\n        Write-Host \\\"Done!\\\"\\n    }\\n}\\ncatch {\\n    Write-Host $_\\n    exit 1\\n}\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-pBXCKbcpwtQ/Ue532irjf6I/AAAAAAAAA-k/UXknumZQf9g/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-7FXKUl0DzNA/Ue533vEyvuI/AAAAAAAAA-s/J1MR-03AQ5Y/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"448\\\" height=\\\"137\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-7VO2ED_AQM4/Ue534-SSWVI/AAAAAAAAA-0/cc_ZmHC4hu8/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-WQFam_YQoo8/Ue535YZOfUI/AAAAAAAAA-8/58QHGMD0dY0/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"391\\\" height=\\\"194\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-oAFMEiiaeCI/Ue536Ou5d6I/AAAAAAAAA_E/1adNWaDcj28/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-ycoNgswTTeM/Ue536lSJTJI/AAAAAAAAA_M/zRdgbu0bNDQ/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"481\\\" height=\\\"107\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-fWNmnsSowyI/Ue537sUflYI/AAAAAAAAA_U/fF85Zqp6eDo/s1600-h/image%25255B15%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-3MfeHizdJro/Ue5387D_kXI/AAAAAAAAA_c/WAq0aIxYQrI/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"441\\\" height=\\\"297\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/jimlamb/archive/2009/11/18/how-to-create-a-custom-workflow-activity-for-tfs-build-2010.aspx\"]],[\"a\",[\"href\",\"http://www.microsoft.com/visualstudio/eng/2013-downloads\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/vstudio/dd647547(v=vs.120).aspx#scripts\"]],[\"a\",[\"href\",\"http://sdrv.ms/1bbZKWJ\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Jim Lamb wrote a \"],[0,[0],1,\"post\"],[0,[],0,\" about how to use a custom activity to match the compiled versions of your assemblies to the TFS build number. This was not a trivial exercise (since you have to edit the workflow itself) but is the best solution for this sort of operation. Interestingly the post was written in November 2009 and updated for TFS 2010 RTM in February 2010.\"]]],[1,\"p\",[[0,[],0,\"I finally got a chance to play with a VM that’s got \"],[0,[1],1,\"TFS 2013\"],[0,[],0,\" Preview installed. I was looking at the changes to the build engine. The Product Team have simplified the default template (they’ve collapsed a lot of granular activities into 5 or 6 larger activities). In fact, if you use the default build template, you won’t even see it (it’s not in the BuildProcessTemplates folder – you have to download it if you want to customize it).\"]]],[1,\"p\",[[0,[],0,\"The good news is that the team have added pre- and post-build and pre- and post-test script hooks into the default workflow. I instantly realised this could be used to solve the assembly-version-matches-build-number problem in a much easier manner.\"]]],[1,\"h2\",[[0,[],0,\"Using the Pre-Build Script\"]]],[1,\"p\",[[0,[],0,\"The solution is to use a PowerShell script that can replace the version in the AssemblyInfo files before compiling with the version number in the build. Here’s the procedure:\"]]],[3,\"ol\",[[[0,[],0,\"Import the UpdateVersion.ps1 script into source control (the script is below)\"]],[[0,[],0,\"Change the build number format of your builds to produce something that contains a version number\"]],[[0,[],0,\"Point the pre-build script argument to the source control path of the script in step 1\"]]]],[1,\"p\",[[0,[],0,\"The script itself is pretty simple – find all the matching files (AssemblyInfo.* by default) in a target folder (the source folder by default). Then extract the version number from the build number using a regex pattern, and do a regex replace on all the matching files.\"]]],[1,\"p\",[[0,[],0,\"If you’re using TFVC, the files are marked read-only when the build agent does a Get Latest, so I had to remove the read-only bit as well. The other trick was getting the source path and the build number – but you can use environment variables when executing any of the pre- or post- scripts (as detailed \"],[0,[2],1,\"here\"],[0,[],0,\").\"]]],[10,0],[1,\"p\",[[0,[],0,\"Save this \"],[0,[3],1,\"script\"],[0,[],0,\" as “UpdateVersion.ps1” and put it into Source Control (I use a folder called $/Project/BuildProcessTemplates/CommonScripts to house all the scripts like this one for my Team Project).\"]]],[1,\"p\",[[1,[],0,0]]],[10,1],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"The open your build and specify the source control path to the pre-build script (leave the arguments empty, since they’re all defaulted) and add a version number to your build number format. Don’t forget to add the script’s containing folder as a folder mapping in the Source Settings tab of your build.\"]]],[1,\"p\",[[1,[],0,2]]],[10,2],[1,\"p\",[[1,[],0,3]]],[10,3],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"Now you can run your build, and your assembly (and exe) versions will match the build number:\"]]],[1,\"p\",[[1,[],0,5]]],[10,4],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"I’ve tested this script using TFVC as well as a TF Git repository, and both work perfectly.\"]]],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Happy versioning!\"]]]]}","published_at":1413967202000,"status":"published","published_by":1},{"id":"d831b783-6130-41d8-a92d-856ea78ac89c","title":"Microsoft Fakes – Customizing the System Whitelist (or, enabling Fakes for classes you’ve always wanted to fake, like WebClient)","slug":"microsoft-fakes--customizing-the-system-whitelist-(or-enabling-fakes-for-classes-youve-always-wanted-to-fake-like-webclient)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-7RSmvbea-NI/T-hM8-M9AyI/AAAAAAAAAZE/RX-xVTCfsFQ/s1600-h/image2.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-3N5Oqrx3-6s/T-hM93wWrLI/AAAAAAAAAZM/aEt9u8h-zbg/image_thumb.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"218\\\"></a>\"}],[\"code\",{\"code\":\"<fakes xmlns=\\\"http://schemas.microsoft.com/fakes/2011/\\\"><br>  <assembly name=\\\"System\\\" version=\\\"4.0.0.0\\\"><br>  <shimgeneration><br>    <add fullname=\\\"System.Net.WebClient\\\"><br>  </add></shimgeneration><br></assembly></fakes><br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"public class HttpFunctions<br>{<br>    public void Download(string url, string fileName)<br>    {<br>        var w = new WebClient();<br>        w.DownloadFile(url, fileName);<br>        <br>    }<br>}<br><br>[TestClass]<br>public class UnitTest1<br>{<br>    [TestMethod]<br>    public void TestFakeWebClient()<br>    {<br>        using (ShimsContext.Create())<br>        {<br>            ShimWebClient.AllInstances.DownloadFileStringString = (w, url, fileName) =&gt;<br>            {<br>                File.WriteAllText(fileName, url);<br>            };<br><br>            var c = new HttpFunctions();<br>            string guidFileName = Guid.NewGuid().ToString() + \\\".html\\\";<br>            c.Download(\\\"http://www.bing.com\\\", guidFileName);<br><br>            Assert.IsTrue(File.Exists(guidFileName));<br>            string contents = File.ReadAllText(guidFileName);<br>            Assert.AreEqual(\\\"http://www.bing.com\\\", contents);<br>            File.Delete(guidFileName);<br>        }<br>    }<br>}<br>\",\"language\":\"csharp; ruler\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"So you’re sitting down planning some tests for your shiny new code, only to find that your code uses WebClient to download a file. No problem – you’ve been reading about Microsoft’s new Fakes framework, so you just right-click the System reference in your test project and select “Create Fakes” and you get a bunch of cool fakes to work with.\"]]],[1,\"p\",[[0,[],0,\"But going a bit deeper into the Rabbit Hole, you realize that there are no fakes for System.Net classes. What gives?\"]]],[1,\"h2\",[[0,[],0,\"The White-List\"]]],[1,\"p\",[[0,[],0,\"It turns out that when you right-click a reference and select “Add Fakes” a fakes file is created for that assembly in the Fakes folder. When you add a Fakes lib for System, you in fact get 2 fakes files: one for mscorlib and one for System.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Since System is a large library, the Fakes framework doesn’t automatically generate you a fake for every System class. One of the classes that doesn’t have a fake created by default is System.Net.WebClient. In order to force a fake Shim for this class, you’ll need to override the default “white-list” of classes. Fortunately this is relatively straight-forward to do.\"]]],[1,\"h2\",[[0,[],0,\"The Fakes Config File\"]]],[1,\"p\",[[0,[],0,\"Double-click System.fakes to open it in the editor – it’s just an XML file. It does come with an XSD, so as you type in this file IntelliSense will guide you (just like the Force, Luke). Within the  tag, add a  tag and within that an  tag and you’re good to go. Here’s my Fakes file for adding a WebClient fake:\"]]],[10,1],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Remember to recompile!\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"Here’s some example code using the WebClient fake:\"]]],[10,2],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"Happy faking!\"]]]]}","published_at":1340656500000,"status":"published","published_by":1},{"id":"2bc46956-c317-44a2-a112-b39db7d286a9","title":"Modernizing Source Control - Migrating to Git","slug":"modernizing-source-control---migrating-to-git","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ed7246a1-1969-4b93-93d2-46c343d7d59e.png\\\" target=\\\"_blank\\\"><img width=\\\"348\\\" height=\\\"349\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9075c016-4d17-48a6-8e18-0647ce7ea74d.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\" size=\\\"2\\\">git tfs clone</font>\"}]],\"markups\":[[\"a\",[\"href\",\"https://git-scm.com/\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/devops/repos/tfvc/overview?view=vsts\"]],[\"a\",[\"href\",\"https://subversion.apache.org/\"]],[\"a\",[\"href\",\"https://live360events.com/Events/Orlando-2018/Sessions/Tuesday/VST04-Modernizing-Your-Source-Control-Migrating-to-Git-from-Team-Foundation-Version-Control-TFVC.aspx\"]],[\"a\",[\"href\",\"https://git-lfs.github.com/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/services/devops/artifacts/\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/VFSForGit\"]],[\"a\",[\"href\",\"https://git-scm.com/book/en/v2/Getting-Started-A-Short-History-of-Git\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/vsts-one-team-project-and-inverse-conway-maneuver\"]],[\"a\",[\"href\",\"https://github.com/git-tfs/git-tfs\"]],[\"a\",[\"href\",\"https://git-scm.com/docs/git-svn\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I remember when I first learned about \"],[0,[0],1,\"Git\"],[0,[],0,\" circa 2012. I was skeptical - \"],[0,[1],1,\"you can change history? What kind of source control system let you change history?\"],[0,[],0,\" However, it seemed to have huge momentum and so I started learning how to use it. Once you get over the initial learning curve - and there is one when you switch from centralized version control systems like \"],[0,[2],1,\"Team Foundation Version Control (TFVC)\"],[0,[],0,\" or \"],[0,[3],1,\"Subversion\"],[0,[],0,\" - I started to see the beauty of Git. And now I believe that teams can benefit enormously if they migrate to Git. I believe that so strongly that I spoke about \"],[0,[4],1,\"this very topic at VSLive! in Orlando\"],[0,[],0,\" earlier this month.\"]]],[1,\"p\",[[0,[],0,\"In this post I want to detail why I think migrating to Git makes sense, common objections I hear, and some common ways you can migrate to Git. Migrating to Git make \"],[0,[1],1,\"business sense\"],[0,[],0,\" as well as \"],[0,[1],1,\"technical sense\"],[0,[],0,\" - so I'll call out business value-adds along the way. I'll primarily be talking about migrating from TFVC, but similar principles apply if you're migrating from other centralized source control systems.\"]]],[1,\"h2\",[[0,[],0,\"Why Git?\"]]],[1,\"p\",[[0,[],0,\"There are several reasons why I think Git is essential for modern teams:\"]]],[3,\"ol\",[[[0,[],0,\"Branches are cheap\"]],[[0,[],0,\"Merging is better\"]],[[0,[],0,\"Code review is baked in via Pull Request\"]],[[0,[],0,\"Better offline workflow\"]]]],[1,\"h3\",[[0,[],0,\"Cheap Branches\"]]],[1,\"p\",[[0,[],0,\"The primary reason I love Git is that \"],[0,[1],1,\"branches are cheap\"],[0,[],0,\". We'll get to the technical reasons why this important next - but the main business benefit of cheap branches lies in the ability to easily isolate (and later merge) development streams. That should be exciting since it means that small changes can be completed, merged and deployed without having to be held hostage by larger, longer-running changes. Delays cost, so anything that eliminates delays is good!\"]]],[1,\"p\",[[0,[],0,\"In centralized version control, a branch is a complete copy of the source - so typically teams keep the number of branches small. With Git, branches are essentially pointers, so creating branches is cheap. This means teams can create a lot of branches. Why does this make a difference anyway? The idea of a branch is to isolate code changes. Typical TFVC branching strategy is \\\"DEV-MAIN-PROD\\\". This is an attempt to isolate code in development (DEV) from code that's being tested (MAIN) and code that's running in production (PROD). That seems at first glance to be exactly what we want branches for - however, there's a catch: what if we have two or ten or twenty features in development? I coach teams to check in early, check in often - but that means that at times the code that's checked in will be unstable. Teams expect this at in the DEV branch. In fact, there's a term for how stable a branch is: hardness. The DEV branch is considered \\\"soft\\\" since it's not always stable - while PROD is supposed to be \\\"hard\\\" - that is, stable. But this branching strategy is flawed in that it isolates code at too coarse a level. What we really want is to isolate more granularly - especially if we want to deploy smaller features when they're complete without having to wait for larger features to be ready for deployment.\"]]],[1,\"p\",[[0,[],0,\"Git allows teams to create a branch \"],[0,[1],1,\"per feature\"],[0,[],0,\" - also commonly referred to as \"],[0,[1],1,\"topic branching\"],[0,[],0,\". You don't want to do this when each branch is an entire copy of the code-base - but since Git branches are pointers, we can create branches liberally. By using a good naming convention and periodically cleaning branches that are stale (that haven't been updated for long periods) teams can get very good at isolating changes, and that makes their entire application lifecycle more resilient and more agile and minimize costly delays.\"]]],[1,\"h3\",[[0,[],0,\"Better Merging\"]]],[1,\"p\",[[0,[],0,\"Merge debt can also be costly - the further away two branches diverge, the more costly and risky merging them becomes. Again, thinking in \\\"business terms\\\", this means you can move faster, with better quality - and what business doesn't want that?\"]]],[1,\"p\",[[0,[],0,\"Let's imaging you have 20 features in flight on a single DEV branch, and you somehow manage to coordinate a merge when all the features are ready to go, you'll probably spend a lot of time working through the merge since there are so many changes. Also, features that are completed quickly are forced to wait until the slowest feature is complete - which is a lot of waste. Or teams decide to merge anyway, knowing that they're merging incomplete code.\"]]],[1,\"p\",[[0,[],0,\"Also, when a file changes in a Git repo, Git records the entire file, not just the diffs (like TFVC). This means that merging between arbitrary branches works. With TFVC, branches have to be related to merge - or you could try a dreaded \\\"baseless merge\\\", which is very error-prone. Even though Git stores the entire file for a change, it does so very efficiently, but because of this merging is far easier for the Git.\"]]],[1,\"p\",[[0,[],0,\"Empirically I find that Git teams have fewer merge conflicts and merge issues than TFVC teams. Let's now imaging that we are using Git and have 20 features in flight - and 3 are ready to be deployed, but we want to test them. If we want to test them individually, no problem - we do a build off the branch which is master (the stable code) plus the branch changes. We can queue 3 builds and test each feature in isolation. We can also merge any branch into any of the others (something you can't easily do in unrelated branches in TFVC), so we can also test them together and make sure that there are no breaking changes in the merge - even before we merge each branch to master! This let's teams deploy features much more rapidly and frequently, eliminating waste along the way.\"]]],[1,\"h3\",[[0,[],0,\"Code Reviews\"]]],[1,\"p\",[[0,[],0,\"One of GitHub's engineers introduced the concept of Pull Requests (PRs) and it's since become ubiquitous in the Git world - even though you don't typically do PRs in your local Git repo. The PR lets developers advertise that their code is ready to be merged. Policies (and reviews) can be built around the PR so that only quality code is merged into master. PRs are dynamic - so if I am the reviewer and comment on some code that a developer has submitted in a PR, the developer can fix the code and I can see the changes \\\"live\\\" in the PR. In contrast, TFVC lets you submit a Code Review work item (only through the Visual Studio IDE) and if the code needs to be changed, a new Code Review needs to be created. The whole code review process is clunky and laborious. However, I find PRs to be unobtrusive - they let us check code quickly, respond and adapt, and finally merge in a really natural manner. The Azure DevOps PR interface is fantastic - and if you add branch policies (available in Azure DevOps) you can enforce links to work items, comment resolution, build verification and even external system checks before a PR is merged. This lets teams \\\"shift left\\\" and build quality into their process in a natural, powerful and unobtrusive manner.\"]]],[1,\"h3\",[[0,[],0,\"Better Offline\"]]],[1,\"p\",[[0,[],0,\"Git is a distributed version control system - it's designed to be used locally and synchronized to a central repo for sharing changes. As such, disconnected workflows are natural and powerful - and since cloning a repository gets the entire history of the repo from day 0, and I can branch and merge locally, the disconnected experience is excellent. TFVC used to require connection to the server to do most source control operations - with local workspaces (circa 2013) some source control operations can be performed offline, but you still need to be connected to the server to branch and merge.\"]]],[1,\"h2\",[[0,[],0,\"Common Objections\"]]],[1,\"p\",[[0,[],0,\"There are four common objections I often hear to migrating to Git:\"]]],[3,\"ol\",[[[0,[],0,\"I can overwrite history\"]],[[0,[],0,\"I have large files\"]],[[0,[],0,\"I have a very large repo\"]],[[0,[],0,\"I don't want to use GitHub\"]],[[0,[],0,\"There's a steep learning curve\"]]]],[1,\"h3\",[[0,[],0,\"Overwriting History\"]]],[1,\"p\",[[0,[],0,\"Git technically does allow you to overwrite history - but (as we know from Spiderman) with great power comes great responsibility! If your teams are careful, they should never have to overwrite history. And if you're synchronizing to Azure DevOps you can also add a security rule that prevents developers from overwriting history (you need the \\\"Force Push\\\" permission enabled to actually sync a repo that's had rewritten history). The point is that every source control system works best when the developers using it understand how it works and which conventions work. While you can't overwrite history with TFVC, you can still overwrite code and do other painful things. In my experience, very few teams have managed to actually overwrite history.\"]]],[1,\"h3\",[[0,[],0,\"Large Files\"]]],[1,\"p\",[[0,[],0,\"Git works best with repos that are small and that do not contain large files (or binaries). Every time you (or your build machines) clone the repo, they get the entire repo with all its history from Day 0. This is great for most situations, but can be frustrating if you have large files. Binary files are even worse since Git just can't optimize how they are stored. That's why \"],[0,[5],1,\"Git LFS\"],[0,[],0,\" was created - this lets you separate large files out of your repos and still have all the benefits of versioning and comparing. Also, if you're used to storing compiled binaries in your source repos - stop! Use \"],[0,[6],1,\"Azure Artifacts\"],[0,[],0,\" or some other package management tool to store binaries you have source code for. However, teams that have large files (like 3D models or other assets) you can use Git LFS to keep your code repo slim and trim.\"]]],[1,\"h3\",[[0,[],0,\"Large Repos\"]]],[1,\"p\",[[0,[],0,\"This used to be a blocker - but fortunately the engineers at Microsoft have been on a multi-year journey to convert all of Microsoft's source code to Git. The Windows team has a repo that's over 300GB in size, and they use Git for source control! How? They invented \"],[0,[7],1,\"Virtual File System (VFS) for Git\"],[0,[],0,\". VFS for Git is a client plugin that lets Git think it has the entire repo - but only fetches files from the upstream repo when a file is touched. This means you can clone your giant repo in a few seconds, and only when you touch files does Git fetch them down locally. In this way, the Windows team is able to use Git even for their giant repo.\"]]],[1,\"h3\",[[0,[],0,\"Git? GitHub?\"]]],[1,\"p\",[[0,[],0,\"There is a lot of confusion about Git vs GitHub. Git is the distributed source control system created by \"],[0,[8],1,\"Linus Torvalds in 2005 for the Linux kernel\"],[0,[],0,\". If you create a repo, you have a fully functioning Git repo on your local machine. However, to share that code, you need to pick a central place that developers can use to synchronize their repos - so if I want your changes, you'd push your changes to the central repo, and I'd pull them from there. We're still both working totally disconnected - but we're able to share our code via this push/pull model. GitHub is a cloud service for hosting these sorts of centralized repos - made famous mostly because it's free for open source projects (so you can host unlimited \"],[0,[1],1,\"public\"],[0,[],0,\" repos). You don't have to use GitHub to use Git - though it's pretty much the de-facto platform for open source code. They do offer private repos too - but if you're an enterprise, you may want to consider Azure Repos since you get unlimited \"],[0,[1],1,\"private\"],[0,[],0,\" repos on Azure Repos. You can also create Git repos in Team Foundation Server (TFS) from TFS 2015 to TFS 2019 (now renamed to Azure DevOps Server).\"]]],[1,\"h3\",[[0,[],0,\"Learning Curve\"]]],[1,\"p\",[[0,[],0,\"There is a learning curve - if you've never used source control before you're probably better off when learning Git. I've found that users of centralized source control (TFVC or SubVersion) battle initially to make the mental shift especially around branches and synchronizing. Once developers grok how Git branches work and get over the fact that they have to commit and then push, they have all the basics they need to be successful in Git. I've never once had a team convert to Git and then decide they want to switch back to centralized source control!\"]]],[1,\"h2\",[[0,[],0,\"Git and Microservices\"]]],[1,\"p\",[[0,[],0,\"Microservices are all the rage today - I won't go into details in this post about why - there's plenty of material available explaining why the industry is trending towards microservices. \"],[0,[9],1,\"Conway's Law\"],[0,[],0,\" tells us that the structure of our architecture is strongly influenced by the structure of our organization. The inverse, Conway's Inverse Maneuver, postulates that you can influence the structure of an organization by the way you architect your systems! If you've been battling to get to microservices within your organization, consider migrating to Git and decomposing your giant central repo into smaller Git repos as a method of influencing your architecture. Perhaps someone has already come up with a \\\"law\\\" for this - if not, I'll coin \\\"Colin's Repo Law\\\" which states that the way that you structure your source code will influence everything else in the DevOps lifecycle - builds, releases, testing and so on. So be sure to structure your source code and repos with the end goal in mind!\"]]],[1,\"h2\",[[0,[],0,\"Migrating to Git\"]]],[1,\"p\",[[0,[],0,\"Before we get to how to migrate, we have to address the issue of history. When teams migrate source control systems, they always ask about history. I push back a bit and inform teams that their old source control system isn't going away, so you don't lose history. For a small period of time, you may have two places to check for history - but most teams don't check history further out than the last month regularly. Some teams may have compliance or regulatory burdens, but these are generally the exception. Don't let the fear of \\\"losing history\\\" prevent you from modernizing your source control!\"]]],[1,\"h3\",[[0,[],0,\"Monorepo or Multirepo?\"]]],[1,\"p\",[[0,[],0,\"The other consideration we have to make is monorepo or multirepo? A monorepo is a Git repo that contains all the code for a system (or even organization). Generally, Git repos should be small - my rule of thumb is the repo boundary should be the deployment boundary. If you always deploy three services at the same time (because they're tightly coupled) you may want to put the code for all three services into a single repo. Then again, you may want to split them and start moving to decouple them - only you can decide what's going to be correct.\"]]],[1,\"p\",[[0,[],0,\"If you decide to split your repo into multiple Git repos, you're going to have to consider what to do with shared code. In TFVC, you have shared code in the same repo as the applications, so you generally just have project references. However, if you split out the app code and the common code, you are going to have to have a way to consume the \"],[0,[1],1,\"compiled\"],[0,[],0,\" shared code in the application code - that's a good use case for package management. Depending on your source control structure, the complexity of your system and your team culture, this may not be easy to do - in that case you may decide to just convert to a monorepo instead of a set of smaller repos.\"]]],[1,\"p\",[[0,[],0,\"The Azure DevOps team decided to use a monorepo even though their system is composed of around 40 microservices. They did this because the source code for Azure DevOps (which Microsoft hosts themselves as a SaaS offering) is the same source code that is used for the on-premises out-of-the-box Azure DevOps Server (previously TFS). Their CI builds are triggered off paths in the repo instead of triggering a build for every component every time the repo is changed. If you decide to use a monorepo, make sure your CI system is capable of doing this - and make sure you organize your source code into appropriate folders for managing your builds!\"]]],[1,\"h3\",[[0,[],0,\"Migrating\"]]],[1,\"p\",[[0,[],0,\"So how can you migrate to Git? There are at least three ways:\"]]],[3,\"ol\",[[[0,[],0,\"Tip migration\"]],[[0,[],0,\"Azure DevOps single branch import\"]],[[0,[10],1,\"Git-tfs\"],[0,[],0,\" import\"]]]],[1,\"h3\",[[0,[],0,\"Tip Migration\"]]],[1,\"p\",[[0,[],0,\"Most teams I work with wish they could reorganize their source control structure - typically the structure the team is using today was set up by a well-meaning developer a decade ago but it's not really optimal. Migrating to Git could be a good opportunity to restructure your repo. In this case, it probably doesn't make sense to migrate history anyway, since you're going to restructure the code (or break the code into multiple repos). The process is simple: create an empty Git repo (or multiple empty repos), then get-latest from TFS and copy/reorganize the code into the empty Git repos. Then just commit and push and you're there! Of course if you have shared code you need to create builds of the shared code to publish to a package feed and then consume those packages in downstream applications, but the Git part is really simple.\"]]],[1,\"h3\",[[0,[],0,\"Single Branch Import\"]]],[1,\"p\",[[0,[],0,\"If you're on TFVC and you're in Azure DevOps (aka VSTS) then you have the option of a simple single-branch import. Just click on \\\"Import repository\\\" from the Azure Repos top level drop-down menu to pop open the dialog. Then enter the path to the branch you're migrating (yes, you can only choose one branch) and if you want history or not (up to 180 days). Then add in a name for the repo and let 'er rip!\"]]],[10,0],[1,\"p\",[[0,[],0,\"There are some limitation here: a single branch and only 180 days of history. However, if you only care about one branch and you're already in Azure DevOps, then this is a no-brainer migration method.\"]]],[1,\"h3\",[[0,[],0,\"Git-tfs\"]]],[1,\"p\",[[0,[],0,\"What if you need to migrate more than a single branch and retain branch relationships? Or you're going to ignore my advice and insist on dragging all your history with you? In that case, you're going to have to use \"],[0,[10],1,\"Git-tfs\"],[0,[],0,\". This is an open-source project that is build to synchronize Git and TFVC repos. But you can use it to do a once-off migration using\"]]],[10,1],[1,\"p\",[[0,[],0,\". Git-tfs has the advantage that it can migrate multiple branches and will preserve the relationships so that you can merge branches in Git after you migrate. Be warned that it can take a while to do this conversion - especially for large repos or repos with long history. You can easily dry-run the migration locally, iron out any issues and then do it for real. There's lots of flexibility with this tool, so I highly recommend it.\"]]],[1,\"p\",[[0,[],0,\"If you're on Subversion, then you can use \"],[0,[11],1,\"Git svn\"],[0,[],0,\" to import your Subversion repo in a similar manner to using Git-tfs.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Modernizing source control to Git has high business value - most notably the ability to effectively isolate code changes, minimize merge debt and integrate unobtrusive code reviews which can improve quality. Add to this the broad user-base for Git and you have a tool that is both powerful and pervasive. With Azure DevOps, you can also add \\\"enterprise\\\" features like branch policies, easily manage large binaries and even large repos - so there's really no reason not to migrate. Migrating to Git will cause some short-term pain in terms of learning curve, but the long term benefits are well worth it.\"]]],[1,\"p\",[[0,[],0,\"Happy source controlling!\"]]]]}","published_at":1545207735000,"status":"published","published_by":1},{"id":"fc147461-3e0f-44f6-bf5b-3eff0aa837f0","title":"Monitoring Check-in Policy Overrides","slug":"monitoring-check-in-policy-overrides","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TZAzVMbFlkI/AAAAAAAAAQU/BltKpBs28gU/s1600-h/image%5B2%5D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TZAzWbl2ASI/AAAAAAAAAQY/Y4s1g_mtKPc/image_thumb.png?imgmax=800\\\" width=\\\"221\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">DimChangeset</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">PolicyOverrideComment</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">PolicyOverrideComment</font>\"}],[\"code\",{\"code\":\"<span style=\\\"color: blue\\\">SELECT<br>    </span><span style=\\\"color: teal\\\">[ChangesetSK]</span><span style=\\\"color: gray\\\">,<br>    </span><span style=\\\"color: teal\\\">[ChangesetID]</span><span style=\\\"color: gray\\\">,<br>    </span><span style=\\\"color: teal\\\">[ChangesetTitle]</span><span style=\\\"color: gray\\\">,<br>    </span><span style=\\\"color: teal\\\">[PolicyOverrideComment]</span><span style=\\\"color: gray\\\">,<br>    </span><span style=\\\"color: teal\\\">[LastUpdatedDateTime]</span><span style=\\\"color: gray\\\">,<br>    </span><span style=\\\"color: teal\\\">[TeamProjectCollectionSK]</span><span style=\\\"color: gray\\\">,<br>    </span><span style=\\\"color: teal\\\">[CheckedInBySK]<br></span><span style=\\\"color: blue\\\">FROM<br>    </span><span style=\\\"color: teal\\\">[Tfs_Warehouse]</span><span style=\\\"color: gray\\\">.</span><span style=\\\"color: teal\\\">[dbo]</span><span style=\\\"color: gray\\\">.</span><span style=\\\"color: teal\\\">[DimChangeset]<br></span><span style=\\\"color: blue\\\">WHERE<br>    </span><span style=\\\"color: teal\\\">[PolicyOverrideComment] </span><span style=\\\"color: gray\\\">IS NOT NULL<br></span>\",\"language\":\"\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://www.amazon.com/Professional-Team-Foundation-Server-ebook/dp/B004S82RRE/ref=sr_1_1?ie=UTF8&m=AG56TWVU5XWC2&s=books&qid=1301295041&sr=8-1\"]],[\"a\",[\"href\",\"http://www.edsquared.com/\"]],[\"a\",[\"href\",\"http://www.notionsolutions.com/\"]],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/c255a1e4-04ba-4f68-8f4e-cd473d6b971f/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’m often asked by customers why there is no way to prevent check-in policy overrides in TFS. Usually I respond along the lines of, “Well, it should be a really rare occurrence (otherwise why have the policy?) and besides, every action against TFS is tracked, so you can monitor overrides and beat ‘chat nicely’ to the developer who is overriding the policies”. Which is all well and fine, but exactly \"],[0,[0],1,\"how do you monitor policy overrides\"],[0,[],0,\"?\"]]],[1,\"p\",[[0,[],0,\"This weekend I checked Amazon to see if I could find \"],[0,[1],1,\"Professional Team Foundation Server 2010\"],[0,[],0,\" for my Kindle – and to my surprise, there it was! \"],[0,[2],1,\"Ed Blankenship\"],[0,[],0,\", one of the authors, is a colleague at \"],[0,[3],1,\"Notion Solutions\"],[0,[],0,\" and is an absolute genius on all things TFS. The other authors are, of course, TFS legends too! I instantly purchased the book and while going through it, found a section in Chapter 7 that speaks about monitoring check-in policy overrides.\"]]],[1,\"p\",[[0,[],0,\"There are 2 main ways of doing so – via email alerts and via the warehouse.\"]]],[1,\"h2\",[[0,[],0,\"Email Notification\"]]],[1,\"p\",[[0,[],0,\"Install the latest version of the \"],[0,[4],1,\"Team Foundation Power Tools\"],[0,[],0,\" – anyone using TFS should always have this installed! One of the features that gets installed with these tools is a richer notifications manger – the Alerts Explorer.\"]]],[1,\"p\",[[0,[],0,\"In VS, go to Team->Alerts Explorer and add a “Check-In to specific folder with policy overriden” alert (under the Check-in alerts section).\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now you’ll get an email whenever someone does something nefarious and overrides the check-in policy you’ve so meticulously planned and put in place…\"]]],[1,\"h2\",[[0,[],0,\"Monitoring via the Warehouse\"]]],[1,\"p\",[[0,[],0,\"The second method of monitoring check-in overrides is to query the warehouse. In the warehouse database (called Tfs_Warehouse by default) there is a table called\"]]],[10,1],[1,\"p\",[[0,[],0,\". One of the columns on this table is\"]]],[10,2],[1,\"p\",[[0,[],0,\", which is null whenever there is no override and which contains the policy override comment (which is mandatory when overriding) otherwise. So you can easily craft a query (and turn it into a report) that looks for check-ins with a\"]]],[10,3],[1,\"p\",[[0,[],0,\". Here’s the T-SQL:\"]]],[10,4],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Happy monitoring!\"]]]]}","published_at":1301328360000,"status":"published","published_by":1},{"id":"f6c6c4cf-b801-4504-81cf-bdf381e616ca","title":"Monitoring Web Applications – Continuous IntelliTrace","slug":"monitoring-web-applications--continuous-intellitrace","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://www.microsoft.com/visualstudio/eng/2013-downloads\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/vstudio/hh398365.aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/intellitrace-tips-and-tricks.html\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/visualstudioalm/archive/2013/09/20/introducing-microsoft-monitoring-agent.aspx\"]],[\"a\",[\"href\",\"http://www.microsoft.com/en-us/download/details.aspx?id=40316\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/visualstudioalm/archive/2013/09/20/performance-details-in-intellitrace.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"If you have \"],[0,[0],1,\"Visual Studio Ultimate\"],[0,[],0,\" and are not using \"],[0,[1],1,\"IntelliTrace in production\"],[0,[],0,\", you should be drawn and quartered. This is arguably the best feature of Visual Studio Ultimate, and in my opinion this feature alone justifies the pricing (never mind Web Performance and Load testing, Code Maps, Code Lens, UML diagrams and Layer diagrams).\"]]],[1,\"p\",[[0,[],0,\"The standalone IntelliTrace collector is amazing, and will run anywhere. It’s especially useful for diagnosing problems in Web Applications running in IIS. (For a recap on how to use this tool, see my series starting \"],[0,[2],1,\"here\"],[0,[],0,\").\"]]],[1,\"p\",[[0,[],0,\"Often when I talk about collecting IntelliTrace logs, people invariably ask, “Why can’t I leave the collector running all the time?”. This depends on how resilient you are to the performance impact of the collector, as well as what “mode” you’re using. If you using the verbose “Call and Events” mode, you’re going to see a performance knock. If you use the “Events Only” mode, you may see less impact (of course the logs won’t be as rich) but even this can be too much degradation.\"]]],[1,\"p\",[[0,[],0,\"About a week ago, Microsoft released the \"],[0,[3],1,\"Microsoft Monitoring Agent\"],[0,[],0,\". This agent can be run “standalone” or be connected to System Center Operations Manager (SCOM) 2012 R2. I’m going to show you how you can run this agent in standalone mode in this post.\"]]],[1,\"h2\",[[0,[],0,\"Why use the Monitoring Agent?\"]]],[1,\"p\",[[0,[],0,\"Ah, we get to the crux of the matter – why use this agent instead of the IntelliTrace standalone collector? The answer is two-fold:\"]]],[3,\"ol\",[[[0,[],0,\"You get performance monitoring “for free” when you use the Monitoring Agent\"]],[[0,[],0,\"You can leave the agent on – permanently\"]],[[0,[],0,\"You can target a specific web application (instead of a whole application pool)\"]]]],[1,\"p\",[[0,[],0,\"Unfortunately (for some) the agent is something you install – not like the IntelliTrace standalone collector that is just xcopy-able. If installing agents in your production environments is not a challenge, then you should be switching to the Monitoring Agent.\"]]],[1,\"h2\",[[0,[],0,\"Running Continuously\"]]],[1,\"p\",[[0,[],0,\"There’s a caveat to running the monitor continuously (isn’t there always?). You really only want to do this in one of the three monitoring modes available – “monitor” mode. (The other two are “trace” and “custom”).\"]]],[1,\"p\",[[0,[],0,\"The IntelliTrace standalone collector comes with two xml configuration files out-the-box: events only (default) and events and call information (trace). “Trace” mode will give you the same as the “events and call information” mode of IntelliTrace standalone collector – it’s verbose, but it’ll knock your performance. You’ll have to be selective about when you run this mode. The “custom” mode let’s you run the collector using a custom IntelliTrace xml file, so you can tailor the logging just so.\"]]],[1,\"p\",[[0,[],0,\"The Monitoring Agent’s “monitor” mode is like the events only (default) setting of the IntelliTrace collector, but even more lightweight. Instead of collecting all events, it only collects exceptions that bubble up to the global exception handler and “slow” events (events that take the server longer than 5 seconds to respond). Also, it’ll only record 60 triggers of each event daily. And since you can specify which web site to monitor, you don’t have to impact your whole application pool.\"]]],[1,\"p\",[[0,[],0,\"The upshot of this is that you can turn monitoring on – and leave it on. Then you can “checkpoint” the log at any time – either when there’s a problem or proactively whenever you want to.\"]]],[1,\"h2\",[[0,[],0,\"Running the Monitoring Agent\"]]],[1,\"p\",[[0,[],0,\"I downloaded and installed the Monitoring Agent from the \"],[0,[4],1,\"download site\"],[0,[],0,\". I ran the installer, and configured it to run standalone (i.e. skipped hooking it up to SCOM). Thereafter I opened a PowerShell command prompt, but the module isn’t imported for some reason. No worries – just run this command:\"]]],[1,\"p\",[[0,[],0,\"Import-Module installPath\\\\Agent\\\\Microsoft.EnterpriseManagement.Modules.PowerShell.dll\"]]],[1,\"p\",[[0,[],0,\"From there, you can follow Larry Guger’s post to see how to \"],[0,[3],1,\"run the agent\"],[0,[],0,\", checkpoint the log and review the results (especially \"],[0,[5],1,\"performance events\"],[0,[],0,\").\"]]],[1,\"p\",[[0,[],0,\"Happy monitoring!\"]]]]}","published_at":1380312060000,"status":"published","published_by":1},{"id":"62b9be8e-8df7-49fb-9f60-d644c2be8e9c","title":"More DSC Release Management Goodness: Readying a Webserver for Deployment","slug":"more-dsc-release-management-goodness-readying-a-webserver-for-deployment","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Unable to find the mof file. Make sure you are generating the MOF in the DSC script in the current directory.</font>\"}],[\"code\",{\"code\":\"Configuration WebServerPreReqs\\n{\\n    Node $AllNodes.where{ $_.Role -eq \\\"WebServer\\\" }.NodeName\\n    {\\n        # tell the node to reboot if necessary\\n        LocalConfigurationManager\\n        {\\n            RebootNodeIfNeeded = $true\\n        }\\n\\n        WindowsFeature WebServerRole\\n        {\\n            Name = \\\"Web-Server\\\"\\n            Ensure = \\\"Present\\\"\\n        }\\n    }\\n}\\n\\nWebServerPreReqs -ConfigurationData $configData\\n\\n# invoke Set-DscLocalConfigurationManager directly since RM doesn't yet support this\\nSet-DscLocalConfigurationManager -Path .\\\\WebServerPreReqs -Verbose\\n\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/15596a48-9aab-418d-b421-c0119d9c9564.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1ec52f6d-cf6f-44ff-badf-db448d8537dc.png\\\" width=\\\"644\\\" height=\\\"338\\\"></a>\"}],[\"html\",{\"html\":\"<font color=\\\"#404b55\\\">Now on the Release Template, right-click the Components node in the toolbox and add in the script component, then drag it onto the designer inside your server block (which you’ll also need to drag on from your list of servers). Then just set the paths and username and password correctly and you’re good to go.</font>\"}],[\"image\",{\"src\":\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9b0ec259-0987-4ea3-b372-224af65cf19a.png\",\"alt\":\"image\",\"title\":\"image\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">[FABFIBERSERVER]: LCM:&nbsp; [ Start&nbsp; Set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br>[FABFIBERSERVER]: LCM:&nbsp; [ Start&nbsp; Resource ]&nbsp; [MSFT_DSCMetaConfiguration]<br>[FABFIBERSERVER]: LCM:&nbsp; [ Start&nbsp; Set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]&nbsp; [MSFT_DSCMetaConfiguration]<br>[FABFIBERSERVER]: LCM:&nbsp; [ End&nbsp;&nbsp;&nbsp; Set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]&nbsp; [MSFT_DSCMetaConfiguration]&nbsp; in 0.0620 seconds.<br>[FABFIBERSERVER]: LCM:&nbsp; [ End&nbsp;&nbsp;&nbsp; Resource ]&nbsp; [MSFT_DSCMetaConfiguration]<br>[FABFIBERSERVER]: LCM:&nbsp; [ End&nbsp;&nbsp;&nbsp; Set&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br>Operation 'Invoke CimMethod' complete.<br>Set-DscLocalConfigurationManager finished in 0.207 seconds.</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">An LCM method call arrived from computer FABFIBERSERVER with user sid S-1-5-21-3349151495-1443539541-1735948571-1106.<br>[FABFIBERSERVER]: LCM:&nbsp; [ Start&nbsp; Resource ]&nbsp; [[WindowsFeature]WebServerRole]<br>[FABFIBERSERVER]: LCM:&nbsp; [ Start&nbsp; Test&nbsp;&nbsp;&nbsp;&nbsp; ]&nbsp; [[WindowsFeature]WebServerRole]<br>[FABFIBERSERVER]:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [[WindowsFeature]WebServerRole] The operation 'Get-WindowsFeature' started: Web-Server<br>[FABFIBERSERVER]:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [[WindowsFeature]WebServerRole] The operation 'Get-WindowsFeature' succeeded: Web-Server<br>[FABFIBERSERVER]: LCM:&nbsp; [ End&nbsp;&nbsp;&nbsp; Test&nbsp;&nbsp;&nbsp;&nbsp; ]&nbsp; [[WindowsFeature]WebServerRole]&nbsp; in 0.8910 seconds.</font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/powershell-dsc-remotely-configuring-a-node-to-rebootnodeifneeded\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/using-powershell-dsc-in-release-management-the-hidden-manual\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In my previous couple of posts (\"],[0,[0],1,\"PowerShell DSC: Configuring a Remote Node to “RebootIfNeeded”\"],[0,[],0,\" and \"],[0,[1],1,\"Using PowerShell DSC in Release Management: The Hidden Manual\"],[0,[],0,\") I started to experiment with Release Management’s new PowerShell DSC capabilities. I’ve been getting some great help from Bishal Prasad, one of the developers on Release Management – without his help I’d never have gotten this far!\"]]],[1,\"h2\",[[0,[],0,\"Meta Mofs\"]]],[1,\"p\",[[0,[],0,\"To configure a node (the DSC parlance for a machine) you need to create a DSC script that configures the LocalConfigurationManager. When I first saw this, I thought this was a great feature – unfortunately, when you invoke the config script, it doesn’t produce a mof file (like “normal” DSC scripts that use resources like File and WindowsFeature) so you can’t use Start-DscConfiguration to push it to remote servers. You need to invoke Set-DscLocalConfigurationManager. The reason is that a config that targets LocalConfigurationManager produces a meta.mof instead of a mof file.\"]]],[1,\"p\",[[0,[],0,\"If you try to run a PowerShell script in Release Management that produces a meta.mof, you’ll see a failure like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Of course this is because Release Management expects a mof file, and if you’re just producing a meta.mof file, the invocation will fail.\"]]],[1,\"p\",[[0,[],0,\"We may see support for meta.mofs in future versions of Release Management (hopefully sooner than later) but until then the workaround is to make sure that you include the LocalConfigurationManager settings inside a config that produces a mof file. Then you include two commands at the bottom of the script: first the command to “compile” the configuration – this produces a mof file as well as a meta.mof file. Then you call Set-DscLocalConfigurationManager explicitly to push the meta.mof and let Release Management handle the mof file. Here’s an example that configures a node to reboot if needed and ensures that the Webserver role is present:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can see that there is a LocalConfigurationManager setting (line 6). Line 19 “compiles” the config – given a list of nodes in $configData that includes just a single node (say fabfiberserver) you’ll see fabfiberserver.mof and fabfiberserver.meta.mof files in the current directory after calling the script. Since RM itself takes care of pushing the mof file, we need to explicitly call Set-DscLocalConfigurationManager in order to push the meta.mof file (line 22).\"]]],[1,\"p\",[[0,[],0,\"Now you can use this script just like you would any other DSC script in RM.\"]]],[1,\"h2\",[[0,[],0,\"Setting up the Release\"]]],[1,\"p\",[[0,[],0,\"Utilizing this script in a Release is easy – create a component that has “Source” set to your build output folder (or a network share for deploying bins that are not built in TFS build) and set the deployment tool to “Run PowerShell on Standard Environment”. I’ve called my component “Run DSC Script”.\"]]],[10,2],[10,3],[10,4],[1,\"p\",[[0,[],0,\"I’ve saved this script as “WebServerPreReqs.ps1” in the Scripts folder of my build output folder – you can see the path there in the ScriptPath parameter. My configData script is also in the scripts folder (remember the ScriptPath and ConfigurationFilePath are relative to the source path that you configure in the component). Now you can start a release!\"]]],[1,\"h3\",[[0,[],0,\"Inspecting the Logs\"]]],[1,\"p\",[[0,[],0,\"Once the release has completed, you can open the tool logs for the “Run DSC Script” component and you’ll see two “sets” of entrties. Both sets are prefixed with [SERVERNAME], indicating which node the logs pertain to. Here we can see a snippet of the Set-DscLocalConfigurationManager invocation logs (explicitly deploying the meta.mof):\"]]],[10,5],[1,\"p\",[[0,[],0,\"Just after these entries, you’ll see a second set of entries – this time for the remainder of the DSC invocation that RM initiates (which deploys the mof):\"]]],[10,6],[1,\"p\",[[0,[],0,\"In the next post I’ll look at using DSC to configure the rest of my webserver features as well as create a script for installing and configuring SQL Server. Then we’ll be in a good position to configure deployment of a web application (and its database) onto an environment that we know has everything it needs to run the application.\"]]],[1,\"p\",[[0,[],0,\"In the meantime – happy releasing!\"]]]]}","published_at":1405005728000,"status":"published","published_by":1},{"id":"3e48503a-f2cf-4afe-8cf9-c561539bd393","title":"More on Fakes – the beta has issues","slug":"more-on-fakes--the-beta-has-issues","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"[TestMethod]<br>public void BaseMethodWontWorkInBeta()<br>{<br>    using (ShimsContext.Create())<br>    {<br>        var shimWorkItemStore = new ShimWorkItemStore();<br>        var shimTfsTeamProjectCollection = new ShimTfsTeamProjectCollection();<br>        var shimTfsConnection = new ShimTfsConnection(shimTfsTeamProjectCollection);<br>        shimTfsConnection.GetServiceOf1<workitemstore>(() =&gt; shimWorkItemStore);<br><br>        var workItemStore = shimTfsTeamProjectCollection.Instance.GetService<workitemstore>();<br><br>        Assert.AreSame(shimWorkItemStore.Instance, workItemStore);<br>    }<br>}</workitemstore></workitemstore>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"[TestMethod]<br>public void BindWontWorkBecauseOfObservableCollectionInBeta()<br>{<br>    using (ShimsContext.Create())<br>    {<br>        var ss = new StubISharedStep();<br>        var sharedStepReference = new StubISharedStepReference<br>        {<br>            FindSharedStep = () =&gt; ss<br>        };<br><br>        var actions = new List<itestaction><br>        {<br>            sharedStepReference<br>        };<br><br>        var fakeTestActions = new ShimTestActionCollection();<br>        fakeTestActions.Bind((IList<itestaction>)actions);<br><br>        Assert.AreEqual(1, fakeTestActions.Instance.Count);<br><br>        var ssr = (ISharedStepReference)(fakeTestActions.Instance[0]);<br>        Assert.AreSame(sharedStepReference, ssr);<br><br>        Assert.AreSame(ss, ssr.FindSharedStep());<br>    }<br>}</itestaction></itestaction>\",\"language\":\"csharp; ruler\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.peterprovost.org/blog/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/03/using-fakes-framework-to-test-tfs-api.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Thanks to \"],[0,[0],1,\"Peter Provost\"],[0,[],0,\" for helping answer a couple of questions I had about Fakes – you can look at some of my code in my previous posts about \"],[0,[1],1,\"using Fakes with the TFS API\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"There were two scenarios that I hit snags with. The first was faking methods in a class that are actually defined on the class’s base class. The second was faking an ObservableCollection.\"]]],[1,\"h2\",[[0,[],0,\"Faking Base Class Methods\"]]],[1,\"p\",[[0,[],0,\"Consider this code:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"The exercise here is to try to fake the TfsTeamProjectCollection.GetService method. However, we can’t do it directly on the TfsTeamProjectCollection object, since the method is defined in its base class, TfsConnection.\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"In line 8, we’re using the ShimTfsConnection constructor that takes in an instance of its child class (the TfsTeamProjectCollection) in order to override the method in the child class. This should work, but Peter told me that this doesn’t work in the Beta – it’ll work when the next release of fakes comes out. The workaround is to use the ShimTfsConnection.AllInstances class to override the GetService method (see my earlier posts where I show how to do this).\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"h2\",[[0,[],0,\"Faking ObservableCollection\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"Let’s look at some more code:\"]]],[10,1],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"The point of this code is to try to fake a TestActionCollection, which is an ObservableCollection. I had successfully managed to fake other collections that were not ObservableCollections (like a WorkItemCollection), and to do that I used the Bind() method of the Shim to bind the fake collection to a list of items. However, I couldn’t get the code to work on the ObservableCollection. (In the above code, I get a NullReferenceException on line 18 when calling the Bind() method.\"]]],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"Peter confirmed that the reason for this is twofold: one, I hadn’t added Fakes for System, which is where the ObservableCollection class resides. This is a performance optimization that the Fakes creation employs so that they don’t create fakes for classes that you’ll never use.\"]]],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"Secondly, and rather unfortunately, even adding that Fake doesn’t work in the Beta. The Fakes creation uses a white-list to generate fakes in the System namespace, and the ObservableCollection isn’t on the white-list in the Beta. In the next release, the System white-list will be customizable, so I am looking forward to getting my grubby paws on that!\"]]],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Unfortunately, this scenario has no work-around in the Beta, so if you’re trying to Fake an ObservableCollection, you’ll have to wait for the next release of Fakes.\"]]],[1,\"p\",[[1,[],0,8]]],[1,\"p\",[[0,[],0,\"I really like the Fakes framework and what it can do for testing, so I am looking forward to seeing what else comes out of the woodwork.\"]]],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"Happy faking!\"]]]]}","published_at":1333564440000,"status":"published","published_by":1},{"id":"e8d39da4-2299-4384-af17-945f8666474c","title":"Moving to Northwest Cadence","slug":"moving-to-northwest-cadence","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/33627038-56b5-4b36-b78e-b436ab752571.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8bab2469-3cb3-4506-b820-9ab1a8060532.png\\\" width=\\\"353\\\" height=\\\"134\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.imaginet.com\"]],[\"a\",[\"href\",\"http://www.nwcadence.com/\"]],[\"a\",[\"href\",\"http://www.ru.ac.za/\"]],[\"em\"],[\"a\",[\"href\",\"http://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture\"]],[\"a\",[\"href\",\"http://www.realpeople.co.za/\"]]],\"sections\":[[10,0],[1,\"p\",[[0,[],0,\"That’s right – I’m moving from \"],[0,[0],1,\"Imaginet\"],[0,[],0,\" to \"],[0,[1],1,\"Northwest Cadence\"],[0,[],0,\" (NWC) at the end of this month. That’s the high-level version – read no further if you don’t need back-story! A huge thanks to all at Imaginet for an awesome 4 years. It was a pleasure working with all of you.\"]]],[1,\"h2\",[[0,[],0,\"Getting into ALM\"]]],[1,\"p\",[[0,[],0,\"I’m often asked how I got into ALM. Well, I studied Computer Science up to Masters at \"],[0,[2],1,\"Rhodes University\"],[0,[],0,\" (which was an awesome 6 years!). When I left in 2002 I knew I could program – and one of my favorite courses of all time by Prof. Terry was building our own parser and compiler (using Pascal) so I knew I had enough \"],[0,[3],1,\"meta-knowledge\"],[0,[],0,\" to learn any language. My first job was for a small company (Systems Fusion) that made ISP software using C++ and \"],[0,[4],1,\"CORBA\"],[0,[],0,\" on Linux. Those were the bad old days – CVS for source control and builds that took about 3 hours using make files. Blegh!\"]]],[1,\"p\",[[0,[],0,\"During that time I got married, and my wife and I didn’t enjoy the Johannesburg lifestyle. So I called up an old study mate who happened to be working for a financial services company in East London. In Oct 2004, we moved to East London and I joined \"],[0,[5],1,\"Real People\"],[0,[],0,\" (a financial services company). We had around 20 developers in the MS stack – SQL server, webservices and ASP.NET websites. The team were very cowboy, using zip files for source control and deploying to Production to test. Even though I didn’t know what ALM was at that time, I knew this wasn’t a sustainable way to do development.\"]]],[1,\"p\",[[0,[],0,\"After 2 or 3 months, I got my grubby paws on Team Foundation Server 2005 beta 2. I (eventually) got it installed and configured and we adopted TFS as our primary ALM tool. Our processes were still chaotic, but at least we were starting to utilize good source control, branching and even automated builds. Over the next 5 years, I became the architect and ALM-guy for the team – when I left in Oct 2010 we had around 60 developers, 15 team projects, 250 build definitions and I’d done countless customizations to work items, templates, reports and builds. I’d also learned (intuitively) a lot about process – the good, the bad and the ugly! We did a lot of things right in those days – but we also had lots of room for improvement.\"]]],[1,\"p\",[[0,[],0,\"During my time as the “accidental admin” for TFS, I spent a lot of time on forums and blogs as I tried to figure out how to do stuff in TFS. Sadly, it appeared that there were very few people in South Africa that were doing any ALM using TFS. Or if they were, they certainly weren’t publishing any content!\"]]],[1,\"h2\",[[0,[],0,\"Notion Solutions\"]]],[1,\"p\",[[0,[],0,\"In 2009 I attended my first TechEd Africa. There I listened to an ALM talk by legend Chris Menegay, who explained that he ran a company called Notion Solutions that did ALM consulting in the US. I loved the idea and thought that there was probably some scope for ALM consulting in SA. I saw Chris again at TechEd Africa in 2010 and this time asked if he did any work in South Africa. We hit if off and Chris offered to hire me or help me start ALM Consulting in SA. Eventually we started Notion South Africa in Oct of 2010, and I officially became an ALM consultant. At around that time, Notion Solutions became part of Imaginet.\"]]],[1,\"h2\",[[0,[],0,\"Seattle\"]]],[1,\"p\",[[0,[],0,\"When I was studying my Masters, I was sponsored by a company in Seattle – a startup that was focusing on FireWire technologies. During June of 2000, the company flew me to Seattle for a 3 week working holiday. It was my fist time flying (I was 22 at the time) and my first time to the US. I instantly fell in love with Seattle – I got to see some of the city, meet a few people and even go camping while I was there. I didn’t even mind the rain! In fact, I loved it so much I decided to move there.\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, the startup company was unable to offer me a position at the end of 2001 when I graduated – they’d gone under in the dot bomb. And so my dream of moving to Seattle went cold.\"]]],[1,\"h2\",[[0,[],0,\"MVP Award\"]]],[1,\"p\",[[0,[],0,\"Zoom forward to 2011 – I was awarded my MVP award in ALM. In Feb 2012, I got to attend the MVP Summit in Bellevue, Seattle. It was great being back in one of my favorite cities of all time! I attended the summit in Feb 2013, and then the summit was changed to Nov, so there was another summit in Nov 2013. Each time I visited Seattle, I was more and more keen to live and work there. Also, one of my best mates from Rhodes moved there in 2006 or so to work for Adobe – even more incentive to move there!\"]]],[1,\"p\",[[0,[],0,\"At MVP Summit in Feb, 2012, I shared a room with Chris Menegay – an experience in itself! When we were chatting, I mentioned that I’d love to live in Seattle. Chris said I should chat to Steven Borg, cofounder and strategist of Northwest Cadence – an ALM company based in Seattle. I didn’t approach Steve then – I wasn’t ready for a move yet. However, at Summit in Feb 2013, I made a tongue-in-cheek comment to Martin Hinshelwood: “If I moved to Seattle, would NWC hire me?”. He immediately suggested that I chat to Steve (I didn’t know that Martin was in the process of moving back to Scotland), which I did. Steve and I hit it off from the first conversation we had, and over the next couple of months we got to know each other and we decided I’d be a fit for NWC (and NWC would be a fit for me!). We’re now processing legal paperwork to get me over to start working – and while we wait, I’m going to be working remotely for NWC from Cape Town (I moved from East London to Cape Town earlier this month).\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"This is going to be an exciting transition for me and my family – but I must mention that I learned a lot during my time with Imaginet. My colleagues were an amazing bunch to work with – a lot of the “old guard” (the original Notion Solutions crowd that I met when I joined) have moved on – most of them to Microsoft – but I still have frequent contact with most of them. Big ups to Steve St. Jean, Ed Blankenship, Donovan Brown, Abel Wang, Dave McKinstry and others! I’ll never forget how confident I was on my first ALM gig – because I knew that if I got stuck I could always reach out to some of the most knowledgeable ALM Consultants on the planet simply by mailing the internal distribution list we had called “NotionTech”. Later it changed to ALMTech, but it was still the same level of awesomeness!\"]]],[1,\"p\",[[0,[],0,\"I’ll still be involved in ALM (though I won’t be working much in SA anymore) and I’ll continue to blog here, so you’ll still see plenty of content from me.\"]]],[1,\"p\",[[0,[],0,\"Blessings!\"]]]]}","published_at":1413977468000,"status":"published","published_by":1},{"id":"98953978-7539-41d5-9f44-d8057976473a","title":"MVP Status Renewed","slug":"mvp-status-renewed","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-Rp7fkYULQCY/UGm0x52eNRI/AAAAAAAAAdM/ifciXfypMy8/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-Br_2VtFCRtM/UGm0yyP7qRI/AAAAAAAAAdU/joJTLXdfhj4/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"598\\\" height=\\\"306\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://mvp.microsoft.com/en-US/findanmvp/Pages/profile.aspx?MVPID=7ef0d661-a068-454d-a5f4-68ad1e3a71d9\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I am pleased to announce that my \"],[0,[0],1,\"MVP status\"],[0,[],0,\" was renewed! A large portion of that is due to this blog and all the hits I’ve gotten – so if you keep reading, I’ll keep posting!\"]]],[10,0],[1,\"p\",[[0,[],0,\"Thanks also to Microsoft – I have really enjoyed being an MVP so far – having access to the other MVPs and the VS and TFS product groups has made my job much more enjoyable, and made me look good too!\"]]],[1,\"p\",[[0,[],0,\"I’ll be here all year, and remember to tip your MVPs…\"]]]]}","published_at":1349137200000,"status":"published","published_by":1},{"id":"fbc62e33-951c-4322-bccd-0b1d7835b1b7","title":"My First VSO Extension: Retry Build","slug":"my-first-vso-extension-retry-build","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"https://github.com/colindembovsky/vso-colinsalmcorner-extensions/blob/master/src/retry-build/images/retry-build-screenshot.png?raw=true\",\"alt\":\"retry-build-screenshot.png\",\"title\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">’define’ is not defined</font>\"}],[\"code\",{\"code\":\"import {BuildHttpClient} from \\\"TFS/Build/RestClient\\\";\\nimport {getCollectionClient} from \\\"VSS/Service\\\";\\nvar retryBuildMenu = (function () {\\n    \\\"use strict\\\";\\n\\n    return &lt;IContributedMenuSource&gt; {\\n        execute: (actionContext: any) =&gt; {\\n            var vsoContext = VSS.getWebContext();\\n            var buildClient = getCollectionClient(BuildHttpClient);\\n\\n            VSS.ready(() =&gt; {\\n                // get the build\\n                buildClient.getBuild(actionContext.id, vsoContext.project.name).then(build =&gt; {\\n                    // and queue it again\\n                    buildClient.queueBuild(build, build.definition.project.id).then(newBuild =&gt; {\\n                        // and navigate to the build summary page\\n                        // e.g. https://myproject.visualstudio.com/DefaultCollection/someproject/_BuildvNext#_a=summary&amp;buildId=1347\\n                        var buildPageUrl = `${vsoContext.host.uri}/${vsoContext.project.name}/_BuildvNext#_a=summary&amp;buildId=${newBuild.id}`;\\n                        window.parent.location.href = buildPageUrl;\\n                    });\\n                });\\n            });\\n        }\\n    };\\n}());\\n\\nVSS.register(\\\"retryBuildMenu\\\", retryBuildMenu);\\n\",\"language\":\"js;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">’define’ is not defined</font>\"}],[\"code\",{\"code\":\"&lt;!DOCTYPE html&gt;\\n&lt;html lang=\\\"en\\\"&gt;\\n&lt;head&gt;\\n    &lt;meta charset=\\\"UTF-8\\\"&gt;\\n    &lt;title&gt;Delete Branch&lt;/title&gt;\\n&lt;/head&gt;\\n&lt;body&gt;\\n    &lt;script src=\\\"sdk/scripts/VSS.SDK.js\\\"&gt;&lt;/script&gt;\\n    &lt;script src=\\\"scripts/main.js\\\"&gt;&lt;/script&gt;\\n    &lt;script&gt;\\n        VSS.init({ setupModuleLoader: true });\\n    &lt;/script&gt;\\n&lt;/body&gt;\\n&lt;/html&gt;\\n\\n\",\"language\":\"xml; highlight\"}],[\"code\",{\"code\":\"&lt;!DOCTYPE html&gt;\\n&lt;html lang=\\\"en\\\"&gt;\\n&lt;head&gt;\\n    &lt;meta charset=\\\"UTF-8\\\"&gt;\\n    &lt;title&gt;Retry Build&lt;/title&gt;\\n&lt;/head&gt;\\n&lt;body&gt;\\n    &lt;script src=\\\"sdk/scripts/VSS.SDK.js\\\"&gt;&lt;/script&gt;\\n    &lt;p&gt;User will never see this&lt;/p&gt;\\n    &lt;script type=\\\"text/javascript\\\"&gt;\\n        // Initialize the VSS sdk\\n        VSS.init({\\n            setupModuleLoader: true,\\n        });\\n\\n        // Wait for the SDK to be initialized, then require the script\\n        VSS.ready(function () {\\n            require([\\\"scripts/main\\\"], function (main) { });\\n        });\\n    &lt;/script&gt;\\n&lt;/body&gt;\\n&lt;/html&gt;\\n\\n\",\"language\":\"xml;\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/integrate/extensions/overview\"]],[\"a\",[\"href\",\"http://pascoal.net/task-board-enhancer/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vso-colinsalmcorner-extensions\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-should-switch-to-build-vnext\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vso-extension-samples\"]],[\"a\",[\"href\",\"http://www.vsipprogram.com/join\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Visual Studio Online (VSO) and TFS 2015 keep getting better and better. One of the coolest features to surface recently is the ability to add (supported) \"],[0,[0],1,\"extensions to VSO\"],[0,[],0,\". My good friend Tiago Pascoal managed to \"],[0,[1],1,\"hack VSO to add extensions\"],[0,[],0,\" a while ago, but it was achieved via browser extensions, not through a supported VSO extensibility framework. Now Tiago can add his extensions in an official manner!\"]]],[1,\"p\",[[0,[],0,\"TL;DR – if you just want the code for the extension, then just go to \"],[0,[2],1,\"this repo\"],[0,[],0,\".\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Retry Build\"]]],[1,\"p\",[[0,[],0,\"I was recently \"],[0,[3],1,\"playing with Build VNext\"],[0,[],0,\" and got a little frustrated that there was no way to retry a build from the list of completed builds in Web Access. I had to click the build definition to queue it. I found this strange, since the build explorer in Visual Studio has an option to retry a build. I was half-way through writing a mail to the Visual Studio Product team suggesting that they add this option, when I had an epiphany: I can write that as an extension! So I did…\"]]],[1,\"p\",[[0,[],0,\"I started by browsing to the \"],[0,[4],1,\"Visual Studio Extensions sample repo\"],[0,[],0,\" on Github. I had to join the \"],[0,[5],1,\"Visual Studio Partner program\"],[0,[],0,\", which took a while since I signed up using my email address but adding my work Visual Studio account (instead of my personal account). Switching the account proved troublesome, but I was able to get it sorted with help from Will Smythe on the Product team. Make sure you’re the account owner and that you specify the correct VSO account when you sign up for the partner program!\"]]],[1,\"p\",[[0,[],0,\"Next I cloned the repo and took a look at the code – it looked fairly straightforward, especially since all I wanted to do with this extension was add a menu command – no new UI at all.\"]]],[1,\"p\",[[0,[],0,\"I followed the instructions for installing the “Contribution Point Guide” so that I could test that extensions worked on my account, as well as actually see the properties of the extension points. It’s a very useful extension to have when you’re writing extensions (does that sounds recursive?).\"]]],[1,\"h3\",[[0,[],0,\"TypeScript\"]]],[1,\"p\",[[0,[],0,\"I’m a huge TypeScript fan, so I wanted to write my extension in TypeScript. There is a sample in the samples repo that has TypeScript, so I got some hints from that. There is a “Delete branch” sample that adds a menu command (really the only thing I wanted to do), so I started from that sample and wrote my extension.\"]]],[1,\"p\",[[0,[],0,\"Immediately I was glad I had decided to use TypeScript – the d.ts (definition files) for the extension frameworks and services is very cool – getting IntelliSense and being able to type the objects that were passed around made discovery of the landscape a lot quicker than if I was just using plain JavaScript.\"]]],[1,\"p\",[[0,[],0,\"The code tuned out to be easy enough. However, when I ran the extension, I kept getting a\"]]],[10,1],[1,\"p\",[[0,[],0,\"error. We’ll come back to that. Let’s first look at main.ts to see the extension:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ol\",[[[0,[],0,\"Lines 1/2: imports of framework objects – these 2 lines were causing an error for me initially\"]],[[0,[],0,\"Line 3: the name of this function is only used on line 27 when we register it\"]],[[0,[],0,\"Line 6: we’re returning an IContributedMenuSource struct\"]],[[0,[],0,\"Line 7: the struct has an ‘execute’ method that is invoked when the user clicks on the menu item\"]],[[0,[],0,\"Line 9: we get a reference to what is essentially the build service\"]],[[0,[],0,\"Line 13: using the build it (a property I discovered on the actionContext object using the Contribution Point sample extension) we can get the completed build object\"]],[[0,[],0,\"Line 15: I simple pop the build back onto the queue – all the other information is already in the build object (like branch, configuration and so on) from the previous queuing of the build\"]],[[0,[],0,\"Line 18: I build an url that points to the summary page for the new build\"]],[[0,[],0,\"Line 19: redirect the browser to the new build url\"]],[[0,[],0,\"Line 13/15: note the use of the .then() syntax – these methods return promises (good asynch programming), so we use the .then() to execute once the async operation has completed\"]],[[0,[],0,\"Line 27: registering the extension using the name (1st arg) which is the name we use in the extension.json file, and the function name we specified on line 3 (the 2nd arg)\"]]]],[1,\"p\",[[0,[],0,\"It was in fact, simpler than I thought it would be. I was expecting to have to create a new build object from the old build object – turns out that wasn’t necessary at all. So I had my code and was ready to run – except that I ran into a snag. When I ran my code, I kept getting\"]]],[10,3],[1,\"p\",[[0,[],0,\". To understand why, we need to quickly understand how the extensions are organized.\"]]],[1,\"h2\",[[0,[],0,\"Anatomy of an Extension\"]]],[1,\"p\",[[0,[],0,\"A VSO extension consists of a couple of key files: the extension.json, the main.html and the main.ts or main.js file.\"]]],[3,\"ul\",[[[0,[],0,\"extension.json – the manifest file for the extension – used to register the extension\"]],[[0,[],0,\"main.html – the main loading page for the extension – used to bootstrap the extension\"]],[[0,[],0,\"main.js (or main.ts) – the main script entry point for the extension – used to provide the starting point for any extension logic\"]]]],[1,\"p\",[[0,[],0,\"The “Build Inspector” sample has a main.ts, but this file doesn’t really do much – it only redirects to the main page of the extension custom UI. So there are no imports or requires. So I was at a bit of a loss as to why I was getting what looked like a require error when my extension was loaded. Here’s the html for the main.html page of the sample “Delete Branch” extension:\"]]],[10,4],[1,\"p\",[[0,[],0,\"You’ll see that the main.js file is imported in line 9, and then we’ve told VSO to use the module loader – necessary for any “require” work. So I was still baffled – here we’re telling the framework that we’re going to be using “require” and I’m getting a require error! (Remember, since the sample doesn’t use any requires in the main.js, it doesn’t error). My main.html page looked exactly the same – and then looked at the items.html page of the sample “Build Inspector” extension, and I got an idea – I need to require my main module, not just load it. Here’s what my main.html ended up looking like:\"]]],[10,5],[1,\"p\",[[0,[],0,\"You can see how instead of just importing the main.js script (like the “Delete Branch” sample) I “require” the main script on line 18. Once I had that, no more errors and I was able to get the extension to work.\"]]],[1,\"p\",[[0,[],0,\"Once I had that worked out, I was able to quickly publish the extension to Azure, change the url in the extension.json file to point to my Azure site url, and I was done! The code is in \"],[0,[2],1,\"this repo\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Writing extensions for VSO is fun, and having a good sample library to start from is great. The “Contribution Points” sample is clever – letting you test the extension loading as well as giving very detailed information about the various hooks and properties available for extensions. Finally, the TypeScript definitions make navigating the APIs available a snap. While my first extension is rather basic, I am really pleased with the extensibility framework that the Product team have devised.\"]]],[1,\"p\",[[0,[],0,\"Happy customizing!\"]]]]}","published_at":1432576733000,"status":"published","published_by":1},{"id":"43d7f0ea-9afc-4ea7-9c43-62d574fb533f","title":".NET Core Multi-Stage Dockerfile with Test and Code Coverage in Azure Pipelines","slug":"net-core-multi-stage-dockerfile-with-test-and-code-coverage-in-azure-pipelines","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"FROM microsoft/dotnet:2.2-sdk AS build-env\\nWORKDIR /app\\nARG version=1.0.0\\n\\n# install npm for building\\nRUN curl -sL https://deb.nodesource.com/setup_8.x | bash - &amp;&amp; apt-get update &amp;&amp; apt-get install -yq nodejs build-essential make\\n\\n# Copy csproj and restore as distinct layers\\nCOPY PartsUnlimited.sln ./\\nCOPY ./src/ ./src\\nCOPY ./test/ ./test\\nCOPY ./env/ ./env\\n\\n# restore for all projects\\nRUN dotnet restore PartsUnlimited.sln\\n\\n# test\\n# use the label to identity this layer later\\nLABEL test=true\\n# install the report generator tool\\nRUN dotnet tool install dotnet-reportgenerator-globaltool --version 4.0.6 --tool-path /tools\\n# run the test and collect code coverage (requires coverlet.msbuild to be added to test project)\\n# for exclude, use %2c for ,\\nRUN dotnet test --results-directory /testresults --logger \\\"trx;LogFileName=test_results.xml\\\" /p:CollectCoverage=true /p:CoverletOutputFormat=cobertura /p:CoverletOutput=/testresults/coverage/ /p:Exclude=\\\"[xunit.*]*%2c[StackExchange.*]*\\\" ./test/PartsUnlimited.UnitTests/PartsUnlimited.UnitTests.csproj\\n# generate html reports using report generator tool\\nRUN /tools/reportgenerator \\\"-reports:/testresults/coverage/coverage.cobertura.xml\\\" \\\"-targetdir:/testresults/coverage/reports\\\" \\\"-reporttypes:HTMLInline;HTMLChart\\\"\\nRUN ls -la /testresults/coverage/reports\\n\\n# build and publish\\nRUN dotnet publish src/PartsUnlimitedWebsite/PartsUnlimitedWebsite.csproj --framework netcoreapp2.0 -c Release -o out /p:Version=${version}\\n\\n# Build runtime image\\nFROM microsoft/dotnet:2.2-aspnetcore-runtime\\nWORKDIR /app\\nEXPOSE 80\\nCOPY --from=build-env /app/src/PartsUnlimitedWebsite/out .\\nENTRYPOINT [\\\"dotnet\\\", \\\"PartsUnlimitedWebsite.dll\\\"]\",\"language\":\"plain;\"}],[\"code\",{\"code\":\"$&gt; docker images --filter \\\"label=test=true\\\" | head -2\\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\\n&lt;none&gt;              &lt;none&gt;              13151da78ddb        2 hours ago         2.53 GB\\n$&gt; docker images | grep partsunlimited\\npartsunlimitedwebsite                      1.0.1                    957346c64b03        2 hours ago         308 MB\",\"language\":\"bash;\"}],[\"code\",{\"code\":\"name: 1.0$(Rev:.r)\\n\\ntrigger:\\n- k8sdevops\\n\\npool:\\n  vmImage: 'Ubuntu-16.04'\\n\\nvariables:\\n  imageName: 'partsunlimitedwebsite:$(build.buildNumber)'\\n\\nsteps:\\n- script: docker build -f Dockerfile -t $(imageName) .\\n  displayName: 'docker build'\\n  continueOnError: true\\n\\n- script: |\\n    export id=$(docker images --filter \\\"label=test=true\\\" -q | head -1)\\n    docker create --name testcontainer $id\\n    docker cp testcontainer:/testresults ./testresults\\n    docker rm testcontainer\\n  displayName: 'get test results'\\n\\n- task: PublishTestResults@2\\n  inputs:\\n    testResultsFormat: 'VSTest'\\n    testResultsFiles: '**/test*.xml' \\n    searchFolder: '$(System.DefaultWorkingDirectory)/testresults'\\n    publishRunAttachments: true\\n  displayName: 'Publish test results'\\n\\n- task: PublishCodeCoverageResults@1\\n  inputs:\\n    codeCoverageTool: 'cobertura'\\n    summaryFileLocation: '$(System.DefaultWorkingDirectory)/testresults/coverage/coverage.cobertura.xml'\\n    reportDirectory: '$(System.DefaultWorkingDirectory)/testresults/coverage/reports'\\n  displayName: 'Publish coverage reports'\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/66d008e1-5e77-433e-aa2e-e60e7a09f357.png\\\" target=\\\"_blank\\\"><img width=\\\"399\\\" height=\\\"292\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ab3fb438-893c-41d7-843c-93415e4f8547.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f370a64b-32c6-4b8b-9a4c-3ebaf1d518cb.png\\\" target=\\\"_blank\\\"><img width=\\\"400\\\" height=\\\"298\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8eb8d73a-a22e-4cf2-8911-02a18c1394e1.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/070e3452-89a4-42ce-a1ff-8099f288cf48.png\\\" target=\\\"_blank\\\"><img width=\\\"401\\\" height=\\\"209\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1a7390b3-18ae-4ea3-bca2-d36148d6b5dd.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8bde6102-1aa0-48c4-9f24-22b89f9b6a3e.png\\\" target=\\\"_blank\\\"><img width=\\\"403\\\" height=\\\"199\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/64ebd5b9-67b6-4cb4-bd5e-6619892f2276.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://blog.ehn.nu/2019/01/running-net-core-unit-tests-with-docker-and-azure-pipelines/\"]],[\"a\",[\"href\",\"https://blog.ehn.nu/about-me/\"]],[\"a\",[\"href\",\"https://docs.docker.com/develop/develop-images/multistage-build/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/PartsUnlimited/tree/k8sdevops\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/PartsUnlimited/blob/k8sdevops/Dockerfile\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/PartsUnlimited/blob/k8sdevops/azure-pipelines.yml\"]],[\"a\",[\"href\",\"https://github.com/tonerdo/coverlet\"]],[\"a\",[\"href\",\"https://docs.docker.com/config/labels-custom-metadata/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I read a great \"],[0,[0],1,\"blogpost\"],[0,[],0,\" recently by my friend and fellow MVP \"],[0,[1],1,\"Jakob Ehn\"],[0,[],0,\". In this post he outlines how he created a \"],[0,[2],1,\"multi-stage Dockerfile\"],[0,[],0,\" to run .NET Core tests. I've always been on the fence about running tests during a container build - I usually run the tests outside and then build/publish the container proper only if the tests pass. However, this means I have to have the test frameworks on the build agent - and that's where doing it inside a container is great, since the container can have all the test dependencies without affecting the host machine. However, if you do this then you'll have test assets in your final container image, which isn't ideal. Fortunately, with multi-stage Dockerfiles you can compile (and/or test) and then create a final image that just has the app binaries!\"]]],[1,\"p\",[[0,[],0,\"I was impressed by Jakob's solution, but I wanted to add a couple enhancements:\"]]],[3,\"ol\",[[[0,[],0,\"Jakob builds the container twice and runs the tests twice: one build for the test runs (in a shell task using the --target arg) and one to build the container proper - which would end up execute the tests again. I wanted to improve this if I could.\"]],[[0,[],0,\"Add code coverage. I think that it's almost silly to not do code coverage if you have tests, so I wanted to see how easy it was to add coverage to the test runs too!\"]]]],[1,\"h2\",[[0,[],0,\"tl;dr\"]]],[1,\"p\",[[0,[],0,\"If you want the final process, have a look at my fork of the PartsUnlimited repo on Github (on the \"],[0,[3],1,\"k8sdevops branch\"],[0,[],0,\"). You'll see the final \"],[0,[4],1,\"Dockerfile\"],[0,[],0,\" and the \"],[0,[5],1,\"azure-pipelines.yml\"],[0,[],0,\" build definition file there.\"]]],[1,\"h2\",[[0,[],0,\"Adding Code Coverage\"]]],[1,\"p\",[[0,[],0,\"I wanted to take things one step further and add code coverage into the mix. Except that doing code coverage in .NET Core is non-trivial. For that it seems you have to use \"],[0,[6],1,\"Coverlet\"],[0,[],0,\". I ended up adding a coverlet.msbuild package reference to my test project and then I just configured the test args for \\\"dotnet test\\\" to specify coverage options in the \\\"dotnet test\\\" command - we'll see that in the Dockerfile next.\"]]],[1,\"h2\",[[0,[],0,\"Removing the Redundancy\"]]],[1,\"p\",[[0,[],0,\"Jakob runs a shell script which builds the container only to the point of running the tests - he doesn't want to build the rest of the container if the tests fail. However, when I was playing with this I realized that if tests fail, then the docker build process fails too - so I didn't worry about the test and final image being in the same process. If the process completes, I know the tests have passed - if not, then I might have to diagnose to figure out if there is a build issue or a test issue, but logging in Azure pipelines is fantastic so that's not too much of a concern.\"]]],[1,\"p\",[[0,[],0,\"The next issue was getting the test and coverage files out of the interim image and have a clean final image without test artifacts. That's where \"],[0,[7],1,\"labels\"],[0,[],0,\" come in. Let's look at the final Dockerfile:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: I'm getting the big bloated .NET Core SDK image which is required to compile, test and publish the app\"]],[[0,[],0,\"Line 6: install npm prerequisites. I could create a custom build image with this on it, but it's really quick if these dependencies don't exist. If you're running on a private agent, this layer is cached so you don't do it on every run anyway.\"]],[[0,[],0,\"Lines 9-12: copy app and test files into the container\"]],[[0,[],0,\"Line 15: restore packages for all the projects\"]],[[0,[],0,\"Line 19: add a label which we can use later to identify this layer\"]],[[0,[],0,\"Line 21: install the report generator tool for coverage reports\"]],[[0,[],0,\"Line 24: run \\\"dotnet test\\\" to invoke the test. I specify the results directory which I'll copy out later and specify a trx logger to get a VSTest results file. The remainder of the args are for coverage: the format is cobertura, I specify a folder and specify some namespaces to exclude (note how I had to use %2c for commas to get this to work correctly)\"]],[[0,[],0,\"Line 26: run the report generator tool to produce html coverage reports\"]],[[0,[],0,\"Line 30: publish the app - this is the only bit I really want in the final image\"]],[[0,[],0,\"Lines 33-37: copy the final binaries into an image based on the .NET Core runtime - which is far lighter than the SDK image the previous steps started on (about 10% of the size)\"]],[[0,[],0,\"Line 36: this is where we do the actual copy of any artifacts we want in the final image\"]]]],[1,\"p\",[[0,[],0,\"When the build completes, we'll end up with a number of interim images as well as a final deployable image with just the app - this is the image we're going to push to container registries and so on. Doing some docker images queries shows how important slimming down the final image is:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can see that we get a 2.53GB image for the SDK build process (it's repo and tag are both <none> since this is an intermediary layer). The final image is only 308MB!\"]]],[1,\"p\",[[0,[],0,\"You'll also note how we used the label in the filter expression to get only the layers that have a label \\\"test=true\\\". If we add the \\\"-q\\\" parameter, we'll get just the id of that layer, which is what we'll need to get the test and coverage files out to publish in the CI build.\"]]],[1,\"h2\",[[0,[],0,\"The Azure Pipelines YML File\"]]],[1,\"p\",[[0,[],0,\"The CI definition turns out to be quite simple:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 13-15: build and tag the docker image using the Dockerfile\"]],[[0,[],0,\"Lines 17-22: get the id of the interim image and create a container. Then copy out the test results files and then delete the container.\"]],[[0,[],0,\"Lines 24-30: publish the test file\"]],[[0,[],0,\"Lines 32-37: publish the coverage results and reports\"]]]],[1,\"h2\",[[0,[],0,\"Final Results\"]]],[1,\"p\",[[0,[],0,\"The final results are fantastic. Below are screenshots of the summary page, the test results page, the coverage report and a drill-down to see coverage for a specific class:\"]]],[10,3],[10,4],[10,5],[10,6],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Running tests (with code coverage) inside a container is actually not that bad - you need to do some fancy footwork after the build to get the test/coverage results, but all in all the process is pleasant. We're able to run tests inside a container (not that this mandates real unit tests - tests that have no external dependencies!), get the results out and publish a super-slim final image.\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1547710769000,"status":"published","published_by":1},{"id":"73fc1d40-d5ef-4321-a09c-70e503fb0b94","title":"New Feature: Lightweight Query Charts in Web Access","slug":"new-feature-lightweight-query-charts-in-web-access","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-nZcBnGFzlGc/Ui3PsaeRsXI/AAAAAAAABD0/s9NkwPtEFXw/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-5hDzIXCKGQc/Ui3PtC_Dq2I/AAAAAAAABD8/MMNMTbeGSFs/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"490\\\" height=\\\"202\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-lsjTEJlqih4/Ui3Ptz66k8I/AAAAAAAABEE/WTkLMUHWPhs/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-v9HqiR3C8fc/Ui3Pumx6AmI/AAAAAAAABEM/X6dT9wGYYms/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"325\\\" height=\\\"222\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-OwrnxvXhJbI/Ui3Pve2V52I/AAAAAAAABEU/vCz9HSPzwY0/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-kYnMTM9IAlk/Ui3PwC6Ei8I/AAAAAAAABEc/ElSIby_MvEM/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"449\\\" height=\\\"219\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-vA_9dr25AUw/Ui3PwquOZnI/AAAAAAAABEk/__b8LRwHWE4/s1600-h/image%25255B15%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-s4ESMhJZND0/Ui3PxelnZeI/AAAAAAAABEs/zlUqNxKDTDk/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"390\\\" height=\\\"191\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.microsoft.com/visualstudio/eng/2013-downloads\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have installed \"],[0,[0],1,\"TFS 2013 RC\"],[0,[],0,\". I upgraded my TFS Express (that I use for mucking around with code) from TFS 2012.3 and everything went smoothly. I then opened up Web Access and was pleased to see one of the best features yet for TFS work items: lightweight charts.\"]]],[1,\"p\",[[0,[],0,\"These charts allow you to quickly and easily create visualizations against your work item queries. Here is a “dashboard” against a query that lists “Tasks” in my Team Project:\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Creating Charts\"]]],[1,\"p\",[[0,[],0,\"Creating charts is really simple. Navigate to Web Access and click “WORK” and then “Queries” to go to the query hub. Select or create create a query. Make sure you add any columns that you want to group by onto the query – for example, State, Assigned To or Iteration Path. Once you’ve saved your query, click on the “Charts” tab:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now enter a name, select the chart type and the field to group by. For example, here I am doing a pie graph grouped by “State”:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can also make a Pivot table – this allows you to select rows and columns. For example, here’s one showing Assigned To vs State:\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Limitations\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, you can only use “count of work items” as the value setting. I tried to see if I could add “Remaining Work” and show remaining work per user, but no dice at the moment. We’ll have to wait for a future update to get this sort of functionality.\"]]],[1,\"h2\",[[0,[],0,\"Wish List\"]]],[1,\"p\",[[0,[],0,\"I’d love to see the ability to “Favourite” one of the charts so that it appears on the landing page. It would also be nice if you could edit the colours, add filters and email the charts (or at least a chart link). Also, you can’t drag-and-drop to re-order the charts. We’ll have to see what the product team gets time to actually squeeze into this feature.\"]]],[1,\"p\",[[0,[],0,\"In the meantime, happy charting!\"]]]]}","published_at":1378766400000,"status":"published","published_by":1},{"id":"b789df7e-8071-4fac-8587-57ce03b9811a","title":"New Task: Tag Build or Release","slug":"new-task-tag-build-or-release","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b28ab04a-96bd-4a61-90ed-392f62ff5417.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d9084b02-a33d-4834-96aa-9e66e4e77264.png\\\" width=\\\"304\\\" height=\\\"129\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1c80eeec-e5cd-46fa-b77a-c79adac0befa.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5a844571-d88c-407d-a4bd-da2f1a956e52.png\\\" width=\\\"304\\\" height=\\\"118\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://bit.ly/cacbuildtasks\"]],[\"a\",[\"href\",\"https://roadtoalm.com/2016/07/08/controlling-build-quality-using-build-tags-and-vsts-release-management/\"]],[\"em\"],[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/docs/build/concepts/process/conditions\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/TagBuild\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have a build/release task pack in the \"],[0,[0],1,\"marketplace\"],[0,[],0,\". I’ve just added a new task that allows you to add tags to builds or releases in the pipeline, inspired by my friend and fellow MVP Rene van Osnabrugge’s \"],[0,[1],1,\"excellent post\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Here are a couple of use cases for this task:\"]]],[3,\"ol\",[[[0,[],0,\"You want to trigger releases, but only for builds on a particular branch with a particular tag. This trigger only works if the build is tagged \"],[0,[2],1,\"during the build\"],[0,[],0,\". So you could add a TagBuild task to your build that is only run \"],[0,[3],1,\"conditionally\"],[0,[],0,\" (for example for buildreason = Pull Request). Then if the condition is met, the tag is set on the build and the release will trigger in turn, but only for builds that have the tag set.\"]]]],[10,0],[3,\"ol\",[[[0,[],0,\"You want to tag a build from a release once a release gets to a certain environment. For example, you can add a TagBuild task and tag the primary build once all the integration tests have passed in the integration environment. That way you can see which builds have passed integration tests simply by querying the tags.\"]]]],[10,1],[1,\"p\",[[0,[],0,\"Of course you can use variables for the tags – so you could tag the build with the release(s) that have made it to prod by specifying $(Release.ReleaseNumber) as the tag value.\"]]],[1,\"p\",[[0,[],0,\"There are of course a ton of other use cases!\"]]],[1,\"h3\",[[0,[],0,\"Tag Types\"]]],[1,\"p\",[[0,[],0,\"You can see the tag type matrix for the “tag type” (which can be set to Build or Release) in the \"],[0,[4],1,\"docs\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Let me know if you have issues or feedback. Otherwise, happy taggin’!\"]]]]}","published_at":1493729352000,"status":"published","published_by":1},{"id":"cbf153fd-bdaa-4831-b9ce-ad13fd99536a","title":"New vNext Config Variable Options in RM Update 4 RC","slug":"new-vnext-config-variable-options-in-rm-update-4-rc","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/33abda31-63d2-4440-8d2f-2faf5348e1fe.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b03d629d-6ae1-4c0b-9481-d447a868158d.png\\\" width=\\\"644\\\" height=\\\"108\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e6106605-d0cc-4ec8-91c1-f1ed36a93f88.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c8f6bff0-ed17-4b2d-bf22-35c00de2bd18.png\\\" width=\\\"644\\\" height=\\\"203\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/976326bb-0e2f-4fa9-9e79-84e2d7577fa2.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ff9bea76-6a78-4897-97c2-4aa42759d64b.png\\\" width=\\\"644\\\" height=\\\"170\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/64fe1bb0-fc6e-4bba-bd68-748b02644429.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/02367d62-9d20-4e97-b57f-7b42e63c5ec9.png\\\" width=\\\"644\\\" height=\\\"446\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4ccfae7c-03d1-4137-8253-8d5ac906eecd.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/82da646b-95e0-4066-9dc0-eee359120e3d.png\\\" width=\\\"440\\\" height=\\\"235\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/516ad343-1284-4999-b7ab-0581329dc5f0.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a1fc6126-479c-4194-8336-b0537a954ac7.png\\\" width=\\\"644\\\" height=\\\"400\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c11f8658-3b9d-4d10-82bb-a7817e5c646e.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ed79f014-2b7f-4919-a1ba-e5994537b0d2.png\\\" width=\\\"644\\\" height=\\\"289\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.microsoft.com/en-us/download/details.aspx?id=44555&WT.mc_id=rss_alldownloads_all\"]],[\"a\",[\"href\",\"http://support.microsoft.com/kb/2994375\"]],[\"strong\"],[\"em\"]],\"sections\":[[1,\"p\",[[0,[0],1,\"Update 4 RC for Release Management\"],[0,[],0,\" was released a few days ago. There are some \"],[0,[1],1,\"good improvements\"],[0,[],0,\" – some are minor, like the introduction of “Agent-based” labels improves readability for viewing agent-based vs non-agent based templates and components. Others are quite significant – like being able to use the Manual Intervention activity and tags in vNext templates, being able to use server-drops as release source and others. By far my favorite new feature of the update is the new variable capabilities.\"]]],[1,\"h2\",[[0,[],0,\"Variables: System, Global, Server, Component and Action\"]]],[1,\"p\",[[0,[],0,\"Be aware that, unfortunately, these capabilities are \"],[0,[2],1,\"only\"],[0,[],0,\" for vNext components (so they won’t work with regular agent-based components or workflows). It’s also unlikely that agent-based components will ever get these capabilities. I’ve mentioned before that I think PowerShell DSC is the deployment mechanism of the future, so you should be investing in it now already. If you’re currently using agent-based components, they do have variables that can be specified at design-time (in the deployment workflow surface) – just as they’ve always had.\"]]],[1,\"p\",[[0,[],0,\"The new vNext variable capabilities allow you to use variables inside your PowerShell scripts without having to pass them or hard-code them. For example, if you define a global variable called “MyGlobalVar” you can just use it by accessing $MyGlobalVar in your PowerShell script.\"]]],[1,\"h3\",[[0,[],0,\"Global Variables\"]]],[1,\"p\",[[0,[],0,\"Global variables are defined under “Administration->Settings->Configuration Variables”. Here you can defined variables, giving them a name, type, default value and description.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Server Variables\"]]],[1,\"p\",[[0,[],0,\"Server variables can be defined on vNext servers under “Configure Paths->Servers”. Same format as System variables.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Component Variables\"]]],[1,\"p\",[[0,[],0,\"vNext components can now have configuration variables defined on them “at design time”.\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can also override values and event specify additional configuration variables when you add the “DSC” component onto the design surface:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Another cool new feature is the fact that ComponentName and ServerName are now dropdown lists on the “Deploy using DSC/PS” and “Deploy using Chef” activities, so you don’t have to type them manually:\"]]],[10,4],[1,\"p\",[[0,[],0,\"All these variables are available inside the script by simple using $\"],[0,[3],1,\"variableName.\"],[0,[],0,\" You may event get to the point where you no longer need a PSConfiguration file at all!\"]]],[1,\"p\",[[0,[],0,\"You can also see all your variables by opening the “Resource Variables” tab:\"]]],[10,5],[1,\"h3\",[[0,[],0,\"System Variables\"]]],[1,\"p\",[[0,[],0,\"RM now exposes a number of system variables for your scripts. These are as follows:\"]]],[3,\"ul\",[[[0,[],0,\"Build directory\"]],[[0,[],0,\"Build number (for component in the release)\"]],[[0,[],0,\"Build definition (for component)\"]],[[0,[],0,\"TFS URL (for component)\"]],[[0,[],0,\"Team project (for component)\"]],[[0,[],0,\"Tag (for server which is running the action)\"]],[[0,[],0,\"Application path (destination path where component is copied)\"]],[[0,[],0,\"Environment (for stage)\"]],[[0,[],0,\"Stage\"]]]],[1,\"p\",[[0,[],0,\"You can access these variable easily by simply using $\"],[0,[3],1,\"name\"],[0,[],0,\" (for example: $BuildDirectory or $Stage). If you mouse over the “?” icon on right of the Component or Server screens, the tooltip will tell you what variables you have access to.\"]]],[10,6],[1,\"h3\",[[0,[],0,\"Release Candidate\"]]],[1,\"p\",[[0,[],0,\"Finally, remember that this Release Candidate (as opposed to CTPs) is “go-live” so you can install it on your production TFS servers and updating to the RTM is supported. There may be minor glitches with the RC, but you’ll get full support from MS if you encounter any.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1413932387000,"status":"published","published_by":1},{"id":"ffdc18cc-651e-415c-ba1a-f286c1460ef1","title":"PaaS and Time Compression in the Cloud","slug":"paas-and-time-compression-in-the-cloud","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://blog.nwcadence.com/\"]],[\"a\",[\"href\",\"http://blog.nwcadence.com/paas-architecture-designing-for-the-cloud/\"]],[\"a\",[\"href\",\"http://blog.nwcadence.com/compressing-time-a-competitive-advantage/\"]],[\"a\",[\"href\",\"https://azureinfo.microsoft.com/rs/microsoftdemandcenter/images/EN-CNTNT-Whitepaper-Timeforthecloud.pdf?mkt_tok=3RkMMJWWfF9wsRokuKvKZKXonjHpfsX97%2BUsXa%2BwlMI%2F0ER3fOvrPUfGjI4CTcFlI%2BSLDwEYGJlv6SgFS7XCMadx37gOUxM%3D\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Recently I got to write a couple of articles which were posted on the \"],[0,[0],1,\"Northwest Cadence Blog\"],[0,[],0,\". I am not going to reproduce them here, so please read them on the NWCadence blog from the links below.\"]]],[1,\"h2\",[[0,[],0,\"PaaS\"]]],[1,\"p\",[[0,[],0,\"The first, \"],[0,[1],1,\"PaaS Architecture: Designing Apps for the Cloud\"],[0,[],0,\" covers some of the considerations you’ll need to make if you’re porting your applications to the Cloud. Moving your website to a VM with IIS is technically “moving to the cloud”, but this is IaaS (infrastructure as a service) rather than PaaS (platform as a service). If you’re going to unlock true scale from the cloud, you’re going to have to move towards PaaS.\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, you can’t simply push any site into PaaS, since you’ll need to consider where and how your data is stored, how your authentication is going to work, and most importantly, how your application will handle scale up – being spread over numerous servers simultaneously. The article deals with some these and other considerations you’ll need to make.\"]]],[1,\"h2\",[[0,[],0,\"Time Compression\"]]],[1,\"p\",[[0,[],0,\"The second article, \"],[0,[2],1,\"Compressing Time: A Competitive Advantage\"],[0,[],0,\" is written off the back of Joe Weinman’s excellent white paper \"],[0,[3],1,\"Time for the Cloud\"],[0,[],0,\". Weinman asserts that “moving to the cloud” is not a guarantee of success in and of itself – companies must strategically utilize the advantages cloud computing offers. While there are many cloud computing advantages, Weinman focuses on what he calls \"],[0,[4],1,\"time compression\"],[0,[],0,\" – the cloud’s ability to speed \"],[0,[4],1,\"time to market\"],[0,[],0,\" as well as \"],[0,[4],1,\"time to scale\"],[0,[],0,\". Again, I consider some of the implications you’ll need to be aware of when you’re moving applications to the cloud.\"]]],[1,\"p\",[[0,[],0,\"Happy cloud computing!\"]]]]}","published_at":1434990668000,"status":"published","published_by":1},{"id":"a26bb194-f177-46f5-83da-facf5894e8e2","title":"Parallel Testing in a Selenium Grid with VSTS","slug":"parallel-testing-in-a-selenium-grid-with-vsts","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"java -jar selenium-server-standalone-2.53.1.jar -role hub\\n\\n\",\"language\":\"\"}],[\"code\",{\"code\":\"{\\n  \\\"capabilities\\\":\\n  [\\n    {\\n      \\\"browserName\\\": \\\"firefox\\\",\\n      \\\"platform\\\": \\\"WINDOWS\\\",\\n      \\\"maxInstances\\\": 1\\n    },\\n    {\\n      \\\"browserName\\\": \\\"internet explorer\\\",\\n      \\\"platform\\\": \\\"WINDOWS\\\",\\n      \\\"maxInstances\\\": 1\\n    }\\n  ],\\n  \\\"configuration\\\":\\n  {\\n    \\\"nodeTimeout\\\": 120,\\n    \\\"nodePolling\\\": 2000,\\n    \\\"timeout\\\": 30000\\n  }\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"{\\n  \\\"capabilities\\\": [\\n    {\\n      \\\"browserName\\\": \\\"firefox\\\",\\n      \\\"platform\\\": \\\"WINDOWS\\\",\\n      \\\"maxInstances\\\": 1\\n    },\\n    {\\n      \\\"browserName\\\": \\\"chrome\\\",\\n      \\\"platform\\\": \\\"WINDOWS\\\"\\n    }\\n  ],\\n  \\\"configuration\\\":\\n  {\\n    \\\"nodeTimeout\\\": 120,\\n    \\\"nodePolling\\\": 2000,\\n    \\\"timeout\\\": 30000\\n  }\\n}\\n\",\"language\":\"js;\"}],[\"code\",{\"code\":\"param(\\n    $port,\\n    [string]$hubHost,\\n    $hubPort = 4444,\\n\\n    [ValidateSet('ie', 'chrome')]\\n    [string]$browser,\\n\\n    [string]$driverPath = \\\"configs/drivers\\\"\\n)\\n\\n$hubUrl = \\\"http://{0}:{1}/grid/register\\\" -f $hubHost, $hubPort\\n$configFile = \\\"./configs/{0}Node.json\\\" -f $browser\\n\\njava -jar selenium-server-standalone-2.53.1.jar -role node -port $port -nodeConfig $configFile -hub $hubUrl -D\\\"webdriver.chrome.driver=$driverPath/chromedriver.exe\\\" -D\\\"webdriver.ie.driver=$driverPath/IEDriverServer.exe\\\"\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\".\\\\startNode.ps1 -port 5555 -hubHost localhost -browser chrome -driverPath c:\\\\selenium\\\\drivers\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\".\\\\startNode.ps1 -port 5556 -hubHost 10.4.0.4 -browser ie -driverPath c:\\\\selenium\\\\drivers\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/09a7359e-58df-4a0f-b501-907537859632.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4d65dca6-6287-43ba-ac99-6cfe5cd52fe8.png\\\" width=\\\"279\\\" height=\\\"367\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/3f834cfa-c5f4-48a1-bd23-e42694af2908.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/53001826-09d1-4183-b66b-f2131679a6a0.png\\\" width=\\\"622\\\" height=\\\"202\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/e80d0617-72c8-438e-a721-d0ec678def6b.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/0d1c0780-b0a2-4fcf-86fe-c34910f9aa5f.png\\\" width=\\\"573\\\" height=\\\"206\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c9160528-daef-461f-8142-dae11f2c9dd0.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/36350e23-fa8f-4ea4-b66e-aa0dcf8bd973.png\\\" width=\\\"420\\\" height=\\\"174\\\"></a>\"}],[\"code\",{\"code\":\"protected void SearchTest()\\n{\\n    driver.Navigate().GoToUrl(baseUrl + \\\"/\\\");\\n    driver.FindElement(By.Id(\\\"search-box\\\")).Clear();\\n    driver.FindElement(By.Id(\\\"search-box\\\")).SendKeys(\\\"tire\\\");\\n\\n    driver.FindElement(By.Id(\\\"search-link\\\")).Click();\\n\\n    // check that there are 3 results\\n    Assert.AreEqual(3, driver.FindElements(By.ClassName(\\\"list-item-part\\\")).Count);\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"public abstract class PartsTests\\n{\\n    private const string defaultBaseUrl = \\\"http://localhost:5001\\\";\\n    private const string defaultGridUrl = \\\"http://10.0.75.1:4444/wd/hub\\\";\\n\\n    protected string baseUrl;\\n    protected string gridUrl;\\n\\n    protected IWebDriver driver;\\n    private StringBuilder verificationErrors;\\n    protected ICapabilities capabilities;\\n    public TestContext TestContext { get; set; }\\n\\n    public PartsTests(ICapabilities capabilities)\\n    {\\n        this.capabilities = capabilities;\\n    }\\n\\n    [TestInitialize]\\n    public void SetupTest()\\n    {\\n        if (TestContext.Properties[\\\"baseUrl\\\"] != null) //Set URL from a build\\n        {\\n            baseUrl = TestContext.Properties[\\\"baseUrl\\\"].ToString();\\n        }\\n        else\\n        {\\n            baseUrl = defaultBaseUrl;\\n        }\\n        Trace.WriteLine($\\\"BaseUrl: {baseUrl}\\\");\\n\\n        if (TestContext.Properties[\\\"gridUrl\\\"] != null) //Set URL from a build\\n        {\\n            gridUrl = TestContext.Properties[\\\"gridUrl\\\"].ToString();\\n        }\\n        else\\n        {\\n            gridUrl = defaultGridUrl;\\n        }\\n        Trace.WriteLine($\\\"GridUrl: {gridUrl}\\\");\\n\\n        driver = new RemoteWebDriver(new Uri(gridUrl), capabilities);\\n        verificationErrors = new StringBuilder();\\n    }\\n\\n    [TestCleanup]\\n    public void Teardown()\\n    {\\n        try\\n        {\\n            driver.Quit();\\n        }\\n        catch (Exception)\\n        {\\n            // Ignore errors if unable to close the browser\\n        }\\n        Assert.AreEqual(\\\"\\\", verificationErrors.ToString());\\n    }\\n\\n    ...\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?&gt;\\n&lt;RunSettings&gt;\\n  &lt;TestRunParameters&gt;\\n    &lt;Parameter name=\\\"baseUrl\\\" value=\\\"http://localhost:5001\\\" /&gt;\\n    &lt;Parameter name=\\\"gridUrl\\\" value=\\\"http://localhost:4444/wd/hub\\\" /&gt;\\n  &lt;/TestRunParameters&gt;\\n&lt;/RunSettings&gt;\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/7fe06d56-1a53-484f-af74-eba2c1bec765.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/809864a8-cb6f-4f92-b957-2673e43eaa91.png\\\" width=\\\"531\\\" height=\\\"226\\\"></a>\"}],[\"code\",{\"code\":\"[TestClass]\\npublic class FFTests : PartsTests\\n{\\n    public FFTests()\\n        : base(DesiredCapabilities.Firefox())\\n    {\\n    }\\n\\n    [TestMethod]\\n    [TestCategory(\\\"Firefox\\\")]\\n    public void Firefox_AddToCartTest()\\n    {\\n        AddToCartTest();\\n    }\\n\\n    [TestMethod]\\n    [TestCategory(\\\"Firefox\\\")]\\n    public void Firefox_SearchTest()\\n    {\\n        SearchTest();\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"code\",{\"code\":\"[TestClass]\\npublic class ChromeTests : PartsTests\\n{\\n    public ChromeTests()\\n        : base(DesiredCapabilities.Chrome())\\n    {\\n    }\\n\\n    [TestMethod]\\n    [TestCategory(\\\"Chrome\\\")]\\n    public void Chrome_AddToCartTest()\\n    {\\n        AddToCartTest();\\n    }\\n\\n    [TestMethod]\\n    [TestCategory(\\\"Chrome\\\")]\\n    public void Chrome_SearchTest()\\n    {\\n        SearchTest();\\n    }\\n}\\n\",\"language\":\"csharp;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/063848b9-a77a-4d82-9599-5f0f1a93e6fd.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a91a95b8-e92d-4169-be8c-a2d36841ae4e.png\\\" width=\\\"197\\\" height=\\\"384\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/063e7832-6a22-4301-82d8-ffd0a5500848.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4c174b99-e3a1-4338-bd68-d31330d9c685.png\\\" width=\\\"438\\\" height=\\\"413\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/62310234-41c7-43d3-a605-e93e7305d975.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/5b12a2c2-31a3-4021-a3aa-2c4954bd1f8e.png\\\" width=\\\"484\\\" height=\\\"150\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/0cdad628-a3c8-441f-a3c4-792b083876c0.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4787dfc5-12c6-4e58-b622-4b341faa045b.png\\\" width=\\\"483\\\" height=\\\"164\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/b6291e37-1f26-4a65-95df-157ebdd9e331.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/a6fbc201-467d-4245-861b-6f2587ca0442.png\\\" width=\\\"395\\\" height=\\\"134\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/d071ed68-ff2f-40bd-a2ab-fa0546d06384.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b61298b8-5bcb-4b90-807c-c7359980dac9.png\\\" width=\\\"296\\\" height=\\\"393\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c9ece48f-a63a-4456-8425-6f6b97568a6f.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/3bb098b4-bff9-4e95-9827-41a41ef2cbb5.png\\\" width=\\\"473\\\" height=\\\"184\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/19d774e5-cff0-4176-b6c0-e82345d97710.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b5a7c052-4c80-443d-80c4-b1be4d03bb29.png\\\" width=\\\"811\\\" height=\\\"215\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/5c5aab1b-4cfe-43ea-8307-70ede93b4d82.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/26ceccc9-a9b0-456e-b261-fcbd44831f82.png\\\" width=\\\"653\\\" height=\\\"323\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/SeleniumHQ/selenium/wiki/Grid2\"]],[\"a\",[\"href\",\"http://www.seleniumhq.org/\"]],[\"em\"],[\"a\",[\"href\",\"http://goo.gl/EoH85x\"]],[\"a\",[\"href\",\"http://selenium-release.storage.googleapis.com/2.53/IEDriverServer_Win32_2.53.1.zip\"]],[\"a\",[\"href\",\"http://chromedriver.storage.googleapis.com/index.html?path=2.22/\"]],[\"a\",[\"href\",\"http://docs.seleniumhq.org/download/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"There are several different types of test – unit tests, functional test, load tests and so on. Generally, unit tests are the easiest to implement and have a high return on investment. Conversely, UI automation tests tend to be incredibly fragile, hard to maintain and don’t always deliver a huge amount of value. However, if you carefully design a good UI automation framework (especially for web testing) you can get some good mileage.\"]]],[1,\"p\",[[0,[],0,\"Even if you get a good framework going, you’re going to want to find ways of executing the tests in parallel, since they can take a while. Enter \"],[0,[0],1,\"Selenium Grid\"],[0,[],0,\".  \"],[0,[1],1,\"Selenium\"],[0,[],0,\" is a “web automation framework”. Under the hood, Selenium actually runs as a server which accepts ReST commands – those commands are wrapped in the Selenium client, so you usually see these HTTP commands. However, since the server is capable of driving a browser via HTTP, you can run tests \"],[0,[2],1,\"remotely\"],[0,[],0,\" – that is, have tests run on one machine that execute over the wire to a browser on another machine. This allows you to scale your test infrastructure (by adding more machines). This is exactly what Selenium Grid does for you.\"]]],[1,\"p\",[[0,[],0,\"My intention with this post isn’t to show how you can create Selenium tests. Rather, I’ll show you some ins and outs of running Selenium tests in parallel in a Grid in a VSTS build/release pipeline.\"]]],[1,\"h2\",[[0,[],0,\"Components for Testing in a Grid\"]]],[1,\"p\",[[0,[],0,\"There are a few moving parts we’ll need to keep track of in order for this to work:\"]]],[3,\"ol\",[[[0,[],0,\"The target site\"]],[[0,[],0,\"The tests\"]],[[0,[],0,\"The Selenium Hub (the master that coordinates the Grid)\"]],[[0,[],0,\"The Selenium Nodes (the machines that are going to be executing the tests)\"]],[[0,[],0,\"Selenium drivers\"]],[[0,[],0,\"VSTS Build\"]],[[0,[],0,\"VSTS Release\"]]]],[1,\"p\",[[0,[],0,\"The cool thing about a Selenium grid is that, from a test perspective, you only need to know about the Selenium Grid hub. The nodes register with the hub and await commands to execute (i.e. run tests). The tests themselves just target the hub: “Hey Hub, I’ve got this test I want you to run for me on a browser with these capabilities…”. The hub then finds a node that meets the required capabilities and executes the tests remotely (via HTTP) on the node. Pretty sweet. This means you can scale the grid out without having to modify the tests at all.\"]]],[1,\"p\",[[0,[],0,\"Again lifting Selenium’s skirts we’ll see that a Selenium Node receives an instruction (like “use Chrome and navigate to google.com”). The node uses a driver for each browser (Firefox doesn’t have a driver, since Selenium knows how to drive it “natively”) to drive the browser. So when you configure a grid, you need to configure the Selenium drivers for each browser you want to test with on the grid (and by configure I mean copy to a folder).\"]]],[1,\"h2\",[[0,[],0,\"Setting Up a Selenium Grid\"]]],[1,\"p\",[[0,[],0,\"In experimenting with the grid, I decided to set up a two-machine Grid in Azure. Selenium Server (used to run the Hub and the Nodes) is a java application, so it’ll run wherever you can run Java. So I spun up a Resource Group with two VMs (Windows 2012 R2) and installed Java (and added the bin folder to the Path), Chrome and Firefox. I then downloaded the \"],[0,[3],1,\"Selenium Server jar file\"],[0,[],0,\", the \"],[0,[4],1,\"IE driver\"],[0,[],0,\" and the \"],[0,[5],1,\"Chrome driver\"],[0,[],0,\" (you can see the instructions on installing \"],[0,[6],1,\"here\"],[0,[],0,\"). I put the IE and Chrome drivers into c:\\\\selenium\\\\drivers on both machines.\"]]],[1,\"p\",[[0,[],0,\"I wanted one machine to be the Hub and run Chrome/Firefox tests, and have the other machine run IE/Firefox tests (yes, Nodes can run happily on the same machine or even on the same machine as the Hub). There are a myriad of options you can specify when you start a Hub or Node, so I scripted a general config that I thought would work as a default case.\"]]],[1,\"p\",[[0,[],0,\"To start the Hub, I created a one-line bat file in c:\\\\selenium\\\\server folder (where I put the server jar file):\"]]],[10,0],[1,\"p\",[[0,[],0,\"This command starts up the Hub using port 4444 (the default) on the machine. Don’t forget to open the firewall for this port!\"]]],[1,\"p\",[[0,[],0,\"Configuring the nodes took a little longer to work out. The documentation is a bit all over the place (and ambiguous) so I eventually settled on putting some config in a JSON file and some I pass in to the startup command. Here’s the config JSON I have for the IE node:\"]]],[10,1],[1,\"p\",[[0,[],0,\"This is only the bare minimum of config that you can specify – there are tons of other options that I didn’t need to bother with. All I wanted was for the Node to be able to run Firefox and IE tests. In a similar vein I specify the config for the Chrome node:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can see this is almost identical to the config of the IE node, except for the second browser type.\"]]],[1,\"p\",[[0,[],0,\"I saved these files as ieNode.json and chromeNode.json in c:\\\\selenium\\\\server\\\\configs respectively.\"]]],[1,\"p\",[[0,[],0,\"Then I created a simple PowerShell script that would let me start a node:\"]]],[10,3],[1,\"p\",[[0,[],0,\"So now I can run the following command on the Hub machine to start a node:\"]]],[10,4],[1,\"p\",[[0,[],0,\"This will start a node that can run Chrome/Firefox tests using drivers in the c:\\\\selenium\\\\server\\\\drivers path running on port 5555. On the other machine, I copied the same files and just ran this command:\"]]],[10,5],[1,\"p\",[[0,[],0,\"This time the node isn’t on the same machine as the Hub, so I used the Azure vNet internal IP address of the Hub – I also specified I want IE/Firefox tests to run on this node.\"]]],[1,\"p\",[[0,[],0,\"Of course I made sure that all these files are in source control!\"]]],[1,\"p\",[[0,[],0,\"Again I had to make sure that the ports I specify were allowed in the Firewall. I just created a single Firewall rule to allow TCP traffic on ports 4444, 5550-5559 on both machines.\"]]],[10,6],[1,\"p\",[[0,[],0,\"I also opened those ports in the Azure network security group that both machines’ network cards are connected to:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Now I can browse to the Selenium console of my Grid:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Now my hub is ready to run tests!\"]]],[1,\"h2\",[[0,[],0,\"Writing Tests for Grid Execution\"]]],[1,\"p\",[[0,[],0,\"The Selenium Grid is capable of running tests in parallel, spreading tests across the grid. Spoiler alert: it doesn’t run tests in parallel. I can hear you now thinking, “What?!? You just said it can run tests in parallel, and now you say it can’t!”. Well, the grid can spread as many tests as you throw at it – but you have to parallelize the tests yourself!\"]]],[1,\"p\",[[0,[],0,\"It turns out that you can do this in Visual Studio 2015 and VSTS – but it’s not pretty. If you open up the Test Explorer Window, you’ll see an option to “Run Tests In Parallel” in the toolbar (next to the Group By button):\"]]],[10,9],[1,\"p\",[[0,[],0,\"Again I hear you thinking: “Just flip the switch! Easy!” Whoa, slow down, Dear Reader – it’s not that easy. You have to consider the \"],[0,[2],1,\"unit\"],[0,[],0,\" of parallelization. In other words – what does it mean to “Run Tests In Parallel”? Well, Visual Studio runs different \"],[0,[2],1,\"assemblies\"],[0,[],0,\" in parallel. Which means that you have to have at least two test projects (assemblies) in order to get any benefit.\"]]],[1,\"p\",[[0,[],0,\"In my case I had two tests that I wanted to run against three browsers – IE, Chrome and Firefox. Of course if you have several hundred tests, you probably have them in different assemblies already – hopefully grouped by something meaningful. In my case I chose to group the tests by browser. Here’s what I ended up doing:\"]]],[3,\"ol\",[[[0,[],0,\"Create a base (abstract) class that contains the Selenium test methods (the actual test code)\"]],[[0,[],0,\"Create three additional projects – one for each browser type – that contains a class that derives from the base class\"]],[[0,[],0,\"Run in parallel\"]]]],[1,\"p\",[[0,[],0,\"It’s a pity really – since the Selenium libraries abstract the test away from the actual browser. That means you can run the same test against any browser that you have a driver for! However, since we’re going to run tests in a Hub, we need to use a special driver called a RemoteWebDriver. This driver is going to connect the test to the hub using “capabilities” that we define (like what browser to run in).\"]]],[1,\"p\",[[0,[],0,\"Let’s consider an example test. Here’s a test I created to check the Search functionality of my website:\"]]],[10,10],[1,\"p\",[[0,[],0,\"This is a pretty simple test – and as I mentioned before, this post isn’t about the Selenium tests themselves as much as it is about \"],[0,[2],1,\"running\"],[0,[],0,\" the tests in a build/release pipeline – so excuse the simple nature of the test code. However, I have to show you some code in order to show you how I got the tests running successfully in the grid.\"]]],[1,\"p\",[[0,[],0,\"You can see how the code assumes that there is a “driver” object and that it is instantiated? There’s also a “baseUrl” object. Both of these are essential to running tests in the grid: the driver is an instantiated RemoteWebDriver object that connects us to the Hub, while baseUrl is the base URL of the site we’re testing.\"]]],[1,\"p\",[[0,[],0,\"The base class is going to instantiate a RemoteWebDriver for each test (in the test initializer). Each child (test) class is going to specify what capabilities the driver should be instantiated with. The driver constructor needs to know the URL of the grid hub as well as the capabilities required for the test. Here’s the constructor and test initializer in the base class:\"]]],[10,11],[1,\"p\",[[0,[],0,\"The constructor takes an ICapabilities object which will allow us to specify how we want the test run (or at least which browser to run against). We hold on to these capabilities. The SetupTest() method then reads the “gridUrl” and the “baseUrl” from the TestContext properties (defaulting values if none are present). Finally it created a new RemoteWebDriver using the gridUrl and capabilities. The Teardown() method calls the driver Quit() method, which closes the browser (ignoring errors) and checks that there are no verification errors. Pretty standard stuff.\"]]],[1,\"p\",[[0,[],0,\"So how do we pass in the gridUrl and baseUrl? To do that we need a runsettings file – this sets the value of the parameters in the TestContext object.\"]]],[1,\"p\",[[0,[],0,\"I added a new XML file to the base project called “selenium.runsettings” with the following contents:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Again I’m using default values for the values of the parameters – this is how I debug locally. Note the “/wd/hub” on the end of the grid hub URL.\"]]],[1,\"p\",[[0,[],0,\"Now I can set the runsettings file in the Test menu:\"]]],[10,13],[1,\"p\",[[0,[],0,\"So what about the child test classes? Here’s what I have for the Firefox tests:\"]]],[10,14],[1,\"p\",[[0,[],0,\"I’ve prepended the test name with Firefox_ (you’ll see why when we run the tests in the release pipeline). I’ve also added the [TestClass] and [TestMethod] attributes as well as [TestCategory]. This is using the MSTest framework, but the same will work with nUnit or xUnit too. Unfortunately the non-MSTest frameworks don’t have the TestContext, so you’re going to have to figure out another method of providing the baseUrl and gridUrl to the test. The constructor is using a vanilla Firefox capability for this test – you can instantiate more complex capabilities if you need them here.\"]]],[1,\"p\",[[0,[],0,\"Just for comparison, here’s my code for the ChromeTests file:\"]]],[10,15],[1,\"p\",[[0,[],0,\"You can see it’s almost identical except for the class name prefix, the [TestCategory] and the capabilities in the constructor.\"]]],[1,\"p\",[[0,[],0,\"Here’s my project layout:\"]]],[10,16],[1,\"p\",[[0,[],0,\"At this point I can run tests (in parallel) against my “local” grid (I started a hub and two nodes locally to test). Next we need to put all of this into a build/release pipeline.\"]]],[1,\"h2\",[[0,[],0,\"Creating a Build\"]]],[1,\"p\",[[0,[],0,\"You could run the tests during the build, but I wanted my UI tests to be run against a test site that I deploy to, so I felt it more appropriate to run the tests in the release. Before we get to that, we have to build the application (and test code) so that it’s available in the release.\"]]],[1,\"p\",[[0,[],0,\"I committed all the code to source control in VSTS and created a build definition. Here’s what the tasks look like:\"]]],[10,17],[1,\"p\",[[0,[],0,\"The first three steps are for building the application and the solution – I won’t bore you with the details. Let’s look at the next five steps though:\"]]],[1,\"p\",[[0,[],0,\"The first three “Copy Files” steps copy the binaries for the three test projects I want (one for IE, for Chrome and Firefox):\"]]],[10,18],[1,\"p\",[[0,[],0,\"In each case I’m copying the compiled assemblies of the test project (from say test/PartsUnlimitedSelenium.Chrome\\\\bin\\\\$(BuildConfiguration) to $(Build.ArtifactStagingDirectory)\\\\SeleniumTests.\"]]],[1,\"p\",[[0,[],0,\"The fourth copy task copies the runsettings file:\"]]],[10,19],[1,\"p\",[[0,[],0,\"The final task publishes the $(Build.ArtifactStagingDirectory) to the server:\"]]],[10,20],[1,\"p\",[[0,[],0,\"After running the build, I have the following drop:\"]]],[10,21],[1,\"p\",[[0,[],0,\"The “site” folder contains the webdeploy package for my site – but the important bit here is that all the test assemblies (and the runsettings file) are in the SeleniumTests folder.\"]]],[1,\"h2\",[[0,[],0,\"Running Tests in the Release\"]]],[1,\"p\",[[0,[],0,\"Now that we have the app code (the site) and the test assemblies in a drop location, we’re ready to define a Release. In the release for Dev I have the following steps:\"]]],[10,22],[1,\"p\",[[0,[],0,\"I have all the steps that I need to deploy the application (in this case I’m deploying to Azure). Again, that’s not the focus of this post. The Test Assemblies task is the important step to look at here:\"]]],[10,23],[1,\"p\",[[0,[],0,\"It turns out to be pretty straightforward. I just make sure that “Test Assembly” includes all the assemblies I want to execute – remember you need at least two in order for “Run In Parallel” to have any effect. For Filter Criteria I’ve excluded IE tests – IE tests seem to fail for all sorts of arbitrary reasons that I couldn’t work out – you can leave this empty (or put in a positive expression) if you want to only run certain tests. I specify the path to the runsettings file, and then in “Override TestRun Parameters” I specify the gridUrl and baseUrl that I want to test in this particular environment. I’ve used variables that I define on the environment so that I can clone this for other environments if I need to.\"]]],[1,\"p\",[[0,[],0,\"Now when I release, I see that the tests run as part of the release. Clicking on the Tests tab I see the test results. I changed the Outcome filter to show Passed tests and configured the columns to show the “Date started” and “Date completed”. Sure enough I can see that the tests are running in parallel:\"]]],[10,24],[1,\"p\",[[0,[],0,\"Now you can see why I wanted to add the prefix to the test names – this lets me see exactly which browsers are behaving and which aren’t (ahem, IE).\"]]],[1,\"h2\",[[0,[],0,\"Final Thoughts\"]]],[1,\"p\",[[0,[],0,\"Running Selenium Tests in a Grid in VSTS is possible –  there are a few hacks required though. You need to create multiple assemblies in order to take advantage of the grid scalability, and this can lead to lots of duplicated and error-prone code (for example when I initially created the Firefox tests, I copied the Chrome class and forgot to change the prefix and [TestCategory] which lead to interesting results). There are probably other ways of dividing your tests into multiple assemblies, and then you could pass the browser in as a Test Parameter and have multiple runs – but then the runs wouldn’t be simultaneous across browsers. A final gotcha is that the runsettings only work for MSTest – if you’re using another framework, chances are you’ll end up creating a json file that you read when the tests start.\"]]],[1,\"p\",[[0,[],0,\"You can see that there are challenges whichever way you slice it up. Hopefully the work the test team is doing in VSTS/TFS will improve this story at some stage.\"]]],[1,\"p\",[[0,[],0,\"For now, happy Grid testing!\"]]]]}","published_at":1469755464000,"status":"published","published_by":1},{"id":"a8dea587-aec4-47ca-9315-be1c5e333c5d","title":"Pimp Your Consoles on Windows","slug":"pimp-your-consoles-on-windows","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"git clone https://github.com/powerline/fonts.git\\n.\\\\install.ps1\\n\",\"language\":\"powershell;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Install-Module Posh-Git</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Install-Module Oh-My-Posh<font face=\\\"Calibri\\\">. Once both modules are installed, you need to edit your <font face=\\\"Courier New\\\">$PROFILE</font> (you can run <font face=\\\"Courier New\\\">code $PROFILE</font> to quickly open your profile file in VSCode). Add the following lines:</font></font>\"}],[\"code\",{\"code\":\"Install-Module posh-git\\nInstall-Module oh-my-posh\\nSet-Theme Paradox\\n\",\"language\":\"powershell;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Get-Theme</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a1a635c3-cc83-461d-a821-fcbb04a06ec0.png\\\" target=\\\"_blank\\\"><img width=\\\"699\\\" height=\\\"308\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f829659d-0185-4c9f-bfb0-8c663b615448.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">omf install agnoster</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">omf install bobthefish</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/413ad7e5-a037-4fad-9691-41fb8798fef6.png\\\" target=\\\"_blank\\\"><img width=\\\"683\\\" height=\\\"387\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1b0ceee7-2ab0-4dfb-a12d-2ba8bcfcd985.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.powershellgallery.com/packages/posh-git/0.7.1\"]],[\"a\",[\"href\",\"https://www.powershellgallery.com/packages/oh-my-posh/2.0.225\"]],[\"a\",[\"href\",\"https://github.com/fish-shell/fish-shell\"]],[\"a\",[\"href\",\"https://github.com/oh-my-fish/oh-my-fish\"]],[\"a\",[\"href\",\"https://github.com/neilpa/cmd-colors-solarized\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I spend a fair amount of time in consoles - specifically PowerShell and Bash (Windows Subsystem for Linux) on my Windows 10 machine. I also work with Git - a lot. So having a cool console that is Git aware is a must. I always recommend \"],[0,[0],1,\"Posh-Git\"],[0,[],0,\" (a PowerShell prompt that shows you which Git branch you're on as well as the branch status). At Ignite this I saw some zsh consoles in VS Code on Mac machines. So I wondered if I could get my consoles to look as cool. And it's not just about the looks - seeing your context in the console is a productivity booster!\"]]],[1,\"p\",[[0,[],0,\"It turns out that other than installing some updated fonts for both PowerShell and Bash, you can get pretty sweet consoles fairly easily.\"]]],[1,\"h2\",[[0,[],0,\"Updating Fonts\"]]],[1,\"p\",[[0,[],0,\"The fonts that the custom shells use are UTF-8, so you'll need UTF-8 fonts installed. You'll also need so-called \\\"powerline\\\" fonts. Fortunately, there's a simple script you can run to install a whole bunch of cool fonts that will work nicely on your consoles.\"]]],[1,\"p\",[[0,[],0,\"Here are the steps for installing the fonts. Open a PowerShell and enter the following commands:\"]]],[10,0],[1,\"p\",[[0,[],0,\"This took about 5 minutes on my machine.\"]]],[1,\"h2\",[[0,[],0,\"PowerShell\"]]],[1,\"p\",[[0,[],0,\"So to pimp out your PowerShell console, you'll need to install a couple modules: \"],[0,[0],1,\"Posh-Git\"],[0,[],0,\" and \"],[0,[1],1,\"Oh-My-Posh\"],[0,[],0,\". Run\"]]],[10,1],[1,\"p\",[[0,[],0,\"and\"]]],[10,2],[10,3],[1,\"p\",[[0,[],0,\"You can of course choose different themes - run\"]]],[10,4],[1,\"p\",[[0,[],0,\"to get a list of themes. One last thing to do - set the background color of your PowerShell console to black (I like to make the opacity 90% too).\"]]],[1,\"p\",[[0,[],0,\"Now if you cd to a git repo, you'll get a Powerline status. Sweet!\"]]],[10,5],[1,\"h2\",[[0,[],0,\"Bash\"]]],[1,\"p\",[[0,[],0,\"You can do the same thing for your Bash console. I like to use \"],[0,[2],1,\"fish shell\"],[0,[],0,\" so you'll have to install that first. Once you have fish installed, you can install \"],[0,[3],1,\"oh-my-fish\"],[0,[],0,\" - a visual package manager for fish (and yes, oh-my-posh is a PowerShell version of oh-my-fish). Once oh-my-fish is installed, use it to install themes. You can install agnoster by running\"]]],[10,6],[1,\"p\",[[0,[],0,\"- I like bobthefish, so I just run\"]]],[10,7],[1,\"p\",[[0,[],0,\". Now my bash console is pimped too!\"]]],[10,8],[1,\"h2\",[[0,[],0,\"Solarized Theme\"]]],[1,\"p\",[[0,[],0,\"One more change you may want to make: update your console colors to the Solarized theme. To do that, follow the instructions from \"],[0,[4],1,\"this repo\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"If you're going to work in a console frequently, you may as well work in a pretty one! Oh-My-Fish and Oh-My-Posh let you quickly and easily get great-looking consoles, and Posh-Git adds in Git context awareness. What's not to love?\"]]],[1,\"p\",[[0,[],0,\"Happy console-ing!\"]]]]}","published_at":1538171135000,"status":"published","published_by":1},{"id":"f1f1aa7d-fb02-4b54-bece-9aab22d7b797","title":"PowerShell DSC: Remotely Configuring a Node to “RebootNodeIfNeeded”","slug":"powershell-dsc-remotely-configuring-a-node-to-rebootnodeifneeded","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"Configuration SomeConfig\\n{\\n   Node someMachine\\n   {\\n      LocalConfigurationManager\\n      {\\n         RebootNodeIfNeeded = $true\\n      }\\n   }\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"Configuration ConfigureRebootOnNode\\n{\\n    param (\\n        [Parameter(Mandatory=$true)]\\n        [ValidateNotNullOrEmpty()]\\n        [String]\\n        $NodeName\\n    )\\n\\n    Node $NodeName\\n    {\\n        LocalConfigurationManager\\n        {\\n            RebootNodeIfNeeded = $true\\n        }\\n    }\\n}\\n\\nWrite-Host \\\"Creating mofs\\\"\\nConfigureRebootOnNode -NodeName fabfiberserver -OutputPath .\\\\rebootMofs\\n\\nWrite-Host \\\"Starting CimSession\\\"\\n$pass = ConvertTo-SecureString \\\"P2ssw0rd\\\" -AsPlainText -Force\\n$cred = New-Object System.Management.Automation.PSCredential (\\\"administrator\\\", $pass)\\n$cim = New-CimSession -ComputerName fabfiberserver -Credential $cred\\n\\nWrite-Host \\\"Writing config\\\"\\nSet-DscLocalConfigurationManager -CimSession $cim -Path .\\\\rebootMofs -Verbose\\n\\n# read the config settings back to confirm\\nGet-DscLocalConfigurationManager -CimSession $cim\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/be427703-7d74-4ad9-9ccf-949db8db04fd.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f7e44cac-1732-4bbc-ac1c-8405fa8a1d06.png\\\" width=\\\"644\\\" height=\\\"342\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.technet.com/b/privatecloud/archive/2013/08/30/introducing-powershell-desired-state-configuration-dsc.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/visualstudioalm/archive/2014/05/22/release-management-for-microsoft-visual-studio-2013-with-update-3-ctp1-is-live.aspx\"]],[\"a\",[\"href\",\"http://technet.microsoft.com/en-us/library/dn249922.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve started to experiment a bit with some \"],[0,[0],1,\"PowerShell DSC\"],[0,[],0,\" – mostly because it’s now \"],[0,[1],1,\"supported in Release Management\"],[0,[],0,\" (in Update 3 CTP at least).\"]]],[1,\"p\",[[0,[],0,\"Sometimes when you apply a configuration to a node (machine), the node requires a reboot (for example adding .NET4.5 requires the node to reboot). You can configure the node to reboot immediately (instead of just telling you “a reboot is required”) by changing a setting in the node’s LocalConfigurationManager. Of course, since this is configuration, it’s tempting to try to do this in a DSC script – for example:\"]]],[10,0],[1,\"p\",[[0,[],0,\"This configuration “compiles” to a mof file and you can apply it successfully. However, it doesn’t actually do anything.\"]]],[1,\"h2\",[[0,[],0,\"Set-DscLocalConfigurationManager on a Remote Node\"]]],[1,\"p\",[[0,[],0,\"Fortunately, there is a way to change the settings on the LocalConfigurationManager remotely – you use the cmdlet Set-DscLocalConfigurationManager with a CimSession object (i.e. you invoke it remotely). I stumbled across this when looking at the documentation for \"],[0,[2],1,\"DSC Local Configuration Manager\"],[0,[],0,\" where the very last sentence says “To see the current Local Configuration Manager settings, you can use the Get-DscLocalConfigurationManager cmdlet. If you invoke this cmdlet with no parameters, by default it will get the Local Configuration Manager settings for the node on which you run it. To specify another node, use the CimSession parameter with this cmdlet.”\"]]],[1,\"p\",[[0,[],0,\"Here’s a script that you can modify to set “RebootNodeIfNeeded” on any node:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Just replace “fabfiberserver” with your node name and .\\\\ the script. The last line of the script reads back the LocalConfigurationManager settings on the remote node, so you should see the RebootNodeIfNeeded setting is true.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Happy configuring!\"]]]]}","published_at":1403800519000,"status":"published","published_by":1},{"id":"ccb6479b-d5ee-4c54-8fcb-6c001c609397","title":"Presenting @ DevDays SA 2011: Cape Town and Johannesburg","slug":"presenting--devdays-sa-2011-cape-town-and-johannesburg","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://www.microsoft.com/southafrica/devdays/\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/ms182532.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd172118.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd998313(VS.100).aspx\"]],[\"a\",[\"href\",\"http://research.microsoft.com/en-us/projects/pex/\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd286729.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/edglas/archive/2010/03/24/web-test-authoring-and-debugging-techniques-for-visual-studio-2010.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/library/dd293540.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd286726.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd647547.aspx#run_tests\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/ff972305.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I am going to be co-presenting with Ahmed Salijee (a Development Solution Specialist) from Microsoft South Africa at \"],[0,[0],1,\"DevDays\"],[0,[],0,\" later this month. We’re going to be showing off a ton of TFS and VS 2010 capabilities around testing applications. Here’s the official blurb for our sessions:\"]]],[1,\"p\",[[0,[],0,\"Visual Studio 2010 offers a wide range of software testing capabilities: manual testing, automated UI testing, database testing, low-level unit testing and even performance testing tools capable of simulating thousands of active users. It also provides support for test case management, defect tracking as well as configuring and running your tests in virtual and physical environments. But how do you know which tools to choose for your project? This session will, via a demo scenario, walk you through the various testing capabilities to assist you with the most effective use of Visual Studio 2010's testing capabilities. This session will be useful to developers as well as various tester roles including functional, performance and automation.\"]]],[1,\"p\",[[0,[],0,\"We’re going to start off with a simple web application: IBuySpy. This is a reference application for an ASP.NET store. We’re going to do some refactoring and then showcase:\"]]],[3,\"ul\",[[[0,[1],1,\"Unit Testing\"],[0,[],0,\"\"]],[[0,[2],1,\"Database Testing\"],[0,[],0,\"\"]],[[0,[3],1,\"Test Driven Development (TDD) using “Generate from Usage”\"],[0,[],0,\"\"]],[[0,[4],1,\"Moles and Pex\"],[0,[],0,\" for isolation and automated code coverage discovery\"]],[[0,[5],1,\"Functional Testing\"],[0,[],0,\"\"]],[[0,[6],1,\"Web\"],[0,[],0,\" and \"],[0,[7],1,\"Load Testing\"],[0,[],0,\"\"]],[[0,[8],1,\"Coded UI Testing\"],[0,[],0,\"\"]],[[0,[9],1,\"Running Automated Tests during Builds\"],[0,[],0,\"\"]],[[0,[],0,\"Lab Management \"],[0,[10],1,\"Build-Deploy-Test Workflow\"],[0,[],0,\" for Automated Testing\"]]]],[1,\"p\",[[0,[],0,\"Hope to see you there!\"]]]]}","published_at":1305066480000,"status":"published","published_by":1},{"id":"c8cebfd3-7a89-4dd9-a8fe-e057705abcb7","title":"Presenting at TechEd Africa 2011","slug":"presenting-at-teched-africa-2011","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://www.teched.co.za/\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd264915.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd409365.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/southafrica/archive/2010/05/28/devdays-2010-guest-blog-post-ahmed-salijee.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/briankel/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"It’s a little over 3 weeks until \"],[0,[0],1,\"Tech Ed Africa 2011\"],[0,[],0,\" starts (it runs from 17 to 20 October). I’ll be presenting a two topics on the Development Track\"]]],[3,\"ul\",[[[0,[],0,\"Demystifying Debugging with Visual Studio 2010 and \"],[0,[1],1,\"IntelliTrace\"],[0,[],0,\"\"]],[[0,[],0,\"Understanding Your Systems with \"],[0,[2],1,\"Architectural Discovery\"],[0,[],0,\" in Microsoft Visual Studio 2010\"]]]],[1,\"p\",[[0,[],0,\"I’ll also be doing a session with \"],[0,[3],1,\"Ahmed Salijee\"],[0,[],0,\" from MS South Africa on the Virtualization Track covering Lab Management.\"]]],[1,\"p\",[[0,[],0,\"The other presenters in the ALM sessions include my boss, Chris Menegay, as well as \"],[0,[4],1,\"Brian Keller\"],[0,[],0,\" from MS in the US.\"]]],[1,\"p\",[[0,[],0,\"If you haven’t registered yet, make sure you do! Feel free to catch me at any time if you have questions about ALM using TFS and VS 2010 (or 2011, for that matter!)\"]]]]}","published_at":1317163440000,"status":"published","published_by":1},{"id":"3d6ba9d8-3575-4066-b1a4-9b2dd4bc0506","title":"Project Plans, Agile, Pizza and Startups","slug":"project-plans-agile-pizza-and-startups","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-Ano-qO91NKg/U0uhHfRlt2I/AAAAAAAABR4/552TpI5ifeQ/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-jVMD_3vJvH0/U0uhIocjoUI/AAAAAAAABR8/KPbzX0nT3qA/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"369\\\" height=\\\"292\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2014/04/integrating-tfs-and-project-server-two.html\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Last week I \"],[0,[0],1,\"posted about how to integrate TFS and Project Server “manually”\"],[0,[],0,\". In the post I did put in a bit of philosophy about why I think project plans can be a Bad Thing. Prasanna Adavi posted a thoughtful comment on my philosophy, and I wanted to reply just as thoughtfully, so I decided to add the reply as a new post rather than just reply to the comments inline.\"]]],[1,\"p\",[[0,[],0,\"Here is Prasanna’s comment:\"]]],[1,\"p\",[[0,[],0,\"“I disagree with you on your notion of how \\\"project plans\\\" work. It is wrong to assume that just because a project manager plans and baselines a project, they will never veer from it. To the contrary, It is an attempt to make the customer think before hand as to what they perceive as the value, and then think twice before they \\\"change\\\" the requirements. A good project manager is never afraid to reset the baseline if it is necessary to do so.\"]]],[1,\"p\",[[0,[],0,\"Moreover, who said that a \\\"waterfall\\\" has to be one huge waterfall? Depending upon the nature of the project (especially Software Projects), they can be scheduled as multiple mini-waterfalls, yielding quick, unique features. Actually, the way I see Agile is multiple mini-waterfalls. But within each waterfall, there needs to be a commitment. Leaving it open ended and reworking things again and again, because customer does not know what he wants, is not really 'embracing change'.\"]]],[1,\"p\",[[0,[],0,\"Think about this: When you order a pizza, do you 'commit' to certain toppings, or do you say lets start making the thing, and we will add toppings as we see the value?\"]]],[1,\"p\",[[0,[],0,\"And finally, If your entire team or company is working on one software project/feature set, then it is great to think in terms of \\\"Agile\\\". But when you are on a 'agile' project that never ends because a customer constantly keeps seeing 'value' in the latest change, that is not a project anymore, and it becomes quite difficult to commit to anything else.”\"]]],[1,\"h2\",[[0,[],0,\"Response\"]]],[1,\"p\",[[0,[],0,\"Of course when I speak about Project Managers and Project Plans, I generalize. Some Project Managers are obviously better than others. In my experience, however, Project Managers that manage software projects still use the wrong paradigm when they produce a plan – and the fundamental paradigm of a Project Plan is that \"],[0,[1],1,\"we know what we’re building\"],[0,[],0,\" and we know \"],[0,[1],1,\"how long each task is going to take\"],[0,[],0,\". If either of these suppositions is incorrect, then the Project Plan’s use diminishes dramatically. Of course in Software Development, it’s nearly always the case that at the beginning of a project (when we’re supposed to be creating the plan) that requirements cannot be known fully – and we can guess how long each task will take, but it’s just that – a guess.\"]]],[1,\"p\",[[0,[],0,\"I like to show a diagram called the “Cone of Uncertainty” when I talk about Project Planning:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The Cone shows how the larger the “thing” we’re estimating is (and the further in the future the end date will be), the more uncertainty there is in that estimation. So planning something small right now has little variance – we might say that changing a table in a database will take 2 hours (with about half an hour variance). But if we plan something big (like a Project) and estimate 3 months, then the variance is likely to be +- 1.5 months. We just don’t know enough now to be more accurate.\"]]],[1,\"p\",[[0,[],0,\"This is why Agile is about working in small sprints (or iterations) between 1 and 4 weeks in length. That time period is a good fit for the kind of estimations that generally apply to detailed tasks on a software project. Planning further ahead (in detail) is simply wasted effort. Of course, if you don’t have any plan further out than your current sprint, that’s a bad thing too – you need to be working towards some larger goal or initiative. So Prasanna is theoretically correct that Agile is a series of mini-waterfalls – in the sense that the larger Project is divided up into smaller iterations. However, to put a lot of effort into Planning all the iterations in detail is simply a waste of time – things change so rapidly that by the time you’re half-way into iteration 1, the plan for iteration 3 is obsolete.\"]]],[1,\"p\",[[0,[],0,\"So being Agile doesn’t imply “not planning”. It just says, to use the 80/20 principle, put 80% of your planning effort into the 20% that matters most – in this case, that’s the current sprint. So plan in detail your next sprint, but wait till you’re nearing the end of sprint 1 before you plan sprint 2 and so on.\"]]],[1,\"p\",[[0,[],0,\"Let’s imagine that you’re on a team working in 2 week sprints. Now if you’re only planning in detail 2 weeks at a time, do you really need a Project Plan? Isn’t a list of requirements and their tasks (a backlog) enough?\"]]],[1,\"p\",[[0,[],0,\"The point I was making on my previous post was that putting a lot of effort into a large and detailed project plan, and then artificially sticking to that plan, is a waste of effort. And if you’re going to change your baseline often, then why baseline in the first place? Rather plan in detail in the short term (the next sprint) and let changes that come from that spring influence the detailed plan for the next.\"]]],[1,\"p\",[[0,[],0,\"Another point is that when you adjust your sprint based on feedback, then you’re “responding to change”. Normally a baseline changes because things are going to take longer than you expected – so the change isn’t based on change (or value), it’s based on poor estimates.\"]]],[1,\"h2\",[[0,[],0,\"Commitment\"]]],[1,\"p\",[[0,[],0,\"Of course just because you’re an Agile team that can respond to change rapidly, that doesn’t mean that you’re at the mercy of the customer’s whims. One of the principles of Agile is to \"],[0,[1],1,\"lock down the sprint\"],[0,[],0,\". So if the customer does decide to change things, they’ll have to wait till at least the next sprint. And of course, since the Product Owner (who represents the customer) is the one that needs to prioritize work, they’ll have to bump something off the backlog if a new change or requirement is added in. In fact, I’d go so far as to say that if your team is doing Agile correctly, you’ll get fewer change requests from your customers and they’ll soon learn to be a lot clearer about what they want and be more sure that they actually want it. Having a detailed Project Plan isn’t necessarily going to make your customers think more about what they want – even if baselines move. But giving them responsibility, having them see that every change shifts the goal-posts, will mean they will feel the change much more “personally”. Also if you deliver something (even if it’s small) at the enf of every sprint, customers get a feeling of momentum. You may even find that the seemingly small piece of functionality that you deliver is enough, and the customer doesn’t really need the rest of The Thing that you would have been working on for the next 2 months. \"],[0,[1],1,\"Rapid feedback on value delivered is far better than explaining why a baseline has to move\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Pizza or Degree?\"]]],[1,\"p\",[[0,[],0,\"Finally, the Pizza analogy. Again, if you go back to the Cone of Uncertainty, there is very little “risk” in getting toppings wrong on a pizza – so of course you commit to the entire pizza when you order it. I think the analogy doesn’t really work. I think a better analogy is a startup company.\"]]],[1,\"p\",[[0,[],0,\"Let’s imagine 2 startups – one called ProjectX and one called AgileX. ProjectX spends 4 weeks getting a detailed plan in place for their product, which is going to take 3 months to build. AgileX in the meantime spends 4 days on a general direction, and a day in detailed planning for their project. They release their product’s V1 after another 2 weeks. The product isn’t fully featured, but what there is of the product works. They get some feedback which changes some of what they know about the product, and adjust accordingly. If they continue to add small features and get feedback in 2 week cycles, they would have released features 15 times before ProjectX’s product comes out the door. They would have had 14 opportunities for feedback from their Customers before ProjectX even had 1 round of feedback on working software. Even if ProjectX followed Prasanna’s “mini-waterfalls”, there’d be less movement. A mini-waterfall would require too much planning for 2 weeks, so ProjectX decides on 3 1-month mini-waterfalls. That still means that they only release 3 times in the same period, and only get feedback on 2 occasions. Still way less than AgileX. Is the more detailed longer-termed planning helping or hindering?\"]]],[1,\"p\",[[0,[],0,\"What do you, dear reader, think of Project Planning?\"]]],[1,\"p\",[[0,[],0,\"Happy planning!\"]]]]}","published_at":1397497740000,"status":"published","published_by":1},{"id":"e1cebc5f-43db-46cc-be63-789f3b64aff7","title":"Protecting a VSTS Web Hook with Basic Authentication","slug":"protecting-a-vsts-web-hook-with-basic-authentication","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"using Microsoft.AspNet.WebHooks;\\nusing Microsoft.AspNet.WebHooks.Payloads;\\nusing System.Threading.Tasks;\\nusing System.Diagnostics;\\n\\nnamespace vsts_webhook_with_auth.Handlers\\n{\\n\\tpublic class VSTSHookHandler : VstsWebHookHandlerBase\\n\\t{\\n\\t\\tpublic override Task ExecuteAsync(WebHookHandlerContext context, WorkItemCreatedPayload payload)\\n\\t\\t{\\n\\t\\t\\tTrace.WriteLine($\\\"Event WorkItemCreated triggered for work item {payload.Resource.Id}\\\");\\n\\t\\t\\treturn base.ExecuteAsync(context, payload);\\n\\t\\t}\\n\\t}\\n}\",\"language\":\"csharp\"}],[\"code\",{\"code\":\"protected override Task&lt;HttpResponseMessage&gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)\\n{\\n\\tvar credentials = ParseAuthorizationHeader(request);\\n\\n\\tif (credentials != null &amp;&amp; CredentialsAreValid(credentials))\\n\\t{\\n\\t\\tvar identity = new BasicAuthenticationIdentity(credentials.Name, credentials.Password);\\n\\t\\tThread.CurrentPrincipal = new GenericPrincipal(identity, null);\\n\\t\\treturn base.SendAsync(request, cancellationToken);\\n\\t}\\n\\telse\\n\\t{\\n\\t\\tvar response = request.CreateResponse(HttpStatusCode.Unauthorized, \\\"Access denied\\\");\\n\\t\\tAddChallengeHeader(request, response);\\n\\t\\treturn Task.FromResult(response);\\n\\t}\\n}\",\"language\":\"csharp\"}],[\"code\",{\"code\":\"protected void Application_Start()\\n{\\n\\tGlobalConfiguration.Configuration.MessageHandlers.Add(new BasicAuthenticationHandler());\\n\\n\\tAreaRegistration.RegisterAllAreas();\\n\\tGlobalConfiguration.Configure(WebApiConfig.Register);\\n\\tFilterConfig.RegisterGlobalFilters(GlobalFilters.Filters);\\n\\tRouteConfig.RegisterRoutes(RouteTable.Routes);\\n\\tBundleConfig.RegisterBundles(BundleTable.Bundles);\\n}\",\"language\":\"csharp; highlight:[3]\"}],[\"code\",{\"code\":\"config.Routes.MapHttpRoute(\\n\\tname: \\\"DefaultApi\\\",\\n\\trouteTemplate: \\\"api/{controller}/{id}\\\",\\n\\tdefaults: new { id = RouteParameter.Optional }\\n);\\n\\nconfig.InitializeReceiveVstsWebHooks();\",\"language\":\"csharp; highlight:[7]\"}],[\"code\",{\"code\":\"  &lt;appSettings&gt;\\n\\t...\\n\\t&lt;add key=\\\"WebHookUsername\\\" value=\\\"vsts\\\"/&gt;\\n\\t&lt;add key=\\\"WebHookPassword\\\" value=\\\"P@ssw0rd\\\"/&gt;\\n\\t&lt;add key=\\\"MS_WebHookReceiverSecret_VSTS\\\" value=\\\"C8B7F962-2B5A-4973-81F3-8888D53CF86E\\\"/&gt;\\n  &lt;/appSettings&gt;\",\"language\":\"xml\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/31e35e22-1262-4041-b8e5-a43bf734d7f2.png\\\" target=\\\"_blank\\\"><img width=\\\"288\\\" height=\\\"168\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b0210614-f998-4aee-953f-8bd3721efc98.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/eabb86ad-f65d-4170-a82e-5119ed0e79d7.png\\\" target=\\\"_blank\\\"><img width=\\\"305\\\" height=\\\"82\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/00633e1a-60d8-449b-9c7b-03fe820588fe.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/44208e26-fd6d-4ba9-9304-700b61e70ac1.png\\\" target=\\\"_blank\\\"><img width=\\\"300\\\" height=\\\"46\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0ee8709a-76d9-4187-94bf-bfeb49b004cd.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8c112bd2-9ccb-44db-85dd-372515f3089a.png\\\" target=\\\"_blank\\\"><img width=\\\"300\\\" height=\\\"108\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/13b944df-e31f-419a-9c8a-66f43396ee71.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/13c9ee7e-d172-410b-87ab-d62d7b943908.png\\\" target=\\\"_blank\\\"><img width=\\\"299\\\" height=\\\"98\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4e9b82ff-9334-4f31-945e-567ee190eb21.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/cdee7c12-ca3b-4803-9e44-a17da781fcf9.png\\\" target=\\\"_blank\\\"><img width=\\\"313\\\" height=\\\"136\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ecb37096-1819-46c7-8bec-50f044f1d429.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fdee901d-4294-4426-99cf-20712849312c.png\\\" target=\\\"_blank\\\"><img width=\\\"295\\\" height=\\\"294\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/207a4d9d-0c08-4967-9b6a-db12a1d28a88.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/docs/marketplace/integrate/service-hooks/get-started\"]],[\"a\",[\"href\",\"https://github.com/aspnet/WebHooks/tree/master/samples/VstsReceiver\"]],[\"a\",[\"href\",\"https://www.getpostman.com/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-webhook-with-auth\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-webhook-with-auth/blob/master/vsts-webhook-with-auth/Handlers/BasicAuthHandler.cs\"]],[\"a\",[\"href\",\"https://github.com/aspnet/WebHooks/blob/master/samples/VstsReceiver/index.html#L18\"]],[\"a\",[\"href\",\"http://blog.getpostman.com/2014/01/28/using-self-signed-certificates-with-postman/\"]],[\"a\",[\"href\",\"https://localhost:44388/api/webhooks/incoming/vsts?code=C8B7F962-2B5A-4973-81F3-8888D53CF86E\"]],[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/docs/integrate/get-started/service-hooks/events\"]],[\"a\",[\"href\",\"https://twitter.com/damovisa/status/882953242468122624\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"VSTS \"],[0,[0],1,\"supports service hooks\"],[0,[],0,\" like Slack, AppVeyor, Bamboo and a host of other ALM tools. You can also create your own hooks using a simple WebHooks API. There's an example \"],[0,[1],1,\"here\"],[0,[],0,\". However, one thing that is missing from the sample is any kind of authentication.\"]]],[1,\"p\",[[0,[],0,\"Why care? Well, simply put - without authentication, anyone could trigger events to your event sink. That may or may not be a big deal, but I prefer to be secure by default.\"]]],[1,\"p\",[[0,[],0,\"Now there are a couple of ways you could do auth - you could use AAD or OpenConnect and get a token and use that for the WebHook subscription. That would probably work, but VSTS won't renew the token automatically (at least I don't think it will) so you'll have to update the webhook subscription manually every time the token expires.\"]]],[1,\"p\",[[0,[],0,\"The other way is to use Basic Auth. When you subscribe to a webhook in VSTS, you can pass a Basic username/password. The username and password are base64 encoded and added to a header for the requests. Assuming your using HTTPS (so that you don't get man-in-the-middle attacks) you can use this for a relatively safe authentication method. Once you extract the username/password from the header, you can validate them however you want.\"]]],[1,\"p\",[[0,[],0,\"In this post I'll cover how to create a Web Hook project that includes Basic Auth as well as logging to Application Insights. I'll also cover how to debug and test your service using \"],[0,[2],1,\"Postman\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"The source code for this stub project is \"],[0,[3],1,\"here\"],[0,[],0,\" so you can just grab that if you want to get going.\"]]],[1,\"h2\",[[0,[],0,\"Creating the Project\"]]],[1,\"p\",[[0,[],0,\"Initially I wanted to create the project in .NET Core. However, I wanted to use a NuGet package and the package unfortunately only supports .NET 4.x. So I'll just use that.\"]]],[1,\"p\",[[0,[],0,\"Open Visual Studio 2017 and click File->New Project and create a new ASP.NET Web Application. Select Web API from the project type dialog (oh how I love that you don't have to do this in ASP.NET Core) and ensure you have \\\"No authentication\\\" (we'll add Basic Auth shortly). This creates a new project and even includes Application Insights.\"]]],[1,\"h3\",[[0,[],0,\"Adding Packages\"]]],[1,\"p\",[[0,[],0,\"We're going to add a few NuGet packages. Right-click the web project and add the following packages: \"],[0,[4],1,\"Microsoft.AspNet.WebHooks.Receivers.VSTS\"],[0,[],0,\" (contains webhook handler abstract class and event payload classes) and \"],[0,[4],1,\"Microsoft.ApplicationInsights.TraceListener\"],[0,[],0,\" (which will send Trace.WriteLines to AppInsights).\"]]],[1,\"p\",[[0,[],0,\"Once the packages are installed, you can (optionally) update all the packages. The project templates sometimes have older package versions, so I usually like to do this so that I'm on the latest NuGet package versions from the get go.\"]]],[1,\"h3\",[[0,[],0,\"Adding a WebHook Handler\"]]],[1,\"p\",[[0,[],0,\"This is the class that will do the work. Add a new folder called \\\"Handlers\\\" and create a new class called \\\"VSTSHookHandler\\\".\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 8: We inherit from VstsWebHookHandlerBase - this base class has abstract methods for all the VSTS service hook events that we can listen for.\"]],[[0,[],0,\"Line 10: We override the async WorkItemCreated event - there are other events that you can override depending on what you need. Add as many overrides as you need. This method also gets the context and the payload for the event for us.\"]],[[0,[],0,\"Line 12: We are writing log entries to Trace - because we've added the AppInsights TraceListener, these end up in AppInsights where we can search for particular messages.\"]],[[0,[],0,\"Line 13: Here is where you will implement your logic to respond to the event. For this stub project, I just call the base method (which is essentially a no-op).\"]]]],[1,\"h3\",[[0,[],0,\"Adding a BasicAuthHandler\"]]],[1,\"p\",[[0,[],0,\"Add a new class to the Handlers folder called BasicAuthHandler. You can get the full class \"],[0,[5],1,\"here\"],[0,[],0,\", but we only need to see the SendAsync method for our discussion:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3: The ParseAuthorizationHeader() method extracts the username/password from the auth header\"]],[[0,[],0,\"Line 5: We check that there are credentials and that they \\\"are valid\\\" (in this case that they match hard-coded values we'll add to the web.config)\"]],[[0,[],0,\"Lines 7,8: We add the auth details to the CurrentPrincipal, which has the effect of marking the request as \\\"authenticated\\\"\"]],[[0,[],0,\"Line 9: We forward the request on to the remainder of the pipeline - which is the VSTSHookHandler class methods at this point\"]],[[0,[],0,\"Lines 13-15: We handle the unauthorized scenario\"]]]],[1,\"h3\",[[0,[],0,\"Configuration\"]]],[1,\"p\",[[0,[],0,\"We can now add the handlers into the message processing pipeline. Open the Global.asax.cs file and modify the Application_Start() method by adding in the highlighted line (and resolving the namespace):\"]]],[10,2],[1,\"p\",[[0,[],0,\"Our auth handler is now configured to trigger on requests.\"]]],[1,\"p\",[[0,[],0,\"Next, open up App_Start/WebApiConfig.cs and modify the Register() method with the highlighted line:\"]]],[10,3],[1,\"p\",[[0,[],0,\"This registers the handler class to respond to VSTS events.\"]]],[1,\"p\",[[0,[],0,\"Finally, open the web.config file and add the following appSetting keys:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"The \"],[0,[4],1,\"MS_WebHookReceiverSecret_VSTS\"],[0,[],0,\" is a \\\"code\\\" that the VSTS webhook requires to be in the query params for the service call. This can be anything as long as it is longer than 32 and less than 128 chars. You can also have \"],[0,[6],1,\"multiple codes\"],[0,[],0,\". This code lets you handle different projects firing the same events - you can tie the code to a project so that you can do project specific logic.\"]],[[0,[4],1,\"WebHookUsername\"],[0,[],0,\" and \"],[0,[4],1,\"WebHookPassword\"],[0,[],0,\" are hardcoded username/password for validation - you can ignore these if you need some other way to validate the values.\"]]]],[1,\"h3\",[[0,[],0,\"Configuring SSL in VS\"]]],[1,\"p\",[[0,[],0,\"To run the site using SSL from VS, you'll need to enable that in the project properties. Click on the web project node (so that it is selected in the Solution Explorer). Then press F4 to open the properties pane (this is different to right-click -> Properties). Change SSL Enabled to true. Make a note of the https URL.\"]]],[10,5],[1,\"p\",[[0,[],0,\"You can now right-click the project and select Properties. Change the startup URL to the https URL so that you always get the SSL site.\"]]],[1,\"h3\",[[0,[],0,\"Testing from PostMan\"]]],[1,\"p\",[[0,[],0,\"You can now run the site. You'll notice when you first run it that the cert is invalid (it's self-signed). In order to get Postman working to test the webhooks, I had to read \"],[0,[7],1,\"these instructions\"],[0,[],0,\". In the end, I just opened the https URL in IE and imported the cert to \"],[0,[4],1,\"Trusted Root Certification Authorities\"],[0,[],0,\". I then shut down Chrome and restarted and all was good.\"]]],[1,\"p\",[[0,[],0,\"I opened Postman and entered this url: \"],[0,[8],1,\"https://localhost:44388/api/webhooks/incoming/vsts?code=C8B7F962-2B5A-4973-81F3-8888D53CF86E\"],[0,[],0,\", changing the method to POST. I made sure that the code was the same value as the \"],[0,[4],1,\"MS_WebHookReceiverSecret_VSTS\"],[0,[],0,\" key in my web.config. I then opened the \"],[0,[9],1,\"docs page\"],[0,[],0,\" and grabbed the sample payload for the WorkItemCreated event and pasted this into the Body for the request. I updated the type to application/json (which adds a header).\"]]],[10,6],[1,\"p\",[[0,[],0,\"Hitting Send returns a 401 - which is expected since we haven't provided a username/password. Nice!\"]]],[10,7],[1,\"p\",[[0,[],0,\"Now let's test adding the auth. Go back to Postman and click on Authorization. Change the Type to \\\"Basic Auth\\\" and enter the username/password that you hard-coded into your web.config. Click \\\"Update Request\\\" to add the auth header:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Now press Send again. We get a 200!\"]]],[10,9],[1,\"p\",[[0,[],0,\"Of course you can now set breakpoints and debug normally. If you open AppInsights Viewer in VS you'll see the traces - these will eventually make their way to AppInsights (remember to update your key when you create a real AppInsights resource in Azure).\"]]],[10,10],[1,\"h2\",[[0,[],0,\"Registering the Secure Hook in VSTS\"]]],[1,\"p\",[[0,[],0,\"You can now clean up the project a bit (remove the home controller etc.) and deploy the site to Azure (or your own IIS web server) using VSTS Build and Release Management (\"],[0,[10],1,\"friends don't let friends right-click Publish\"],[0,[],0,\") and you're ready to register the hook in VSTS. Open your VSTS Team Project and head to Configuration->Service Hooks. Add a new service hook. Enter the URL (including the code query param) for your service. Then just enter the same username/password you have in the website web.config and you're good to go! Hit test to make sure it works.\"]]],[10,11],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Securing your WebHooks from VSTS isn't all that hard - just add the BasicAuthHandler and configure the basic auth username/password in the WebHook subscription in VSTS. Now you can securely receive events from VSTS. I would really like to see the VSTS team update the NuGet packages to support .NET Core WebAPI, but the 4.x version is fine in the interim.\"]]],[1,\"p\",[[0,[],0,\"Happy hooking!\"]]]]}","published_at":1500090174000,"status":"published","published_by":1},{"id":"f2f2a399-17f3-4866-8b86-6859183a6586","title":"QFE Fixes VS 2010 SP1 Bug on Test Agents","slug":"qfe-fixes-vs-2010-sp1-bug-on-test-agents","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">\\\"Attempted to access an unloaded AppDomain. (Exception from HRESULT: 0x80131014)\\\"</font>\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"http://blogs.msdn.com/b/lab_management/archive/2011/06/23/new-qfe-for-visual-studio-2010-testing-tools.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Last week I was helping a customer get up and running with Lab Management. In an effort to make sure they’re up to date, I installed TFS and VS 2010 SP1 on their TFS machine as well as in all the Lab machines.\"]]],[1,\"p\",[[0,[],0,\"Everything was going smoothly until we started running automated tests. The tests would return as “not executed” and the error was:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The strange thing was that if we turned off \"],[0,[0],1,\"all\"],[0,[],0,\" the diagnostic adaptors, the tests ran successfully\"]]],[1,\"p\",[[0,[],0,\"After some investigation, we found out that this was a known bug in SP1. Yesterday, however, a \"],[0,[1],1,\"QFE was released that fixes this issue\"],[0,[],0,\". If you’ve installed TFS and VS 2010 SP1 on your TFS and in your lab machines, make sure you apply this QFE if you want to run automated tests with diagnostic data adaptors!\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1308928500000,"status":"published","published_by":1},{"id":"2f651e25-6374-4511-979b-c23a9fb6406b","title":"Real Config Handling for DSC in RM","slug":"real-config-handling-for-dsc-in-rm","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"Script SetConStringDeployParam\\n{\\n    GetScript = { @{ Name = \\\"SetDeployParams\\\" } }\\n    TestScript = { $false }\\n    SetScript = {\\n        $paramFilePath = \\\"c:\\\\temp\\\\Site\\\\FabrikamFiber.Web.SetParameters.xml\\\"\\n\\n        $paramsToReplace = @{\\n            \\\"__FabFiberExpressConStr__\\\" = \\\"data source=fabfiberdb;database=FabrikamFiber-Express;User Id=lab;Password=P2ssw0rd\\\"\\n            \\\"__SiteName__\\\" = \\\"Default Web Site\\\\FabrikamFiber\\\"\\n        }\\n\\n        $content = gc $paramFilePath\\n        $paramsToReplace.GetEnumerator() | % {\\n            $content = $content.Replace($_.Key, $_.Value)\\n        }\\n        sc -Path $paramFilePath -Value $content\\n    }\\n    DependsOn = \\\"[File]CopyWebDeployFiles\\\"\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"cScriptWithParams SetConStringDeployParam\\n{\\n    GetScript = { @{ Name = \\\"SetDeployParams\\\" } }\\n    TestScript = { $false }\\n    SetScript = {\\n        $paramFilePath = \\\"c:\\\\temp\\\\Site\\\\FabrikamFiber.Web.SetParameters.xml\\\"\\n\\n        $paramsToReplace = @{\\n            \\\"__FabFiberExpressConStr__\\\" = $conStr\\n            \\\"__SiteName__\\\" = $siteName\\n        }\\n\\n        $content = gc $paramFilePath\\n        $paramsToReplace.GetEnumerator() | % {\\n            $content = $content.Replace($_.Key, $_.Value)\\n        }\\n        sc -Path $paramFilePath -Value $content\\n    }\\n    cParams =\\n    @{\\n        conStr = $conStr;\\n        siteName = $siteName;\\n    }\\n    DependsOn = \\\"[File]CopyWebDeployFiles\\\"\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"Configuration FabFibWeb_Site\\n{\\n    Import-DscResource -Name cScriptWithParams\\n\\n    Node $ServerName\\n    {\\n        Log WebServerLog\\n        {\\n            Message = \\\"Starting Site Deployment. AppPath = $applicationPath.\\\"\\n        }\\n\\n        #\\n        # Deploy a website using WebDeploy\\n        #\\n        File CopyWebDeployFiles\\n        {\\n            Ensure = \\\"Present\\\"         \\n            SourcePath = \\\"$applicationPath\\\\_PublishedWebsites\\\\FabrikamFiber.Web_Package\\\"\\n            DestinationPath = \\\"c:\\\\temp\\\\Site\\\"\\n            Recurse = $true\\n            Force = $true\\n            Type = \\\"Directory\\\"\\n        }\\n\\n        cScriptWithParams SetConStringDeployParam\\n        {\\n            GetScript = { @{ Name = \\\"SetDeployParams\\\" } }\\n            TestScript = { $false }\\n            SetScript = {\\n                $paramFilePath = \\\"c:\\\\temp\\\\Site\\\\FabrikamFiber.Web.SetParameters.xml\\\"\\n\\n                $paramsToReplace = @{\\n                    \\\"__FabFiberExpressConStr__\\\" = $conStr\\n                    \\\"__SiteName__\\\" = $siteName\\n                }\\n\\n                $content = gc $paramFilePath\\n                $paramsToReplace.GetEnumerator() | % {\\n                    $content = $content.Replace($_.Key, $_.Value)\\n                }\\n                sc -Path $paramFilePath -Value $content\\n            }\\n            cParams =\\n            @{\\n                conStr = $ConStr;\\n                siteName = $SiteName;\\n            }\\n            DependsOn = \\\"[File]CopyWebDeployFiles\\\"\\n        }\\n        \\n        Script DeploySite\\n        {\\n            GetScript = { @{ Name = \\\"DeploySite\\\" } }\\n            TestScript = { $false }\\n            SetScript = {\\n                &amp; \\\"c:\\\\temp\\\\Site\\\\FabrikamFiber.Web.deploy.cmd\\\" /Y\\n            }\\n            DependsOn = \\\"[cScriptWithParams]SetConStringDeployParam\\\"\\n        }\\n    }\\n}\\n\\n# command for RM\\nFabFibWeb_Site\\n\\n&lt;# \\n#test from command line\\n$ServerName = \\\"fabfiberserver\\\"\\n$applicationPath = \\\"\\\\\\\\rmserver\\\\builddrops\\\\__ReleaseSite\\\\__ReleaseSite_1.0.0.3\\\"\\n$conStr = \\\"testing\\\"\\n$siteName = \\\"site Test\\\"\\nFabFibWeb\\nStart-DscConfiguration -Path .\\\\FabFibWeb -Verbose -Wait\\n#&gt;\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9c1fe5be-55bb-49c4-b51b-eeb6588e82b2.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3afcf23f-dd5f-4ad5-bbbd-44b91f8da5c2.png\\\" width=\\\"489\\\" height=\\\"393\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d1fc2e69-c46c-49a9-8f2c-2ff2e22062ed.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/449bb9ac-2243-451e-945c-160302f3a9ed.png\\\" width=\\\"433\\\" height=\\\"292\\\"></a>\"}],[\"code\",{\"code\":\"Configuration CopyCustomResource\\n{\\n    Node $ServerName\\n    {\\n        File CopyCustomResource\\n        {\\n            Ensure = \\\"Present\\\"\\n            SourcePath = \\\"$applicationPath\\\\$modSubFolder\\\\$modName\\\"\\n            DestinationPath = \\\"$env:ProgramFiles\\\\WindowsPowershell\\\\Modules\\\\$modName\\\"\\n            Recurse = $true\\n            Force = $true\\n            Type = \\\"Directory\\\"\\n        }\\n    }\\n}\\n\\n&lt;#\\n# test from command line\\n$ServerName = \\\"fabfiberserver\\\"\\n$applicationPath = \\\"\\\\\\\\rmserver\\\\builddrops\\\\__ReleaseSite\\\\__ReleaseSite_1.0.0.3\\\"\\n$modSubfolder = \\\"CustomResources\\\"\\n$modName = \\\"DSC_ColinsALMCorner.com\\\"\\n#&gt;\\n\\n# copy the resource locally\\n#cp \\\"$applicationPath\\\\$modSubFolder\\\\$modName\\\" $env:ProgramFiles\\\\WindowsPowerShell\\\\Modules -Force -Recurse\\n\\n# command for RM\\nCopyCustomResource\\n\\n&lt;#\\n# test from command line\\nCopyCustomResource\\nStart-DscConfiguration -Path .\\\\CopyCustomResource -Verbose -Wait\\n#&gt;\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3a7e1710-d951-4208-a764-5c178e547b36.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d2e3a8e8-83e9-4c55-9f79-c31ccd2a3b73.png\\\" width=\\\"436\\\" height=\\\"294\\\"></a>\"}],[\"code\",{\"code\":\"function Get-TargetResource \\n{\\n    [CmdletBinding()]\\n     param \\n     (         \\n       [parameter(Mandatory = $true)]\\n       [ValidateNotNullOrEmpty()]\\n       [string]\\n       $GetScript,\\n  \\n       [parameter(Mandatory = $true)]\\n       [ValidateNotNullOrEmpty()]\\n       [string]$SetScript,\\n\\n       [parameter(Mandatory = $true)]\\n       [ValidateNotNullOrEmpty()]\\n       [string]\\n       $TestScript,\\n\\n       [Parameter(Mandatory=$false)]\\n       [System.Management.Automation.PSCredential] \\n       $Credential,\\n\\n       [Parameter(Mandatory=$false)]\\n       [Microsoft.Management.Infrastructure.CimInstance[]]\\n       $cParams\\n     )\\n\\n    $getTargetResourceResult = $null;\\n\\n    Write-Debug -Message \\\"Begin executing Get Script.\\\"\\n \\n    $script = [ScriptBlock]::Create($GetScript);\\n    $parameters = $psboundparameters.Remove(\\\"GetScript\\\");\\n    $psboundparameters.Add(\\\"ScriptBlock\\\", $script);\\n    $psboundparameters.Add(\\\"customParams\\\", $cParams);\\n\\n    $parameters = $psboundparameters.Remove(\\\"SetScript\\\");\\n    $parameters = $psboundparameters.Remove(\\\"TestScript\\\");\\n\\n    $scriptResult = ScriptExecutionHelper @psboundparameters;\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"function ScriptExecutionHelper \\n{\\n    param \\n    (\\n        [ScriptBlock] \\n        $ScriptBlock,\\n    \\n        [System.Management.Automation.PSCredential] \\n        $Credential,\\n\\n        [Microsoft.Management.Infrastructure.CimInstance[]]\\n        $customParams\\n    )\\n\\n    $scriptExecutionResult = $null;\\n\\n    try\\n    {\\n        $executingScriptMessage = \\\"Executing script: {0}\\\" -f ${ScriptBlock} ;\\n        Write-Debug -Message $executingScriptMessage;\\n\\n        $executingScriptArgsMessage = \\\"Script params: {0}\\\" -f $customParams ;\\n        Write-Debug -Message $executingScriptArgsMessage;\\n\\n        # bring the cParams into memory\\n        foreach($cVar in $customParams.GetEnumerator())\\n        {\\n            Write-Debug -Message \\\"Creating value $($cVar.Key) with value $($cVar.Value)\\\"\\n            New-Variable -Name $cVar.Key -Value $cVar.Value\\n        }\\n\\n        if($null -ne $Credential)\\n        {\\n           $scriptExecutionResult = Invoke-Command -ScriptBlock $ScriptBlock -ComputerName . -Credential $Credential\\n        }\\n        else\\n        {\\n           $scriptExecutionResult = &amp;$ScriptBlock;\\n        }\\n        Write-Debug -Message \\\"Completed script execution\\\"\\n        $scriptExecutionResult;\\n    }\\n    catch\\n    {\\n        # Surfacing the error thrown by the execution of Get/Set/Test script.\\n        $_;\\n    }\\n}\\n\",\"language\":\"ps;\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/using-webdeploy-in-vnext-releases\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/DSC_ColinsALMCorner.com\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/new-vnext-config-variable-options-in-rm-update-4-rc\"]],[\"u\"]],\"sections\":[[1,\"p\",[[0,[],0,\"In my \"],[0,[0],1,\"previous post\"],[0,[],0,\" I showed you how to use PowerShell DSC and Release Management to configure machines and deploy an application. There was one part of the solution that I wasn’t satisfied with, and in the comments section you’ll see that @BigFan picks it up: the configuration is hard-coded.\"]]],[1,\"h2\",[[0,[],0,\"cScriptWithParams Resource\"]]],[1,\"p\",[[0,[],0,\"The primary reason I’ve had to hard-code the configuration is that I use the Script resource heavily. Unfortunately the Script resource cannot utilize configuration (or parameters)! I do explain this in my previous post (see the section headed “A Note on Script Resource Parameters”). For a while I tried to write my own custom resource, but eventually abandoned that project. However, after completing my previous post, I decided to have another stab at the problem. And voila! I created a custom Script resource that (elegantly, I think) can be parameterized. You can get it from \"],[0,[1],1,\"GitHub\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Let’s first look at how to utilize the new resource – I’ll discuss how I created the resource after that.\"]]],[1,\"h2\",[[0,[],0,\"Parameterized Scripts\"]]],[1,\"p\",[[0,[],0,\"In my previous solution, the Script resource that executed the Webdeploy command (which deploys my web application) looks like this:\"]]],[10,0],[1,\"p\",[[0,[],0,\"You can see how lines 9 and 10 are hard-coded. Ideally these values should be read from a configuration somewhere.\"]]],[1,\"p\",[[0,[],0,\"Here’s what the script looks like when you use the cScriptWithParams resource:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Some notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: The name of the resource is “cScriptWithParams” – the custom Script resource I created. In order to use this custom resource, you need the line “Import-DscResource -Name cScriptWithParams” at the top of your Configuration script (above the first Node element).\"]],[[0,[],0,\"Lines 9/10: The values for the connection string and site name are now variables instead of hard-coded\"]],[[0,[],0,\"Lines 19-23: This is the property that allows you to “pass in” values for the variables. It’s a hash-table of string key-value pairs, where the key is the name of the variable used in any of the Get, Set or Test scripts and the value is the value you want to set the variable to. We could get the values from anywhere – a DSC config file (where we would have $Node.ConStr for example) – in this case it’s from 2 global variables called $conStr and $siteName (we’ll see later where these get specified).\"]]]],[1,\"h2\",[[0,[],0,\"Removing Config Files Altogether\"]]],[1,\"p\",[[0,[],0,\"Now that we can (neatly) parameterize the custom scripts we want to run, we can use the \"],[0,[2],1,\"new config variable options in RM\"],[0,[],0,\" to completely remove the need for a config file. Of course you could still use the config file if you wanted to. Here’s the final script for deploying my web application:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3: Importing the custom resource (presumes the custom resource is “installed” locally – see next section for how to do this)\"]],[[0,[],0,\"Line 5: I leverage the $ServerName variable that RM sets – I don’t have to hard-code the node name\"]],[[0,[],0,\"Lines 15-23: Copy the Webdeploy files from the build drop location to a local folder (again I use an RM parameter, $applicationPath, which is the drop folder)\"]],[[0,[],0,\"Lines 25-49: Almost the same Script resource we had before, but subtly changed to handle variables by changing it to a cScriptWithParams resource.\"]],[[0,[],0,\"Lines 33/34: The hard-coded values have been replaced with variables.\"]],[[0,[],0,\"Lines 43-47: We need to supply a hash-table of key/value pairs for our parameterized scripts. In this case, we need to supply conStr and siteName. For the values, we pass in $conStr and $siteName, which RM will feed in for us (we’ll specify these on the Release Template itself)\"]],[[0,[],0,\"Line 64: “Compile” the configuration (into a .mof file) for RM to push to the target server\"]],[[0,[],0,\"Lines 66-74: If you test this script from the command line, you just create the variables required and execute it. This is exactly what RM does under the hood when executing this script.\"]]]],[1,\"h3\",[[0,[],0,\"Using the Script in a Release Template\"]]],[1,\"p\",[[0,[],0,\"Now that we have the script, let’s see how we consume it. (Of course it’s checked into source control, along with the Custom Resource, and part of a build so that it ends up in the build drop folder with our application. Of course – goes without saying!)\"]]],[1,\"p\",[[0,[],0,\"We define the vNext Component the same way we did last time:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Nothing magical here – this really just defines the root folder of the build drop for use in the deployment.\"]]],[1,\"p\",[[0,[],0,\"Next we create the vNext template using our desired vNext release path. On the designer, you’ll see the major difference: we’re defining the variables on the surface itself:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Our script uses $ServerName (which you can see is set to fabfiberserver). It also uses ConStr and SiteName (these are the parameter values we specified in lines 44/45 of the above script – $ConStr  and $SiteName). Of course if we deploy to another server (say in our production environment) we would simply specify other values for that server.\"]]],[1,\"h2\",[[0,[],0,\"Deploying a Custom Resource\"]]],[1,\"p\",[[0,[],0,\"The final trick is how you deploy the custom resource. To import it using Import-DSCResource, you need to have it in ProgramFiles\\\\WindowsPowerShell\\\\Modules. If you’re testing the script from you workstation, you’ll need to copy it to this path on your workstation. You’ll also need to copy it to that folder on the target server. Sounds like a job for a DSC script with a File resource! Unfortunately it can’t be part of the web application script we created above since it needs to be on the server before you run the Import-DscResource command. No problem – we’ll run 2 scripts on the template. Here’s the script to deploy the custom resource:\"]]],[10,5],[1,\"p\",[[0,[],0,\"This is very straight-forward:\"]]],[3,\"ul\",[[[0,[],0,\"Line 3: Again we’re using RM’s variable so that don’t have to hard-code the node name\"]],[[0,[],0,\"Lines 5-13: Copy the resource files to the PowerShell modules folder\"]],[[0,[],0,\"Line 26: Use this to copy the resource locally to your workstation for testing it\"]],[[0,[],0,\"Line 29: This “compiles” this config file before RM deploys it (and executes it) on the target server\"]],[[0,[],0,\"Lines 17-23 and 31-35: Uncomment these to run this from the command line for testing\"]]]],[1,\"p\",[[0,[],0,\"Here’s how to use the script in a release template:\"]]],[10,6],[1,\"p\",[[0,[],0,\"By now you should be able to see how this designer is feeding values to the script!\"]]],[1,\"h2\",[[0,[],0,\"cScriptWithParams: A Look Inside\"]]],[1,\"p\",[[0,[],0,\"To make the cScriptWithParams custom resource, I copied the out-of-the-box script and added the cParams hash-table parameter to the Get/Set/Test TargetResource functions. I had some issues with type conversions, so I eventually changed the HashTable to an array of Microsoft.Management.Infrastructure.CimInstance. I then make sure this gets passed to the common function that actually invokes the script (ScriptExecutionHelper). Here’s a snippet from the Get-TargetResource function:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 24-26: the extra parameter I added\"]],[[0,[],0,\"Line 36: I add the $cParams to the $psboundparameters that will be passed to the ScriptExecutionHelper function\"]],[[0,[],0,\"Line 41: this is the original call to the ScriptExecutionHelper function\"]]]],[1,\"p\",[[0,[],0,\"Finally, I customized the ScriptExecutionHelper function to utilize the parameters:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 11/12: The new “hashtable” of variables\"]],[[0,[],0,\"Lines 25-30: I use New-Variable to create global variables for each key/value pair in $customParams\"]],[[0,[],0,\"The remainder of the script is unmodified\"]]]],[1,\"p\",[[0,[],0,\"The only limitation I hit was the the values \"],[0,[3],1,\"must be strings\"],[0,[],0,\" – I am sure this has to do with the way the values are serialized when a DSC configuration script is “compiled” into a .mof file.\"]]],[1,\"p\",[[0,[],0,\"As usual, happy deploying!\"]]]]}","published_at":1414690549000,"status":"published","published_by":1},{"id":"eaf9eda4-c107-41c6-a3dc-3225b35ecd12","title":"Reflections on DSC for Release Management","slug":"reflections-on-dsc-for-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"WindowsFeature WebServerRole\\n{\\n    Name = \\\"Web-Server\\\"\\n    Ensure = \\\"Present\\\"\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"Script AllowWMI \\n{\\n    GetScript = { @{ Name = \\\"AllowWMI\\\" } }\\n    TestScript = { $false }\\n    SetScript = \\n    {\\n        Set-NetFirewallRule -DisplayGroup \\\"Windows Management Instrumentation (WMI)\\\" -Enabled True\\n    }\\n}\\n\\nWindowsFeature WebServerRole\\n{\\n    Name = \\\"Web-Server\\\"\\n    Ensure = \\\"Present\\\"\\n}\\n\\nWindowsFeature WebMgmtConsole\\n{\\n    Name = \\\"Web-Mgmt-Console\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\nWindowsFeature WebAspNet\\n{\\n    Name = \\\"Web-Asp-Net\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\nWindowsFeature WebNetExt\\n{\\n    Name = \\\"Web-Net-Ext\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\nWindowsFeature WebAspNet45\\n{\\n    Name = \\\"Web-Asp-Net45\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\nWindowsFeature WebNetExt45\\n{\\n    Name = \\\"Web-Net-Ext45\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\nWindowsFeature WebHttpRedirect\\n{\\n    Name = \\\"Web-Http-Redirect\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\nWindowsFeature WebWinAuth\\n{\\n    Name = \\\"Web-Windows-Auth\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\nWindowsFeature WebScriptingTools\\n{\\n    Name = \\\"Web-Scripting-Tools\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"#\\n# Install MVC4\\n#\\nPackage MVC4\\n{\\n    Name = \\\"Microsoft ASP.NET MVC 4 Runtime\\\"\\n    Path = \\\"$AssetPath\\\\AspNetMVC4Setup.exe\\\"\\n    Arguments = \\\"/q\\\"\\n    ProductId = \\\"\\\"\\n    Ensure = \\\"Present\\\"\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"#\\n# Install webdeploy\\n#\\nPackage WebDeploy\\n{\\n    Name = \\\"Microsoft Web Deploy 3.5\\\"\\n    Path = \\\"$AssetPath\\\\WebDeploy_amd64_en-US.msi\\\"\\n    Arguments = \\\"ADDLOCAL=MSDeployFeature,MSDeployAgentFeature\\\"\\n    ProductId = \\\"\\\"\\n    Ensure = \\\"Present\\\"\\n    Credential = $Credential\\n    DependsOn = \\\"[WindowsFeature]WebServerRole\\\"\\n}\\n\\n#\\n# Enable webdeploy in the firewall\\n#\\nScript WebDeployFwRule\\n{\\n    GetScript = \\n    {\\n        write-verbose \\\"Checking WebDeploy Firewall exception status\\\"\\n        $Rule = Get-NetFirewallRule -DisplayName \\\"WebDeploy_TCP_8172\\\"\\n        Return @{\\n            Result = \\\"DisplayName = $($Rule.DisplayName); Enabled = $($Rule.Enabled)\\\"\\n        }\\n    }\\n    SetScript =\\n    {\\n        write-verbose \\\"Creating Firewall exception for WebDeploy\\\"\\n        New-NetFirewallRule -DisplayName \\\"WebDeploy_TCP_8172\\\" -Direction Inbound -Action Allow -Protocol TCP -LocalPort 8172\\n    }\\n    TestScript =\\n    {\\n        if (Get-NetFirewallRule -DisplayName \\\"WebDeploy_TCP_8172\\\" -ErrorAction SilentlyContinue) \\n        {\\n            write-verbose \\\"WebDeploy Firewall exception already exists\\\"\\n            $true\\n        } \\n        else \\n        {\\n            write-verbose \\\"WebDeploy Firewall exception does not exist\\\"\\n            $false\\n        }\\n    }\\n    DependsOn = \\\"[Package]WebDeploy\\\"\\n}\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"#\\n# MMA\\n# Since this comes in an exe that can't be run silently, first copy the exe to the node,\\n# then unpack it. Then use the Package Resource with custom args to install it from the\\n# unpacked msi.\\n#\\nFile CopyMMAExe\\n{\\n    SourcePath = \\\"$AssetPath\\\\MMASetup-AMD64.exe\\\"\\n    DestinationPath = \\\"c:\\\\temp\\\\MMASetup-AMD64.exe\\\"\\n    Force = $true\\n    Type = \\\"File\\\"\\n    Ensure = \\\"Present\\\"\\n}\\n\\nScript UnpackMMAExe\\n{\\n    DependsOn =\\\"[File]CopyMMAExe\\\"\\n    TestScript = { $false }\\n    GetScript = {\\n        @{\\n            Result = \\\"UnpackMMAExe\\\"\\n        }\\n    }\\n    SetScript = {\\n        Write-Verbose \\\"Unpacking MMA.exe\\\"\\n        $job = Start-Job { &amp; \\\"c:\\\\temp\\\\MMASetup-AMD64.exe\\\" /t:c:\\\\temp\\\\MMA /c }\\n        Wait-Job $job\\n        Receive-Job $job\\n    }\\n}\\n\\nPackage MMA\\n{\\n    Name = \\\"Microsoft Monitoring Agent\\\"\\n    Path = \\\"c:\\\\temp\\\\MMA\\\\MOMAgent.msi\\\"\\n    Arguments = \\\"ACTION=INSTALL ADDLOCAL=MOMAgent,ACSAgent,APMAgent,AdvisorAgent AcceptEndUserLicenseAgreement=1 /qn /l*v c:\\\\temp\\\\MMA\\\\mmaInstall.log\\\"\\n    ProductId = \\\"\\\"\\n    Ensure = \\\"Present\\\"\\n    Dependson = \\\"[Script]UnpackMMAExe\\\"\\n}\\n\",\"language\":\"ps;\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/using-webdeploy-in-vnext-releases\"]],[\"em\"],[\"a\",[\"href\",\"http://1drv.ms/1IXntHL\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"A couple of months ago I did a series of posts (\"],[0,[0],1,\"this one\"],[0,[],0,\" has the summary of all my RM/DSC posts) about using PowerShell DSC in Release Management. I set out to see if I could create a DSC script that RM could invoke that would prep the environment and install the application. I managed to get it going, but never felt particularly good about the final solution – it always felt a little bit hacky. Not the entire solution per se – really just the \"],[0,[1],1,\"application\"],[0,[],0,\" bit.\"]]],[1,\"p\",[[0,[],0,\"The main reason for this was the fact that I need to hack the Script Resource in order to let me run commands on the target node with parameters. Initially I thought that the inability to do this natively in DSC was short-sighted from the architecture of DSC – but the more I thought about it, the more I realized that I was trying to shoehorn application installation into DSC.\"]]],[1,\"p\",[[0,[],0,\"DSC scripts should be declarative – my scripts were mostly declarative, but the application-specific parts of the script were very much imperative – and that started to smell.\"]]],[1,\"h2\",[[0,[],0,\"Idempotency\"]]],[1,\"p\",[[0,[],0,\"I wrote about what I consider to be the most important mental shift when working with PowerShell DSC – idempotency. The scripts you create need to be idempotent – that is they need to end up in the same end state no matter what the starting state is. This works really well for the environment that an application needs to run in – but it doesn’t really work so well for the application itself.\"]]],[1,\"p\",[[0,[],0,\"My conclusion is simple: use DSC to specify the \"],[0,[1],1,\"environment\"],[0,[],0,\", and use plain ol’ PowerShell to install your \"],[0,[1],1,\"application\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"PowerShell DSC resources are split into 3 actions – Get, Test and Set. The Get method gets the state of the resource on the node. The Set method “makes it so” – it enforces the state the script specifies. The Test method checks to see if the target node’s state matches the state the script specifies. Let’s consider an example: the WindowsFeature resource. Consider the following excerpt:\"]]],[10,0],[1,\"p\",[[0,[],0,\"When executing, this resource will check the corresponding WindowsFeature (IIS) on the target node using the Test method. If IIS is present, no action is taken (the node state matches the desired state specified in the script). If it’s not installed, the Set method is invoked to install/enable the IIS. Of course if we simply wanted to query the state of the WindowsFeature, the Get method would tell us the state (installed or not) of IIS.\"]]],[1,\"p\",[[0,[],0,\"This Get-Test-Set paradigm works well for environments – however, it starts to break down when you try to apply it to an application. Consider a Web Application with a SQL Database backend. How to you test if the application is in a particular state? You could check the schema of the database as an indication of the state; you could check if the site exists as an indication of the web site state. Of course this may not be sufficient for checking the state of your application.\"]]],[1,\"p\",[[0,[],0,\"(On a side note, if you’re using WebDeploy to deploy your website and you’re using Database Projects, you don’t need to worry, since these mechanisms are idempotent).\"]]],[1,\"p\",[[0,[],0,\"The point is, you may be deploying an application that doesn’t use an idempotent mechanism. In either case, you’re better off not trying to shoehorn application installation into DSC. Also, Release Management lets you execute both DSC and “plain” PowerShell against target nodes – so use them both.\"]]],[1,\"h2\",[[0,[],0,\"WebServerPreReqs Script\"]]],[1,\"p\",[[0,[],0,\"I also realized that I never published my “WebServerPreReqs” script. I use this script to prep a Web Server for my web application. There are four major sections to the script: Windows Features, runtimes, WebDeploy and MMC.\"]]],[1,\"p\",[[0,[],0,\"First, I ensure that Windows is in the state I need it to be – particularly IIS. I ensure that IIS is installed, as well as some other options like Windows authentication. Also, I ensure that the firewall allows WMI.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Next I install any runtimes my website requires – in this case, the MVC framework. You need to supply a network share somewhere for the installer – of course you could use a File resource as well, but you’d still need to have a source somewhere.\"]]],[10,2],[1,\"p\",[[0,[],0,\"I can’t advocate WebDeploy as a web deployment mechanism enough – if you’re not using it, you should be! However, in order to deploy an application remotely using WebDeploy, the WebDeploy agent needs to be running on the target node and the firewall port needs to be opened. No problem – easy to specify declaratively using DSC. I add the required arguments to get the installer to deploy and start the WebDeploy agent (see the Arguments setting in the Package WebDeploy resource). I also use a Script resource to Get-Test-Set the firewall rule for WebDeploy:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Finally, I wanted to make sure that MMC is installed so that I can monitor my application using Application Insights. This one was a little tricky since there isn’t an easy way to install the agent quietly – I have to unzip the installer and then invoke the MSI within. However, it’s still not that hard.\"]]],[10,4],[1,\"p\",[[0,[],0,\"After running this script against a Windows Server in any state, I can be sure that the server will run my application – no need to guess or hope.\"]]],[1,\"p\",[[0,[],0,\"You can download the entire script from \"],[0,[2],1,\"here\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Release Management\"]]],[1,\"p\",[[0,[],0,\"Now releasing my application is fairly easy in Release Management – execute two vNext script tasks: the first runs WebServerPreReqs DSC against the target node; the second runs a plain PowerShell script that invokes WebDeploy for my application using the drop folder of my build as the source.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"PowerShell DSC is meant to be declarative – any time you’re doing any imperative scripting, rip it out and put it into plain PowerShell. Typically this split is going to be along the line of \"],[0,[1],1,\"environment\"],[0,[],0,\" vs \"],[0,[1],1,\"application\"],[0,[],0,\". Use DSC for environment and plain PowerShell scripts for application deployment.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1421694359000,"status":"published","published_by":1},{"id":"09155b6b-ceeb-4416-baaa-04299c6970bd","title":"Release Management 2015 with Build vNext: Component to Artifact Name Matching and Other Fun Gotchas","slug":"release-management-2015-with-build-vnext-component-to-artifact-name-matching-and-other-fun-gotchas","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"param(\\n    [string]$srcDir,\\n    [string]$targetDir,\\n    [string]$fileFilter = \\\"*.*\\\"\\n)\\n\\nif (-not (Test-Path $targetDir)) {\\n    Write-Host \\\"Creating $targetDir\\\"\\n    mkdir $targetDir\\n}\\n\\nWrite-Host \\\"Executing xcopy /y '$srcDir\\\\$fileFilter' $targetDir\\\"\\nxcopy /y \\\"$srcDir\\\\$fileFilter\\\" $targetDir\\n\\nWrite-Host \\\"Done!\\\"\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a3119a20-6a71-4853-8ddc-641d544411fb.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/195bdd4c-4072-4709-9fcd-a0a63af64d70.png\\\" width=\\\"290\\\" height=\\\"151\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3f6230c3-b086-4ad6-be08-7637a600b25a.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5ad836ef-244d-46d5-a9fa-465161d97093.png\\\" width=\\\"351\\\" height=\\\"143\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d851e635-0537-49d6-98a5-f3c02d09db99.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c7cb6d5f-e997-4899-abad-a8ce8343c067.png\\\" width=\\\"312\\\" height=\\\"236\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/72187323-5516-4637-b0de-4789ae8c22b5.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3e2dd5b6-b22d-44e3-b785-ebfb16aaa240.png\\\" width=\\\"292\\\" height=\\\"250\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f080521b-ab6f-4dff-85f5-478cd4c77b96.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7ff7d1ae-0492-4ece-a747-7d2e9882e902.png\\\" width=\\\"244\\\" height=\\\"219\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f1d0cb99-2307-4847-bc0a-b6485e2ce1a6.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f515207a-6b1d-455e-b53b-e368e47e3853.png\\\" width=\\\"333\\\" height=\\\"285\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8c7af39b-f730-4585-ba6b-fb41ca58befb.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/98897692-3bd7-4f88-85ce-e311e02ad465.png\\\" width=\\\"360\\\" height=\\\"69\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/30296ba5-af98-49bd-9618-cd50fd3fe04a.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/50078e51-ce0c-4f62-8a3e-78cd05e09024.png\\\" width=\\\"334\\\" height=\\\"74\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.codewrecks.com/blog/index.php/2015/06/30/manage-artifacts-with-tfs-build-vnext/\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/visualstudioalm/archive/2014/11/11/what-s-new-in-release-management-for-vs-2013-update-4.aspx\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=b18DjXWyWuc\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve been setting up a couple of VMs in Azure with a TFS demo. Part of the demo is release management, and I finally got to upgrade Release Management to the 2015 release. I wanted to test integrating with the new build vNext engine. I faced some “fun” gotchas along the way. Here are my findings.\"]]],[1,\"h2\",[[0,[],0,\"Fail: 0 artifacts(s) found\"]]],[1,\"p\",[[0,[],0,\"After upgrading Release Management server, I gleefully associated a component with my build vNext build. I was happy when the build vNext builds appeared in the drop-down. Since I was using the root of the output folder, I simply selected “\\\\” as the location of the component (since I have several folders that I want to use via scripts, so I usually just specify the root of the drop folder).\"]]],[1,\"p\",[[0,[],0,\"I then queued the release – and the deployment failed almost instantly. “0 artifact(s) found corresponding to the name ‘FabFiber’ for BuildId: 91”. After a bit of head-scratching and head-to-table-banging, I wondered if the error was hinting at the fact that RM is actually looking for a published artifact named “FabFiber” in my build. Turns out that was correct.\"]]],[1,\"h2\",[[0,[],0,\"Component Names and Artifact Names\"]]],[1,\"p\",[[0,[],0,\"To make a long story short: you have to match the component name in Release Management with the artifact name in your build vNext “Publish Artifact” task. This may seem like a good idea, but for me it’s a pain, since I usually split my artifacts into Scripts, Sites, DBs etc. and publish each as a separate artifact so that I get a neat folder layout for my artifacts. Since I use PowerShell scripts to deploy, I used to specify the root folder “\\\\” for the component location and then used Scripts\\\\someScript.ps1 as the path to the script. So I had to go back to my build and add a PowerShell script to first put all the folders into a “root” folder for me and then use a single “Publish Artifacts” task to publish the neatly laid out folder structure. I looked at \"],[0,[0],1,\"this post\"],[0,[],0,\" from my friend Ricci Gian Maria to get some inspiration!\"]]],[1,\"p\",[[0,[],0,\"Here’s the script that I created and checked into source control:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now I have a couple of PowerShell tasks that copy the binaries (and other files) in the staging directory – which I am using as the root folder for my artifacts. I configure the msbuild arguments to publish the website webdeploy package to $(build.stagingDirectory)\\\\FabFiber, so I don’t need to copy it, since it’s already in the staging folder. For the DB components and scripts:\"]]],[3,\"ul\",[[[0,[],0,\"I configure the copy scripts to copy my DB components (dacpacs and publish.xmls) so I need 2 scripts which have the following args respectively:\"]],[[0,[],0,\"-srcDir MAIN\\\\FabFiber\\\\FabrikamFiber.CallCenter\\\\FabrikamFiber.Schema\\\\bin\\\\$(BuildConfiguration) -targetDir $(build.stagingDirectory)\\\\db -fileFilter *.dacpac\"]],[[0,[],0,\"-srcDir MAIN\\\\FabFiber\\\\FabrikamFiber.CallCenter\\\\FabrikamFiber.Schema\\\\bin\\\\$(BuildConfiguration) -targetDir $(build.stagingDirectory)\\\\DB -fileFilter *.publish.xml\"]],[[0,[],0,\"I copy the scripts folder directly from the workspace into the staging folder using these arguments:\"]],[[0,[],0,\"-srcDir MAIN\\\\FabFiber\\\\DscScripts -targetDir $(build.stagingDirectory)\\\\Scripts\"]],[[0,[],0,\"Finally, I publish the artifacts like so:\"]]]],[10,1],[1,\"p\",[[0,[],0,\"Now my build artifact (yes, a single artifact) looks as follows:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Back in Release Management, I made sure I had a component named “FabFiber” (to match the name of the artifact from the Publish Artifact task). I then also supplied “\\\\FabFiber” as the root folder for my components:\"]]],[10,3],[1,\"p\",[[0,[],0,\"That at least cleared up the “cannot find artifact” error.\"]]],[1,\"p\",[[0,[],0,\"A bonus of this is that you can now use server drops for releases instead of having to use shared folder drops. Just remember that if you choose to do this, you have to set up a ReleaseManagementShare folder. See \"],[0,[1],1,\"this post\"],[0,[],0,\" for more details (see point 7). I couldn’t get this to work for some reason so I reverted to a shared folder drop on the build.\"]]],[1,\"h2\",[[0,[],0,\"Renaming Components Gotcha\"]]],[1,\"p\",[[0,[],0,\"During my experimentation I renamed the component in Release Management that I was using in the release. This caused some strange behavior when trying to create releases: the build version picker was missing:\"]]],[10,4],[1,\"p\",[[0,[],0,\"I had to open the release template and set the component from the drop-down everywhere that it was referenced!\"]]],[10,5],[1,\"p\",[[0,[],0,\"Once that was done, I got the build version picker back:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Deployments started working again – \"],[0,[2],1,\"my name is Boris, and I am invincible\"],[0,[],0,\"!\"]]],[10,7],[1,\"h2\",[[0,[],0,\"The Parameter is Incorrect\"]]],[1,\"p\",[[0,[],0,\"A further error I encountered had to do with the upgrade from RM 2013. At least, I think that was the cause. The deployment would copy the files to the target server, but when the PowerShell task was invoked, I got a failure stating (helpfully – not!), “The parameter is incorrect.”\"]]],[10,8],[1,\"p\",[[0,[],0,\"At first I thought it was an error in my script – turns out that all you have to do to resolve this one is re-enter the password in the credentials for the PowerShell task in the release template. All of them. Again. Sigh… Hopefully this is just me and doesn’t happen to you when you upgrade your RM server.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I have to admit that I have a love-hate relationship with Release Management. It’s fast becoming more of a HATE-love relationship though. The single feature I see that it brings to the table is the approval workflow – the client is slow, the workflows are clunky and debugging is a pain.\"]]],[1,\"p\",[[0,[],0,\"I really can’t wait for the release of Web-based Release Management that will use the same engine as the build vNext engine, which should mean a vastly simpler authoring experience! Also the reporting and charting features we should see around releases are going to be great.\"]]],[1,\"p\",[[0,[],0,\"For now, the best advice I can give you regarding Release Management is to make sure you invest in agent-less deployments using PowerShell scripts. That way your upgrade path to the new Web-based Release Management will be much smoother and you’ll be able to reuse your investments (i.e. your scripts).\"]]],[1,\"p\",[[0,[],0,\"Perhaps your upgrade experiences will be happier than mine – I can only hope, dear reader, I can only hope.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1438185314000,"status":"published","published_by":1},{"id":"7875fd49-8c7c-4557-a611-4e663979078d","title":"Running Data-Driven Unit Tests with SpreadSheets on Sharepoint","slug":"running-data-driven-unit-tests-with-spreadsheets-on-sharepoint","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Consolas\\\"><font size=\\\"2\\\"><span><font color=\\\"#0000ff\\\">public</font></span><font color=\\\"#000000\\\">&nbsp;</font><span><font color=\\\"#0000ff\\\">static</font></span><font color=\\\"#000000\\\">&nbsp;</font><span><font color=\\\"#0000ff\\\">void</font></span><font color=\\\"#000000\\\"> MapNetworkDrive(</font><span><font color=\\\"#0000ff\\\">string</font></span><font color=\\\"#000000\\\"> localDrive, </font><span><font color=\\\"#0000ff\\\">string</font></span></font></font>\"}],[\"html\",{\"html\":\"<font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\"> serverDrive)<br>{<br>&nbsp;&nbsp;&nbsp; </font><span><font color=\\\"#0000ff\\\">var</font></span><font color=\\\"#000000\\\"> drive = </font><span><font color=\\\"#0000ff\\\">new</font></span><font color=\\\"#000000\\\">&nbsp;</font><span><font color=\\\"#2b91af\\\">NetworkDrive</font></span></font></font>\"}],[\"html\",{\"html\":\"<font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\">();<br>&nbsp;&nbsp;&nbsp; drive.PromptForCredentials = </font><span><font color=\\\"#0000ff\\\">false</font></span></font></font>\"}],[\"html\",{\"html\":\"<font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\">;<br>&nbsp;&nbsp;&nbsp; drive.ShareName = serverDrive;<br>&nbsp;&nbsp;&nbsp; drive.LocalDrive = localDrive;<br>&nbsp;&nbsp;&nbsp; drive.Force = </font><span><font color=\\\"#0000ff\\\">true</font></span></font><font color=\\\"#000000\\\" size=\\\"2\\\">;<br>&nbsp;&nbsp;&nbsp; drive.MapDrive();<br>}</font></font>\"}],[\"code\",{\"code\":\"<font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\">[</font><span><font color=\\\"#2b91af\\\">ClassInitialize</font></span></font></font><font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\">]<br></font><span><font color=\\\"#0000ff\\\">public</font></span><font color=\\\"#000000\\\">&nbsp;</font><span><font color=\\\"#0000ff\\\">static</font></span><font color=\\\"#000000\\\">&nbsp;</font><span><font color=\\\"#0000ff\\\">void</font></span><font color=\\\"#000000\\\"> Setup(</font><span><font color=\\\"#2b91af\\\">TestContext</font></span></font></font><font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\"> context)<br>{<br></font><span><font color=\\\"#2b91af\\\">    NetworkDrive</font></span><font color=\\\"#000000\\\">.MapNetworkDrive(</font><span><font color=\\\"#a31515\\\">@\\\"z:\\\\\\\"</font></span><font color=\\\"#000000\\\">, </font><span><font color=\\\"#a31515\\\">@\\\"\\\\\\\\server\\\\sites\\\\DefaultCollection\\\\Project\\\\TestMatrices\\\"</font></span></font><font color=\\\"#000000\\\" size=\\\"1\\\"><font size=\\\"2\\\">);<br>}</font>\\t</font></font>\",\"language\":\"\"}],[\"code\",{\"code\":\"<font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\">[</font><span><font color=\\\"#2b91af\\\">TestMethod</font></span></font></font><font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\">()]<br>[</font><span><font color=\\\"#2b91af\\\">Description</font></span><font color=\\\"#000000\\\">(</font><span><font color=\\\"#a31515\\\">\\\"Testing database component to add error information\\\"</font></span><font color=\\\"#000000\\\">)]</font></font></font><font face=\\\"Consolas\\\"><font color=\\\"#000000\\\"><br><font size=\\\"2\\\">[</font></font><font size=\\\"2\\\"><span><font color=\\\"#2b91af\\\">DataSource</font></span><font color=\\\"#000000\\\">(</font><span><font color=\\\"#a31515\\\">\\\"System.Data.Odbc\\\"</font></span><font color=\\\"#000000\\\">, </font><span><font color=\\\"#a31515\\\">@\\\"Dsn=Excel Files;dbq=z:\\\\Error\\\\ULP_Error_ErrorManager_Add.xlsx;defaultdir=.;driverid=1046;maxbuffersize=2048;pagetimeout=5\\\"</font></span><font color=\\\"#000000\\\">, </font><span><font color=\\\"#a31515\\\">\\\"ErrorAdd$\\\"</font></span><font color=\\\"#000000\\\">, </font><span><font color=\\\"#2b91af\\\">DataAccessMethod</font></span></font></font><font face=\\\"Consolas\\\"><font size=\\\"2\\\"><font color=\\\"#000000\\\">.Sequential)]<br></font><span><font color=\\\"#0000ff\\\">public</font></span><font color=\\\"#000000\\\">&nbsp;</font><span><font color=\\\"#0000ff\\\">void</font></span></font><font color=\\\"#000000\\\" size=\\\"2\\\"> AddErrorTest()<br>{</font></font>\",\"language\":\"\"}],[\"code\",{\"code\":\"<font color=\\\"#000000\\\" size=\\\"2\\\" face=\\\"Consolas\\\">    ...</font>\",\"language\":\"\"}],[\"code\",{\"code\":\"<font color=\\\"#000000\\\" size=\\\"2\\\" face=\\\"Consolas\\\">}</font>\",\"language\":\"\"}],[\"code\",{\"code\":\"<font face=\\\"Arial\\\">There’s one other gotcha that you might see – the test run on the build fails saying that the data source could not be opened. In this case, log into your build server as your build service (tfsbuild or whatever identity you’re using) and open any Excel file – make sure that you are not being prompted for your name and initials (a popup that appears the very first time you use Excel). This will break the build until you’ve dismissed this dialogue once.</font>\",\"language\":\"\"}],[\"code\",{\"code\":\"<font face=\\\"Arial\\\">A last aside: when trying to set up the document library for the spreadsheets, we could not get the Explorer View working in Sharepoint – we just got a “Web page could not be opened”. After using Fiddler, I could see an HTTP 405 error. Googling a bit, I found that in order for Sharepoint’s own WebDAV to work (that’s what the Sharepoint Explorer View uses), you need to uninstall IIS’s WebDAV feature! Go figure…</font>\",\"language\":\"\"}],[\"code\",{\"code\":\"<font face=\\\"Arial\\\">Happy testing!</font>\",\"language\":\"\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.codeproject.com/KB/system/mapnetdrive.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Data-driven unit tests are a great way to run lots of tests. You define a test harness (or method) and tie it to a spreadsheet in an Excel file to “drive” the test – each row in the spreadsheet is one iteration of the test case. I am not going to go into any more detail here – there are plenty of examples about how to set these up. Another advantage of using Excel for data-driven tests is that testers love Excel. They can open and edit the spreadsheet and add / edit / remove test cases at will.\"]]],[1,\"p\",[[0,[],0,\"When a developer creates a test method, they can specify the spreadsheet to connect to using the [DataSource] attribute. Usually, you’d have a spreadsheet in your solution and set it to deploy in the test settings file (or by marking its properties to copy to output on a build) and then use the |DataDirectory| placeholder in the [DataSource] attribute to tell the test to look in the build output folder for the Excel file to open.\"]]],[1,\"p\",[[0,[],0,\"There are some drawbacks to doing this if your test team is maintaining the spreadsheets. They’ll need to have access to the source folder (out of Visual Studio) in order to change the spreadsheet. This is hardly ideal. A much better approach would be to set up a document library on a fileshare. Going one step better, set up a document library on Sharepoint – that way you get version history on the test data spreadsheets.\"]]],[1,\"p\",[[0,[],0,\"So if you decide to put the spreadsheets in a Sharepoint document library, you’ll have to decide on which site you want to store your docs on and then jump through a few hoops to hook up the tests to the Sharepoint documents. It makes sense to use the Team Project Portal for these sorts of documents, but any Sharepoint site will do.\"]]],[1,\"p\",[[0,[],0,\"Once you’ve uploaded your Excel files to a document library (don’t forget to enable Versioning if you want to track changes to these files), you have to do 2 things to get the test methods to work.\"]]],[1,\"p\",[[0,[],0,\"Firstly, you’ll have to map a network drive to the document library in the [ClassInitialize] method of your test class. This maps the document library of sharepoint to a local drive letter. You need to do this programmatically since the mapping won’t persist after you log out. Make sure that the account that is running the build controller has at least read access to the document library! I downloaded (and tweaked) \"],[0,[0],1,\"this project from CodeProject\"],[0,[],0,\" to get some code for mapping network drives. I then created a method that map the drive:\"]]],[10,0],[10,1],[10,2],[10,3],[1,\"p\",[[0,[],0,\"I’m telling the library no to prompt for credentials and setting Force to true will unmap if the mapping exists already. You can then call this method from the [ClassInitialize] method:\"]]],[10,4],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Here I am mapping the z: to the root of the test document library on the Sharepoint portal for my Project. Once this is done, the [DataSource] attribute can just reference any spreadsheet from within this folder:\"]]],[10,5],[10,6],[10,7],[10,8],[10,9],[10,10]]}","published_at":1289342280000,"status":"published","published_by":1},{"id":"85ea1e67-05c2-4a2a-be40-e21b26b9ae6a","title":"Running DB Data Generation and Unit Tests in TeamBuild 2010","slug":"running-db-data-generation-and-unit-tests-in-teambuild-2010","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TLXD8GfKUPI/AAAAAAAAALU/9AjFDPLyJ9s/s1600-h/schemaexplorer3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"Schema explorer\\\" border=\\\"0\\\" alt=\\\"Schema explorer\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TLXD9-ToJVI/AAAAAAAAALY/EaEyRWt3-OQ/schemaexplorer_thumb1.png?imgmax=800\\\" width=\\\"219\\\" height=\\\"484\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TLXEANaMFqI/AAAAAAAAALc/g-08J70UYqk/s1600-h/datageneration3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"Data generation plan\\\" border=\\\"0\\\" alt=\\\"Data generation plan\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TLXECothJbI/AAAAAAAAALg/dU8o_Ny0K34/datageneration_thumb1.png?imgmax=800\\\" width=\\\"551\\\" height=\\\"484\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TLXEFMowDMI/AAAAAAAAALk/xKzkmQrIlOI/s1600-h/testSettings3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"Unit test settings\\\" border=\\\"0\\\" alt=\\\"Unit test settings\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TLXEHr_9qzI/AAAAAAAAALo/dC0LIUmUOxk/testSettings_thumb1.png?imgmax=800\\\" width=\\\"390\\\" height=\\\"484\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TLXEJACDllI/AAAAAAAAALs/19v_YQA8L7g/s1600-h/testspasslocallyPNG3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"Tests pass locally\\\" border=\\\"0\\\" alt=\\\"Tests pass locally\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TLXEKzXyI9I/AAAAAAAAALw/7m08cILeasM/testspasslocallyPNG_thumb1.png?imgmax=800\\\" width=\\\"644\\\" height=\\\"195\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TLXENju0bAI/AAAAAAAAAL0/zXiFUsAJkLM/s1600-h/buildfailed3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"Build only partially succeeds\\\" border=\\\"0\\\" alt=\\\"Build only partially succeeds\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TLXEQdrasFI/AAAAAAAAAL4/8DAo39beE4Q/buildfailed_thumb1.png?imgmax=800\\\" width=\\\"490\\\" height=\\\"484\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TLXERqFXHzI/AAAAAAAAAL8/DHw2_zm2VEo/s1600-h/pathfailed3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"The deployment path is wrong\\\" border=\\\"0\\\" alt=\\\"The deployment path is wrong\\\" src=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TLXETD1EetI/AAAAAAAAAMA/QOGKNFKmtOQ/pathfailed_thumb1.png?imgmax=800\\\" width=\\\"644\\\" height=\\\"148\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TLXEThTmjQI/AAAAAAAAAME/0BQEbrTNHic/s1600-h/modifiedtestproject4.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"Modified test project\\\" border=\\\"0\\\" alt=\\\"Modified test project\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TLXEUWQOH3I/AAAAAAAAAMI/KNjDzioKb10/modifiedtestproject_thumb2.png?imgmax=800\\\" width=\\\"222\\\" height=\\\"136\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TLXEVP_jh9I/AAAAAAAAAMM/q_8jj31KEMw/s1600-h/deploysettings4.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"deploy settings\\\" border=\\\"0\\\" alt=\\\"deploy settings\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TLXEW5Nu4LI/AAAAAAAAAMQ/j0d8gQIOdWs/deploysettings_thumb2.png?imgmax=800\\\" width=\\\"644\\\" height=\\\"232\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TLXEXuI48DI/AAAAAAAAAMU/G4pOLXZ6m7E/s1600-h/configurebuildtestsettings3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"Configure build test settings\\\" border=\\\"0\\\" alt=\\\"Configure build test settings\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TLXEZsiXKCI/AAAAAAAAAMY/waoJeA41ilM/configurebuildtestsettings_thumb1.png?imgmax=800\\\" width=\\\"644\\\" height=\\\"252\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TLXEbMl8i4I/AAAAAAAAAMc/hxaV4dlmxiY/s1600-h/completed3.png\\\"><img style=\\\"BORDER-RIGHT-WIDTH: 0px; DISPLAY: inline; BORDER-TOP-WIDTH: 0px; BORDER-BOTTOM-WIDTH: 0px; BORDER-LEFT-WIDTH: 0px\\\" title=\\\"completed\\\" border=\\\"0\\\" alt=\\\"completed\\\" src=\\\"http://lh4.ggpht.com/_d41Ixos7YsM/TLXEd30ndbI/AAAAAAAAAMg/P36cUt6IXI4/completed_thumb1.png?imgmax=800\\\" width=\\\"644\\\" height=\\\"417\\\"></a>\"}]],\"markups\":[[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Using “Data Dude” (or VS DB Professional) in VS 2010 allows you to bring Agile to your database development. You can import a database schema and add it to source control – so you get linking to work items as well as branching and labelling etc. Furthermore, you can use TeamBuild to build a .schema file, and then use vsdbcmd.exe to deploy schema changes (incrementally) to databases. You can also create unit tests and data generation plans.\"]]],[1,\"p\",[[0,[],0,\"Importing the schema and building the schema in TeamBuild is straightforward. However, if you want to add running data generation and unit tests in your build, you have to jump through a few hoops.\"]]],[1,\"p\",[[0,[],0,\"Let’s go through an example.\"]]],[1,\"p\",[[0,[],0,\"First, we’ll import the schema into a database project using the SQL 2008 Database Wizard project template.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Next, right click the Data Generation Plans folder and add a new Data Generation plan. The details are not important for this example, so just accept the defaults. Press F5 to generate data to the database.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now go to the schema view and select a stored proc. Right click and select “Create Unit Tests”.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Create a new test project and fill in some tests. Again, the exact details here are not critical – just make sure you’ve got some tests that pass when you run them from Visual Studio.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Next you’ll have to create a build. Check you solution into source control. Then right click the builds node of the project and create a new build definition. Choose the workspace and the drop location, and leave everything else defaulted. This generates a standard build that will compile the database project and then run the unit tests.\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, this build will only partially succeed – the unit tests will fail. This is due to the fact that the paths to the .dbproj and .dgen files are different during a TeamBuild than when we build in VS. So we have to modify the configs.\"]]],[10,4],[10,5],[1,\"p\",[[0,[],0,\"Right click the app.config  of the Test Project. Copy and paste it so that you have a copy of the config. Rename this to \"],[0,[0],1,\"buildserver\"],[0,[],0,\".dbunittest.config where \"],[0,[0],1,\"buildserver\"],[0,[],0,\" is the name of your build server (this could also be the name of the user running the build). The .dbunittest is important since TeamBuild looks for *.dbunittest.config if we override the default config.\"]]],[1,\"p\",[[0,[],0,\"Now remove all the tags except the  and its contents. Make sure that the very 1st character of the file is the <, else TeamBuild won’t read this config correctly. Add “..\\\\Sources\\\\” into the paths to the .dbproj file and .dgen files (just after the ..\\\\..\\\\).\"]]],[1,\"p\",[[0,[],0,\"<DatabaseUnitTesting>\"],[1,[],0,0],[0,[],0,\"    <DatabaseDeployment DatabaseProjectFileName=\\\"..\\\\..\\\\..\\\\Sources\\\\TailspinToys\\\\TailspinToys.dbproj\\\"\"],[1,[],0,1],[0,[],0,\"        Configuration=\\\"Debug\\\" />\"],[1,[],0,2],[0,[],0,\"    <DataGeneration DataGenerationFileName=\\\"..\\\\..\\\\..\\\\Sources\\\\TailspinToys\\\\Data Generation Plans\\\\UnitTest.dgen\\\"\"],[1,[],0,3],[0,[],0,\"        ClearDatabase=\\\"true\\\" />\"],[1,[],0,4],[0,[],0,\"    <ExecutionContext Provider=\\\"System.Data.SqlClient\\\" ConnectionString=\\\"Data Source=TRAIN-TFS;Initial Catalog=TailspinToys;Integrated Security=True;Pooling=False\\\"\"],[1,[],0,5],[0,[],0,\"        CommandTimeout=\\\"30\\\" />\"],[1,[],0,6],[0,[],0,\"    <PrivilegedContext Provider=\\\"System.Data.SqlClient\\\" ConnectionString=\\\"Data Source=TRAIN-TFS;Initial Catalog=TailspinToys;Integrated Security=True;Pooling=False\\\"\"],[1,[],0,7],[0,[],0,\"        CommandTimeout=\\\"30\\\" />\"],[1,[],0,8],[0,[],0,\"DatabaseUnitTesting>\"],[1,[],0,9]]],[1,\"p\",[[0,[],0,\"Open up the app.config in the Test project. Add AllowConfigurationOverride=”true” to the  tag. This prompts TeamBuild to look for the config we created in the previous step.\"]]],[1,\"p\",[[0,[],0,\"<DatabaseUnitTesting AllowConfigurationOverride=\\\"true\\\">\"]]],[10,6],[1,\"p\",[[0,[],0,\"Finally we need to create a new test settings file. Right click the Solution Items folder and add a new item – select the Test Settings item. You can name this whatever you want to and set whatever you want for your settings. The important bit here is to go to the “Deployment” tag and check the “Enable Deployment”. Then click the “Add” button and browse to the \"],[0,[0],1,\"buildserver\"],[0,[],0,\".dbunittest.config file (you’ll have to change the file-type drop-down to *.*). Check in your solution.\"]]],[10,7],[1,\"p\",[[0,[],0,\"Check your solution into source control.\"]]],[1,\"p\",[[0,[],0,\"Finally we’ll modify the build. Edit the build definition that you created before. In the Process tab, click the ellipses after the Automated Test Settings to get the test settings dialogue. Set the Test Settings File to be the test settings we just created. If you can’t see the file, it may be that you haven’t checked it into source control – this browser browses source control, not your local drive.\"]]],[10,8],[1,\"p\",[[0,[],0,\"Now queue a build and check that your tests are passing!\"]]],[10,9]]}","published_at":1287013080000,"status":"published","published_by":1},{"id":"76a570a7-36fe-42e5-8805-57f783afbe02","title":"Running Lab Management with XP SP3 Machines","slug":"running-lab-management-with-xp-sp3-machines","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-5JMaUoD2vsQ/TfnobWAUwyI/AAAAAAAAAQk/oxtyWf30Ucs/s1600-h/image%25255B3%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-zzOipzaI7HI/TfnocA_gTbI/AAAAAAAAAQo/no1SmH4izqE/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"368\\\" height=\\\"63\\\"></a>\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"http://social.msdn.microsoft.com/Forums/en-US/vststest/thread/df043823-ffcf-46a4-9e47-1c4b8854ca13/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"If you’re running Lab Management with lab machines with XP SP3, you may run into a problem where the Test Agent is never ready for running tests.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The environment was starting up fine and configuring both network isolation as well as the workflow agent. However, the Test Capability was continually not ready – the event logs had a message like this:\"]]],[1,\"p\",[[0,[],0,\"Unable to connect to the controller on 'TFSMachine.Domain:6901'. The agent can connect to the controller but the controller cannot connect to the agent because of following reason: An error occurred while processing the request on the server: System.IO.IOException: The write operation failed, see inner exception. ---> System.ComponentModel.Win32Exception: \"],[0,[0],1,\"The message or signature supplied for verification has been altered.\"]]],[1,\"p\",[[0,[],0,\"Scratching around led me to this rather helpful blog site: \"],[0,[1],1,\"Troubleshooting Guide for Visual Studio Test Controller and Agent\"],[0,[],0,\". The problem is mentioned there – it’s a problem with a Windows hotfix (KB968389) which you simply need to uninstall from the XP Lab machine. Reboot and you’re good to go.\"]]],[1,\"p\",[[0,[],0,\"Happy Labbing!\"]]]]}","published_at":1308255840000,"status":"published","published_by":1},{"id":"c4d7be22-4772-4cfc-b912-c41a9d9dcaba","title":"Running Load Tests in Lab Management: Perfmon Issues","slug":"running-load-tests-in-lab-management-perfmon-issues","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"I am preparing for the demo at DevDays, and we wanted to run a Load Test using our Lab environment. However, when trying to access the performance counters on the lab machines, I got “Access Denied” errors. That lead to some searching on why this is the case.\"]]],[1,\"p\",[[0,[],0,\"To access performance counters on the lab machines, you have to have 2 things in place:\"]]],[1,\"p\",[[0,[],0,\"1. The Remote Registry service must be running on each lab machine you want to monitor\"]]],[1,\"p\",[[0,[],0,\"2. The user that you’re running VS out of (or the identity of the Test Controller) needs to be an administrator on the lab machines.\"]]],[1,\"h2\",[[0,[],0,\"Remote Registry Service\"]]],[1,\"p\",[[0,[],0,\"This one’s easy – open the services snap-in and start the service (set it to start automatically if you want to).\"]]],[1,\"h2\",[[0,[],0,\"Permissions\"]]],[1,\"p\",[[0,[],0,\"This one was a little harder. In my lab, I have the computers connecting to a workgroup, so I couldn’t just add notiontraining\\\\colind as an administrator (since the lab machines aren’t on the notiontraining domain, which is the domain I am running on). So I created a local user on each lab machine with the name “colind” and the same password as notiontraining\\\\colind. I then added the local colind user into the local admin group.\"]]],[1,\"p\",[[0,[],0,\"Now I can fire up VS on my box (logged in as notiontraining\\\\colind) and can add the performance counters to each lab machine without any issues.\"]]],[1,\"p\",[[0,[],0,\"Happy Load Testing!\"]]]]}","published_at":1305590340000,"status":"published","published_by":1},{"id":"034cf4fd-cdd5-4a23-8759-3b17e3807a7a","title":"Running Selenium Tests in Docker using VSTS Release Management","slug":"running-selenium-tests-in-docker-using-vsts-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/25d0d00c-4ccc-4026-900b-f50755bb9b6c.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/3d91a7fa-2c3e-4cdb-aa9c-54d9b6491fbd.png\\\" width=\\\"745\\\" height=\\\"282\\\"></a>\"}],[\"code\",{\"code\":\"private void Test(ICapabilities capabilities)\\n{\\n    var driver = new RemoteWebDriver(new Uri(HubUrl), capabilities);\\n    driver.Navigate().GoToUrl(BaseUrl);\\n    // other test steps here\\n}\\n\\n[TestMethod]\\npublic void HomePage()\\n{\\n    Test(DesiredCapabilities.Chrome());\\n    Test(DesiredCapabilities.Firefox());\\n}\\n\",\"language\":\"csharp; highlight\"}],[\"code\",{\"code\":\"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\" ?&gt;\\n&lt;RunSettings&gt;\\n    &lt;TestRunParameters&gt;\\n        &lt;Parameter name=\\\"BaseUrl\\\" value=\\\"http://bing.com\\\" /&gt;\\n        &lt;Parameter name=\\\"HubUrl\\\" value=\\\"http://localhost:4444/wd/hub\\\" /&gt;\\n    &lt;/TestRunParameters&gt;\\n&lt;/RunSettings&gt;\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/637a3469-5954-4028-a336-2af270dd3d08.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/21e1beba-4284-457a-8a7b-ab3e696965d9.png\\\" width=\\\"351\\\" height=\\\"222\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c75aac1c-c4d9-4e97-8843-bbba84245a84.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/1bc5d71f-aa64-44ea-baa2-8782da1090d2.png\\\" width=\\\"773\\\" height=\\\"340\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">azure vm docker create --data-disk-size 22 --vm-size \\\"Standard_d1_v2\\\" --image-urn $urn --admin-username vsts --admin-password $password --nic-name \\\"cd-dockerhost-nic\\\" --vnet-address-prefix \\\"10.2.0.0/24\\\" --vnet-name \\\"cd-dockerhost-vnet\\\" --vnet-subnet-address-prefix \\\"10.2.0.0/24\\\" --vnet-subnet-name \\\"default\\\" --public-ip-domain-name \\\"cd-dockerhost\\\"&nbsp; --public-ip-name \\\"cd-dockerhost-pip\\\" --public-ip-allocationmethod \\\"dynamic\\\" --name \\\"cd-dockerhost\\\" --resource-group \\\"cd-docker\\\" --storage-account-name \\\"cddockerstore\\\" --location \\\"westus\\\" --os-type \\\"Linux\\\" --docker-cert-cn \\\"cd-dockerhost.westus.cloudapp.azure.com\\\"</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/1307331b-df06-40fa-a28e-8861f282e19d.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/e5770c4a-f7b1-4d40-aae0-b807fa449e23.png\\\" width=\\\"551\\\" height=\\\"319\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/0c178c4b-f5d5-449e-a6e1-1e5636f5529f.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4a1bd38f-c704-45ae-b152-a7ddae0e32c8.png\\\" width=\\\"467\\\" height=\\\"205\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">docker -H $dockerhost --tls info</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/179e94fd-6868-4e0d-911c-5b6c39fd9e21.png\\\"><img title=\\\"SNAGHTML2b3ba0\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"SNAGHTML2b3ba0\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/e426e1bb-4e15-4d3a-bfa9-3be04d47aa63.png\\\" width=\\\"494\\\" height=\\\"208\\\"></a>\"}],[\"code\",{\"code\":\"hub:\\n  image: selenium/hub\\n  ports:\\n    - \\\"4444:4444\\\"\\n  \\nchrome-node:\\n  image: selenium/node-chrome\\n  links:\\n    - hub\\n\\nff-node:\\n  image: selenium/node-firefox\\n  links:\\n    - hub\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/7c1234cf-a831-4827-96fc-fda2f3fd8246.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/faad0481-94e6-4ee8-9703-2bed37159ae6.png\\\" width=\\\"511\\\" height=\\\"367\\\"></a>\"}],[\"code\",{\"code\":\"docker -H $dockerhost --tls run --env VSTS_ACCOUNT=$vstsAcc --env VSTS_TOKEN=$pat --env VSTS_POOL=docker -it microsoft/vsts-agent\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/43793dbc-bffe-48c5-ba75-c9ba2cb74725.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/e32df3b8-00b6-4a57-898e-8afe7d417363.png\\\" width=\\\"412\\\" height=\\\"312\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/413ccf0a-5a6a-4477-a81d-d807c3b0fc4d.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/9be0bed0-d4aa-49b5-8fa2-f85d988fcc6c.png\\\" width=\\\"550\\\" height=\\\"161\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/ee82ca0b-2d9a-480a-baef-e54639c71a77.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/2e0114be-d110-474b-b8d6-b3bb55e47db6.png\\\" width=\\\"327\\\" height=\\\"341\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/b37910bc-6cf2-42be-b0f3-a8f708432712.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/154ff891-cb9e-4dc6-8fc3-01d85fa3cd61.png\\\" width=\\\"343\\\" height=\\\"305\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/85392ae0-41e8-46eb-ac18-aae5075780d6.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/6f263221-b3b4-4233-b0a3-da3bab7f6bd5.png\\\" width=\\\"769\\\" height=\\\"386\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/bf70ea38-90fa-4904-91dd-bd81b9663ab7.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/b81be1bd-09a0-42ed-84f8-a43b95d3f6d9.png\\\" width=\\\"364\\\" height=\\\"301\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/5ddee09a-106f-4bb6-8cf9-cef95d08a19f.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/302c3c26-3251-4c25-8922-cb9866bc3866.png\\\" width=\\\"690\\\" height=\\\"278\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">SSL error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:581)</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/b55b6e90-9922-4986-b8bb-bbab1ae00767.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/c92d14c5-ff6c-46a3-af19-2d6ad0ade7e4.png\\\" width=\\\"787\\\" height=\\\"193\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">run -d -P --name selenium-hub selenium/hub</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">run -d --link selenium-hub:hub selenium/node-chrome</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">run -d --link selenium-hub:hub selenium/node-firefox</font>\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-docker-tests\"]],[\"a\",[\"href\",\"https://fishshell.com/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-docker-tests/blob/master/scripts/createDockerHost.fish\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=ms-vscs-rm.docker\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-docker/issues/38\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/parallel-testing-in-a-selenium-grid-with-vsts\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"The other day I was doing a POC to run some Selenium tests in a Release. I came across some Selenium docker images that I thought would be perfect – you can spin up a Selenium grid (or hub) container and then join as many node containers as you want to (the node container is where the tests will actually run). The really cool thing about the node containers is that the container is configured with a browser (there are images for Chrome and Firefox) meaning you don’t have to install and configure a browser or manually run Selenium to join the grid. Just fire up a couple containers and you’re ready to test!\"]]],[1,\"p\",[[0,[],0,\"The source code for this post is on \"],[0,[0],1,\"Github\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Here’s a diagram of the components:\"]]],[10,0],[1,\"h2\",[[0,[],0,\"The Tests\"]]],[1,\"p\",[[0,[],0,\"To code the tests, I use Selenium WebDriver. When it comes to instantiating a driver instance, I use the RemoteWebDriver class and pass in the Selenium Grid hub URL as well as the capabilities that I need for the test (including which browser to use) – see line 3:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Line 4 includes a setting that is specific to the test – in this case the first page to navigate to.\"]]],[1,\"p\",[[0,[],0,\"When running this test, we need to be able to pass the environment specific values for the HubUrl and BaseUrl into the invocation. That’s where we can use a runsettings file.\"]]],[1,\"h3\",[[0,[],0,\"Test RunSettings\"]]],[1,\"p\",[[0,[],0,\"The runsettings file for this example is simple – it’s just XML and we’re just using the TestRunParameters element to set the properties:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can of course add other settings to the runsettings file for the other environment specific values you need to run your tests. To test the setting in VS, make sure to go to Test->Test Settings->Select Test Settings File and browse to your runsettings file.\"]]],[1,\"h2\",[[0,[],0,\"The Build\"]]],[1,\"p\",[[0,[],0,\"The build is really simple – in my case I just build the test project. Of course in the real world you’ll be building your application as well as the test assemblies. The key here is to ensure that you upload the test assemblies as well as the runsettings file to the drop (more on what’s in the runsettings file later). The runsettings file can be uploaded using two methods: either copy it using a Copy Files task into the artifact staging directory – or you can mark the file’s properties in the solution to “Copy Always” to ensure it’s copied to the bin folder when you compile. I’ve selected the latter option.\"]]],[1,\"p\",[[0,[],0,\"Here’s what the properties for the file look like in VS:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Here’s the build definition:\"]]],[10,4],[1,\"h2\",[[0,[],0,\"The Docker Host\"]]],[1,\"p\",[[0,[],0,\"If you don’t have a docker host, the fastest way to get one is to spin it up in Azure using the Azure CLI – especially since that will create the certificates to secure the docker connection for you! If you’ve got a docker host already, you can skip this section – but you will need to know where the certs are for your host for later steps.\"]]],[1,\"p\",[[0,[],0,\"Here are the steps you need to take to do that (I did this all in my Windows Bash terminal):\"]]],[3,\"ol\",[[[0,[],0,\"Install node and npm\"]],[[0,[],0,\"Install the azure-cli using “npm install –g azure-cli”\"]],[[0,[],0,\"Run “azure login” and log in to your Azure account\"]],[[0,[],0,\"Don’t forget to set your subscription if you have more than one\"]],[[0,[],0,\"Create an Azure Resource Group using “azure group create <name> <location>”\"]],[[0,[],0,\"Run “azure vm image list –l westus –p Canonical” to get a list of the Ubuntu images. Select the Urn of the image you want to base the VM on and store it – it will be something like “Canonical:UbuntuServer:16.04-LTS:16.04.201702240”. I’ve saved the value into $urn for the next command.\"]],[[0,[],0,\"Run the azure vm docker create command – something like this:\"]]]],[10,5],[1,\"p\",[[0,[],0,\"Here’s the run from within my bash terminal:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Here’s the result in the Portal:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Once the docker host is created, you’ll be able to log in using the certs that were created. To test it, run the following command:\"]]],[10,8],[10,9],[1,\"p\",[[0,[],0,\"I’ve included the commands in a \"],[0,[1],1,\"fish\"],[0,[],0,\" script \"],[0,[2],1,\"here\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"The docker-compose.yml\"]]],[1,\"p\",[[0,[],0,\"The plan is to run multiple containers – one for the Selenium Grid hub and any number of containers for however many nodes we want to run tests in. We can call docker run for each container, or we can be smart and use docker-compose!\"]]],[1,\"p\",[[0,[],0,\"Here’s the docker-compose.yml file:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Here we define three containers – named hub, chrome-node and ff-node. For each container we specify what image should be used (this is the image that is passed to a docker run command). For the hub, we map the container port 4444 to the host port 4444. This is the only port that needs to be accessible outside the docker host. The node containers don’t need to map ports since we’re never going to target them directly. To connect the nodes to the hub, we simple use the links keyword and specify the name(s) of the containers we want to link to – in this case, we’re linking both nodes to the hub container. Internally, the node containers will use this link to wire themselves up to the hub – we don’t need to do any of that plumbing ourselves - really elegant!\"]]],[1,\"h2\",[[0,[],0,\"The Release\"]]],[1,\"p\",[[0,[],0,\"The release requires us to run docker commands to start a Selenium hub and then as many nodes as we need. You can install \"],[0,[3],1,\"this extension\"],[0,[],0,\" from the marketplace to get docker tasks that you can use in build/release. Once the docker tasks get the containers running, we can run our tests, passing in the hub URL so that the Selenium tests hit the hub container, which will distribute the tests to the nodes based on the desired capabilities. Once the tests complete, we can optionally stop the containers.\"]]],[1,\"h3\",[[0,[],0,\"Define the Docker Endpoint\"]]],[1,\"p\",[[0,[],0,\"In order to run commands against the docker host from within the release, we’ll need to configure a docker endpoint. Once you’ve installed the docker extension from the marketplace, navigate to your team project and click the gear icon and select Services. Then add a new Docker Host service, entering your certificates:\"]]],[10,11],[1,\"h3\",[[0,[],0,\"Docker VSTS Agent\"]]],[1,\"p\",[[0,[],0,\"We’re almost ready to create the release – but you need an agent that has the docker client installed so that it can run docker commands! The easiest way to do this – is to run the vsts agent docker image on your docker host. Here’s the command:\"]]],[10,12],[1,\"p\",[[0,[],0,\"I am connecting this agent to a queue called docker – so I had to create that queue in my VSTS project. I wanted a separate queue because I want to use the docker agent to run the docker commands and then use the hosted agent to run the tests – since the tests need to run on Windows. Of course I could have just created a Windows VM with the agent and the docker bits – that way I could run the release on the single agent.\"]]],[1,\"h3\",[[0,[],0,\"The Release Definition\"]]],[1,\"p\",[[0,[],0,\"Create a new Release Definition and start from the empty template. Set the build to the build that contains your tests so that the tests become an artifact for the release. Conceptually, we want to spin up the Selenium containers for the test, run the tests and then (optionally) stop the containers. You also want to deploy your app, typically before you run your tests – I’ll skip the deployment steps for this post. You can do all three of these phases on a single agent – as long as the agent has docker (and docker-compose) installed and VS 2017 to run tests. Alternatively, you can do what I’m doing and create three separate phases – the docker commands run against a docker-enabled agent (the VSTS docker image that I we just got running) while the tests run off a Windows agent. Here’s what that looks like in a release:\"]]],[10,13],[1,\"p\",[[0,[],0,\"Here are the steps to get the release configured:\"]]],[3,\"ol\",[[[0,[],0,\"Create a new Release Definition and rename the release by clicking the pencil icon next to the name\"]],[[0,[],0,\"Rename “Environment 1” to “Test” or whatever you want to call the environment\"]],[[0,[],0,\"Add a “Run on agent” phase (click the dropdown next to the “Add Tasks” button)\"]],[[0,[],0,\"Set the queue for that phase to “docker” (or whatever queue you are using for your docker-enabled agents)\"]]]],[10,14],[3,\"ol\",[[[0,[],0,\"In this phase, add a “Docker-compose” task and configure it as follows:\"]]]],[10,15],[3,\"ol\",[[[0,[],0,\"Change the action to “Run service images” (this ends up calling docker-compose up)\"]],[[0,[],0,\"Uncheck Build Images and check Run in Background\"]],[[0,[],0,\"Set the Docker Host Connection\"]],[[0,[],0,\"In the next phase, add tasks to deploy your app (I’m skipping these tasks for this post)\"]],[[0,[],0,\"Add a VSTest task and configure it as follows:\"]]]],[10,16],[3,\"ol\",[[[0,[],0,\"I’m using V2 of the Test Agent task\"]],[[0,[],0,\"I update the Test Assemblies filter to find any assembly with UITest in the name\"]],[[0,[],0,\"I point the Settings File to the runsettings file\"]],[[0,[],0,\"I override the values for the HubUrl and BaseUrl using environment variables\"]],[[0,[],0,\"Click the ellipses button on the Test environment and configure the variables, using the name of your docker host for the HubUrl (note also how the port is the port from the docker-compose.yml file):\"]]]],[10,17],[3,\"ol\",[[[0,[],0,\"In the third (optional) phase, I use another Docker Compose task to run docker-compose down to shut down the containers\"]]]],[10,18],[3,\"ol\",[[[0,[],0,\"This time set the Action to “Run a Docker Compose command” and enter “down” for the Command\"]],[[0,[],0,\"Again use the docker host connection\"]]]],[1,\"p\",[[0,[],0,\"We can now queue and run the release!\"]]],[1,\"p\",[[0,[],0,\"My release is successful and I can see the tests in the Tests tab (don’t forget to change the Outcome filter to Passed – the grid defaults this to Failed):\"]]],[10,19],[1,\"h2\",[[0,[],0,\"Some Challenges\"]]],[1,\"h3\",[[0,[],0,\"Docker-compose SSL failures\"]]],[1,\"p\",[[0,[],0,\"I could not get the docker-compose task to work using the VSTS agent docker image. I kept getting certificate errors like this:\"]]],[10,20],[1,\"p\",[[0,[],0,\"I did \"],[0,[4],1,\"log an issue\"],[0,[],0,\" on the VSTS Docker Tasks repo, but I’m not sure if this is a bug in the extension or the VSTS docker agent. I was able to replicate this behavior locally by running docker-compose. What I found is that I can run docker-compose successfully if I explicitly pass in the ca.pem, cert.pem and key.pem files as command arguments – but if I specified them using environment variables, docker-compose failes with the SSL error. I was able to run docker commands successfully using the Docker tasks in the release – but that would mean running three commands (assuming I only want three containers) in the pre-test phase and another three in the post-test phase to stop each container. Here’s what that would look like:\"]]],[10,21],[1,\"p\",[[0,[],0,\"You can use the following commands to run the containers and link them (manually doing what the docker-compose.yml file does):\"]]],[10,22],[10,23],[10,24],[1,\"p\",[[0,[],0,\"To get the run for this post working, I just ran the docker-compose from my local machine (passing in the certs explicitly) and disabled the Docker Compose task in my release.\"]]],[1,\"p\",[[0,[],0,\"EDIT (3/9/2017): I figured out the issue I was having: when I created the docker host I wasn’t specifying a CN for the certificates. The default is *, which was causing my SSL issues. When I configured the CN correctly using\"]]],[1,\"p\",[[0,[],0,\"--docker-cert-cn” \\\"cd-dockerhost.westus.cloudapp.azure.com\\\", everything worked nicely.\"]]],[1,\"h3\",[[0,[],0,\"Running Tests in the Hosted Agent\"]]],[1,\"p\",[[0,[],0,\"I also could not get the test task to run successfully using the hosted agent – but it did run successfully if I used a private windows agent. This is because at this time VS 2017 is not yet installed on the hosted agent. Running tests from the hosted agent will work just fine once VS 2017 is installed onto it.\"]]],[1,\"h2\",[[0,[],0,\"Pros and Cons\"]]],[1,\"p\",[[0,[],0,\"This technique is quite elegant – but there are pros and cons.\"]]],[1,\"p\",[[0,[],0,\"Pros:\"]]],[3,\"ul\",[[[0,[],0,\"Get lots of Selenium nodes registered to a Selenium hub to enable lots of parallel testing (refer to my previous blog on how to \"],[0,[5],1,\"run tests in parallel in a grid\"],[0,[],0,\")\"]],[[0,[],0,\"No config required – you can run tests on the nodes as-is\"]]]],[1,\"p\",[[0,[],0,\"Cons:\"]]],[3,\"ul\",[[[0,[],0,\"Only Chrome and Firefox tests supported, since there are only docker images for these browsers. Technically you could join any node you want to to the hub container if you wanted other browsers, but at that point you may as well configure the hub outside docker anyway.\"]]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"I really like how easy it is to get a Selenium grid up and running using Docker. This should make testing fast – especially if you’re running tests in parallel. Once again VSTS makes advanced pipelines easy to tame!\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1489106605000,"status":"published","published_by":1},{"id":"73e616a4-0a4a-47a1-a97c-80ecba1f7319","title":"Running the New DotNet Core VSTS Agent in a Docker Container","slug":"running-the-new-dotnet-core-vsts-agent-in-a-docker-container","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"FROM microsoft/dotnet:1.0.0-core\\n\\n# defaults - override them using --build-arg\\nARG AGENT_URL=://github.com/Microsoft/vsts-agent/releases/download/v2.104.0/vsts-agent-ubuntu.14.04-x64-2.104.0.tar.gz\\nARG AGENT_NAME=docker\\nARG AGENT_POOL=default\\n\\n# you must supply these to the build command using --build-arg\\nARG VSTS_ACC\\nARG PAT\\n\\n# install git\\n#RUN apt-get update &amp;&amp; apt-get -y install software-properties-common &amp;&amp; apt-add-repository ppa:git-core/ppa\\nRUN apt-get update &amp;&amp; apt-get -y install git\\n\\n# create a user\\nRUN useradd -ms /bin/bash agent\\nUSER agent\\nWORKDIR /home/agent\\n\\n# download the agent tarball\\n#RUN curl -Lo agent.tar.gz $AGENT_URL &amp;&amp; tar xvf agent.tar.gz &amp;&amp; rm agent.tar.gz\\nCOPY *.tar.gz .\\nRUN tar xzf *.tar.gz &amp;&amp; rm -f *.tar.gz\\nRUN bin/Agent.Listener configure --url https://$VSTS_ACC.visualstudio.com --agent $AGENT_NAME --pool $AGENT_POOL --acceptteeeula --auth PAT --token $PAT --unattended\\n\\nENTRYPOINT ./run.sh\",\"language\":\"\"}],[\"code\",{\"code\":\"docker build . --build-arg VSTS_ACC=myVSTSAcc --build-arg PAT=abd64… --build-arg AGENT_POOL=docker –t colin/agent\\n\",\"language\":\"\"}],[\"code\",{\"code\":\"&gt; docker run -it colin/agent:latest\\n\\nScanning for tool capabilities.\\nConnecting to the server.\\n2016-07-28 17:56:57Z: Listening for Jobs\\n\\n\",\"language\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">The job has been abandoned because agent docker did not renew the lock. Ensure agent is running, not sleeping, and has not lost communication with the service.</font>\"}]],\"markups\":[[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=colinsalmcorner.colinsalmcorner-buildtasks\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/VersionAssemblies\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/ReplaceTokens\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-task-lib\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-agent/blob/master/README.md\"]],[\"a\",[\"href\",\"https://docs.docker.com/docker-for-windows/\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/docker\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-agent/releases\"]],[\"a\",[\"href\",\"https://myVSTSAcc.visualstudio.com\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This week I finally got around to updating my \"],[0,[0],1,\"VSTS extension\"],[0,[],0,\" (which bundle x-plat \"],[0,[1],1,\"VersionAssembly\"],[0,[],0,\" and \"],[0,[2],1,\"ReplaceTokens\"],[0,[],0,\" tasks) to use the new \"],[0,[3],1,\"vsts-task-lib\"],[0,[],0,\", which is used by the new \"],[0,[4],1,\"DotNet Core vsts-agent\"],[0,[],0,\". One of the bonuses of the new agent is that it can run in a DotNet Core Docker container! Since I am running \"],[0,[5],1,\"Docker for Windows\"],[0,[],0,\", I can now (relatively) easily spin up a test agent in a container to run test – a precursor to running the agent in a container as the \"],[0,[6],1,\"de-facto\"],[0,[],0,\" method of running agents!\"]]],[1,\"p\",[[0,[],0,\"All you need to do this is a Dockerfile with a couple of commands that do the following:\"]]],[3,\"ol\",[[[0,[],0,\"Install Git\"]],[[0,[],0,\"Create a non-root user and switch to it (since the agent won’t run as root)\"]],[[0,[],0,\"Copy the agent tar.gz file and extract it\"]],[[0,[],0,\"Configure the agent to connect it to VSTS\"]]]],[1,\"p\",[[0,[],0,\"Pretty simple.\"]]],[1,\"h2\",[[0,[],0,\"The Dockerfile\"]]],[1,\"p\",[[0,[],0,\"Let’s take a look at the Dockerfile (which you can find \"],[0,[7],1,\"here in Github\"],[0,[],0,\") for an agent container:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: We start with the DotNet Core 1.0.0 image\"]],[[0,[],0,\"Lines 4-6: We create some arguments and set defaults\"]],[[0,[],0,\"Lines 9-10: We create some args that don’t have defaults\"]],[[0,[],0,\"Line 14: Install Git\"]],[[0,[],0,\"This installs Git 2.1.4 from the official Jesse packages. We should be installing Git 2.9, but the only way to install it from a package source is to add a package source (line 13, which I commented out). Unfortunately apt-add-repository is inside the package software-properties-common, which introduces a lot of bloat to the container which I decided against. The VSTS agent will work with Git 2.1.4 (at least at present) so I was happy to leave it at that.\"]],[[0,[],0,\"Line 17: create a user called agent\"]],[[0,[],0,\"Line 18: switch to the agent user\"]],[[0,[],0,\"Line 19: switch to the agent home directory\"]],[[0,[],0,\"Line 23: Use this to download the tarball as part of building the container. Do it if you have enough bandwidth. I ended up downloading the tarball and putting it in the same directory as the Dockerfile and using Line 24 to copy it to the container\"]],[[0,[],0,\"Line 24: Extract the tarball and then delete it\"]],[[0,[],0,\"Line 25: Run the command to configure the agent in an unattended mode. This uses the args supplied through the file or from the docker build command to correctly configure the agent.\"]],[[0,[],0,\"Line 27: Set an entrypoint – this is the command that will be executed when you run the container.\"]]]],[1,\"p\",[[0,[],0,\"Pretty straightforward. To build the image, just cd to the Dockerfile folder and download the agent tarball (from \"],[0,[8],1,\"here\"],[0,[],0,\") if you’re going to use Line 23 (otherwise if you use Line 22, just make sure Line 4 has the latest release URL for Ubuntu 14.04 or use the AGENT_URL arg to supply it when building the image). Then run the following command:\"]]],[10,1],[3,\"ul\",[[[0,[],0,\"Mandatory: VSTS_ACC (which is the 1st part of your VSTS account URL – so for \"],[0,[9],1,\"https://myVSTSAcc.visualstudio.com\"],[0,[],0,\" the VSTS_ACC is myVSTSAcc.\"]],[[0,[],0,\"Mandatory: PAT – your Personal Auth Token\"]],[[0,[],0,\"Optional: AGENT_POOL – the name of the agent pool you want the agent to register with\"]],[[0,[],0,\"Optional: AGENT_NAME – the name of the agent\"]],[[0,[],0,\"Optional: AGENT_URL – the URL to the Ubuntu 14.04 agent (if using Line 22)\"]],[[0,[],0,\"The –t is the tag argument. I use colin/agent.\"]]]],[1,\"p\",[[0,[],0,\"This creates a new image that is registered with your VSTS account!\"]]],[1,\"p\",[[0,[],0,\"Now that you have an image, you can simply run it whenever you need your agent:\"]]],[10,2],[1,\"p\",[[0,[],0,\"After the docker run command, you should see the agent listening for jobs.\"]]],[1,\"h2\",[[0,[],0,\"Gotcha – Self-Updating Agent\"]]],[1,\"p\",[[0,[],0,\"One issue I did run into is that I had downloaded agent 2.104.0. When the first build runs, the agent checks to see if there’s a new version available. In my case, 2.104.1 was available, so the agent updated itself. It also restarts – however, if it’s running in a container, when the agent stops, the container stops. The build fails with this error message:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Running the container again starts it with the older agent again, so you get into a loop. Here’s how to break the loop:\"]]],[3,\"ol\",[[[0,[],0,\"Run docker run -it --entrypoint=/bin/bash colin/agent:latest\"]],[[0,[],0,\"This starts the container but just creates a prompt instead of starting the agent\"]],[[0,[],0,\"In the container, run “./run.sh”. This will start the agent.\"]],[[0,[],0,\"Start a build and wait for the agent to update. Check the version in the capabilities pane in the Agent Queue page in VSTS. The first build will fail with the above “renew lock” error.\"]],[[0,[],0,\"Run a second build to make sure the agent is working correctly.\"]],[[0,[],0,\"Now exit the container (by pressing Cntr-C and then typing exit).\"]],[[0,[],0,\"Commit the container to a new image by running docker commit --change='ENTRYPOINT ./run.sh' <containerId> (you can get the containerId by running docker ps)\"]],[[0,[],0,\"Now when you run the container using docker run –it colin/agent:latest your agent will start and will be the latest version. From there on, you’re golden!\"]]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Overall, I was happy with how (relatively) easy it was to get an agent running in a container. I haven’t yet tested actually compiling a DotNet Core app – that’s my next exercise.\"]]],[1,\"p\",[[0,[],0,\"Happy Dockering!\"]]]]}","published_at":1469754726000,"status":"published","published_by":1},{"id":"7fba557e-7fb4-4693-a79b-6d154f6b3924","title":"Serverless Parallel Selenium Grid Testing with VSTS and Azure Container Instances","slug":"serverless-parallel-selenium-grid-testing-with-vsts-and-azure-container-instances","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/46f87e72-d338-4297-9073-273943493756.png\\\" target=\\\"_blank\\\"><img width=\\\"418\\\" height=\\\"303\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/39464a65-4351-482d-a373-35bf0698f964.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1216c9cf-e8f7-4bc7-b347-9df978e01b72.png\\\" target=\\\"_blank\\\"><img width=\\\"525\\\" height=\\\"369\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ad7f904a-53b5-4306-be3d-ddc014715926.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0aeb7471-3c09-4222-b543-79a45379ff1d.png\\\" target=\\\"_blank\\\"><img width=\\\"616\\\" height=\\\"355\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/18590a8c-0030-4c27-8de0-61b4f2e89785.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a2fdc707-53ce-4a0a-8d2f-e7848ab25361.png\\\" target=\\\"_blank\\\"><img width=\\\"676\\\" height=\\\"187\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fb613512-3b5e-40c0-8871-c101d354f659.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6d503890-0576-4b3d-9ca6-7b5c32010607.png\\\" target=\\\"_blank\\\"><img width=\\\"697\\\" height=\\\"238\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a5f18dfd-dad5-48eb-b674-ad97f52ec3be.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/db90eece-c231-4220-916a-d457e3e2e458.png\\\" target=\\\"_blank\\\"><img width=\\\"703\\\" height=\\\"345\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0e9cd7da-310f-44fc-89c7-59366c2a7c7b.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ca200252-5c15-43d4-8ee7-24f05aa5962a.png\\\" target=\\\"_blank\\\"><img width=\\\"737\\\" height=\\\"291\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4511e36c-eebd-4dcb-bdd6-c0faaf0f1739.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c8fe1b7c-0b13-482e-8cbb-d1f216dbf64e.png\\\" target=\\\"_blank\\\"><img width=\\\"766\\\" height=\\\"306\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c9ea1534-7837-4fcc-8281-b4563a1151e3.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e7c9e2bd-e6d9-460c-bb32-145efe1d9d09.png\\\" target=\\\"_blank\\\"><img width=\\\"459\\\" height=\\\"562\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4b409e7e-450b-40c1-9d9f-f224d1e1dc26.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a2107607-e037-4265-95c4-cfd3ed7b9294.png\\\" target=\\\"_blank\\\"><img width=\\\"668\\\" height=\\\"366\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4baf0c8f-7c86-451e-89d6-e8dadb856477.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5b832b40-7d1d-402a-a89a-df873d7f1b0e.png\\\" target=\\\"_blank\\\"><img width=\\\"718\\\" height=\\\"377\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/96fc634f-35ef-40ba-9452-c343d5317dba.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9ff88399-5fd8-481b-843e-7df415a3086a.png\\\" target=\\\"_blank\\\"><img width=\\\"725\\\" height=\\\"182\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f6b6f455-76c6-4154-af96-c5d2379b3f7c.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/35b5d859-8559-4567-92de-6b0c0043266d.png\\\" target=\\\"_blank\\\"><img width=\\\"767\\\" height=\\\"341\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c8bc9329-a1e2-4242-a454-be7de62c768f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/273d0d68-f2e5-4efd-b02d-406926793dd8.png\\\" target=\\\"_blank\\\"><img width=\\\"768\\\" height=\\\"470\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8468eca2-7d31-44ea-bf8a-eadad866f847.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://colinsalmcorner.com/post/parallel-testing-in-a-selenium-grid-with-vsts\"]],[\"a\",[\"href\",\"https://colinsalmcorner.com/post/running-selenium-tests-in-docker-using-vsts-release-management\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/container-instances/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-aci\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-aci/blob/master/Scripts/selenium.release.runsettings\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-aci/blob/master/Scripts/WebAppInfrastructure.json\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-aci/blob/master/Scripts/VSTS-Selenium-ACI.release.yaml\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-aci/blob/master/.vsts-ci.yml\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-selenium-aci/blob/master/WebApp.ReleaseDefinition.json\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/container-instances/container-instances-quotas#region-availability\"]],[\"em\"],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/pipelines/library/service-endpoints?view=vsts\"]],[\"a\",[\"href\",\"https://bit.ly/cacbuildtasks\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/pipelines/agents/pools-queues?view=vsts#creating-agent-pools-and-queues\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/organizations/accounts/use-personal-access-tokens-to-authenticate?view=vsts#create-personal-access-tokens-to-authenticate-access\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I've written before about Selenium testing (\"],[0,[0],1,\"Parallel Testing in a Selenium Grid with VSTS\"],[0,[],0,\" and \"],[0,[1],1,\"Running Selenium Tests in Docker using VSTS and Release Management\"],[0,[],0,\"). The problem with these solutions, however, is that you need a VM! However, I was setting up a demo last week and decided to try to solve this challenge using \"],[0,[2],1,\"Azure Container Instances\"],[0,[],0,\" (ACI), and I have a neat solution.\"]]],[1,\"p\",[[0,[],0,\"So why ACI? Why not a Kubernetes cluster? This solution would definitely work in a k8s cluster, but I wanted something more light-weight. If you don't have a k8s cluster, spinning one up just for Selenium testing seemed a bit heavy handed. Also, I discovered that ACI now lets you spin up multiple containers in the same ACI group via a yaml file.\"]]],[1,\"p\",[[0,[],0,\"I've created a \"],[0,[3],1,\"GitHub repo\"],[0,[],0,\" with all the source code for this post so you can follow along - it includes the following:\"]]],[3,\"ul\",[[[0,[],0,\"a sample app (boilerplate File->New Project->MVC) with unit tests\"]],[[0,[],0,\".NET Core Selenium test project and tokenized \"],[0,[4],1,\"runsettings\"],[0,[],0,\" files\"]],[[0,[5],1,\"ARM template\"],[0,[],0,\" for Azure Web App\"]],[[0,[6],1,\"Tokenized yaml file\"],[0,[],0,\" for the ACI infrastructure\"]],[[0,[7],1,\"CI yaml build definition\"],[0,[],0,\" (yay for build-as-code!)\"]],[[0,[8],1,\"json release definition\"],[0,[],0,\" (release-as-code isn't yet available, so we'll have to do the release via the designer)\"]]]],[1,\"h2\",[[0,[],0,\"Architecture\"]]],[1,\"p\",[[0,[],0,\"Once we've built and package the application code, the release process (which we model in the Release Definition) is as follows:\"]]],[3,\"ol\",[[[0,[],0,\"Provision infrastructure - both for the app as well as the ACI group with the Selenium hub, worker nodes and VSTS agents\"]],[[0,[],0,\"Deploy the app\"]],[[0,[],0,\"Install .NET Core and execute \\\"dotnet test\\\", publishing test results\"]],[[0,[],0,\"Tear down the ACI\"]]]],[1,\"p\",[[0,[],0,\"The component architecture for the ACI is pretty straight-forward: we run containers for the following:\"]]],[3,\"ol\",[[[0,[],0,\"A Selenium Hub - this will listen for test requests and match the requested capabilities with a worker node\"]],[[0,[],0,\"Selenium nodes - Chrome and Firefox worker nodes\"]],[[0,[],0,\"VSTS Agent(s) - these connects to VSTS and execute tests as part of a release\"]]]],[1,\"p\",[[0,[],0,\"To run in parallel I'm going to use a multi-configuration phase - so if you want to run Firefox and Chrome tests simultaneously you'll need at least 2 VSTS agents (and 2 pipelines!). ACI \"],[0,[9],1,\"quotas\"],[0,[],0,\" will let you spin up groups with up to 4 processors and 14GB of memory, so for this example we'll use a Selenium hub, 2 workers and 2 VSTS agents each using .5 processor and .5GB memory.\"]]],[1,\"p\",[[0,[],0,\"One concern is security - how do you secure the test rig since it's running in the public cloud? Fortunately, this solution doesn't require any external ports - the networking is all internal (the worker nodes and VSTS agent connect to the hub using \\\"internal\\\" ports) and the VSTS agent itself connects out to VSTS and does not require incoming connections. So we don't have to worry about securing endpoints - we don't even expose any!\"]]],[1,\"p\",[[0,[],0,\"Let's take a look at the architecture of this containers in ACI:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"We are running 5 containers: selenium-hub, selenium-chrome and selenium-firefox and 2 vsts-agents\"]],[[0,[],0,\"The only port we have to think about is the hub port, 4444, which is available internally - we don't have any external ports\"]],[[0,[],0,\"The vsts-agent is connected to our VSTS account, and again no external port is required\"]]]],[1,\"p\",[[0,[],0,\"The trick to getting multiple Selenium nodes within the group to connect to the hub was to change the ports that they register themselves on - all taken care of in the yaml definition file.\"]]],[1,\"p\",[[0,[],0,\"As for the tests themselves, since I want to run the tests inside a vsts-agent container, they have to be written in .NET Core. Fortunately that's not really a big deal - except that some methods (like the screenshot method) are not yet implemented in the .NET Core Selenium libraries.\"]]],[1,\"p\",[[0,[],0,\"We'll get into the nitty-gritty of how to use this ACI in a release pipeline for testing your app, but before we do that let's quickly consider how to deploy this setup in the first place.\"]]],[1,\"h3\",[[0,[],0,\"Permanent or Transient?\"]]],[1,\"p\",[[0,[],0,\"There are two ways you can run the ACI - \"],[0,[10],1,\"permanently\"],[0,[],0,\" or \"],[0,[10],1,\"transiently\"],[0,[],0,\". The permanent method spins up the ACI and leaves it running full-time, while the transient method spins the ACI up as part of an application pipeline (and then tears it down again after testing). If you are cost-sensitive, you probably want to opt for the transient method, though this will add a few minutes to your releases. That shouldn't be too much of a problem since this phase of your pipeline is running integration tests which you probably expect to take a bit longer. If you are optimizing for speed, then you probably just want to spin the ACI up in a separate pipeline and just assume it's up when your application pipeline runs. Fortunately the scripts/templates are exactly the same for both methods! In this post I'll show you the transient method - just move the tasks for creating/deleting the ACI to a separate pipeline if you prefer the ACI to be up permanently.\"]]],[1,\"h2\",[[0,[],0,\"Putting it all Together\"]]],[1,\"p\",[[0,[],0,\"To put this all together, you need the following:\"]]],[3,\"ul\",[[[0,[],0,\"A VSTS account\"]],[[0,[],0,\"An Azure subscription\"]],[[0,[],0,\"A \"],[0,[11],1,\"Service Endpoint to the Azure sub\"],[0,[],0,\" from VSTS\"]],[[0,[12],1,\"Colin's ALM Corner Build Extension pack\"],[0,[],0,\" installed onto your VSTS account\"]],[[0,[],0,\"An \"],[0,[13],1,\"agent queue in VSTS\"],[0,[],0,\" (the ACI VSTS agents will register onto this queue)\"]],[[0,[],0,\"A \"],[0,[14],1,\"personal access token\"],[0,[],0,\" (PAT) to your VSTS account\"]]]],[1,\"p\",[[0,[],0,\"For the source code you can link directly to my \"],[0,[3],1,\"GitHub repo\"],[0,[],0,\". However, if you want to change the code, then you'll either need to fork it to your own GitHub account or import the repo into your VSTS account.\"]]],[1,\"h3\",[[0,[],0,\"The Build\"]]],[1,\"p\",[[0,[],0,\"The build is really simple - and since it's yaml-based, the \"],[0,[7],1,\"code is already there\"],[0,[],0,\". To create a build definition, enable the YAML build preview on your VSTS account, then browse to the Build page. Create a new YAML build and point to the .vsts-ci.yml file. The build compiles code, versions the assemblies, runs unit tests with code coverage and finally publishes the web app as a webdeploy package:\"]]],[10,1],[1,\"h3\",[[0,[],0,\"The Release\"]]],[1,\"p\",[[0,[],0,\"You'll need to import the release definition to create it. First download the \"],[0,[8],1,\"WebApp.ReleaseDefinition.json\"],[0,[],0,\" file (or clone the repo) so that you have the file on disk. Now, if you're using the \\\"old\\\" release view, just navigate to the Releases page and click the + button at the top of the left menu, then select \\\"Import release pipeline\\\". If you're using the new preview release view, you'll need to create a dummy release (since the landing page doesn't show the toolbar). Once you've created a dummy release, click the \\\"+ New\\\" button in the toolbar and click \\\"Import a pipeline\\\". Then browse to the WebApp.ReleaseDefinition.json file and click import. Once it's imported, you'll need to fix up a few settings:\"]]],[1,\"h4\",[[0,[],0,\"Variables\"]]],[1,\"p\",[[0,[],0,\"Click on the Variables tab and update the names for:\"]]],[3,\"ul\",[[[0,[],0,\"RGName - the name of the resource group for all the infrastructure\"]],[[0,[],0,\"WebAppName - the name of the web app (must be globally unique in Azure)\"]],[[0,[],0,\"ACIName - the name for the Azure Container Instance\"]],[[0,[],0,\"Location - I'd suggest you leave this on WestUS for the ACI quotas\"]],[[0,[],0,\"VSTSAccount - the name of your VSTS account (i.e. the bit before .visualstudio.com)\"]],[[0,[],0,\"VSTSPool - the name of the agent pool that you created earlier\"]],[[0,[],0,\"VSTSToken - your PAT. Make sure to padlock this value (to make it secret)\"]]]],[10,2],[1,\"h4\",[[0,[],0,\"Artifacts\"]]],[1,\"p\",[[0,[],0,\"Click on the Pipeline tab top open the designer. You're going to have to delete and recreate the artifacts, since the id's are specific to my VSTS, so I cleared them in the definition json file. The primary artifact (so add this first) is your web app build - so add a new artifact of type \\\"Build\\\" and point to the WebApp build. Make sure the \\\"Source alias\\\" of this artifact is set to \\\"WebApp\\\" to preserve the paths in the tasks. You can also enable the CD trigger (to queue a release when a new build is available) if you want to. Now add another artifact - this time point to the source code repo (either on GitHub or your VSTS account) and alias this artifact as \\\"infra\\\" to preserve paths.\"]]],[1,\"h4\",[[0,[],0,\"Job Queues\"]]],[1,\"p\",[[0,[],0,\"Now click on the Tasks tab. Click on the \\\"Provision Infrastructure\\\" job header and then select \\\"Hosted Linux Preview\\\" for the agent pool (we're running some Azure CLI commands via bash, so we need an Ubuntu agent). You can repeat this for the last job \\\"Tear down ACI\\\".\"]]],[10,3],[1,\"p\",[[0,[],0,\"The \\\"Deploy using WebDeploy\\\" task requires a Windows agent, so change the queue on this job to \\\"Hosted VS2017\\\". Finally, change the \\\"Run Tests\\\" queue to the agent queue you created earlier (with the same name as the VSTSPool variable).\"]]],[1,\"h4\",[[0,[],0,\"Azure Endpoints\"]]],[1,\"p\",[[0,[],0,\"You'll need to click on each \\\"Azure\\\" task (the Azure CLI tasks, the Azure Resource Group Deployment task and the Azure App Service Deploy task and configure the correct Azure endpoint:\"]]],[10,4],[1,\"p\",[[0,[],0,\"You should now be able to save the definition - remove \\\"Copy\\\" from the name before you do!\"]]],[1,\"h2\",[[0,[],0,\"The Jobs\"]]],[1,\"p\",[[0,[],0,\"Let's have a quick look at the tasks and how they are configured:\"]]],[1,\"h3\",[[0,[],0,\"Provision Infrastructure\"]]],[1,\"p\",[[0,[],0,\"This job provisions infrastructure for the web app (via ARM template) as well as the ACI (via Azure CLI). The first task executes the ARM template, passing in the WebAppName. The template creates a Free tier App Service Plan and the App service itself. Next we replace some tokens in the ACI yaml definition file - this is somewhat akin to a Kubernetes pod file. In the file we specify that we require 5 containers: the Selenium hub (which opens port 4444), the worker nodes (one firefox and one chrome, running on different ports and connecting to the hub via localhost:4444) and 2 VSTS agents (with environment variables for the VSTS account, pool and PAT) so that the agent can connect to VSTS. I had to specify agent names since both containers get the same hostname, so if you don't and the agents have the same name, the 2nd agent would override the 1st agent registration.\"]]],[1,\"p\",[[0,[],0,\"Finally we invoke the Azure CLI task using an inline script to create the ACI using the yaml file:\"]]],[10,5],[1,\"p\",[[0,[],0,\"The script itself is really a one-liner to \\\"az container create\\\" and we pass in the resource group name, the ACI name and the path to the yaml file.\"]]],[1,\"h3\",[[0,[],0,\"Deploy using WebDeploy\"]]],[1,\"p\",[[0,[],0,\"The deploy job is a single task: Deploy Azure App Service. This has to run on a windows agent because it's invoking webdeploy. We specify the App name from the variable and point to the webdeploy zip file (the artifact from the build).\"]]],[10,6],[1,\"p\",[[0,[],0,\"Of course a real application may require more deployment steps - but this single step is enough for this demo.\"]]],[1,\"h3\",[[0,[],0,\"Run Tests\"]]],[1,\"p\",[[0,[],0,\"This job should be executing on the agent queue that you've configured in variables - this is the queue that the ACI agents are going to join in the first job. The first task installs the correct .NET core framework. We then replace the tokens in the runsettings file (to set the browser and the BaseURL). Then we execute \\\"dotnet test\\\" and finally publish the test results.\"]]],[10,7],[1,\"p\",[[0,[],0,\"You'll notice that I have unset \\\"Publish test results\\\" in the dotnet test task. This is because the run is always published as \\\"VSTest Test Run\\\" - there's no way to distinguish which browser the test run is for. We tweak the test run title in the Publish Test Results step:\"]]],[10,8],[1,\"p\",[[0,[],0,\"You'll also notice that we only have a single job - so how does the parallelization work? If you click on the Job name, you'll see that we've configured the parallelization settings for the job:\"]]],[10,9],[1,\"p\",[[0,[],0,\"We're \\\"splitting\\\" the values for the variable \\\"Browser\\\" - in this case it's set to \\\"chrome,firefox\\\". In other words, this job will spawn twice - once for Browser=chrome and once for Browser=firefox. I've set the maximum number of agents to 2 since we only have 2 anyway.\"]]],[1,\"h3\",[[0,[],0,\"Teardown ACI\"]]],[1,\"p\",[[0,[],0,\"Finally we tear down the ACI in a single Azure CLI inline script where we call \\\"az container delete\\\" (passing in the resource group and ACI names):\"]]],[10,10],[1,\"p\",[[0,[],0,\"To ensure that this job always runs (even if the tests fail) we configure the Advanced options for the job itself, specifying that it should always run:\"]]],[10,11],[1,\"h2\",[[0,[],0,\"Run It!\"]]],[1,\"p\",[[0,[],0,\"Now that we have all the pieces in place, we can run it! Once it has completed, we can see 2 \\\"Run Test\\\" jobs were spawned:\"]]],[10,12],[1,\"p\",[[0,[],0,\"If we navigate to the Test tab, we can see both runs (with the browser name):\"]]],[10,13],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Using ACI we can get essentially serverless parallel Selenium tests running in a release pipeline. We're only charged for the compute that we actually used in Azure, so this is a great cost optimization. We also gain parallelization or just better test coverage (we are running the same tests in 2 browsers). All in all this proved to be a useful experiment!\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1536286328000,"status":"published","published_by":1},{"id":"2e56728e-68e8-47ea-aa70-2a97595dc16e","title":"Source Control Operations During Deployments in Release Management","slug":"source-control-operations-during-deployments-in-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"<p>param(\\n    [string]$targetPath,\\n    [string]$tfArgs\\n)\\n\\ntry {\\n    $tf = \\\"$pwd\\\\tf.exe\\\"\\n    Push-Location\\n\\n    if (-not(Test-Path $targetPath)) {\\n        mkdir $targetPath\\n    }\\n\\n    cd $targetPath\\n    &amp;$tf $tfArgs.Split(\\\" \\\")\\n    \\n    if (-not($?)) {\\n        throw \\\"TF.exe failed\\\"\\n    }\\n}\\nfinally {\\n    Pop-Location\\n}\\n</p><p>&nbsp;</p>\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">-command ./tf.ps1 –targetPath ‘__TargetPath__’ –tfArgs ‘__TFArgs__’</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6f0e72f1-be2f-4569-a5d3-f3b2c069b3df.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7111413d-0c74-4c8c-a08c-a13cd6d3cbe8.png\\\" width=\\\"644\\\" height=\\\"361\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">-command ./tf.ps1 -targetPath '__TargetPath__' -tfArgs 'workspace /new /noprompt /collection:</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">http://rmserver:8080/tfs/__TPC__</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\"> \\\"__WorkspaceName__\\\"'</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/95bf9577-cdf4-4095-8428-a0e5f787cb19.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/148962db-f550-4af0-8c16-87f080600911.png\\\" width=\\\"644\\\" height=\\\"322\\\"></a>\"}]],\"markups\":[[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Before we start: Don’t ever do this.\"]]],[1,\"p\",[[0,[],0,\"But if you really have to, then it can be done. There are actually legitimate cases for doing source control operations during a deployment. For example, you don’t have source control and you get “files” from a vendor that need to be deployed to servers. Or you have a vendor application that has “extensions” that are just some sort of script file that is deployed onto the server – so you don’t compile anything for customizations. Typically these sorts of applications are legacy applications.\"]]],[1,\"h2\",[[0,[],0,\"Simple Solution: Install Team Explorer on the Target Servers\"]]],[1,\"p\",[[0,[],0,\"The simplest way to do source control operations is just to install Team Explorer on your target server. Then you can use the “Run Command” tool from Release Management and invoke tf.exe directly, or create a script that does a number of tf operations.\"]]],[1,\"p\",[[0,[],0,\"However, I was working at a customer where they have hundreds of servers, so they don’t want to have to manually maintain Team Explorer on all their servers.\"]]],[1,\"h2\",[[0,[],0,\"Creating a TF.exe Tool\"]]],[1,\"p\",[[0,[],0,\"Playing around a bit, I realized that you can actually invoke tf.exe on a machine that doesn’t have Team Explorer. You copy tf.exe to the target machine – as well as all its dependencies – and you’re good to go. Fortunately it’s not a huge list of files – around 20 altogether.\"]]],[1,\"p\",[[0,[],0,\"That covers the exe itself – however, a lot of TF commands are “location dependent” – they use the directory you’re in to give context to the command. For example, running “tf get” will get files for the current directory (assuming there is a mapping in the workspace). When RM deploys a tool to the target server, it copies the tool files to a temporary directory and executes them from there. This means that we need a script that can “remember” the path where the tool (tf.exe) is but execute from a target folder on the target server.\"]]],[1,\"p\",[[0,[],0,\"PowerShell is my scripting language of choice – so here’s the PowerShell script to wrap the tf.exe call:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 2: We pass in the $targetPath – this is the path on the target server we want to perform tf commands from\"]],[[0,[],0,\"Line 3: We in $tfArgs – these are the arguments to pass to tf.exe\"]],[[0,[],0,\"Line 7-8: get the path to tf.exe and store it\"]],[[0,[],0,\"Line 10-12: if the $targetPath does not exist, create it\"]],[[0,[],0,\"Line 14: change directory to the $targetPath\"]],[[0,[],0,\"Line 15: Invoke tf.exe passing the $tfArgs we passed in as parameters\"]],[[0,[],0,\"Line 17-19: Since this script invokes tf.exe, you could get a failure from the invocation, but have the script still “succeed”. In order to make sure the deployment fails if tf.exe fails, we need to check if the tf.exe invocation succeeded or not – that’s what these lines are doing\"]],[[0,[],0,\"Line 22: Change directory back to the original directory we were in – not strictly necessary, but “clean”\"]]]],[1,\"p\",[[0,[],0,\"Here’s the list of dependencies for tf.exe:\"]]],[3,\"ul\",[[[0,[],0,\"Microsoft.TeamFoundation.Build.Client.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.Build.Common.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.Client.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.Common.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.TestManagement.Client.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.VersionControl.Client.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.VersionControl.Common.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.VersionControl.Common.Integration.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.VersionControl.Common.xml\"]],[[0,[],0,\"Microsoft.TeamFoundation.VersionControl.Controls.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.WorkItemTracking.Client.DataStoreLoader.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.WorkItemTracking.Client.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.WorkItemTracking.Client.QueryLanguage.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.WorkItemTracking.Common.dll\"]],[[0,[],0,\"Microsoft.TeamFoundation.WorkItemTracking.Proxy.dll\"]],[[0,[],0,\"Microsoft.VisualStudio.Services.Client.dll\"]],[[0,[],0,\"Microsoft.VisualStudio.Services.Common.dll\"]],[[0,[],0,\"TF.exe\"]],[[0,[],0,\"TF.exe.config\"]]]],[1,\"p\",[[0,[],0,\"Open up the Release Management client and navigate to Inventory->Tools. Click New to create a new tool, and specify a good name and description. For the command, specify “powershell” and for arguments type the following:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Note that the quotes around the parameters __TargetPath__ and __TFArgs__ should be single-quotes.\"]]],[1,\"p\",[[0,[],0,\"Finally, click “Add” on the Resources section and add all the tf files – don’t forget the tf.ps1 file!\"]]],[10,2],[1,\"h2\",[[0,[],0,\"Creating TF Actions\"]]],[1,\"p\",[[0,[],0,\"Once you have the tf.exe tool, you can then create TF.exe actions – like “Create Workspace” and “Get Files”. Let’s do “Create Workspace”:\"]]],[1,\"p\",[[0,[],0,\"Navigate to Inventory->Actions and click “New”. Enter an appropriate name and description. I created a new Category called “TFS Source Control” for these actions, but this is up to you. For “Tool used” specify the TF.exe tool you just created. When you select this tool, it will bring in the arguments for the tool – we’re going to edit those to be more specific for this particular Action. I set my arguments to:\"]]],[10,3],[10,4],[10,5],[1,\"p\",[[0,[],0,\"(Note where the single and double quotes are).\"]]],[1,\"p\",[[0,[],0,\"The parameters are as follows:\"]]],[3,\"ul\",[[[0,[],0,\"__TargetPath__: the path we want to create the workspace in\"]],[[0,[],0,\"__TPC__: the name of the Team Project Collection in the rmserver TFS – this can be totally hardcoded (if you only have one TFS server) or totally dynamic (if you have multiple TFS servers). In this case, we have a single server but can run deployments for several collections, so that’s why this parameter is “partly hardcoded” and “partly dynamic”\"]],[[0,[],0,\"__WorkspaceName__: the name we want to give to the workspace\"]]]],[1,\"h2\",[[0,[],0,\"Using Create Workspace Action in a Release Template\"]]],[1,\"p\",[[0,[],0,\"Now that you have the action, you can use it in a release template:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Here you can see that I’ve create some other actions (Delete Workspace and TF Get) to perform other TF.exe commands. This workflow deletes the workspace called “Test”, then creates a new Workspace in the “c:\\\\files” folder, and then gets a folder from source control. From there, I can copy or run or do whatever I need to with the files I got from TFS.\"]]],[1,\"p\",[[0,[],0,\"Happy releasing from Source Control (though you can’t really be \"],[0,[0],1,\"happy\"],[0,[],0,\" about this – it’s definitely a last-resort).\"]]]]}","published_at":1413478623000,"status":"published","published_by":1},{"id":"d80f4a55-6924-4893-addf-d204e37dcb05","title":"Staging Servers Must Die – Or Must They?","slug":"staging-servers-must-die--or-must-they","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://readwrite.com/2016/01/22/staging-servers\"]],[\"a\",[\"href\",\"https://launchdarkly.com\"]],[\"em\"],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/automated-buildswhy-theyre-absolutely-essential-(part-1)\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-absolutely-need-to-unit-test\"]],[\"a\",[\"href\",\"http://www.sonarqube.org/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/templates/\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/powershell/dsc/overview\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/library/vs/alm/release/overview-rmpreview\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/web-sites-staged-publishing/\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/documentation/articles/app-service-web-test-in-production-get-start/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Edith Harbaugh published a though-provoking post called \"],[0,[0],1,\"Staging Servers Must Die\"],[0,[],0,\" with the byline “so continuous delivery may live.” She asserts something which I’d never really considered before: that separate, cascading Dev, QA, Staging and Prod environments is a hangover from Waterfall development.\"]]],[1,\"h2\",[[0,[],0,\"Agility = No Build or Staging?\"]]],[1,\"p\",[[0,[],0,\"Harbaugh makes some bold assertions about what she calls “DevOps 2.0”. First, she states that teams should ditch the concept of build (which she calls antiquated). Developers should be checking source into their mainline and and deploying immediately to Prod – with feature flags. The flag of a new feature being deployed defaults to “off for everyone” – no need to keep staging in sync with Prod, and no delay. The QA team are then given access to the feature, then beta customers, and slowly the number of users with access to the feature is increased until everyone has it and the feature is “live”.\"]]],[1,\"p\",[[0,[],0,\"She calls out four problems with cascading environments. The first one is time: she argues that a pipeline of environments slows delivery since builds have to be queued and then progressively moved through the pipeline. Secondly, staging environments increase costs since they require infrastructure. Thirdly, she says that the effectiveness of staging environments is moot since they can almost never reproduce production exactly. Finally, she recounts bad experiences where she needed users to test on staging servers, and users continually logged into Prod instead of Staging (or vice-versa) and so the effectiveness of having a staging environment became eclipsed user confusion.\"]]],[1,\"h2\",[[0,[],0,\"Feature Flags\"]]],[1,\"p\",[[0,[],0,\"I think that Harbaugh’s view of feature flags may be a tad biased, since she is the CEO of a \"],[0,[1],1,\"LaunchDarkly\"],[0,[],0,\", a product that allows developers to introduce and manage feature flags. Still, feature flags are a great solution to some of the challenges she lists. However, feature flags are hard to code and manage (that’s why she has a product that helps teams manage it).\"]]],[1,\"p\",[[0,[],0,\"LaunchDarkly is a really neat idea – in your code, you call an API that queries LaunchDarkly to determine if this feature is on for this user. Then you can manage which users have which features outside the app in LaunchDarkly – great for A/B testing or releasing features to beta customers and so on.\"]]],[1,\"p\",[[0,[],0,\"Feature flags always sound great in theory, but how do you manage database schema differences? How do you fix a bug (what bug?) – do you need a feature flag for the bug fix? What about load testing a new feature – do you do that against Prod?\"]]],[1,\"h2\",[[0,[],0,\"Agility\"]]],[1,\"p\",[[0,[],0,\"So are feature flags and ditching builds and staged environments the way to increase agility and progress to “DevOps 2.0”? It may be in some cases, but I don’t think so. Automated deployment doesn’t make you DevOps – DevOps is far more that just that.\"]]],[1,\"p\",[[0,[],0,\"Here are my thoughts on what you should be thinking about in your DevOps journey.\"]]],[1,\"h3\",[[0,[],0,\"Microservices\"]]],[1,\"p\",[[0,[],0,\"You may be able to go directly to microservices, but even if you can’t (and in some cases you probably shouldn’t), you should be thinking about breaking large, monolithic applications into smaller, loosely-coupled components. Besides better architecture, isolating components allows you \"],[0,[2],1,\"deployment granularity\"],[0,[],0,\". That is, you can deploy a component of your application without having to deploy the entire application. This makes for much faster cycles, since teams that complete functionality in one component can deploy immediately without waiting for teams that are working on other components to be ready to deploy. Smaller, more frequent, asynchronous deployments are far better than large, infrequent, synchronized deployments.\"]]],[1,\"h3\",[[0,[],0,\"Automated Builds with Automated Testing\"]]],[1,\"p\",[[0,[],0,\"This has always seemed so fundamental to me – I battle to understand why so many dev teams do not have \"],[0,[3],1,\"builds\"],[0,[],0,\" and \"],[0,[4],1,\"unit tests\"],[0,[],0,\". This is one of my biggest disagreements with Harbaugh – when a developer checks in, the code should trigger a build that not only compiles, but goes through a number of quality checks. The most non-negotiable is unit testing with coverage analysis – that way you have some measure of code quality. Next, consider static code analysis, and better yet, integration with \"],[0,[5],1,\"SonarQube\"],[0,[],0,\" or some other technical debt management system.\"]]],[1,\"p\",[[0,[],0,\"Every build should produce metrics about the quality of your code – tests passed/failed, coverage percentage, maintainability indexes and so on. You should know these things about your code – deploying directly to production (even with feature switches) bypasses any sort of quality analysis on your code.\"]]],[1,\"p\",[[0,[],0,\"Your build should also produce a deployable package – that is \"],[0,[2],1,\"environment agnostic\"],[0,[],0,\". You should be able to deploy your application to any environment, and have the deployment process take care of environment specific configuration.\"]]],[1,\"p\",[[0,[],0,\"Beyond unit testing, you should be creating automated integration tests. These should be running on an environment (we’ll discuss that shortly) so that you’re getting quality metrics back frequently. These tests typically take longer to run than unit tests, so they should at least be run on a schedule if you don’t want them running on each check-in. Untested code should never be deployed to production – that means you’re going to have to invest into keeping your test suites sharp – treat your test code as “real” code and help it to help you!\"]]],[1,\"h3\",[[0,[],0,\"Automated Deployment with Infrastructure As Code\"]]],[1,\"p\",[[0,[],0,\"Harbaugh does make a good point – that cascading dev/test/staging type pipelines originate in Waterfall. I constantly try to get developers to separate \"],[0,[2],1,\"branch\"],[0,[],0,\" from \"],[0,[2],1,\"environment\"],[0,[],0,\" in their minds – it’s unfortunate that we have dev/test/prod branches and dev/test/prod environments – that makes developers think that the code on a branch is the code in the environment. This is almost never the case – I usually recommend a dev/prod branching structure and let the \"],[0,[2],1,\"build\"],[0,[],0,\" track which code is in which environment (with proper versioning and labeling of course).\"]]],[1,\"p\",[[0,[],0,\"So we should \"],[0,[2],1,\"repurpose\"],[0,[],0,\" our cascading environments – call them integration and load or something appropriate if you want to. You need somewhere to run all these shiny tests you’ve invested in. And go Cloud – pay as you use models mean that you don’t have to have hardware idling – you’ll get much more efficient usage of environments that are spun up/down as you need them. However, if you’re spinning environments up and down, you’ll need to use Infrastructure as Code in some form to automate the deployment and configuration of your infrastructure – \"],[0,[6],1,\"ARM Templates\"],[0,[],0,\", \"],[0,[7],1,\"DSC scripts\"],[0,[],0,\" and the like.\"]]],[1,\"p\",[[0,[],0,\"You’ll then also need a tool for managing deployments in the pipeline – for example, \"],[0,[8],1,\"Release Management\"],[0,[],0,\". Release Management allows you to define tasks – that can deploy build outputs or run tests or do whatever you want to – in a series of environments. You can automate the entire pipeline (stopping when tests fail) or insert manual approval points. You can then configure triggers, so when a new build is available the pipeline automatically triggers for example. Whichever tool you use though, you’ll need a way to monitor what builds are where in which pipelines. And you can of course deploy directly to Production when it is appropriate to do so, so the pipeline won’t slow critical bugfixes if you don’t want it to.\"]]],[1,\"h3\",[[0,[],0,\"Load Testing\"]]],[1,\"p\",[[0,[],0,\"So what about load and scale testing? Doing this via feature switches is almost impossible if you don’t want to bring your production environment to a grinding halt. If you’re frequently doing this, then consider replication of your databases so that you always have an exact copy of production that you can load test against. Of course, most teams can use a subset of prod and extrapolate results – so you’ll have to decide if matching production \"],[0,[2],1,\"exactly\"],[0,[],0,\" is actually necessary for load testing.\"]]],[1,\"p\",[[0,[],0,\"Having large enough datasets should suffice – load testing should ideally be a relative operation. In other words, you’re not testing for an absolute number, like how many requests per second your site can handle. Rather, you should be base lining and comparing runs. Execute load test on current code to set a base line, then implement some performance improvement, then re-run the tests. You now compare the two runs to see if your tweaks were effective. This way, you don’t necessarily need an exact copy of production data or environments – you just need to run the tests with the same data and environment so that comparisons make sense.\"]]],[1,\"h3\",[[0,[],0,\"A/B Testing\"]]],[1,\"p\",[[0,[],0,\"Of course feature switches can be manipulated and managed in such as way as to enable A/B testing – having some users go to “version A” and some to “version B” of your application. It’s still possible to do A/B testing without deploying to production – for example, using \"],[0,[9],1,\"deployment slots\"],[0,[],0,\" in Azure. In an Azure site, you’d create a staging slot on your production site. The staging slot can have the same config as your production slot or have different config, so it could point to production databases if necessary. Then you’d \"],[0,[10],1,\"use Traffic Manager to divert some percentage of traffic to the staging slot\"],[0,[],0,\" until you’re happy (which the users will be unaware of – they go to the production URL and are none the wiser that they’ve been redirected to the staging slot). Then just swap the slots – instant deployment, no data loss, no confusion.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Staging environments shouldn’t die – they should be repurposed, rising like a Phoenix out of the ashes of Waterfall’s cremation. Automated builds with solid automated testing (which requires staging infrastructure) should be what you’re aiming for. That way, you can deploy into production quickly \"],[0,[2],1,\"with confidence,\"],[0,[],0,\" something that’s hard to do if you deploy directly to production, even with feature switches.\"]]],[1,\"p\",[[0,[],0,\"Happy staging!\"]]]]}","published_at":1454087557000,"status":"published","published_by":1},{"id":"d0849c5c-4646-4faf-b9d8-cb6d50eddad4","title":"Subtle MSF Agile Enhancement: Adding Bugs to the Backlogs","slug":"subtle-msf-agile-enhancement-adding-bugs-to-the-backlogs","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-_s4I_h3U0GE/UPkfCs912GI/AAAAAAAAAik/nE11zNDH0mM/s1600-h/image%25255B6%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-J-Mvk3lt_aM/UPkfD8yr8XI/AAAAAAAAAis/CWxAevsAU8A/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"452\\\" height=\\\"109\\\"></a>\"}],[\"code\",{\"code\":\"witadmin exportwitd /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Bug.xml /n:Bug<br><br>\",\"language\":\"\"}],[\"code\",{\"code\":\"<field name=\\\"Story Points\\\" refname=\\\"Microsoft.VSTS.Scheduling.StoryPoints\\\" type=\\\"Double\\\" reportable=\\\"measure\\\" formula=\\\"sum\\\"><br>  <helptext>The size of work estimated for implementing this user story</helptext><br></field><br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"<column percentwidth=\\\"33\\\"><br>  <group label=\\\"Planning\\\"><br>    <column percentwidth=\\\"100\\\"><br>      <control fieldname=\\\"Microsoft.VSTS.Scheduling.StoryPoints\\\" type=\\\"FieldControl\\\" label=\\\"Story Points\\\" labelposition=\\\"Left\\\"><br>      <control fieldname=\\\"Microsoft.VSTS.Common.StackRank\\\" type=\\\"FieldControl\\\" label=\\\"Stack Rank\\\" labelposition=\\\"Left\\\" numberformat=\\\"DecimalNumbers\\\" maxlength=\\\"10\\\" emptytext=\\\"<None>\\\"><br>      <control fieldname=\\\"Microsoft.VSTS.Common.Priority\\\" type=\\\"FieldControl\\\" label=\\\"Priority\\\" labelposition=\\\"Left\\\"><br>      <control fieldname=\\\"Microsoft.VSTS.Common.Severity\\\" type=\\\"FieldControl\\\" label=\\\"Severity\\\" labelposition=\\\"Left\\\"><br>    </control></control></control></control></column><br>  </group><br></column><br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"<states><br>  <state value=\\\"New\\\"><br>    <fields><br>      ...<br>    </fields><br>  </state><br>  <state value=\\\"Active\\\"><br>     ...<br><states><br></states></state></states>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"<transition from=\\\"\\\" to=\\\"New\\\"><br>  <reasons><br>    <defaultreason value=\\\"New\\\"><br>    <reason value=\\\"Build Failure\\\"><br>  </reason></defaultreason></reasons><br></transition><br><transition from=\\\"New\\\" to=\\\"Active\\\"><br>  <reasons><br>    <defaultreason value=\\\"Activated\\\"><br>  </defaultreason></reasons><br>  <fields><br>    <field refname=\\\"Microsoft.VSTS.Common.ActivatedBy\\\"><br>      <allowexistingvalue><br>      <copy from=\\\"currentuser\\\"><br>      <validuser><br>      <required><br>    </required></validuser></copy></allowexistingvalue></field><br>    <field refname=\\\"Microsoft.VSTS.Common.ActivatedDate\\\"><br>      <serverdefault from=\\\"clock\\\"><br>    </serverdefault></field><br>  </fields><br></transition><br><transition from=\\\"Active\\\" to=\\\"New\\\"><br>  <reasons><br>    <defaultreason value=\\\"Deactivated\\\"><br>  </defaultreason></reasons><br>  <fields><br>    <field refname=\\\"Microsoft.VSTS.Common.ActivatedBy\\\"><br>      <empty><br>    </empty></field><br>    <field refname=\\\"Microsoft.VSTS.Common.ActivatedDate\\\"><br>      <empty><br>    </empty></field><br>  </fields><br></transition><br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"witadmin importwitd /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Bug.xml<br>\",\"language\":\"\"}],[\"code\",{\"code\":\"witadmin exportcategories /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Categories.xml<br><br>witadmin exportcommonprocessconfig /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:CommonProcessConfig.xml<br><br>\",\"language\":\"\"}],[\"code\",{\"code\":\"<category refname=\\\"Microsoft.RequirementCategory\\\" name=\\\"Requirement Category\\\"><br>  <defaultworkitemtype name=\\\"User Story\\\"><br>  <workitemtype name=\\\"Bug\\\"><br></workitemtype></defaultworkitemtype></category><br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"<bugworkitems category=\\\"Microsoft.BugCategory\\\"><br>  <states><br>    <state type=\\\"Proposed\\\" value=\\\"New\\\"><br>    <state type=\\\"InProgress\\\" value=\\\"Active\\\"><br>    <state type=\\\"Complete\\\" value=\\\"Closed\\\"><br>    <state type=\\\"Resolved\\\" value=\\\"Resolved\\\"><br>  </state></state></state></state></states><br></bugworkitems><br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"witadmin importcategories /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:Categories.xml<br><br>witadmin importcommonprocessconfig /collection:http://localhost:8080/tfs/defaultcollection /p:Code /f:CommonProcessConfig.xml<br><br>\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-6vMxmvHL80s/UPkfEhGQinI/AAAAAAAAAiw/SEMEzDHViwU/s1600-h/image%25255B10%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-vRoqf54FmVc/UPkfGJnsqfI/AAAAAAAAAi8/A4q6v31f2Ps/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"480\\\" height=\\\"157\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-N0yHRDkdwoA/UPkfG6QTB-I/AAAAAAAAAjA/OhqEAfrvG3Q/s1600-h/image%25255B14%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-cHA0lNQ6C5A/UPkfH-8Km5I/AAAAAAAAAjM/sFy8RfdYha0/image_thumb%25255B6%25255D.png?imgmax=800\\\" width=\\\"427\\\" height=\\\"207\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-R9Q3P16rVx0/UPkfInUJoqI/AAAAAAAAAjQ/o7gPmH_xCeA/s1600-h/image%25255B19%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-DhJ19VDIoqE/UPkfJlh0NrI/AAAAAAAAAjc/4ClGvDkz_NM/image_thumb%25255B9%25255D.png?imgmax=800\\\" width=\\\"432\\\" height=\\\"234\\\"></a>\"}]],\"markups\":[[\"em\"],[\"strong\"],[\"u\"],[\"a\",[\"href\",\"http://pascoal.net/2013/02/team-foundation-task-board-enhancer-version-0-6-1-released/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"TFS’s Work Item Tracking system is amazing – the flexibility and customizability of the system is fantastic. It also allows your tool to enforce your processes – which is always a good thing for efficiency in any team!\"]]],[1,\"p\",[[0,[],0,\"When I install / configure / customize TFS at my customers, I usually suggest that they start with one of the three out-of-the-box Process Templates – CMMI, Scrum or Agile. Then after having run a couple of sprints or iterations using these templates, decide what changes are required and implement them slowly and over time.\"]]],[1,\"p\",[[0,[],0,\"So how do you decide which template to start off with? At a \"],[0,[0],1,\"very\"],[0,[],0,\" high level, CMMI is the most formal of the templates. Scrum and Agile are very similar in most respects, and I only recommend Scrum for Scrum purists. My template of choice for most teams is the MSF Agile Template.\"]]],[1,\"h2\",[[0,[],0,\"Scrum vs Agile\"]]],[1,\"p\",[[0,[],0,\"Scrum sort-of-noses-ahead-slightly-early-on because:\"]]],[3,\"ul\",[[[0,[],0,\"You can see Bugs along with Product Backlog items\"]]]],[1,\"p\",[[0,[],0,\"Agile comes-from-behind-and-smashes-Scrum-to-smithereens because:\"]]],[3,\"ul\",[[[0,[],0,\"The dashboarding and reporting are more comprehensive out-the-box\"]],[[0,[],0,\"The bug States make more sense\"]],[[0,[],0,\"In Scrum, Bugs are Approved and Committed – what does that really mean? I prefer Active-Resolved-Closed\"]],[[0,[],0,\"Scrum Bugs can’t be “resolved” during Check-in because there is no resolved state\"]],[[0,[],0,\"Scrum Bugs can’t be verified in MTM, since there is no resolved state\"]]]],[1,\"p\",[[0,[],0,\"So it’s really the dashboards and Bug lifecycle that pushes me to recommending Agile over Scrum.\"]]],[1,\"h2\",[[0,[],0,\"Bugs in Planning\"]]],[1,\"p\",[[0,[],0,\"One of the pain points in the Agile template is planning around bugs. I see two challenges:\"]]],[3,\"ol\",[[[0,[],0,\"You have a Bug that is known, and you plan to fix it in a future Iteration. To do this (including planning time and resources to do the work) you would have to create a User Story that is a “copy” of the Bug so that the User Story can be part of the backlog and you can log tasks against the bug. This does mean some duplication, and in the latest Agile template (6.1) there is a Reason when you transition from “Active” to “Resolved” called \\\"Copied to Backlog” that is designed for this scenario.\"]],[[0,[],0,\"You find a Bug during the Sprint and you need to fix it. Again, you’d need to “copy” the Bug to a User Story and add this into the backlog. This will surface as unplanned work, which is what you want.\"]]]],[1,\"p\",[[0,[],0,\"Of course, you can just fix the bug without the User Story, but then you’ll have to decide where to log the tasks so that they actually appear in the backlogs.\"]]],[1,\"h2\",[[0,[],0,\"The Solution: Modify Agile Bugs to make them more “Scrummy”\"]]],[1,\"p\",[[0,[],0,\"At one of my customers, we made some minor changes to the Agile template that mitigate the “copy Bug to User Story” pain. It allows you to bring the Bugs into the backlogs (just like the Scrum template) so you end up with the best of both Agile and Scrum templates. The only down side is that if you have a lot of bugs, you can end up cluttering your backlogs, but this is a problem you’d have in the Scrum template anyway.\"]]],[1,\"p\",[[0,[],0,\"To do this you need to add a New state to the Bug Work Item – that will make the Bug and User Story States match more closely (New, Active, Resolved, Closed). User Stories have an extra “Removed” state, but that’s not required on the Bug. Secondly, you need to add the Bug Work Item to the Requirements Category, allowing the Product Backlog to show Bugs.\"]]],[1,\"p\",[[0,[],0,\"Before you start, make sure you backup as you go along (by source controlling your files) and do this on a test TFS Server before attempting this on a production server! If you get the error below, don’t panic. Check your configurations and try again. I wish there was a better description of exactly what is wrong, but unfortunately there doesn’t appear to be any more detail, so you’ll have to revert and try again.\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Add Story Points and the “New” State to the Bug Work Item\"]]],[1,\"p\",[[0,[],0,\"To edit the Bug Work Item, open it using the Process Template editor or from the command line. I’ll show you how to do this from the command line and in the XML itself. I’m going to connect to localhost (my TFS server) to the DefaultCollection and work with a TeamProject called “Code”. Of course you’ll have to change the server, collection and team project name for your scenario.\"]]],[1,\"p\",[[0,[],0,\"From a command line, export the Bug Work Item Template:\"]]],[10,1],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"In the  section, copy the Story Points field from the User Story work item type or just paste in the following:\"]]],[10,2],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"Then look for the “Planning” group in the  and change it to the following (to add the Story Points control):\"]]],[10,3],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"Now edit the  and  elements of the Bug. You can simply copy/paste the  and change the value to “New” to create the New state.\"]]],[10,4],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"Then in  remove the  transition and add the following:\"]]],[10,5],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"Now you can import the Work Item Type to make the change to your Bug Work Item:\"]]],[10,6],[1,\"p\",[[1,[],0,5]]],[1,\"h2\",[[0,[],0,\"Exposing Bugs on the Backlogs\"]]],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"To do this, you need to edit the Categories and the CommonProcessConfiguration. First let’s export those:\"]]],[10,7],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Open Categories.xml and add Bugs to the Requirements Category:\"]]],[10,8],[1,\"p\",[[1,[],0,8]]],[1,\"p\",[[0,[],0,\"Open CommonProcessConfig.xml and add the “New” state to the mapping:\"]]],[10,9],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"Finally, import the Categories and CommonProcessConfig files:\"]]],[10,10],[1,\"p\",[[1,[],0,10]]],[1,\"h2\",[[0,[],0,\"The Results\"]]],[1,\"p\",[[1,[],0,11]]],[1,\"p\",[[0,[],0,\"Now you’ll be able to add Bugs on your Product Backlog, as well as do work breakdowns for Bugs in the Sprint Backlog.\"]]],[1,\"p\",[[1,[],0,12]]],[10,11],[1,\"p\",[[1,[],0,13]]],[1,\"p\",[[0,[],0,\"You’ll probably want to add Work Item Type as a Column to your Product and Iteration Backlogs (press the Column Options button shown in the figure above) or you can also edit the AgileProcessConfig file (works similarly to the CommonProcessConfig – see witadmin exportagileprocessoconfiguration for more details).\"]]],[1,\"p\",[[1,[],0,14]]],[10,12],[1,\"p\",[[1,[],0,15]]],[10,13],[1,\"p\",[[1,[],0,16]]],[1,\"p\",[[0,[],0,\"Thanks to my good friend Theo Kleynhans for working with me on this!\"]]],[1,\"p\",[[1,[],0,17]]],[1,\"p\",[[0,[],0,\"Happy (product back-) logging!\"]]],[1,\"p\",[[1,[],0,18]]],[1,\"p\",[[0,[1,2],2,\"UPDATE:\"],[0,[],0,\" If you want to make some subtle changes to your Task Board (including showing the Type of the “Requirement” and the ID of the Work Item) and you don’t mind an unofficial “hack” – then my good friend Tiago Pascoal has a Task Board Plugin – read about it in \"],[0,[3],1,\"this post\"],[0,[],0,\".\"]]]]}","published_at":1358539680000,"status":"published","published_by":1},{"id":"d64076ea-5dd2-4256-83d1-2193124f9e60","title":"TechDays and Jhb Training Class","slug":"techdays-and-jhb-training-class","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://www.microsoft.com/southafrica/techdays/Home.aspx\"]],[\"strong\"],[\"a\",[\"href\",\"http://sdrv.ms/STrGoG\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I will be co-presenting with Ahmed Salijee at \"],[0,[0],1,\"TechDays\"],[0,[],0,\" in Durban, Johannesburg and Cape Town (12, 16 and 19 Oct respectively). Check out the website for details on exact dates and venues – there is tons of new stuff to show off, so this is going to be a really exciting event!\"]]],[1,\"p\",[[0,[],0,\"Here’s the blurb for the demo Ahmed and I will be doing:\"]]],[1,\"p\",[[0,[1],1,\"End to End Application Lifecycle Management with Visual Studio and Team Foundation Server 2012\"]]],[1,\"p\",[[0,[],0,\"Microsoft’s application lifecycle management tooling is all about enabling teams to deliver great software. In this demo-packed session, learn how to engage early and often with your project’s stakeholders to ensure that your team is building the right software for them. Discover tooling to help you more effectively plan and track work by using the new web-based project management tools. Learn how to bridge the divide between development and operations by utilizing IntelliTrace in your production environments and integrating Microsoft System Center with Microsoft Visual Studio Team Foundation Server. And, developers can stay on-task and “in the zone” with the new “My Work” and code review features. In addition to making your team more productive, we show you how you can boost your overall code quality with new features such as code clone and an overhauled unit testing story in Visual Studio 2012. We will walk you through an end to end scenario from conception to production, leveraging capabilities across the define, build and operate phases. Feature covered include storyboarding, manual and automated testing, build, source control, code review, intellitrace and lab management.\"]]],[1,\"h2\",[[0,[],0,\"Training Course\"]]],[1,\"p\",[[0,[],0,\"I am also going to be running a 2 day overview class in Johannesburg at Bytes in Midrand on 19/20 November. This course will quickly upgrade you from VS/TFS 2010 to VS/TFS 2012 so that you can get going quickly. The cost is R3500 per seat. You can get the course flyer \"],[0,[2],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Hope to see you at TechDays or at the training course!\"]]]]}","published_at":1349136840000,"status":"published","published_by":1},{"id":"a18b9c80-5f9c-410f-b264-a0535d2cd7fb","title":"Terraform all the Things with VSTS","slug":"terraform-all-the-things-with-vsts","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a7b67174-34d0-4fd8-82cc-278ace77a965.png\\\" target=\\\"_blank\\\"><img width=\\\"261\\\" height=\\\"249\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/52a5a94e-38fd-4f9d-a219-f229830bc1a9.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"variable \\\"stack_config\\\" {\\n  type = \\\"map\\\"\\n\\n  default = {\\n    dev = {\\n      name             = \\\"webapp\\\"\\n      rg_name_prefix   = \\\"cd-terra\\\"\\n      plan_name_prefix = \\\"cdterra\\\"\\n      app_name_prefix  = \\\"cdterraweb\\\"\\n    }\\n\\n    uat = {\\n      name             = \\\"webapp\\\"\\n      rg_name_prefix   = \\\"cd-terra\\\"\\n      plan_name_prefix = \\\"cdterra\\\"\\n      app_name_prefix  = \\\"cdterraweb\\\"\\n    }\\n  }\\n}\\n\",\"language\":\"javascript;\"}],[\"code\",{\"code\":\"locals {\\n  env        = \\\"${var.environment[terraform.workspace]}\\\"\\n  secrets    = \\\"${var.secrets[terraform.workspace]}\\\"\\n  stack      = \\\"${var.stack_config[terraform.workspace]}\\\"\\n  created_by = \\\"${var.created_by}\\\"\\n  stack_name = \\\"${local.stack[\\\"name\\\"]}\\\"\\n\\n  env_name   = \\\"${terraform.workspace}\\\"\\n  release    = \\\"${var.release}\\\"\\n  ...\\n  app_name   = \\\"${local.stack[\\\"app_name_prefix\\\"]}-${local.env_name}\\\"\\n}\\n\",\"language\":\"javascript;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6260f976-0208-4c25-992f-016a8d4b1a0b.png\\\" target=\\\"_blank\\\"><img width=\\\"342\\\" height=\\\"181\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a541f404-0800-4406-9ad0-0c07c6fd8eef.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"#!/bin/bash -e\\n\\necho \\\"*********** Initialize backend\\\"\\necho \\\"access_key = \\\\\\\"${1}\\\\\\\"\\\" &gt; ../backend.tfvars\\n$2/terraform init -backend-config=../backend.tfvars -no-color\\n\\necho \\\"\\\"\\necho \\\"*********** Create or select workspace\\\"\\nif [ $($2/terraform workspace list | grep $3 | wc -l) -eq 0 ]; then\\n  echo \\\"Create new workspace $3\\\"\\n  $2/terraform workspace new $3 -no-color\\nelse\\n  echo \\\"Switch to workspace $3\\\"\\n  $2/terraform workspace select $3 -no-color\\nfi\\n\\necho \\\"\\\"\\necho \\\"*********** Run 'plan'\\\"\\n$2/terraform plan --var-file=../global.tfvars --var-file=../release.tfvars -var=\\\"release=$4\\\" --out=./tf.plan -no-color -input=false\\n\\necho \\\"\\\"\\necho \\\"*********** Run 'apply'\\\"\\n$2/terraform apply -no-color -input=false -auto-approve ./tf.plan\\n\",\"language\":\"bash;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/61839421-84e8-48fe-89d4-fa625c0bd91b.png\\\" target=\\\"_blank\\\"><img width=\\\"296\\\" height=\\\"257\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/22aa154b-6cbf-4016-b8a9-dc4ae8af1c39.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/206156c7-baaf-45b5-87f7-64b296e8ffa2.png\\\" target=\\\"_blank\\\"><img width=\\\"299\\\" height=\\\"199\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fa3010fe-4573-4a5f-8a6a-65ff9796f81e.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7185ec85-8478-4061-8635-ce6f8164b4bd.png\\\" target=\\\"_blank\\\"><img width=\\\"280\\\" height=\\\"239\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f2bb76d7-11b5-4349-b438-6ec1197132b4.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5ac6f588-deed-473b-a467-21dcb76cce28.png\\\" target=\\\"_blank\\\"><img width=\\\"308\\\" height=\\\"240\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6030a6c6-ff43-43ce-9886-4077de7bec0e.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/32e3266c-bafa-4e0d-8d49-707a0e3ba377.png\\\" target=\\\"_blank\\\"><img width=\\\"359\\\" height=\\\"279\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/60974a3f-a4a1-44a5-bb97-420472670fec.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/045d196f-8243-46d9-b5e4-a6d427e73895.png\\\" target=\\\"_blank\\\"><img width=\\\"315\\\" height=\\\"151\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/047a7b47-438d-441c-b646-a62ffff7a91a.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-authoring-templates\"]],[\"a\",[\"href\",\"https://www.terraform.io/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/vsts-terraform-sample\"]],[\"a\",[\"href\",\"https://marketplace.visualstudio.com/items?itemName=petergroenewegen.PeterGroenewegen-Xpirit-Vsts-Release-Terraform\"]],[\"a\",[\"href\",\"http://bit.ly/cacbuildtasks\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I've done a fair amount of \"],[0,[0],1,\"ARM template\"],[0,[],0,\" authoring. It's not as bad as XML, but the JSON can get laborious. A number of my colleagues use \"],[0,[1],1,\"Terraform\"],[0,[],0,\" templates and I was recently on a project that was using these templates. I quickly did a couple PluralSight Terraform classes to get up to speed and then started hacking away. In this post I'll jot down a couple thoughts about how we structure Terraform projects and how to deploy them using VSTS. The source code for this post is on \"],[0,[2],1,\"Github\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Stacks\"]]],[1,\"p\",[[0,[],0,\"When we create Terraform projects, we divide them into \\\"stacks\\\". These are somewhat independent, loosely-coupled components of the full infrastructure we're deploying. Let's take the example of an Azure App Service with deployment slots that connects to an Azure SQL database and has Application Insights configured. In this scenario, we have three \\\"stacks\\\": SQL, WebApp and AppInsights. We then have an additional \\\"stack\\\" for the Terraform remote state (an Azure blob) and finally a folder for scripts. Here's what our final folder structure looks like:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Follow the instructions in the README.md file for initializing the backend using the state folder. Note that backend.tfvars and secrets.tfvars are ignored by the .gitignore file so should not be committed to the repo.\"]]],[1,\"h2\",[[0,[],0,\"Workspace = Environment\"]]],[1,\"p\",[[0,[],0,\"Thinking ahead, we may want to create different environments. This is where Terraform workspaces come in handy - we use them to represent different environments. That way we can have a single template and can re-use it in multiple environments. So if you look at webapp/variables.tf, you'll see this snippet:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can see how we have different maps for different workspaces. To consume the environment (or workspace) specific variables, we use the locals resource in our main.tf scripts. The common format is something like this:\"]]],[10,2],[1,\"p\",[[0,[],0,\"We create local variables for env, secrets and stack by dereferencing the appropriate workspace's map values. Now we can use \\\"${local.app_name}\\\" and the value will be an environment-specific value. Also note how we have a variable called \\\"release\\\" - we add this as a tag to all the Azure resources being created so that we can tie the resource to the release that created/updated it.\"]]],[1,\"h2\",[[0,[],0,\"Releases in VSTS\"]]],[1,\"p\",[[0,[],0,\"Now we get to the really interesting bit: how we run the templates in a release pipeline in VSTS. I tried a couple of marketplace Terraform extensions, but wasn't happy with the results. The most promising one was Peter Groenewegen's \"],[0,[3],1,\"extension\"],[0,[],0,\", but it was not workspace aware and while it did download Terraform so that I could run Terraform using the hosted agents, it didn't preserve Terraform on the path. I eventually ditched it for a plain ol' bash script. To perform Terraform operations, we have to:\"]]],[3,\"ol\",[[[0,[],0,\"Replace tokens in the release.tfvars file\"]],[[0,[],0,\"Download the Terraform executable\"]],[[0,[],0,\"Run the Terraform apply bash script for each stack in order\"]]]],[1,\"p\",[[0,[],0,\"I ended up using the Hosted Ubuntu 1604 hosted agent for the agent phase - I don't know for sure, but I suspect this is running in a container - in any case, it's super fast to start up and execute. Because I'm running the build on Linux, I wrote the scripts I used in bash - but you can easily create equivalent PowerShell scripts if you really want to - though VSTS will run bash scripts on Windows agents just fine - although the paths are different.\"]]],[1,\"h3\",[[0,[],0,\"Artifact\"]]],[1,\"p\",[[0,[],0,\"For the release to work, it needs access to the terraform templates. I create a new release and use the source repo as the incoming artifact with an alias \\\"infra\\\". You can add multiple artifacts, so if you're going to deploy code after deploying infrastructure, then you can add in the build as another artifact.\"]]],[1,\"h3\",[[0,[],0,\"Variables\"]]],[1,\"p\",[[0,[],0,\"In the release, I define a number of variables:\"]]],[10,3],[1,\"p\",[[0,[],0,\"I tried to use the environment variable format for ARM_ACCESS_KEY, ARM_CLIENT_ID etc. but found I had to supply these explicitly - which I do via the release.tfvars file. The release.tfvars file has tokens that are replaces with the environment values at deploy time. If you add more environment-specific variables, then you need to add their tokens in the tfvars file and add the variable into the variables section of the release. One last note: I use $(Release.EnvironmentName) as the value for the Environment variable - but this needs a different value for the \\\"destroy\\\" environment (each environment I have in the pipeline has a corresponding \\\"destroy\\\" environment for destroying the resources). You can see how I specify \\\"dev\\\" as the Environment name for the \\\"destroy dev\\\" environment.\"]]],[1,\"h3\",[[0,[],0,\"Download Terraform\"]]],[1,\"p\",[[0,[],0,\"This is only required if you're using the hosted agents - if you're using a private agent, then you're better off downloading terraform and adding it to the PATH. However, in the scripts folder I have a bash script (download-terraform.sh) that downloads Terraform using curl (from a URL specified in a variable) and untars it to the path specified in the TerraformPath variable. From that point on, you can use $(TerraformPath)\\\\terraform for any Terraform operations.\"]]],[1,\"h3\",[[0,[],0,\"Applying a Stack\"]]],[1,\"p\",[[0,[],0,\"The scripts folder contains the script for applying a stack (run-terraform.sh). Let's dig into the script:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 3 - 5: Initialize the backend. Output the access_key to the root backend.tfvars file (remember this won't exist in the repo since this file is ignored in .gitignore)\"]],[[0,[],0,\"Lines 7-15: Create or select the workspace (environment)\"]],[[0,[],0,\"Lines 17-19: Run terraform plan passing in global variables from global.tfvars, environment-specific variables now encapsulated in release.tfvars and pass in the release number (for tagging)\"]],[[0,[],0,\"Lines 21-23: Run terraform apply using the plan generated in the previous command\"]]]],[1,\"h3\",[[0,[],0,\"Destroying a Stack\"]]],[1,\"p\",[[0,[],0,\"Destroying a stack is almost the same as applying one - the script (run-terraform-destroy.sh) just does a \\\"plan -destroy\\\" to preview the operations before calling terraform destroy.\"]]],[1,\"h3\",[[0,[],0,\"The Release Steps\"]]],[1,\"p\",[[0,[],0,\"Now we can see what these blocks look like in the pipeline. Here's a pipeline with a dev and a \\\"destroy dev\\\" environment:\"]]],[10,5],[1,\"p\",[[0,[],0,\"The dev environment triggers immediately after the release is created, while the \\\"destroy dev\\\" environment is a manual-only trigger.\"]]],[1,\"p\",[[0,[],0,\"Let's see what's in the dev environment:\"]]],[10,6],[1,\"p\",[[0,[],0,\"There you can see 5 tasks: replace variables, download Terraform and then an apply for each stack (in this case we have 3 stacks). The order here is important only because the WebApp stack reads output variables from the state data of the SQL and AppInsights deployments (to get the AppInsights key and SQL connection strings). Let's take a closer look at each task:\"]]],[1,\"h4\",[[0,[],0,\"Replace Variables\"]]],[1,\"p\",[[0,[],0,\"For this I use my trusty ReplaceTokens task from my \"],[0,[4],1,\"build and release extension pack\"],[0,[],0,\". Specify the folder (the root) that contains the release.tfvars file and the file search format, which is just release.tfvars:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Next we use a Shell task to run the download Terraform script, which expects the path to install to as well as the URL for the Terraform binary to download:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Finally we use a Shell task for each stack - the only change is the working folder (under Advanced) needs to be the stack folder - otherwise everything else stays the same:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Success! Here you can see a run where nothing was changed except the release tag - the templates are idempotent, so we let Terraform figure out what changes (if any) are necessary.\"]]],[10,10],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Terraform feels to me to be a more \\\"enterprise\\\" method of creating infrastructure as code than using pure ARM templates. It's almost like what TypeScript is to JavaScript - Terraform has better sharing and state awareness and allows for more maintainable and better structured code. Once I had iterated a bit on how to execute the Terraform templates in a release, I got it down to a couple really simple scripts. These could even be wrapped into custom tasks.\"]]],[1,\"p\",[[0,[],0,\"Happy deploying!\"]]]]}","published_at":1533690534000,"status":"published","published_by":1},{"id":"31d67685-9c54-4430-8ba0-b8f8e167510f","title":"Test Case Manager: Customize Failure and Resolution Type","slug":"test-case-manager-customize-failure-and-resolution-type","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-t4a_kS-0_WQ/UUi0PYbJc0I/AAAAAAAAApM/oRpIK3_2rUc/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-GCU-UWuNV7g/UUi0QjIpIGI/AAAAAAAAApU/sxN5-ZNBTo4/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"364\\\" height=\\\"199\\\"></a>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">tcm fieldmapping /export /collection:<strong>collectionURL</strong> /teamproject:<strong>Project</strong> /type:FailureType /mappingFile:FailureTypes.xml</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">tcm fieldmapping /export /collection:<strong>collectionURL</strong> /teamproject:<strong>Project</strong> /type:ResolutionType /mappingFile:ResolutionTypes.xml</font>\"}],[\"code\",{\"code\":\"<!--?xml version=\\\"1.0\\\" encoding=\\\"utf-16\\\"?--><br><testfailuretypes><br>  <testfailuretype name=\\\"Regression\\\"><br>  <testfailuretype name=\\\"New Issue\\\"><br>  <testfailuretype name=\\\"Known Issue\\\"><br>  <testfailuretype name=\\\"Unknown\\\"><br></testfailuretype></testfailuretype></testfailuretype></testfailuretype></testfailuretypes><br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"<!--?xml version=\\\"1.0\\\" encoding=\\\"utf-16\\\"?--><br><testresolutionstates><br>  <testresolutionstate name=\\\"Configuration issue\\\"><br>  <testresolutionstate name=\\\"Needs investigation\\\"><br>  <testresolutionstate name=\\\"Product issue\\\"><br>  <testresolutionstate name=\\\"Test issue\\\"><br></testresolutionstate></testresolutionstate></testresolutionstate></testresolutionstate></testresolutionstates><br>\",\"language\":\"xml; ruler\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">tcm fieldmapping /import /collection:<strong>collectionURL</strong> /teamproject:<strong>Project</strong> /type:FailureType /mappingFile:FailureTypes.xml</font>\"}],[\"html\",{\"html\":\"<font size=\\\"2\\\" face=\\\"Courier New\\\">tcm fieldmapping /import /collection:<strong>collectionURL</strong> /teamproject:<strong>Project</strong> /type:ResolutionType /mappingFile:ResolutionTypes.xml</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-4Q1g78FyB44/UUi0Rg70FdI/AAAAAAAAApc/l2FlHrIvBjM/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-hy4l-RVHqcE/UUi0TiAe36I/AAAAAAAAApk/gvBaS1v1EjY/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"313\\\" height=\\\"339\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-PQkxMGznEbQ/UUi0U-wkp9I/AAAAAAAAAps/pijAP-bwSN8/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-DYoCHNhi4Us/UUi0WOenaNI/AAAAAAAAAp0/14-5SeK8km4/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"312\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/bharry/archive/2013/01/30/announcing-visual-studio-2012-update-2-vs2012-2.aspx\"]],[\"a\",[\"href\",\"http://go.microsoft.com/fwlink/?LinkId=273878\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In Test Case Manager, you can open a test run that has failed and set the Failure and Resolution types for the failure.\"]]],[10,0],[1,\"p\",[[0,[],0,\"I’ve had a lot of customers ask me if it’s possible to customize the lists. Up until now, the answer has been no. However, in the middle of a bunch of improvements released in the CTPs of QU2 (Quarterly Update 2), \"],[0,[0],1,\"Brian Harry mentioned\"],[0,[],0,\" that it is now possible to customize the lists. The CTP he was referring to is NOT A GO LIVE CTP, so rather \"],[0,[1],1,\"download CTP4\"],[0,[],0,\" of the update (which IS GO LIVE) if you want to try this on a production server.\"]]],[1,\"p\",[[0,[],0,\"The customization is only possible via the command line. And even that is hard to figure out. So here are the 4 commands that you need.\"]]],[1,\"p\",[[0,[],0,\"Export the Current Lists\"]]],[1,\"p\",[[0,[],0,\"Open a developer command prompt, and type the following commands (substituting your collection URL and team project accordingly):\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"This will export the two lists for you. They are pretty straight-forward and self-explanatory:\"]]],[10,3],[10,4],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Now edit the lists, and then use the import commands:\"]]],[1,\"p\",[[1,[],0,1]]],[10,5],[1,\"p\",[[1,[],0,2]]],[10,6],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"That’s it – restart (not just refresh) Test Case Manager and you’re good to go.\"]]],[1,\"p\",[[1,[],0,4]]],[10,7],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"Of course they new values appear in the Plan Results page, and though I haven’t tested it, I presume they’ll be in the warehouse too:\"]]],[1,\"p\",[[1,[],0,6]]],[10,8],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1363751640000,"status":"published","published_by":1},{"id":"63319a5b-a0d2-42a5-a06b-ade5a1081cc7","title":"Test Result Traceability Matrix Tool","slug":"test-result-traceability-matrix-tool","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/54f8b5ce-b27c-44c6-8930-0e9f6b02ffee.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0239fdda-4c98-4efc-a156-db27d9ca9646.png\\\" width=\\\"766\\\" height=\\\"304\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.modernrequirements.com/smartexcel4tfs/\"]],[\"a\",[\"href\",\"http://1drv.ms/1AUErAE\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I am often asked if there is a way to see a “traceability matrix” in TFS. Different people define a “traceability matrix” in different ways. If you want to see how many tests there are for a set of requirements, then you can use \"],[0,[0],1,\"SmartExcel4TFS\"],[0,[],0,\". However, this doesn’t tell you what state the current tests are in – so you can’t see how many tests are passing / failing etc.\"]]],[1,\"h2\",[[0,[],0,\"Test Points\"]]],[1,\"p\",[[0,[],0,\"Of course this is because there is a difference between a test case and a test point in TFS. A test point is the combination of Test Case, Test Suite and Test Configuration. So let’s say you have Test ABC in Suite 1 and Suite 2 and have it for 2 configurations (Win7 and Win8, for example). Then you’ll really have 1 test case and 4 test points (2 suites x 2 configurations). So if you want to know “is this test case passing?” you really have to ask, “Is this test case passing in this suite and for this configuration?”.\"]]],[1,\"p\",[[0,[],0,\"However, you can do a bit of a “cheat” by making an assumption: if the most recent result is Pass/Fail/Not Run/Blocked, then assume the “result of the test” is Pass/Fail/Not Run/Blocked. Of course if the “last result” is failed, you’d have to find exactly which suite/configuration the failure relates to in order to get any detail. Anyway, for most situations this assumption isn’t too bad.\"]]],[1,\"h2\",[[0,[],0,\"Test Result Traceability Matrix Tool\"]]],[1,\"p\",[[0,[],0,\"Given the assumption that the most recent test point result is the “result” of the Test Case, it’s possible to create a “test result traceability matrix”. If you plot Requirement vs Test Case in a grid, and then color the intersecting cells with the appropriate “result”, you can get a good idea of what state tests are in in relation to your requirements. So I’ve written a utility that will generate this matrix for you (see the bottom of this post for the link).\"]]],[1,\"p\",[[0,[],0,\"Here’s the output of a run:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The first 3 columns are:\"]]],[3,\"ul\",[[[0,[],0,\"Requirement ID\"]],[[0,[],0,\"Requirement Title\"]],[[0,[],0,\"Requirement State\"]]]],[1,\"p\",[[0,[],0,\"Then I sum the total of the test case results per category for that requirement – you can see that Requirement 30 has 2 Passed Tests and 1 Failed test (also 0 blocked and 0 not run). If you move along the same row, you’ll see the green and red blocks where the tests cases intersect with their requirements. The colors are as follows:\"]]],[3,\"ul\",[[[0,[],0,\"Green = Passed\"]],[[0,[],0,\"Red = Failed\"]],[[0,[],0,\"Orange = Blocked\"]],[[0,[],0,\"Blue = Not Run\"]]]],[1,\"p\",[[0,[],0,\"You can see I’ve turned on conditional formatting for the 4 totals columns. I’ve also added filtering to the header, so you can sort / filter the requirements on id, title or state.\"]]],[1,\"h2\",[[0,[],0,\"Some Notes\"]]],[1,\"p\",[[0,[],0,\"This tool requires the following arguments:\"]]],[3,\"ol\",[[[0,[],0,\"TpcUrl – the URL to the team project collection\"]],[[0,[],0,\"ProjectName – the name of the Team Project you’re creating the matrix for\"]],[[0,[],0,\"(Optional) RequirementQueryName – if you don’t specify this, you’ll get the matrix for all requirements in the team project. Alternatively, you can create a flat-list query to return only requirements you want to see (for example all Stories in a particular area path) and the matrix will only show those requirements.\"]]]],[1,\"p\",[[0,[],0,\"I speak of “requirements” – the tool essentially gets all the work items in the “requirements category” as a top-level query and then fetches all work items in the “test case category” that are linked to the top-level items. So this will work as long as your process template has a Requirements / Test Case category.\"]]],[1,\"p\",[[0,[],0,\"The tool isn’t particularly efficient – so if you have large numbers of requirements, test cases and test plans the tool could take a while to run. Also, the tool selects the first “requirementsQuery” that matches the name you pass in – so make sure the name of your requirements query is unique. The tool doesn’t support one-hop or tree queries for this query either.\"]]],[1,\"p\",[[0,[],0,\"Let me know what you think!\"]]],[1,\"h2\",[[0,[],0,\"Download\"]]],[1,\"p\",[[0,[],0,\"Here’s a \"],[0,[1],1,\"link to the executable\"],[0,[],0,\": you’ll need Team Explorer 2013 and Excel to be installed on the machine you run this tool from. To run it, download and extract the zip. The open up a console and run TestResultMatrix.exe.\"]]],[1,\"p\",[[0,[],0,\"Happy matrix generating!\"]]]]}","published_at":1411154902000,"status":"published","published_by":1},{"id":"f1538d9f-d573-4cda-9846-e29c4329f8b0","title":"Testing in Production: Routing Traffic During a Release","slug":"testing-in-production-routing-traffic-during-a-release","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f863725e-2677-44d2-801b-b47d56b7c930.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/269dc0f8-c80b-4a80-8f06-91ca74aeb113.png\\\" width=\\\"421\\\" height=\\\"258\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3732c09c-f70e-4358-838d-98553f39e19b.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/900b2a20-072c-4f73-9e35-2a626ca8a461.png\\\" width=\\\"414\\\" height=\\\"182\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1fdf1cdb-3870-4ea4-a74d-5c38a6d69fb6.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/29c131e0-2413-47ec-944c-4c1844dd96e8.png\\\" width=\\\"410\\\" height=\\\"166\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/841a61a8-c97c-4be5-948d-876eb20b4d75.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bb224c40-b900-4187-b0ec-8c456be61e8b.png\\\" width=\\\"419\\\" height=\\\"142\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/de4dbdb2-e9fc-4772-abc3-3d64f629d583.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/26843d57-249c-4f73-a686-77af531c6027.png\\\" width=\\\"436\\\" height=\\\"184\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b68db4d3-1553-4a7f-a7b2-a84ac73f5231.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d79818cf-6180-44ea-8e95-e1939d4f4655.png\\\" width=\\\"435\\\" height=\\\"181\\\"></a>\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Professor_Farnsworth\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/RouteTraffic\"]],[\"a\",[\"href\",\"http://bit.ly/cacbuildtasks\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"DevOps is a journey that every team should at least have started by now. Most of the engagements I have been on in the last year or so have been in the build/release automation space. There are still several practices that I think teams must invest in to remain competitive – unit testing during builds and integration testing during releases are crucial foundations for more advanced DevOps, which I’ve blogged about (a lot) before. However, Application Performance Monitoring (APM) is also something that I believe is becoming more and more critical to successful DevOps teams. And one application of monitoring is \"],[0,[0],1,\"hypothesis driven development\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"Hypothesis Driven Development using App Service Slots\"]]],[1,\"p\",[[0,[],0,\"There are some prerequisites for hypothesis driven development: you need to have metrics that you can measure (I highly, highly recommend using Application Insights to gather the metrics) and you have to have a hypothesis that you can quickly test. Testing in production is the best way to do this – but how do you manage that?\"]]],[1,\"p\",[[0,[],0,\"If you’re deploying to Azure App Services, then it’s pretty simple: create a deployment slot on the Web App that you can deploy the “experimental” version of your code to and divert a small percentage of traffic from the real prod site to the experimental slot. Then monitor your metrics. If you’re happy, swap the slots, instantly promoting the experiment. If it does not work, then you’ve failed fast – and you can back out.\"]]],[1,\"p\",[[0,[],0,\"Sounds easy. But how do you do all of that in an automated pipeline? Well, you can already deploy to a slot using VSTS and you can already swap slots using OOB tasks. What’s missing is the ability to route a percentage of traffic to a slot.\"]]],[1,\"h3\",[[0,[],0,\"Route Traffic Task\"]]],[1,\"p\",[[0,[],0,\"To quote \"],[0,[1],1,\"Professor Farnsworth\"],[0,[],0,\", “Good news everyone!” There is now a \"],[0,[2],1,\"VSTS task\"],[0,[],0,\" in my \"],[0,[3],1,\"extension pack\"],[0,[],0,\" that allows you to configure a percentage of traffic to a slot during your release – the Route Traffic task. To use it, just deploy the new version of the site to a slot and then drop in a Route Traffic task to route a percentage of traffic to the staging site. At this point, you can approve or reject the experiment – in both cases, take the traffic percentage down to 0 to the slot (so that 100% traffic goes to the production slot) ad then if the experiment is successful, swap the slots.\"]]],[1,\"h3\",[[0,[],0,\"What He Said – In Pictures\"]]],[1,\"p\",[[0,[],0,\"To illustrate that, here’s an example. In this release I have DEV and QA environments (details left out for brevity), and then I’ve split prod into Prod-blue, blue-cleanup and Prod-success. There is a post-approval set on Prod-blue. For both success and failure of the experiment, approve the Prod-blue environment. At this stage, blue-cleanup automatically runs, turning the traffic routing to 0 for the experimental slot. Then Prod-success starts, but it has a pre-approval set that you can approve only if the experiment is successful: it swaps the slots.\"]]],[1,\"p\",[[0,[],0,\"Here is the entire release in one graphic:\"]]],[10,0],[1,\"p\",[[0,[],0,\"In Prod-blue, the incoming build is deployed to the “blue” slot on the web app:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Next, the Route Traffic task routes a percentage of traffic to the blue slot (in this case, 23%):\"]]],[10,2],[1,\"p\",[[0,[],0,\"If you now open the App Service in the Azure Portal, click on “Testing in Production” to view the traffic routing:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now it’s time to monitor the two slots to check if the experiment is successful. Once you’ve determined the result, you can approve the Prod-blue environment, which automatically triggers the blue-cleanup environment, which updates the traffic routing to route 0% traffic to the blue slot (effectively removing the traffic route altogether).\"]]],[10,4],[1,\"p\",[[0,[],0,\"Then the Prod-success environment is triggered with a manual pre-deployment configured – reject to end the experiment (if it failed) or approve to execute the swap slot task to make the experimental site production.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Whew! We were able to automate an experiment fairly easily using the Route Traffic task!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Using my new Route Traffic task, you can easily configure traffic routing into your pipeline to conduct true A/B testing. Happy hypothesizing!\"]]]]}","published_at":1494400407000,"status":"published","published_by":1},{"id":"fc1a4cee-4641-4220-9fc8-ae0169f6279c","title":"Testing Windows 8 Applications","slug":"testing-windows-8-applications","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-I6x_uS68ESU/UaWZOhisFqI/AAAAAAAAAu8/l3xWuuXiFAc/s1600-h/image%25255B32%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-nWJEM_EYiCI/UaWZPs9XvMI/AAAAAAAAAvE/_QUAawKlOEQ/image_thumb%25255B14%25255D.png?imgmax=800\\\" width=\\\"348\\\" height=\\\"126\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-5SxT5qs9SV0/UaWZQedH3TI/AAAAAAAAAvM/fgHVncyjoTg/s1600-h/image%25255B36%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-dKFRG8M0WSY/UaWZRYtPImI/AAAAAAAAAvU/OMklrZRH24c/image_thumb%25255B16%25255D.png?imgmax=800\\\" width=\\\"288\\\" height=\\\"155\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-fTKG6fPGwd8/UaWZVK9YKkI/AAAAAAAAAvc/xlimPTb9dbs/s1600-h/image%25255B40%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-VxKbvHw50ec/UaWZWCm_suI/AAAAAAAAAvk/QVQ0aCFwIKE/image_thumb%25255B18%25255D.png?imgmax=800\\\" width=\\\"377\\\" height=\\\"180\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-dJKA2o4CsXE/UaWZXYQCJzI/AAAAAAAAAvs/F4VGdqCOAso/s1600-h/image%25255B48%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-4QXp2G5r8O4/UaWZYSQ827I/AAAAAAAAAv0/2I7ZBcr0LbY/image_thumb%25255B22%25255D.png?imgmax=800\\\" width=\\\"413\\\" height=\\\"195\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-Izb-AdV1k8c/UaWZaF3ECqI/AAAAAAAAAv8/qRlq7ldv1jM/s1600-h/image%25255B51%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-Bhc7KJB9gxQ/UaWZbIccvoI/AAAAAAAAAwE/q8LuLDY5Cfw/image_thumb%25255B23%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"206\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-gwif932gvJ8/UaWZb2O3PSI/AAAAAAAAAwM/bz36bpCriJ4/s1600-h/image%25255B55%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-S8OkszFKnv8/UaWZdGCEWGI/AAAAAAAAAwU/zBK3Op3zAbs/image_thumb%25255B25%25255D.png?imgmax=800\\\" width=\\\"368\\\" height=\\\"112\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-kxlWdSOhguY/UaWZd5Y4HmI/AAAAAAAAAwc/JazNXdG-m2o/s1600-h/image%25255B6%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-Egp4nLOWtvk/UaWZeiYIWTI/AAAAAAAAAwk/Os-T8KGIhxs/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"327\\\" height=\\\"147\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-iMbJBcXOgWA/UaWZfkjcDrI/AAAAAAAAAws/hHGNq2UQje0/s1600-h/image%25255B2%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-M7ogMhQDeJo/UaWZgbiiGyI/AAAAAAAAAw0/Yb_aeOUt2so/image_thumb.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"237\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-vucVix_l_zc/UaWZhFFr_NI/AAAAAAAAAw8/VGhqFGdP9s4/s1600-h/image%25255B9%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-UUozqlAg3y8/UaWZh-QWV4I/AAAAAAAAAxE/3Fn5a_qHVHU/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"70\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-OcdCcKd0m-0/UaWZix65UTI/AAAAAAAAAxM/V0Ixcyvs0fk/s1600-h/image%25255B59%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-OuQiiecW754/UaWZjp8uZ3I/AAAAAAAAAxU/H3KmpgUEeZc/image_thumb%25255B26%25255D.png?imgmax=800\\\" width=\\\"368\\\" height=\\\"219\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-VH5B7GgrVMk/UaWZkhvGBSI/AAAAAAAAAxc/TTw79uWwAYc/s1600-h/image%25255B63%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-M9c2fUhInY8/UaWZlcX1bLI/AAAAAAAAAxk/RLA2ER6DanQ/image_thumb%25255B27%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"146\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-2xpyYcWWGM0/UaWZmZdFk6I/AAAAAAAAAxs/Wx2V0h5MPkU/s1600-h/image%25255B67%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-aKT-lbrs3dE/UaWZnodqwHI/AAAAAAAAAx0/s8CGRY4PciE/image_thumb%25255B29%25255D.png?imgmax=800\\\" width=\\\"300\\\" height=\\\"362\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-GpYMjbvtjbQ/UaWZpNefE0I/AAAAAAAAAx8/gPRu_xmMTWI/s1600-h/image%25255B24%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-mDCLg1-e5pM/UaWZqa36DdI/AAAAAAAAAyE/yXIrKubGUo8/image_thumb%25255B10%25255D.png?imgmax=800\\\" width=\\\"377\\\" height=\\\"197\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-CvsrlXUn7yE/UaWZreROIgI/AAAAAAAAAyM/3canWnFyUwo/s1600-h/image%25255B74%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-CSBY1s2l_bo/UaWZsPSzavI/AAAAAAAAAyU/6bkgNX05bfM/image_thumb%25255B32%25255D.png?imgmax=800\\\" width=\\\"267\\\" height=\\\"302\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-w8isJCIS5Cw/UaWZvYaQHyI/AAAAAAAAAyc/IFkn5JII8NQ/s1600-h/image%25255B75%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-O5SnIY7qYkg/UaWZw6O7EaI/AAAAAAAAAyk/z5G-BpxjOnE/image_thumb%25255B33%25255D.png?imgmax=800\\\" width=\\\"357\\\" height=\\\"283\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-Bvt9nNOrABU/UaWZzBs2WEI/AAAAAAAAAys/6k2NDZshoO0/s1600-h/image%25255B28%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-cGvh55ALWmQ/UaWZ0k_eHvI/AAAAAAAAAy0/dHMqEDNboT0/image_thumb%25255B12%25255D.png?imgmax=800\\\" width=\\\"278\\\" height=\\\"356\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-LHlArFUPrwM/UaWZ1hWTpiI/AAAAAAAAAy8/x2py-ZCrdO4/s1600-h/image%25255B79%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-K-fW3YPD1vM/UaWZ3cLhyMI/AAAAAAAAAzE/NSywHq3mA8M/image_thumb%25255B35%25255D.png?imgmax=800\\\" width=\\\"398\\\" height=\\\"290\\\"></a>\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"http://qunitmetro.github.io/QUnitMetro/\"]],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe\"]],[\"a\",[\"href\",\"http://www.microsoft.com/visualstudio/eng/downloads#d-additional-software\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/hh405417(v=vs.110)\"]],[\"a\",[\"href\",\"http://channel9.msdn.com/posts/Manual-Testing-Of-Windows-8-Metro-Style-Applications\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/library/dd380742(v=vs.110).aspx\"]],[\"a\",[\"href\",\"http://dev.bennage.com/blog/2012/08/15/unit-testing-winjs/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Windows Store applications are slowly becoming more popular. If you’re going to do any Windows Store development, you’ll need to shift a few paradigms in how you code – and how you test. There are hundreds of webcasts and blogs detailing Windows Store programming, but I want to explore testing Windows Store apps.\"]]],[1,\"h2\",[[0,[],0,\"The Summary (or, TL;DR)\"]]],[1,\"p\",[[0,[],0,\"There’s ok news and bad news. The good news is you can test Windows Store apps – the bad news is, it’s hard to do it properly.\"]]],[1,\"p\",[[0,[],0,\"First let’s consider unit test capabilities:\"]]],[1,\"p\",[[0,[],0,\"\"],[0,[0],1,\"JavaScript\"],[0,[],0,\" \"],[0,[0],1,\".NET\"],[0,[],0,\"  \"],[0,[0],1,\"Test Project Template\"],[0,[],0,\" QUnit for Metro has a project template Out-of-the-box using “Unit Test Library for Windows Store apps”  \"],[0,[0],1,\"Run Tests in Test Explorer\"],[0,[],0,\" No Yes  \"],[0,[0],1,\"Run Tests in Team Build\"],[0,[],0,\" No Using Interactive Build Agent (No code coverage)  \"],[0,[0],1,\"Coded UI Tests\"],[0,[],0,\" No No\"]]],[1,\"p\",[[0,[],0,\"Here’s a summary of Manual Testing capabilities:\"]]],[1,\"p\",[[0,[],0,\"\"],[0,[0],1,\"Capability\"],[0,[],0,\" \"],[0,[0],1,\"Yes or No?\"],[0,[],0,\"  Test on Local Machine No  Test on Remote Device Yes  Action Log Yes  Manual Code Coverage No  Event Log Yes  IntelliTrace No  Voice and Video Recording No  System Information Yes  Test Impact Analysis No  Manual Screen Captures Yes  Fast Forwarding No\"]]],[1,\"p\",[[0,[],0,\"Read on if you want some more details and explanations!\"]]],[1,\"h2\",[[0,[],0,\"Unit Testing (.NET)\"]]],[1,\"p\",[[0,[],0,\"If you want to unit test your application, you’ll need to add a Unit Test project. For Windows Store apps using .NET, you get a project template out-the-box.\"]]],[10,0],[1,\"p\",[[0,[],0,\"When you add this project to your solution, you’ll see that the test project actually has a structure that is very similar to a Windows Store app – it has a certificate and an app manifest too. This is because if you’re going to use and of the Windows capabilities in your tests (like File Storage, Camera, Location and so on) you’re going to need to enable them in the app manifest, just like you would a normal app.\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can run tests that are in the project in the Test Explorer window, but automating them as part of a Team Build will require you to run the Build Agent in the Interactive Mode (as opposed to as a service) since the Windows capabilities require interaction with the desktop. Furthermore, you won’t get Code Coverage when running these tests either through the Test Explorer or in a Team Build.\"]]],[1,\"h2\",[[0,[],0,\"Unit Testing (JavaScript)\"]]],[1,\"p\",[[0,[],0,\"For JavaScript apps, you’re almost out of luck. You can install \"],[0,[1],1,\"QUnit for Metro\"],[0,[],0,\" which gives similar functionality to the C# Unit Test Library. The difference is you’ll need to set the test project as your Start project and then run that to actually execute your tests – there is no “headless” way to run these tests. If you’ve installed \"],[0,[2],1,\"Chutzpah Test Adapter\"],[0,[],0,\", then you’ll see the tests in the Test Explorer Window, but they’ll fail if you try to run them. There’s no integration with Team Build, so you can’t run automated tests using this framework.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Once you’ve created a QUnit Test Project, you have to add links to the js files in the project you want to test. Then you write your QUnit tests, and run the application (setting the test project as the start up project). When you run that, it’ll execute your test cases:\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Manual Testing (.NET or JavaScript)\"]]],[1,\"p\",[[0,[],0,\"The manual testing story for both .NET and JavaScript apps is the same. The trick here is that you can’t test on the local machine – you’ll have to have a separate Windows 8 device to run your app on. The steps you need to follow for manual testing are as follows:\"]]],[1,\"p\",[[0,[],0,\"Step 1: Install the Remote Debugger on your remote device\"]]],[3,\"ul\",[[[0,[],0,\"Download the \"],[0,[3],1,\"Remote Debugger\"],[0,[],0,\" for your architecture (x64, x86 or ARM)\"]],[[0,[],0,\"Install the debugger on your remote device (amazingly, the ARM exe installs even on WinRT!)\"]],[[0,[],0,\"Launch the “Microsoft Test Tools Adapter” application from the Start screen\"]]]],[10,4],[1,\"p\",[[0,[],0,\"Step 2: Create a TFS Build to build your application\"]]],[1,\"p\",[[0,[],0,\"Step 3: Push your application to your remote device via MTM\"]]],[3,\"ul\",[[[0,[],0,\"Launch MTM and go to the Test Tab. Look for the links “Modify” and “Install Windows Store App”.\"]]]],[10,5],[3,\"ul\",[[[0,[],0,\"Press “Modify” and connect to your remote device.\"]]]],[10,6],[3,\"ul\",[[[0,[],0,\"Enter the name of the remote device and press the “Test” link. You’ll probably be prompted for credentials (if you’re logged in as a different user). In my case, my development machine is on a domain and my remote device (a Surface RT) is not – so I enter my Microsoft Account details to connect to the device.\"]]]],[10,7],[3,\"ul\",[[[0,[],0,\"Once you press enter, MTM will attempt to connect to the remote device. Make sure your device is not locked and that the “Microsoft Test Tools Adapter” is running.\"]]]],[10,8],[3,\"ul\",[[[0,[],0,\"Next you’ll have to click the “Install Windows Store App” in MTM to push your app. Browse to the drop folder of your application and find the appx file for your application.\"]]]],[10,9],[3,\"ul\",[[[0,[],0,\"Once you click “Open”, MTM will push the app to your device. If this is the fist time you’re doing it, you’ll need to get a developer licence (on the remote device) – MTM will pause until you complete this step. Go to the remote device, and open the Desktop app if you don’t see anything happening. Once you’ve signed in with your Microsoft Account details, you’ll see a notice saying that a developer license has been installed.\"]],[[0,[],0,\"MTM then also installs your app’s certificate. Again, you’ll have to go to the Desktop on the remote device and allow the certificate to install before MTM actually installs the app.\"]]]],[1,\"p\",[[0,[],0,\"Step 4: Set the build for your Test Plan\"]]],[3,\"ul\",[[[0,[],0,\"This ensures a tie-in from the executable you have and any bugs that you file\"]]]],[1,\"p\",[[0,[],0,\"Step 5: Configure Test Settings\"]]],[3,\"ul\",[[[0,[],0,\"You can try to collect video or IntelliTrace or do Test Impact Analysis – none of those collectors work.\"]]]],[10,10],[3,\"ul\",[[[0,[],0,\"What will work is the Action Log, Event Log and System Information. Just set them for the “Local” role, even though these are the collectors that will run on the remote device.\"]]]],[10,11],[1,\"p\",[[0,[],0,\"Step 6: Execute Your Tests\"]]],[3,\"ul\",[[[0,[],0,\"Click on a Test and hit “Run” to run the test\"]],[[0,[],0,\"When you do, you’ll see a message pop up on the Remote Device notifying you that a test session has started. All your actions are now being recorded (tap by tap, as it were).\"]]]],[10,12],[3,\"ul\",[[[0,[],0,\"As you complete each step, remember to mark it as “Pass or Fail” in the Test Runner that is running on your development machine\"]],[[0,[],0,\"Press “Screenshot” to take a screenshot – this actually pulls the remote device’s screen onto the development machine and allows you to mark a rectangle to be attached to the test results.\"]]]],[1,\"p\",[[0,[],0,\"When you log a bug, you’ll get detailed info. The Action Log opens really nicely as an HTML page with links showing where you tapped and so on.\"]]],[10,13],[10,14],[1,\"h2\",[[0,[],0,\"Exploratory Testing\"]]],[1,\"p\",[[0,[],0,\"This works in exactly the same manner – except you don’t have a test case to follow. You can filter the steps out when you log bugs (or test cases) as you go.\"]]],[10,15],[1,\"h2\",[[0,[],0,\"Coded UI Testing\"]]],[1,\"p\",[[0,[],0,\"I was really hoping that this would be something that you could do from Visual Studio – except, that it’s not. Here’s what happens when you generate a coded UI test from a test case action recording:\"]]],[10,16],[1,\"p\",[[0,[],0,\"That’s right, sports fans – no coded UI support yet.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Unfortunately it’s a bit of a bleak outlook at the moment for testing Windows Store apps. Even when you can do some testing, you’re limited, and you’re going to have to invest significant effort. I am hoping that the story will improve with future versions of TFS and VS. For now, the best value-for-time-and-money is on manual testing using a remote device. If your Windows Store App is written in .NET, you can do some unit testing, but you’ll have to make your build agent interactive to run the tests as part of a build. If you’ve got JavaScript Windows Store Apps, then try out the QUnit for Metro extension, but it feels a bit hacky and you won’t get any build integration.\"]]],[1,\"h2\",[[0,[],0,\"Links\"]]],[3,\"ul\",[[[0,[],0,\"There is a great \"],[0,[4],1,\"MSDN article\"],[0,[],0,\" about testing Windows Store Apps\"]],[[0,[],0,\"Brian Keller does a great \"],[0,[5],1,\"11 minute video\"],[0,[],0,\" showing some of these capabilities\"]],[[0,[],0,\"The \"],[0,[6],1,\"support matrix for Coded UI and Action Recordings\"],[0,[],0,\" currently showing no support for Windows Store Apps\"]],[[0,[],0,\"Christopher Bennage apparently got some \"],[0,[7],1,\"unit tests working for his JavaScript Windows Store Apps\"],[0,[],0,\", though I couldn’t seem to get the “headless” experience he claims. I also didn’t like polluting my application with tests (since his method has the tests in the App itself).\"]]]],[1,\"p\",[[0,[],0,\"(Un)Happy Windows Store App Testing!\"]]]]}","published_at":1369839720000,"status":"published","published_by":1},{"id":"d63df9b7-5cc6-4091-b42a-3a8c0c548517","title":"TF50299: The value named ‘xxx’ was not found when evaluating a condition","slug":"tf50299-the-value-named-xxx-was-not-found-when-evaluating-a-condition","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">ResultMessage : Job extension had an unhandled error: System.Exception: TF50299: The value named '070001' was not found when evaluating a condition.</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">… AND ‘070001’ = ‘MyProject’…</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">… AND TeamProject = ‘MyProject’…</font>\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Recently while working at a customer, we configured mail alerts for TFS. We checked that the SMTP server was correct and that we could send mail from the application tiers – everything looked correct, but still there were no mails.\"]]],[1,\"p\",[[0,[],0,\"In the event log, we found this “helpful” error (note the sarcasm):\"]]],[10,0],[1,\"p\",[[0,[],0,\"Unfortunately, there was woefully little information about this error.\"]]],[1,\"p\",[[0,[],0,\"The Team Project Collection was upgraded from a TFS2008 server, so it inherited a lot of “old” notifications. We eventually scanned the notifications, and found one that had a rather strange clause in it:\"]]],[10,1],[1,\"p\",[[0,[],0,\"It seemed strange to me that there appeared to be value parameters on the left and the right of the = operator – more usual conditions look like\"]]],[10,2],[1,\"p\",[[0,[],0,\"So we deleted this notification, and suddenly all the mails started coming through. It seems like this exception crashes the notification job completely – it would be nice if it just skipped this notification clause and then continued with the other notifications!\"]]],[1,\"p\",[[0,[],0,\"Moral of the story – make sure your notifications have properly formed condition clauses!\"]]]]}","published_at":1313469060000,"status":"published","published_by":1},{"id":"2634783f-2b60-4d70-9ca5-db396d520576","title":"TFS 2010 is Complete ALM","slug":"tfs-2010-is-complete-alm","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-gDcaqtC53vw/Th2GePbOBqI/AAAAAAAAASU/13r6pNkf0zs/s1600-h/Slide%252520Pic%25255B3%25255D.jpg\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"Slide Pic\\\" border=\\\"0\\\" alt=\\\"Slide Pic\\\" src=\\\"http://lh5.ggpht.com/-lX_lsp4k8hM/Th2GfKxzsTI/AAAAAAAAASY/F4pL4aDfB0k/Slide%252520Pic_thumb%25255B1%25255D.jpg?imgmax=800\\\" width=\\\"441\\\" height=\\\"346\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TYNRwyJLzOI/AAAAAAAAAQE/N7yLJehFg14/s1600-h/image%5B3%5D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TYNRxqxPFxI/AAAAAAAAAQI/Ptual4ICg5U/image_thumb%5B1%5D.png?imgmax=800\\\" width=\\\"382\\\" height=\\\"243\\\"></a>\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://subversion.apache.org/\"]],[\"a\",[\"href\",\"http://www.atlassian.com/software/jira/\"]],[\"a\",[\"href\",\"http://www.collab.net/\"]],[\"a\",[\"href\",\"http://jazz.net/\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/vstudio/ff637362\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd264915.aspx\"]],[\"a\",[\"href\",\"http://www.eclipse.org/\"]],[\"a\",[\"href\",\"http://www.microsoft.com/downloads/en/details.aspx?displaylang=en&FamilyID=af1f5168-c0f7-47c6-be7a-2a83a6c02e57\"]]],\"sections\":[[10,0],[1,\"p\",[[0,[0],1,\"(This slide shows the capabilities of TFS 2010 in 1 page. The orange blocks are TFS 2010 Features)\"]]],[1,\"p\",[[0,[],0,\"Some products in the Application Lifecycle Management (ALM) space provide only slices of ALM capabilities. For example, \"],[0,[1],1,\"Subversion\"],[0,[],0,\" (SVN) is a good source repository. \"],[0,[2],1,\"Atlassian’s Jira\"],[0,[],0,\" provides issue tracking (and some other modules that bolt on). Other products (like \"],[0,[3],1,\"Collabnet\"],[0,[],0,\" and the \"],[0,[4],1,\"Jazz\"],[0,[],0,\" platform) are more comprehensive ALM offerings.\"]]],[1,\"p\",[[0,[],0,\"I’m not going to dig too deeply into why I think \"],[0,[5],1,\"TFS 2010\"],[0,[],0,\" outshines all of these products, but I do want to focus on two main aspects that I think put TFS ahead: comprehensiveness and integration.\"]]],[1,\"h2\",[[0,[],0,\"TFS 2010 is Complete (or, “TFS, You Complete Me”)\"]]],[1,\"p\",[[0,[],0,\"TFS isn’t \"],[0,[0],1,\"just\"],[0,[],0,\" Source Control. It isn’t \"],[0,[0],1,\"just\"],[0,[],0,\" Work Item Tracking. It isn’t \"],[0,[0],1,\"just\"],[0,[],0,\" Lab Management, Automated Builds, Code Analysis, Automated UI Tests, Requirements Management and Project Management, Reporting and Dashboards, Performance Profiling and Load Testing, Manual Testing and Code Coverage. It is, in fact, \"],[0,[0],1,\"all of these\"],[0,[],0,\" (and more).\"]]],[1,\"p\",[[0,[],0,\"Which leads to the second major benefit of using TFS 2010 for ALM:\"]]],[1,\"h2\",[[0,[],0,\"TFS 2010 is Integrated (or, “TFS, You’re One For All, And All in One”)\"]]],[1,\"p\",[[0,[],0,\"Unlike many other “comprehensive” ALM products, TFS was designed to provide all the ALM functionality you could want \"],[0,[0],1,\"from the ground up\"],[0,[],0,\". It’s not just a hodge-podge of different pieces stitched together with fragile “bridges” and “connectors” (though TFS is extensible). That’s why you can link check-ins to work items and have builds report what changesets and what work items are included in a build. That’s why you can pull a report that shows your requirements with a breakdown of tasks used to implement the requirement (and a roll-up of completed and remaining work) as well as testing effort against that requirement as well as bugs against that requirement. That’s why you can get a tester to log a bug using just a title and have the system automatically attach video of the test session, historical debug information (\"],[0,[6],1,\"IntelliTrace\"],[0,[],0,\"), event logs and other diagnostic data in seconds – your developers will never close another bug with “no repro” again. And so on, and so on.\"]]],[10,1],[1,\"p\",[[0,[0],1,\"(This is a screen-shot of the “Stories Overview” report – this shows your requirements (stories) together with a roll up of development effort, test progress and bug status – all in one place)\"]]],[1,\"p\",[[0,[],0,\"This ability to tie work items and check-ins and builds and tests and bugs, almost effortlessly, is made possible by the fact that TFS 2010 provides all these features from a centralized repository. All source code, all work items, all test results, all reports live in a single place – meaning you can link almost anything to almost anything else – and it means you can analyse your entire ALM process from start to end (using the centralized data warehouse, of course).\"]]],[1,\"p\",[[0,[],0,\"This all means immense productivity gains for your development team. Add to that portals and wiki’s, ad-hoc reporting, event notifications, the Microsoft Test Manager tool for manual testing and Lab Management, integration into Visual Studio (and even other IDE’s like \"],[0,[7],1,\"Eclipse\"],[0,[],0,\" via \"],[0,[8],1,\"Team Explorer Everywhere\"],[0,[],0,\") and a host of other great process tools, and you can concentrate on creating applications quickly, continually improving on quality and enhance your process along the way.\"]]]]}","published_at":1300484160000,"status":"published","published_by":1},{"id":"68d775d8-bf4f-4873-b533-ce18cf1ca599","title":"TFS 2012 Upgrade Bugs–again?","slug":"tfs-2012-upgrade-bugsagain","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/bharry/archive/2013/01/11/tfs-2012-update-1-hotfix.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"On January 11, \"],[0,[0],1,\"Brian Harry blogged\"],[0,[],0,\" about some bugs in the TFS 2012 upgrade process (as well as the link to the KB patch for fixing the bugs). I was upgrading a customer in December last year and we hit one of the “symptoms” that was fixed with the patch.\"]]],[1,\"p\",[[0,[],0,\"I was at another customer about two weeks ago and we applied the patch before importing Team Project Collections and of course we didn’t hit any of the bugs. It was a great week for me!\"]]],[1,\"p\",[[0,[],0,\"Unfortunately, I didn’t have a great week last week when I went to upgrade another customer from TFS 2010 to TFS 2012. I applied the patch from Brian’s blog and while the Team Project Collections imported successfully, some other very strange things started to happen.\"]]],[1,\"p\",[[0,[],0,\"Most of the “bugs” occur around security – not being able to see collections that you have access to (or seeing collections you don’t have access to) for example. But we also had some obscure issues that were also security related, but were harder to diagnose.\"]]],[1,\"p\",[[0,[],0,\"For example, we had 22 Test Plans, and suddenly 4 of them were “broken”. If you tried to connect to these Test Plans, a popup appeared saying, “Team Foundation Server is currently offline. Try again later”. This of course wasn’t the case, since the server was up and running and we could access 18 other Test Plans! The event logs on the TFS server had an exception “Given key was not present in the dictionary”.\"]]],[1,\"p\",[[0,[],0,\"We also noticed that if you accesses a Test Plan that wasn’t “broken” and went to the Results pane, you got an “Object reference not set to an instance of an object” error message. Not cool…\"]]],[1,\"p\",[[0,[],0,\"Fortunately, being an MVP has its advantages. I mailed the champs list and within a couple of hours, members of the product team were helping me. Eventually they provided me with a script to run against the TFS database that sorted the weird issues, and my customer was able to get back to work. In fact, when the team provided me the script they told me that they had already found this bug and were including it in a second patch for the upgrade process. I don’t have a timeline on when this patch will be released – but I am confident that the product team has it in hand!\"]]],[1,\"h2\",[[0,[],0,\"Unofficial Analysis\"]]],[1,\"p\",[[0,[],0,\"I through back to why out of the three upgrades I’ve done, two seemed to have issues while the 3rd was error free. In both cases where there were issues, changes had been made to users on the AD domain that the TFS server was on – users had been deleted after they had touched the old TFS. I suspect that something in the identities logic wasn’t catering for “missing” users when you install TFS and then import a Team Project Collection that has “old” users.\"]]],[1,\"h2\",[[0,[],0,\"Quarterly Pain?\"]]],[1,\"p\",[[0,[],0,\"I love the fact that TFS on-premise is going to a quarterly release cycle. This is good news, especially for those of us who like new features and who like to be at the forefront. However, I hope that the product team improves the upgrade process. Of course this process is mammoth, and the number of permutations they have to cater for is staggering, so I’m not saying it’s going to be a cakewalk. However, if we’re going to be eager for four updates a year, we need to feel confident that they’ll “just work”.\"]]],[1,\"p\",[[0,[],0,\"A big thank you to Chandru Ramakrishnan and his fellows for the speedy help!\"]]],[1,\"p\",[[0,[],0,\"Happy upgrading!\"]]]]}","published_at":1359407820000,"status":"published","published_by":1},{"id":"9637e458-6c2d-412b-b915-92a95a12cc37","title":"TFS 2013 Default Build – The GetEnvironmentVariable<T> Activity","slug":"tfs-2013-default-build--the-getenvironmentvariablet-activity","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-5OVWfqZPPkM/UnDu0K_LnqI/AAAAAAAABGA/oXyjcYarb5k/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-00NCvm_o7q0/UnDu1DcgQ3I/AAAAAAAABGI/C1Mr6qKBzUg/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"378\\\" height=\\\"174\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-tt6xVmnj5vM/UnDu1nGoizI/AAAAAAAABGM/StHphcJ337A/s1600-h/image%25255B15%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-k1fmm_xtxDI/UnDu2GCgn8I/AAAAAAAABGY/8zFRa_42DKs/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"321\\\" height=\\\"82\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-qFLuPawXlUY/UnDu2xY8NEI/AAAAAAAABGg/709A8LdbqWY/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/--Ol0hxvXLLU/UnDu3eK08QI/AAAAAAAABGo/0k4wbjVV828/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"369\\\" height=\\\"162\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/07/matching-binary-version-to-build-number.html\"]],[\"strong\"],[\"u\"]],\"sections\":[[1,\"p\",[[0,[],0,\"If you’ve upgraded to TFS 2013, then you’ll notice that there’s a new Default Build template. In fact, to support Git repositories, the product team moved the default template into a super-secret-database-backed-folder-you-can’t-get-hold-of-place in TFS. This means that you won’t see it in the BuildProcessTemplates folder.\"]]],[1,\"p\",[[0,[],0,\"But the product team did make the default template quite flexible by building in pre- and post-build and pre- and post-test script arguments. To see how to use a pre-build script, refer to my post about \"],[0,[0],1,\"versioning assemblies to match build number\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"So if you’re going to customize the default template, you’ll have to download it first. Once you download it, you’ll see that it’s quite a bit “smaller” than the old default template. The team has “bundled” a bunch of very fine-grained activities into higher level activities. However, this means that some of the items that existed in the 2012 default template no longer exist. For example, the AssociateChangeSetsAndWorkItems activity in 2012 returns the associated work items – but the 2013 AssociateChanges activity has no return parameter. So how do you get the associated changesets? Another example is the SourcesDirectory – this used to be available from an assign activity (which created the variable and put it into a variable) in 2012 – but there’s no variable for this value in the 2013 workflow.\"]]],[1,\"p\",[[0,[],0,\"How then do you get these values? I’ll show you how you can get access to them via a new Activity called “GetEnvironmentVariable”. We’ll do this for SourcesDirectory and associated changes.\"]]],[1,\"h2\",[[0,[],0,\"Downloading the TfvcTemplate.12.xaml Template\"]]],[1,\"p\",[[0,[],0,\"If you’re going to customize the workflow for the default activity, you’ll need to download it. In VS 2013, Go to the Team Explorer and click on Builds. Click “New Build Definition”. Click on the Process tab. Now expand the “Show Details” button on the right to show the template details. You’ll see a “Download” link. Click it and save the template somewhere (possibly to the BuildProcessTemplates folder?)\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now you can open the template to edit it (don’t forget to add it into Source Control!).\"]]],[1,\"p\",[[0,[],0,\"Once it’s open, add in your variables – I’m just scoping mine to the whole workflow, so I’ll add them with the root “Overall build process” activity. Click on “Variables” and add a string variable called “sourcesDir” and an IList variable called “associatedChangesets”.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now find the place where you want to get the variables – in my case, I’ll go right down to the bottom of the Try-Catch in the RunOnAgent activity just after the “Publish Symbols…” activity.\"]]],[1,\"p\",[[0,[1,2],2,\"BEWARE\"],[0,[],0,\": The finally of this Try-Catch invokes a “ResetEnvironment” activity which will clear all the environment variables. If you need variables after this point of the workflow, be sure to remove this activity.\"]]],[1,\"p\",[[0,[],0,\"From the toolbox, under the “Team Foundation Build Activities” section, drag on a GetEnvironmentVariable activity. I made the type “String” for the 1st activity, and renamed it to “Get Sources Dir”. Then press F4 to get the properties of the activity – set the result to “sourcesDir”.\"]]],[1,\"p\",[[0,[],0,\"The name parameter you can get from an enumeration - Microsoft.TeamFoundation.Build.Activities.Extensions.WellKnownEnvironmentVariables. This enum has a list of all the variables you can query using the activity.\"]]],[1,\"p\",[[0,[],0,\"I set the value to “Microsoft.TeamFoundation.Build.Activities.Extensions.WellKnownEnvironmentVariables.SourcesDirectory”\"]]],[10,2],[1,\"p\",[[0,[],0,\"Now drag on another GetEnvironmentVariable activity and set the type to IList (optionally change the name). Set the result to “associatedChangesets” and the name to “Microsoft.TeamFoundation.Build.Activities.Extensions.WellKnownEnvironmentVariables.AssociatedChangesets”. (You’ll see AssociatedCommits too if you’re doing a Git build customization).\"]]],[1,\"p\",[[0,[],0,\"That’s all there is to it – you can now use the variables however you need to.\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1383165240000,"status":"published","published_by":1},{"id":"9def5255-6e02-4201-b8d5-c29436b3d12d","title":"TFS and Project Server Integration: Tips from the Trenches (Part 1)","slug":"tfs-and-project-server-integration-tips-from-the-trenches-(part-1)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_07.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_165.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_6118.html\"]],[\"a\",[\"href\",\"http://www.microsoft.com/project/en/us/default.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/chrisfie/archive/2010/07/06/microsoft-project-server-and-team-foundation-server-2010-ctp-virtual-machine.aspx\"]],[\"strong\"]],\"sections\":[[1,\"h2\",[[0,[],0,\"Links to this series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 2 – Setup\"],[0,[],0,\"\"]],[[0,[1],1,\"Part 3 – Configuration\"],[0,[],0,\"\"]],[[0,[2],1,\"Part 4 – Synchronizing Hierarchies from TFS\"]]]],[1,\"p\",[[0,[],0,\"Development teams often work in conjunction with a Project Management Office (PMO). A common scenario is the PMO creating high level requirements and the Dev team keeping the Project plans up to date manually in order to report progress back to the PMO. This process means duplication – the PMO task and the TFS requirement. The problem is even worse if the PMO is tracking detailed tasks. Another complication is having to update tasks in both systems.\"]]],[1,\"p\",[[0,[],0,\"That’s where the TFS Project Server integration comes into play. This integration keeps TFS and \"],[0,[3],1,\"Project Server\"],[0,[],0,\" up to date via 2 way synchronization. You can kick the tires a bit with \"],[0,[4],1,\"this virtual machine and labs\"],[0,[],0,\" that demo the capabilities. You can download the integrator from your MSDN subscription downloads.\"]]],[1,\"p\",[[0,[],0,\"I recently worked on an integration for a customer – and these posts are going to detail some of the gotchas that I ran across. This post will focus on the some of the limitations of the integration.\"]]],[1,\"h2\",[[0,[],0,\"Supported Scenarios\"]]],[1,\"p\",[[0,[],0,\"There are 2 scenario’s that are supported by the Integration: High Level Task Roll-up and Detailed Task Breakdown.\"]]],[1,\"p\",[[0,[],0,\"In the High Level Task Roll-up, the PMO creates high level tasks (in Project) that map to requirements in TFS. Only the requirements in TFS are sync’d to Project Server.\"]]],[1,\"p\",[[0,[],0,\"In the Detailed Task Breakdown, the requirements are created on the Project Server and then broken down into tasks on TFS. The tasks are also sync’d to Project Server.\"]]],[1,\"p\",[[0,[],0,\"This table compares the 2 approaches:\"]]],[1,\"p\",[[0,[],0,\"\"],[0,[5],1,\"High Level Task Roll-up\"],[0,[],0,\" \"],[0,[5],1,\"Detailed Task Breakdown\"],[0,[],0,\"  Only requirements are sync’d Requirements and tasks are sync’d  PMO only gets high level progress PMO can do detailed resource planning  Best when mapped to Agile Template Best when mapped to CMMI Template\"]]],[1,\"h2\",[[0,[],0,\"Limitations\"]]],[1,\"p\",[[0,[],0,\"There are 2 limitations that you need to be aware of when doing the integration.\"]]],[1,\"h2\",[[0,[],0,\"Time Tracking\"]]],[1,\"p\",[[0,[],0,\"Since TFS has no notion of when work was completed (it tracks only the total work completed and remaining work), you can’t use this synchronization to perform time tracking from the TFS side. If you don’t care about that, then you haven’t got a problem. If you care, then your developers will have to track time in the PWA timesheet.\"]]],[1,\"p\",[[0,[],0,\"On a related note, though you can assign multiple resources to a task in Project, you can’t on TFS – so make sure your PMO understands this!\"]]],[1,\"h2\",[[0,[],0,\"Hierarchy Sync\"]]],[1,\"p\",[[0,[],0,\"This is more of an irritation than a limitation. Project Server has some sort of limitation that it requires parent tasks to exist and be sync’d before the child tasks can be sync’d.\"]]],[1,\"p\",[[0,[],0,\"Once you’ve connected a Team Project (in TFS) to an Enterprise Project (in Project Server), you’ll need to tell TFS which work items you want to sync to the Project Server. You do this by setting the “Sync to Enterprise” field on your work item to true and selecting the Enterprise Project you want to sync the work item to.\"]]],[1,\"p\",[[0,[],0,\"Here’s the gotcha: if you create a hierarchy in TFS, you need to sync level by level. Start by setting the top level items of the hierarchy to sync – then wait until they are sync’d. Then set the next level of items to sync. Wait for them to sync before next level and so on and so on. Once the items are sync’d you can create child items (and set them to sync) without further problems.\"]]],[1,\"h2\",[[0,[],0,\"Names and Permissions\"]]],[1,\"p\",[[0,[],0,\"It’s best to use Active Directory for the Enterprise Resources. The sync engine matches Enterprise Resources to TFS users using the display name – if you’re not using Active Directory Sync in the PWA, then make sure the display name of the Enterprise Resource matched the Display Name of the AD User exactly.\"]]],[1,\"p\",[[0,[],0,\"If your PMO has customized permissions for Enterprise users in the PWA, then you may run into issues. For example, if your PMO does not grant the “Create Tasks” permission (if they don’t want everyone creating tasks in the Project) then the sync engine won’t be able to sync from TFS to Project Server – the engine uses the “Created By” identity to create tasks on the Enterprise Project.\"]]],[1,\"p\",[[0,[],0,\"In the next post, I’ll talk about setup and configuration of the sync engine.\"]]]]}","published_at":1310069340000,"status":"published","published_by":1},{"id":"18858a91-6592-461e-ae0d-3866c194a008","title":"TFS and Project Server Integration: Tips from the Trenches (Part 2)","slug":"tfs-and-project-server-integration-tips-from-the-trenches-(part-2)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-hCEnV4A9a3k/ThWU89XjpzI/AAAAAAAAARM/m1GGl6RGoso/s1600-h/image10.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-KST5sPIGgu0/ThWU-K7stHI/AAAAAAAAARQ/mQgvu4Xcid0/image_thumb4.png?imgmax=800\\\" width=\\\"383\\\" height=\\\"190\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-if6zP0elJ1c/ThWU-0dqjDI/AAAAAAAAARU/7a856iiaBGI/s1600-h/image18.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-_wSLbHjFIfw/ThWVAKJGH0I/AAAAAAAAARY/F5Xb4pkFpkE/image_thumb8.png?imgmax=800\\\" width=\\\"417\\\" height=\\\"287\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-8EP177a3cDc/ThWVAxu5NcI/AAAAAAAAARc/0uNQBSwX3Ls/s1600-h/image23.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-6dr6IkYLDYI/ThWVB9kyqDI/AAAAAAAAARg/WEEzu2TNlVA/image_thumb11.png?imgmax=800\\\" width=\\\"326\\\" height=\\\"205\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-FR4urAO2qWU/ThWVCTKKWPI/AAAAAAAAARk/q5GiJUdkTaQ/s1600-h/image27.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-HJ9-69v0WDc/ThWVDqakRvI/AAAAAAAAARo/wSS7Uv7cXUc/image_thumb13.png?imgmax=800\\\" width=\\\"477\\\" height=\\\"187\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-RGPNdXyxxHc/ThWVEI5LbcI/AAAAAAAAARs/OZTiF6ULbbo/s1600-h/image31.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-STa_dLxbT00/ThWVE_NgQzI/AAAAAAAAARw/XNa_flU9rXo/image_thumb15.png?imgmax=800\\\" width=\\\"292\\\" height=\\\"313\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_165.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_6118.html\"]],[\"em\"],[\"strong\"]],\"sections\":[[1,\"h2\",[[0,[],0,\"Links to this series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1 – Considerations\"],[0,[],0,\"\"]],[[0,[1],1,\"Part 3 – Configuration\"],[0,[],0,\"\"]],[[0,[2],1,\"Part 4 – Synchronizing Hierarchies from TFS\"]]]],[1,\"h2\",[[0,[],0,\"Part 2: Setup and Configuration\"]]],[1,\"p\",[[0,[],0,\"The best way to do this is to follow the instructions from the “Configuration Quick Reference” page in the Integration help file. In this post I’ll add some summaries and screen shots to augment the manual a bit.\"]]],[1,\"h2\",[[0,[],0,\"Required Accounts\"]]],[3,\"ol\",[[[0,[],0,\"An account that will be used to configure the integration \"],[0,[3],1,\"globally\"],[0,[],0,\" – or at least at the Project Collection<->PWA level. (I used the \"],[0,[4],1,\"tfssetup\"],[0,[],0,\" account that was used to install and configure TFS). (For the rest of this post, this account will be referred to as tfssetup)\"]],[[0,[],0,\"Accounts that will be used to manage mappings \"],[0,[3],1,\"per project\"],[0,[],0,\" – at the TeamProject<->Enterprise Project level (these can be Project Admins or Team Leads on the TFS side). (This can also be the tfssetup account if you want – for this post, I will use the \"],[0,[4],1,\"colind\"],[0,[],0,\" account)\"]],[[0,[],0,\"The TFS Service account (\"],[0,[4],1,\"tfsservice\"],[0,[],0,\" for this post).\"]]]],[1,\"h2\",[[0,[],0,\"Configuring Permissions on TFS\"]]],[3,\"ol\",[[[0,[],0,\"Make sure the global config account (\"],[0,[4],1,\"tfssetup\"],[0,[],0,\") is a in the \"],[0,[4],1,\"Team Foundation Administrator\"],[0,[],0,\" group\"]],[[0,[],0,\"Open the TFS Admin Console and click on the Application Tier node\"]],[[0,[],0,\"Click on the “Group Membership” link in the Application Tier Summary section\"]],[[0,[],0,\"Add the user (tfssetup) to the Team Foundation Administrator group\"]]]],[10,0],[3,\"ol\",[[[0,[],0,\"Make sure each account that will be managing mapping at the project level (\"],[0,[4],1,\"colind\"],[0,[],0,\") has “Administer Project Server integration” permissions.\"]],[[0,[],0,\"Open the TFS Admin Console and click on the “Team Project Collections” node\"]],[[0,[],0,\"Click on the Project Collection you want to configure\"]],[[0,[],0,\"Click on “Administer Security”\"]],[[0,[],0,\"Select the group that the user is in (or add the user explicitly) and make sure the “Administer Project Server integration” permission is allowed\"]]]],[10,1],[3,\"ol\",[[[0,[],0,\"Make sure the resources you are going to use on the project are part of some group on the TFS Team Project that you will be mapping to Project Server (usually just place them into the Contributors group).\"]]]],[1,\"h2\",[[0,[],0,\"Configuring Permissions on Project Server 2010\"]]],[3,\"ol\",[[[0,[],0,\"Configure service account permissions on the Project Server Application in Sharepoint Central Admin\"]],[[0,[],0,\"Open the Sharepoint central admin console and click on the “Manage Service Applications” link\"]]]],[10,2],[3,\"ol\",[[[0,[],0,\"Look for the “Project Server Service Application” and click next to the name (don’t click the hyperlink – you just want to select the row, not follow the link – see the red ‘x’ in the picture below). This highlights the row and activates some buttons in the ribbon at the top.\"]]]],[10,3],[3,\"ol\",[[[0,[],0,\"Click on the “Permissions” button in the ribbon.\"]],[[0,[],0,\"Add the TFSService account and make sure it has full control checked in the permissions section.\"]]]],[10,4],[1,\"h2\",[[0,[],0,\"Configuring Permissions in PWA\"]]],[1,\"p\",[[0,[],0,\"To configure the integration, you’ll need to create 2 user accounts on the PWA – one for the TFSService account and one for the account you’ll use to register the PWA (TFSSetup). You can add them to the Administrator group on PWA or assign the accounts the minimum permissions as described in the Integration help file.\"]]],[1,\"p\",[[0,[],0,\"Each user that is going to have tasks must be created as an enterprise resource. Make sure the display name matches the display name for the user in Active Directory – and it’s recommended that you let Project server sync users from Active Directory.\"]]],[1,\"p\",[[0,[],0,\"The resources will need to be part of the Team Members group in Project Server (or have the same permission set).\"]]],[1,\"p\",[[0,[],0,\"Once you’ve created the Enterprise Project, make sure that you select “Build Team” and assign the enterprise resources to the project.\"]]],[1,\"p\",[[0,[],0,\"So now you’ve got the permissions set and the components installed – next we’ll configure the bits.\"]]]]}","published_at":1310069640000,"status":"published","published_by":1},{"id":"7a86a013-7ebc-4486-a36f-72ef87e858d0","title":"TFS and Project Server Integration: Tips from the Trenches (Part 3)","slug":"tfs-and-project-server-integration-tips-from-the-trenches-(part-3)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tfsadmin ProjectServer /RegisterPWA /pwa:<strong>pwaUrl</strong> /tfs:<strong>tfsUrl</strong></font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">TF244079: An error occurred while retrieving the URL for shared services</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tfsadmin ProjectServer /MapPWAToCollection /pwa:<strong>pwaUrl</strong> /collection:<strong>tpcUrl</strong></font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tfsadmin ProjectServer /UploadFieldMapping /collection:<strong>tpcUrl </strong>/useDefaultFieldMappings</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tfsadmin ProjectServer /MapPlanToTeamProject /collection:<strong>tpcUrl</strong> /enterpriseProject:<strong>epmProjectName</strong> /teamProject:<strong>teamProjectName</strong> /workItemTypes:<strong>typeList</strong> <strong>/nofixedwork</strong></font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_07.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_6118.html\"]],[\"strong\"],[\"a\",[\"href\",\"http://server/pwa\"]],[\"a\",[\"href\",\"http://server:8080/tfs\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/project/archive/2011/05/16/project-2010-sp1.aspx\"]],[\"a\",[\"href\",\"http://server:8080/tfs/DefaultCollection\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"Links to this series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1 – Considerations\"],[0,[],0,\"\"]],[[0,[1],1,\"Part 2 – Setup\"],[0,[],0,\"\"]],[[0,[2],1,\"Part 4 – Synchronizing Hierarchies from TFS\"]]]],[1,\"h2\",[[0,[],0,\"Part 3: Configuring the Bits\"]]],[1,\"p\",[[0,[],0,\"Now that you’ve installed the Integrator, you need to configure it. This happens at various levels:\"]]],[3,\"ol\",[[[0,[],0,\"Register the PWA with TFS\"]],[[0,[],0,\"Associate a Project Collection with a registered PWA\"]],[[0,[],0,\"Specify mappings (or use the default mapping)\"]],[[0,[],0,\"Associate a Team Project with one (or more) Enterprise Projects\"]],[[0,[],0,\"Mark Tasks for synchronization\"]]]],[1,\"p\",[[0,[],0,\"You only need to configure the Project Collection <-> PWA once – but for each Team Project that you want to map to an Enterprise Project, you’ll need to configure a mapping (again, usually just once).\"]]],[1,\"p\",[[0,[],0,\"I logged into a client machine with VS 2010 SP1 installed (that’s where tfsadmin.exe comes from) as TFSSetup (my admin account for the integration).\"]]],[1,\"h2\",[[0,[],0,\"Registering a PWA\"]]],[1,\"p\",[[0,[],0,\"To register a PWA, open a VS command prompt and use the following command:\"]]],[10,0],[1,\"p\",[[0,[],0,\"where\"]]],[3,\"ul\",[[[0,[3],1,\"pwaUrl\"],[0,[],0,\" is the url to your PWA – like \"],[0,[4],1,\"http://server/pwa\"],[0,[],0,\"\"]],[[0,[3],1,\"tfsUrl\"],[0,[],0,\" is the url to your TFS services – like \"],[0,[5],1,\"http://server:8080/tfs\"]]]],[1,\"h2\",[[0,[],0,\"TF244079 when trying RegisterPWA\"]]],[1,\"p\",[[0,[],0,\"If you get the following (helpful!) error:\"]]],[10,1],[1,\"p\",[[0,[],0,\"then you probably haven’t installed the latest cumulative updates for Project Server (\"],[0,[6],1,\"Project 2010 SP1\"],[0,[],0,\" is also available – if you install SP1, make sure you run the Sharepoint Configuration Wizard after installing the SP).\"]]],[1,\"h2\",[[0,[],0,\"Associate a Project Collection to a Registered PWA\"]]],[1,\"p\",[[0,[],0,\"To map a Project Collection to a PWA, open a VS command prompt and use the following command:\"]]],[10,2],[1,\"p\",[[0,[],0,\"where\"]]],[3,\"ul\",[[[0,[3],1,\"pwaUrl\"],[0,[],0,\" is the url to your PWA – like \"],[0,[4],1,\"http://server/pwa\"],[0,[],0,\"\"]],[[0,[3],1,\"tpcUrl\"],[0,[],0,\" is the url to the Project collection – like \"],[0,[7],1,\"http://server:8080/tfs/DefaultCollection\"]]]],[1,\"h2\",[[0,[],0,\"Specifying Mappings\"]]],[1,\"p\",[[0,[],0,\"You can customize the mappings yourself if you want to deviate from the default mapping – consult the help file if you want to do this. For this example, I used the default mapping.\"]]],[1,\"p\",[[0,[],0,\"To specify the default mapping, use the following command:\"]]],[10,3],[1,\"p\",[[0,[],0,\"where \"],[0,[3],1,\"tpcUrl\"],[0,[],0,\" is the url to the Project collection – like \"],[0,[7],1,\"http://server:8080/tfs/DefaultCollection\"]]],[1,\"h2\",[[0,[],0,\"Mapping a Team Project to an Enterprise Project\"]]],[1,\"p\",[[0,[],0,\"Now you can map a Team Project to an Enterprise Project. Use the following command:\"]]],[10,4],[1,\"p\",[[0,[],0,\"where\"]]],[3,\"ul\",[[[0,[3],1,\"tpcUrl\"],[0,[],0,\" is the url to the Project collection – like \"],[0,[7],1,\"http://server:8080/tfs/DefaultCollection\"],[0,[],0,\"\"]],[[0,[3],1,\"epmProjectName\"],[0,[],0,\" is the name of the Project Plan on Project Server (use “” to enclose the name if it contains spaces)\"]],[[0,[3],1,\"teamProjectName\"],[0,[],0,\" is the name of the Team Project in TFS (use “” to enclose the name if it contains spaces)\"]],[[0,[3],1,\"typeList\"],[0,[],0,\" is the list of types you want to sync – if you specify multiple types, use “” to enclose the comma-separated list without spaces after the commas (for example, “Requirement,Task” or “User Story,Task”)\"]],[[0,[3],1,\"nofixedwork\"],[0,[],0,\" specifies no fixed work in Project (consult the Integration help file for more info on this switch).\"]]]],[1,\"p\",[[0,[],0,\"In the next post, we’ll look at how to synchronize work items.\"]]]]}","published_at":1310069700000,"status":"published","published_by":1},{"id":"4b0d3491-425a-4610-b009-91c2bd27cc89","title":"TFS and Project Server Integration: Tips from the Trenches (Part 4)","slug":"tfs-and-project-server-integration-tips-from-the-trenches-(part-4)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-DiUEKWlOL2M/ThWVlTpOusI/AAAAAAAAAR0/0ZQJtMJ9B3s/s1600-h/image9.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-m4sQKrnaa0w/ThWVmb8gZOI/AAAAAAAAAR4/OF99BnFSmEc/image_thumb5.png?imgmax=800\\\" width=\\\"473\\\" height=\\\"106\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">TF285010: The following user is not a valid Project Server resource: Colin Dembovsky. Add the Team Foundation user to the enterprise resource pool.</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Project Server Sync: Successfully submitted the request to Project Server.</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-ZUC52YTDwG8/ThWVm3QksmI/AAAAAAAAAR8/b92ZitpkHNo/s1600-h/image19.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-qh_Umvrkooo/ThWVn-dFRPI/AAAAAAAAASA/Y1ZVK-KUicU/image_thumb11.png?imgmax=800\\\" width=\\\"376\\\" height=\\\"243\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-Hx7oQ9GDWnM/ThWVomY03LI/AAAAAAAAASE/z9Cc66V6A2M/s1600-h/image23.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-KDRvLb4CPK4/ThWVpUBLMII/AAAAAAAAASI/ph7MklEuKxM/image_thumb13.png?imgmax=800\\\" width=\\\"401\\\" height=\\\"153\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-5euIUUkQIK8/ThWVqq6C8ZI/AAAAAAAAASM/F_aIaTF3lck/s1600-h/image27.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-LQW1p3l0-PQ/ThWVrb8F-pI/AAAAAAAAASQ/NfTVW_XhLcM/image_thumb15.png?imgmax=800\\\" width=\\\"551\\\" height=\\\"113\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_07.html\"]],[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/07/tfs-and-project-server-integration-tips_165.html\"]],[\"strong\"]],\"sections\":[[1,\"h2\",[[0,[],0,\"Links to this series:\"]]],[3,\"ul\",[[[0,[0],1,\"Part 1 – Considerations\"],[0,[],0,\"\"]],[[0,[1],1,\"Part 2 – Setup\"],[0,[],0,\"\"]],[[0,[2],1,\"Part 3 – Configuration\"]]]],[1,\"h2\",[[0,[],0,\"Part 4: Synchronizing Hierarchies from TFS to Project\"]]],[1,\"p\",[[0,[],0,\"One limitation of the synchronization is that you need to sync parent items before child items. One way of getting work items into TFS and Project is to use Excel. Open Excel, go to the Team Tab and click “New List”. In the dialogue, select the Team Project that is associated and select “Input List” to get some default columns. In the ribbon at the top, select “Choose Columns” and add “Project Server Submit” and “Project Server Enterprise Project”. Finally, press the “Add Tree Level” button to add a level for each level in your hierarchy.\"]]],[1,\"p\",[[0,[],0,\"Now you can enter your work items.\"]]],[1,\"p\",[[0,[],0,\"When you’re ready to submit to TFS, change the Project Server Submit value on \"],[0,[3],1,\"only\"],[0,[],0,\" the 1st level of the hierarchy to “Yes” and select the Enterprise Project in the Project Server Enterprise Project column (there will be a value here for each Enterprise Project that is associated to this Team Project). In the image below, you can see that I’ve only set the High Level Requirements to sync.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Hit the Publish button in the ribbon to publish to TFS. The Integration engine will kick in and sync the high level items to Project Server. You can check this by waiting a short while (a minute or two) and then checking the history field of the work items that you’ve sync’d.\"]]],[1,\"h2\",[[0,[],0,\"TF285010: Not a valid Project Server Resource\"]]],[1,\"p\",[[0,[],0,\"Often when you’re doing synchronization, you’ll come across this error message:\"]]],[10,1],[1,\"p\",[[0,[],0,\"This is actually a generic error message (which could mean that the user is not an enterprise resource or has not been added to the resource pool for the Enterprise Project) or it could be a permissions issue. Check that the TFSService account (the integration identity) has the correct permissions.\"]]],[1,\"p\",[[0,[3],1,\"You must also make sure that you publish the Enterprise Project\"],[0,[],0,\". Synchronization only works with published projects.\"]]],[1,\"p\",[[0,[],0,\"(Note: To get the sync engine to kick in again, I usually make a change to the title field of the work item).\"]]],[1,\"p\",[[0,[],0,\"Make sure you can see the following message in the history of the high level work items:\"]]],[10,2],[1,\"h2\",[[0,[],0,\"Approvals\"]]],[1,\"p\",[[0,[],0,\"So now the changes have been submitted to the Enterprise Project – if required, the Project Manager (or Timesheet approver) will need to approve the changes and publish the project.\"]]],[10,3],[1,\"p\",[[0,[],0,\"If you log onto your PWA, you’ll see some task approvals waiting (in this case it’s a New Task Request). Approve the requests (you can preview if you want to and you can add comments when approving). Once you’ve approved the tasks, you’ll get \"],[0,[3],1,\"approvals again\"],[0,[],0,\". This time there are “edits” to the tasks – hours worked, work item type and other fields. Again, you can preview if you want to – make sure you approve this 2nd approval!\"]]],[1,\"p\",[[0,[],0,\"Once you have approved the tasks (each one needs 2 approvals) you must \"],[0,[3],1,\"publish the project\"],[0,[],0,\"! Open it in Project Professional and select File->Publish. Once you’ve done that, you’ll be able to see the work items in the Project Plan:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now if you look at the history of Work Item in TFS, you’ll see a couple more edits from the sync engine. Note the approval that comes in (as well as the comment) \"],[0,[3],1,\"are from the 2nd approval\"],[0,[],0,\" on the PWA.\"]]],[1,\"h2\",[[0,[],0,\"Publishing the Next Level\"]]],[1,\"p\",[[0,[],0,\"Now you can go back to your spreadsheet and change the Project Server Sync to yes on the next level of the hierarchy. I also added remaining work to the tasks at the same time. Once again, make sure you see the “Successfully submitted to Project Server” message in the history. Then do the double approval on the PWA and don’t forget to publish the plan!\"]]],[1,\"p\",[[0,[],0,\"You can also go ahead and enter predecessor information on the Project Plan to make sure that your resources are levelled and so on. Once you publish, you’ll see the changes (again the 2nd approval) on the work items in TFS. You’ll also notice the start dates coming in from the Project Plan as well as the padlock on the hierarchy – when you sync a hierarchy to Project Server, the links are locked within TFS. If you want to change the parenting, do it from the Project Plan.\"]]],[10,5]]}","published_at":1310069820000,"status":"published","published_by":1},{"id":"5711ae92-a06d-4001-abe5-bc90f2aa37b0","title":"Tip: Creating Task Groups with Azure Service Endpoint Parameters","slug":"tip-creating-task-groups-with-azure-service-endpoint-parameters","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ed25d298-ee8b-467e-a3e0-4dbddcce6284.png\\\" target=\\\"_blank\\\"><img width=\\\"340\\\" height=\\\"165\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7ed2a0f6-7ce1-484e-9330-be24aae46a5f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/fa1a36fe-da88-4587-92af-3ca1f3374b7e.png\\\" target=\\\"_blank\\\"><img width=\\\"300\\\" height=\\\"154\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c08befdc-0ff9-48f2-ad4e-8cdb17451e2c.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/40bffa53-42ee-4477-b0e1-20c992792be8.png\\\" target=\\\"_blank\\\"><img width=\\\"242\\\" height=\\\"289\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d3ef4821-e41b-4942-805e-65c3bdad5ce0.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/746d677d-6ae8-4d5e-af7f-cb84fffd122e.png\\\" target=\\\"_blank\\\"><img width=\\\"320\\\" height=\\\"171\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/404885a7-146c-46b8-91da-cfb16b3a31cb.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/074d6252-b72e-42a3-a73a-67c772f8b83e.png\\\" target=\\\"_blank\\\"><img width=\\\"330\\\" height=\\\"114\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/afc66c70-15dd-4c4d-af1e-027d612c77ff.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7cc4b761-099d-43b0-a3de-4c14fff9c8bf.png\\\" target=\\\"_blank\\\"><img width=\\\"308\\\" height=\\\"102\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6c4508de-8141-4d2d-8323-d97a5aaccd08.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ee0aee48-8a4c-4f0d-84e6-f038ad3a9659.png\\\" target=\\\"_blank\\\"><img width=\\\"313\\\" height=\\\"158\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/390bba92-c4bb-4645-b56c-a144a243d3cb.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/668ba059-3ca9-45d6-a3e4-a488c9d74234.png\\\" target=\\\"_blank\\\"><img width=\\\"296\\\" height=\\\"173\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0862b9b8-ce28-4533-83d6-8f3257aa9674.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b0cf3e6c-30f7-4256-bfb6-4344e9e4b521.png\\\" target=\\\"_blank\\\"><img width=\\\"306\\\" height=\\\"155\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9836eed6-1101-4be4-a4b5-791a55eb78e5.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b0dce6d9-e3d0-45c9-abf5-7830a902e60d.png\\\" target=\\\"_blank\\\"><img width=\\\"324\\\" height=\\\"73\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/993e7370-8313-42f6-a3c2-87f0b355f9cb.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I've been working on some pretty complicated infrastructure deployment pipelines using my release management tool of choice (of course): VSTS Release Management. In this particular scenario, we're deploying a set of VMs to a region. We then want to deploy exactly the same setup but in a different region. Conceptually, this is like duplicating infrastructure between different datacenters.\"]]],[1,\"p\",[[0,[],0,\"Here's what the DEV environment in a release could look like:\"]]],[10,0],[1,\"p\",[[0,[],0,\"If we're duplicating this to 5 regions, we'd need to clone the environment another 4 times. However, that would mean that any updates to any tasks would need to be duplicated over all 5 regions. It's easy to forget to update or to fat-finger a copy - isn't there a better way to maintain sets of tasks? I'm glad you asked…\"]]],[1,\"h3\",[[0,[],0,\"DRY - Don't Repeat Yourself\"]]],[1,\"p\",[[0,[],0,\"DRY (Don't Repeat Yourself) is a common coding practice - any time you find yourself copying code, you should extract it into a function so that you only have to maintain that logic in a single place. We can do the same thing in a release (or build) using Task Groups. Task Groups are like functions that you can call from releases (or builds) from many places - but maintain in a single place. Just like functions, they have parameters that you can set when you \\\"call\\\" them. Click the selector (checkmark icon to the right of each task) to select all the tasks you want to group, right-click and select \\\"Create task group\\\":\"]]],[10,1],[1,\"p\",[[0,[],0,\"A popup asks for the name of the Task Group and bubbles up all the parameters that are used in the tasks within the group. You can update the defaults and descriptions and click Create (helpful hint: make variables for all the values so that the variable becomes the default rather than a hard-coded value - this will make it easier to re-use the Task Group when you clone environments later):\"]]],[10,2],[1,\"p\",[[0,[],0,\"So far, so good:\"]]],[10,3],[1,\"p\",[[0,[],0,\"However, there's a snag: looking at the parameters section, you'll notice that \"],[0,[0],1,\"we don't have any parameter for the Azure Service Endpoint\"],[0,[],0,\". Let's open the tasks and update the value in the dropdown to $(AzureSubscription):\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now you can see that the parameter is bubble up and surfaced as a parameter on the Task Group - it even has the dropdown with the Service Endpoints. Nice!\"]]],[10,5],[1,\"h3\",[[0,[],0,\"Consuming the Task Group\"]]],[1,\"p\",[[0,[],0,\"Open up the release again. You'll see that you now have a new parameter on the Task Group: the AzureSubscription. We'll select the DEV sub from the dropdown.\"]]],[10,6],[1,\"p\",[[0,[],0,\"Also note how the phase is now a single \\\"task\\\" (which is just a call to the Task Group). Under the hood, when the release is created, Release Management deletes the task group and replaces it with the tasks from the Task Group - so any values that are likely to change or be calculated on the fly should be variables.\"]]],[1,\"p\",[[0,[],0,\"Let's now clone the DEV environment to UAT-WESTUS and to UAT-EASTUS.\"]]],[10,7],[1,\"p\",[[0,[],0,\"If we edit the UAT-WESTUS, we can edit the service endpoint (and any other parameters) that we need to for this environment:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Excellent! Now we can update the Task Group in a single place even if we're using it in dozens of environments. Of course you'd need to update the other parameter values to have environment-specific values (Scopes) in the Variables section.\"]]],[10,9],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Task Groups are a great way to keep your releases (or builds) DRY - even allowing you to parameterize the Azure Service Endpoint so that you can duplicate infrastructure across different subscriptions or regions in Azure.\"]]],[1,\"p\",[[0,[],0,\"Happy deploying!\"]]]]}","published_at":1524708223000,"status":"published","published_by":1},{"id":"35e3b882-cfa5-4de3-a9e1-368c30c8876d","title":"Tips and Tricks for Complex IaaS Deployments Using VSTS Deployment Groups","slug":"tips-and-tricks-for-complex-iaas-deployments-using-vsts-deployment-groups","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d7bd08ff-5b27-47b4-b0f8-4aeb94295793.png\\\" target=\\\"_blank\\\"><img width=\\\"340\\\" height=\\\"102\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/56561a03-700d-476e-9c63-7a6e90c7fa83.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2aa9823e-f332-4cbe-be09-427257de1fb8.png\\\" target=\\\"_blank\\\"><img width=\\\"233\\\" height=\\\"341\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/50b0fd52-f99c-4a97-bbbf-2306512d0f68.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"{\\n    \\\"name\\\": \\\"[parameters('settings').vms[copyIndex()].name]\\\",\\n    \\\"type\\\": \\\"Microsoft.Compute/virtualMachines\\\",\\n    \\\"location\\\": \\\"[resourceGroup().location]\\\",\\n    \\\"apiVersion\\\": \\\"2017-03-30\\\",\\n    \\\"dependsOn\\\": [\\n      ...\\n    ],\\n    \\\"properties\\\": {\\n      \\\"hardwareProfile\\\": {\\n        \\\"vmSize\\\": \\\"[parameters('settings').vms[copyIndex()].size]\\\"\\n      },\\n      \\\"osProfile\\\": {\\n        \\\"computerName\\\": \\\"[parameters('settings').vms[copyIndex()].name]\\\",\\n        \\\"adminUsername\\\": \\\"[parameters('adminUsername')]\\\",\\n        \\\"adminPassword\\\": \\\"[parameters('adminPassword')]\\\"\\n      },\\n      \\\"storageProfile\\\": {\\n        \\\"imageReference\\\": \\\"[parameters('settings').vms[copyIndex()].imageReference]\\\",\\n        \\\"osDisk\\\": {\\n          \\\"createOption\\\": \\\"FromImage\\\"\\n        },\\n        \\\"dataDisks\\\": [\\n            {\\n                \\\"lun\\\": 0,\\n                \\\"name\\\": \\\"[concat(parameters('settings').vms[copyIndex()].name,'-datadisk1')]\\\",\\n                \\\"createOption\\\": \\\"Attach\\\",\\n                \\\"managedDisk\\\": {\\n                    \\\"id\\\": \\\"[resourceId('Microsoft.Compute/disks/', concat(parameters('settings').vms[copyIndex()].name,'-datadisk1'))]\\\"\\n                }\\n            }\\n        ]\\n      },\\n      \\\"networkProfile\\\": {\\n        \\\"networkInterfaces\\\": [\\n          {\\n            \\\"id\\\": \\\"[resourceId('Microsoft.Network/networkInterfaces', concat(parameters('settings').vms[copyIndex()].name, if(equals(parameters('settings').vms[copyIndex()].name, 'JumpBox'), '-nicpub', '-nic')))]\\\"\\n          }\\n        ]\\n      }\\n    },\\n    \\\"resources\\\": [\\n      {\\n        \\\"name\\\": \\\"[concat(parameters('settings').vms[copyIndex()].name, '/TeamServicesAgent')]\\\",\\n        \\\"type\\\": \\\"Microsoft.Compute/virtualMachines/extensions\\\",\\n        \\\"location\\\": \\\"[resourceGroup().location]\\\",\\n        \\\"apiVersion\\\": \\\"2015-06-15\\\",\\n        \\\"dependsOn\\\": [\\n          \\\"[resourceId('Microsoft.Compute/virtualMachines/', concat(parameters('settings').vms[copyIndex()].name))]\\\"\\n        ],\\n        \\\"properties\\\": {\\n          \\\"publisher\\\": \\\"Microsoft.VisualStudio.Services\\\",\\n          \\\"type\\\": \\\"TeamServicesAgent\\\",\\n          \\\"typeHandlerVersion\\\": \\\"1.0\\\",\\n          \\\"autoUpgradeMinorVersion\\\": true,\\n          \\\"settings\\\": {\\n            \\\"VSTSAccountName\\\": \\\"[parameters('vstsAccount')]\\\",\\n            \\\"TeamProject\\\": \\\"[parameters('vstsTeamProject')]\\\",\\n            \\\"DeploymentGroup\\\": \\\"[parameters('vstsDeploymentGroup')]\\\",\\n            \\\"Tags\\\": \\\"[parameters('settings').vms[copyIndex()].tags]\\\"\\n          },\\n          \\\"protectedSettings\\\": {\\n            \\\"PATToken\\\": \\\"[parameters('vstsPat')]\\\"\\n          }\\n        }\\n      },\\n      ...\\n\",\"language\":\"javascript; highlight:[43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66]\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2d415fda-4852-4327-adcd-07bd80234a0e.png\\\" target=\\\"_blank\\\"><img width=\\\"331\\\" height=\\\"99\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5d918fca-64d5-4b20-a027-6afcb74dca7e.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d5c254eb-a976-47a4-a9eb-7b375a8d080c.png\\\" target=\\\"_blank\\\"><img width=\\\"314\\\" height=\\\"187\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0858d323-3c20-4bf1-9211-1d90279e32df.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"if ($PSVersionTable.PSVersion.Major -lt 5) {\\n    $powershell5Url = \\\"https://go.microsoft.com/fwlink/?linkid=839516\\\"\\n    wget -Uri $powershell5Url -OutFile \\\"wmf51.msu\\\"\\n    Start-Process .\\\\wmf51.msu -ArgumentList '/quiet' -Wait\\n}\\n\",\"language\":\"powershell;\"}],[\"code\",{\"code\":\"Import-Module PackageManagement\\nInstall-PackageProvider -Name NuGet -MinimumVersion 2.8.5.201 -Force\\n\",\"language\":\"powershell;\"}],[\"code\",{\"code\":\"Install-Module -Name xActiveDirectory -Force\\nInstall-Module -Name xNetworking -Force\\nInstall-Module -Name xStorage -Force\\nInstall-Module -Name xDSCDomainjoin -Force\\nInstall-Module -Name xComputerManagement -Force\\n\",\"language\":\"powershell;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Install-Module AzureRM -AllowClobber -Force</font>\"}],[\"code\",{\"code\":\"Script SMBConfig\\n{\\n\\tGetScript = { @{ Result = Get-SmbServerConfiguration } }\\n\\tTestScript =\\n\\t{\\n\\t\\t$config = Get-SmbServerConfiguration\\n\\t\\t$needConfig = $config.EnableSMB2Protocol -and (-not ($config.EnableSMB1Protocol))\\n\\t\\tif ($needConfig) {\\n\\t\\t\\t\\tWrite-Host \\\"SMB settings are not correct.\\\" \\n\\t\\t}\\n\\t\\t$needConfig\\n\\t}\\n\\tSetScript =\\n\\t{\\n\\t\\tWrite-Host \\\"Configuring SMB settings\\\" \\n\\t\\tSet-SmbServerConfiguration -EnableSMB1Protocol $false -Force\\n\\t\\tSet-SmbServerConfiguration -EnableSMB2Protocol $true -Force\\n\\t}\\n}\\n\",\"language\":\"powershell;\"}],[\"code\",{\"code\":\"Configuration DiskConfig\\n{\\n\\tparam\\n\\t(\\n\\t\\t[Parameter(Mandatory)]\\n\\t\\t[string]$dataDiskId\\n\\t)\\n\\n\\t# import DSC Resources \\n\\tImport-DscResource -ModuleName PSDscResources\\n\\tImport-DscResource -ModuleName xStorage\\n\\n\\tNode localhost\\n\\t{\\n        LocalConfigurationManager\\n\\t\\t{\\n\\t\\t\\tActionAfterReboot = 'ContinueConfiguration'\\n\\t\\t\\tConfigurationMode = 'ApplyOnly'\\n\\t\\t\\tRebootNodeIfNeeded = $true\\n\\t\\t}\\n\\n        xWaitforDisk DataDisk\\n        {\\n            DiskId = $dataDiskId\\n            RetryIntervalSec = 60\\n            RetryCount = 3\\n        }\\n\\n        xDisk FVolume\\n        {\\n            DiskId = $dataDiskId\\n            DriveLetter = 'F'\\n            FSLabel = 'Data'\\n            DependsOn = \\\"[xWaitforDisk]DataDisk\\\"\\n        }\\n    }\\n}\\n\\n# work out what disk number the data disk is on\\n$dataDisks = Get-Disk -FriendlyName \\\"Microsoft Virtual Disk\\\" -ErrorAction SilentlyContinue\\nif ($dataDisk -eq $null) {\\n\\t$dataDisk = Get-Disk -FriendlyName \\\"Microsoft Storage Space Device\\\" -ErrorAction SilentlyContinue\\n}\\n# filter to GPT partitions\\n$diskNumber = 2\\nGet-Disk | Out-Host -Verbose\\n$dataDisk = Get-Disk | ? { $_.PartitionStyle -eq \\\"RAW\\\" -or $_.PartitionStyle -eq \\\"GPT\\\" }\\nif ($dataDisk -eq $null) {\\n\\tWrite-Host \\\"Cannot find any data disks\\\"\\n} else {\\n\\tif ($dataDisk.GetType().Name -eq \\\"Object[]\\\") {\\n\\t\\tWrite-Host \\\"Multiple data disks\\\"\\n\\t\\t$diskNumber = $dataDisk[0].Number\\n\\t} else {\\n\\t\\tWrite-Host \\\"Found single data disk\\\"\\n\\t\\t$diskNumber = $dataDisk.Number\\n\\t}\\n}\\nWrite-Host \\\"Using $diskNumber for data disk mounting\\\"\\n\\nDiskConfig -ConfigurationData .\\\\ConfigurationData.psd1 -dataDiskId \\\"$($diskNumber)\\\"\\nStart-DscConfiguration -Wait -Force -Path .\\\\DiskConfig -Verbose \\n\",\"language\":\"powershell;\"}],[\"code\",{\"code\":\"WindowsFeature DNS\\n{\\n        Ensure = \\\"Present\\\"\\n        Name = \\\"DNS\\\"\\n        DependsOn = \\\"[xADDomain]ADDomain\\\"\\n}\\n\\nxDnsServerAddress DnsServerAddress\\n{\\n        Address        = '10.10.0.4', '127.0.0.1'\\n        InterfaceAlias = 'Ethernet 2'\\n        AddressFamily  = 'IPv4'\\n        DependsOn = \\\"[WindowsFeature]DNS\\\"\\n}\\n\",\"language\":\"powershell;\"}],[\"code\",{\"code\":\"param($rgName, $vnetName, $dnsAddress)\\n$vnet = Get-AzureRmVirtualNetwork -ResourceGroupName $rgName -Name $vnetName\\nif ($vnet.DhcpOptions.DnsServers[0] -ne $dnsAddress) {\\n    $vnet.DhcpOptions.DnsServers = @($dnsAddress)\\n    Set-AzureRmVirtualNetwork -VirtualNetwork $vnet\\n}\\n\",\"language\":\"powershell;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">Restart-Machine -ComputerName localhost -Force</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c7a4e385-8587-4347-a7a5-77aff6f7bb7b.png\\\" target=\\\"_blank\\\"><img width=\\\"300\\\" height=\\\"135\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/a2dd904d-386d-4f9a-8a47-a200a8aba5e7.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/92f15bf4-6607-4aff-85d7-a1d1a102cec2.png\\\" target=\\\"_blank\\\"><img width=\\\"322\\\" height=\\\"211\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/be0c7bbe-79ed-4cb1-85e6-ed9b61d7ca42.png\\\" border=\\\"0\\\"></a>\"}],[\"code\",{\"code\":\"param (\\n    [string]$ResourceGroupName,\\n    [string[]]$VMNames = @(),\\n    $TimeoutMinutes = 2,\\n    $DelaySeconds = 30\\n)\\n\\nWrite-Host \\\"Delay for $DelaySeconds seconds.\\\"\\nStart-Sleep -Seconds $DelaySeconds\\n\\nif($VMNames.Count -eq 0)\\n{\\n    $VMNames = (Get-AzureRmVm -ResourceGroupName $ResourceGroupName).Name\\n    Write-Host \\\"Getting VM names.\\\"\\n}\\n\\n$seconds = 10\\n$desiredStatus = \\\"PowerState/running\\\"\\n\\nforeach($vmName in $VMNames)\\n{\\n    $timer = [Diagnostics.Stopwatch]::StartNew()\\n    Write-Host \\\"Getting statuses of VMs.\\\"\\n    $statuses = (Get-AzureRmVm -ResourceGroupName $ResourceGroupName -VMName $vmName -Status).Statuses\\n    $status = $statuses | Where-Object { $_.Code -eq $desiredStatus }\\n    while($status -eq $null -and ($timer.Elapsed.TotalMinutes -lt $TimeoutMinutes))\\n    {\\n        Write-Verbose \\\"Retrying in $($seconds) seconds.\\\"\\n        Start-Sleep -Seconds $seconds\\n        $statuses = (Get-AzureRmVm -ResourceGroupName $ResourceGroupName -VMName $vmName -Status).Statuses\\n        $status = $statuses | Where-Object { $_.Code -eq $desiredStatus }\\n    }\\n\\n    if($timer.Elapsed.TotalMinutes -ge $TimeoutMinutes)\\n    {\\n        Write-Error \\\"VM restart exceeded timeout.\\\"\\n    }\\n    else\\n    {\\n        Write-Host \\\"VM name $($vmName) has current status of $($status.DisplayStatus).\\\"\\n    }\\n}\\n\",\"language\":\"powershell;\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-authoring-templates\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/build-release/concepts/definitions/release/what-is-release-management\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/vsts/build-release/concepts/definitions/release/deployment-groups/\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/Microsoft/vsts-tasks/tree/master/Tasks/AzurePowerShell\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/virtual-machines/windows/extensions-customscript\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/resources/templates/201-vm-domain-join/\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/install-and-configure-sql-server-using-powershell-dsc\"]],[\"a\",[\"href\",\"https://github.com/PowerShell/xStorage\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/azure/security/azure-security-disk-encryption\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Recently I was working with a customer that was struggling with test environments. Their environments are complex and take many weeks to provision and configure - so they are generally kept around even though some of them are not frequently used. Besides a laborious, error-prone manual install and configuration process that usually takes over 10 business days, the team has to maintain all the clones of this environment. This means that at least two senior team members are required just to maintain existing dev and test environments as well as create new ones.\"]]],[1,\"p\",[[0,[],0,\"Using \"],[0,[0],1,\"Azure ARM templates\"],[0,[],0,\" and VSTS \"],[0,[1],1,\"Release Management\"],[0,[],0,\" with \"],[0,[2],1,\"Deployment Groups\"],[0,[],0,\", we were able to show how we could spin up the entire environment in just under two hours. That's a 50x improvement in lead time! And it's more consistent since the entire process is automated and the scripts are all source controlled so there's auditability. This is a huge win for the team. Not only can they spin up an environment in a fraction of the time they are used to, they can now decommission environments that are not frequently used (some environments were only used twice a year). That means they have less maintenance to worry about. When they need an environment for a short time, they spin it up, used it and then throw it away. They've also disseminated \\\"tribal knowledge\\\" from a few team members' heads to a button click - meaning anyone can create a new environment now.\"]]],[1,\"p\",[[0,[],0,\"This was my first time working with a larger scale provisioning and configuration project that uses Deployment Groups - and this post documents some of the lessons that we learned along the way.\"]]],[1,\"h3\",[[0,[],0,\"A Brief Glossary\"]]],[1,\"p\",[[0,[],0,\"Before we jump into the tips, I need to get some definitions out of the way. In VSTS, a Release Definition is made up of multiple Environments. Typically you see DEV, STAGE and PROD but you can have multiple \\\"environments\\\" that target the same set of machines.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The above VSTS release has three \\\"environments\\\":\"]]],[3,\"ul\",[[[0,[],0,\"Infrastructure Provision\"]],[[0,[],0,\"Runs an ARM template to provision VMs, VNets, Storage and any other infrastructure required for the environment\"]],[[0,[],0,\"Infrastructure Config\"]],[[0,[],0,\"Configure the OS of each machine, DNS and any other \\\"low-level\\\" settings\"]],[[0,[],0,\"App Install\"]],[[0,[],0,\"Install and configure the application(s)\"]]]],[1,\"p\",[[0,[],0,\"This separation also allows you to run the \\\"Infrastructure Provision\\\" environment and then set it to a manual trigger and just trigger the config environment - particularly useful when you're developing the pipeline, since you can skip environments that end up being no-ops but take a couple minutes to pass through.\"]]],[1,\"p\",[[0,[],0,\"Within an Environment, you can have 1..\"],[0,[3],1,\"n\"],[0,[],0,\" phases. You specify tasks inside a phase - these are the smallest unit of work in the release.\"]]],[10,1],[1,\"p\",[[0,[],0,\"In the above image, there are several phases within the \\\"Infrastructure Config\\\" environment. Each phase (in this case) is running a single task, but you can run as many tasks as you need for that particular phase.\"]]],[1,\"p\",[[0,[],0,\"There are three types of phases: agentless, agent-based or deployment-group based. You can think of agentless phases as phases that are executed on VSTS. Agent-based phases are executed on agent(s) in a build or release queue. Deployment Group phases are executed on all agents (with optional tag matching) within the specified Deployment Group. The agent for agent-based or deployment-group based is the same agent under the hood - the difference is that deployment group agents are only referenced through the Deployment Group while build/release agents are accessed through queues. You'd typically use agent queues for build servers or for \\\"proxy servers\\\" in releases (where the tasks are executing on the proxy but acting on other machines). Deployment Groups are used when you don't know the machines ahead of time - like when you're spinning up a set of machines in the cloud on demand. They also allow you to target multiple machines at the same time.\"]]],[1,\"p\",[[0,[],0,\"The VSTS Deployment agent joins a machine (this can be any machine anywhere that can connect to VSTS) to a Deployment Group. The agent is cross-platform (runs on DotNET Core) so it can run on practically any machine anywhere. It connects out to VSTS meaning you don't need to open incoming firewall ports at all. The agent runs on the machine and so any scripts you write can execute locally - which simplifies configuration dramatically. Executing remote instructions is typically much harder to do - you have to think about your connection and security and so on. Executing locally is much easier.\"]]],[1,\"h2\",[[0,[],0,\"TL;DR - The Top 10 Tips and Tricks\"]]],[1,\"p\",[[0,[],0,\"Here are my top 10 tips and tricks:\"]]],[3,\"ol\",[[[0,[],0,\"Spin Up Azure VMs with the VSTS Deployment Agent Extension\"]],[[0,[],0,\"This allows you to configure everything else locally on each machine\"]],[[0,[],0,\"Use Tagging for Parallelization and Specialization\"]],[[0,[],0,\"Tagging the VSTS agent allows you to repeat the same actions on many machines in parallel and/or distinguish machines for unique actions\"]],[[0,[],0,\"Use Phases to Start New Sessions\"]],[[0,[],0,\"Each phase in an Environment gets a new session, which is useful in a number of scenarios\"]],[[0,[],0,\"Update Your PowerShell PackageProviders and Install DSC Modules\"]],[[0,[],0,\"If you're using DSC, install the modules in a separate step to ensure that they are available when you run DSC scripts. You may need to update your Package Providers for this to work\"]],[[0,[],0,\"Install Azure PowerShell and use the \"],[0,[4],1,\"Azure PowerShell task\"]],[[0,[],0,\"If you're going to be doing any scripting to Azure, you can quickly install Azure PowerShell so that you can use the Azure PowerShell task\"]],[[0,[],0,\"Use PowerShell DSC for OS Configuration\"]],[[0,[],0,\"Configuring Windows Features, firewalls and so on is best done with PowerShell DSC\"]],[[0,[],0,\"Use Plain PowerShell for Application Install and Config\"]],[[0,[],0,\"Expressing application state can be challenging - so use \\\"plain\\\" PowerShell for application install and config\"]],[[0,[],0,\"Attaching Data Disks in Azure VMs\"]],[[0,[],0,\"If you add data disks in your ARM template, you still need to mount them in the OS of the VM\"]],[[0,[],0,\"Configuring DNS on Azure VNets\"]],[[0,[],0,\"If you create an Active Directory Domain Controller or DNS, you'll need to do some other actions on the VNet too\"]],[[0,[],0,\"Wait on machines when they reboot\"]],[[0,[],0,\"If you reboot a machine and don't pause, the subsequent deployment steps fail because the agent goes offline.\"]]]],[1,\"p\",[[0,[],0,\"In the next section I'll dig into each tip.\"]]],[1,\"h3\",[[0,[],0,\"Tip 1: Spin Up Azure VMs with the VSTS Deployment Agent Extension\"]]],[1,\"p\",[[0,[],0,\"You can install the VSTS Deployment Agent (or just \\\"the agent\\\" for the remainder of this post) on any machine using a simple script. The script downloads the agent binary and configures it to connect the agent to your VSTS account and to the specified Deployment Group. However, if you're spinning up machines by using an ARM template, you can also install the agent via the VSTS extension. In order to do this you need a Personal Access Token (or PAT), the name of the VSTS account, the name of the Deployment Group and optionally some tags to tag the agent with. Tags will be important when you're distinguishing between machines in the same Deployment Group later on. You'll need to create the Deployment Group in VSTS before you run this step.\"]]],[1,\"p\",[[0,[],0,\"Here's a snippet of an ARM template that adds the extension to the Deployment Group:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"The extension is defined in the highlighted lines\"]],[[0,[],0,\"The \\\"settings\\\" section of the extension is where you specify the VSTS account name, team project name, deployment group name and comma-separated list of tags for the agent. You also need to supply a PAT that has access to join machines to the Deployment Group\"]],[[0,[],0,\"You can also specify a \\\"Name\\\" property if you want the agent name to be custom. By default it will be machineName-DG (so if the machine name is WebServer, the agent will be named WebServer-DG.\"]]]],[1,\"p\",[[0,[],0,\"Now you have a set of VMs that are bare-boned but have the VSTS agent installed. They are now ready for anything you want to throw at them - and you don't need to worry about ports or firewalls or anything like that.\"]]],[1,\"p\",[[0,[],0,\"There are some useful extensions and patterns for configuring VMs such as \"],[0,[5],1,\"Custom Script\"],[0,[],0,\" or \"],[0,[6],1,\"Join Domain\"],[0,[],0,\". The problem with these scripts is that the link to the script has to be either in a public place or in a blob store somewhere, or they assume existing infrastructure. This can complicate deployment. Either you need to publish your scripts publically or you have to deal with uploading scripts and generating SAS tokens. So I recommend just installing the VSTS agent and let it do everything else that you need to do - especially since the agent will download artifacts (like scripts and build binaries) as a first step in any deployment phase.\"]]],[1,\"h3\",[[0,[],0,\"Tip 2: Use Tagging for Parallelization and Specialization\"]]],[1,\"p\",[[0,[],0,\"Tags are really important for Deployment Groups. They let you identify machines or groups of machines within a Deployment Group. Let's say you have a load balanced application with two webservers and a SQL server. You'd probably want identical configuration for the webservers and a completely different configuration for the SQL server. In this case, tag two machines with WEBSERVER and the other machine with SQL. Then you'll define the tasks in the phase  - when the phase runs, it executes all the tasks on all the machines that match the filter - for example, you can target all WEBSERVER machines with a script to configure IIS. These will execute in parallel (you can configure it to work serially if you want to) and so you'll only specify the tasks a single time in the definition and you'll speed up the deployment.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Be careful though: multiple tags use AND (not OR) logic. This means if you want to do something like join a domain on machines with WEBSERVER and SQL, you would think you could specify WEBSERVER, SQL as the tag filter in the phase tag filter. But since the tags are joined with an AND, you'll see the phase won't match any machines. So you'd have to add a NODE tag (or something similar) and apply it to both webservers and SQL machine and then target NODE for things you want to do on all the machines.\"]]],[10,4],[1,\"p\",[[0,[],0,\"The above image shows the tag filtering on the Phase settings. Note too the parallelization settings.\"]]],[1,\"h3\",[[0,[],0,\"Tip 3: Use Phases to Start New Sessions\"]]],[1,\"p\",[[0,[],0,\"At my customer we were using Windows 2012 R2 machines. However, we wanted to use PowerShell DSC for configuring the VMs and you need Windows Management Framework 5.0 to get DSC. So we executed a PowerShell task to upgrade the PowerShell to 5.x:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: This script checks the major version of the current PowerShell\"]],[[0,[],0,\"Lines 2,3: If it's less than 5, then the script downloads PowerShell 5.1 (the path to the installer can be update to whichever PowerShell version you need)\"]],[[0,[],0,\"Line 4: The installer is invoked with the quiet parameter\"]]]],[1,\"p\",[[0,[],0,\"However, if we then called a task right after the update task, we'd still get the old PowerShell since all tasks within a phase are executed in\"],[0,[3],1,\" the same session\"],[0,[],0,\". We just added another phase with the same Deployment Group settings - the second phase started a new session and we got the upgraded PowerShell.\"]]],[1,\"p\",[[0,[],0,\"This doesn't work for environment variables though. When you set machine environment variables, you have to restart the agent. The VSTS team are working on providing a task to do this, but for now you have to reboot the machine. We'll cover how to do this in Tip 10.\"]]],[1,\"h3\",[[0,[],0,\"Tip 4: Update Your PowerShell PackageProviders and Install DSC Modules\"]]],[1,\"p\",[[0,[],0,\"You really should be using PowerShell DSC to configure Windows. The notation is succinct and fairly easy to read and Windows plays nicely with DSC. However, if you're using custom modules (like xNetworking) you have to ensure that the modules are installed. You can pre-install all the modules so that your scripts can assume the modules are already installed. To install modules you'll need to update your Package Providers. Here's how to do it:\"]]],[10,6],[1,\"p\",[[0,[],0,\"You'll need to start a new Phase in order to pick up the new packages. Then you'll be able to install modules:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Not all machines need all the modules, but this step is so quick I found it easier to just enumerate and install all the modules anyway. That way I know that any machine could run any DSC script I throw at it.\"]]],[1,\"h3\",[[0,[],0,\"Tip 5: Install Azure PowerShell\"]]],[1,\"p\",[[0,[],0,\"If you're going to do anything against Azure from the VMs (in our case we were downloading binaries from a blob store) then you'll want to use the Azure PowerShell task. This task provides an authenticated context (via a preconfigured endpoint) so you don't have to worry about adding passwords or anything to your script. However, for it to work, you'll need to install Azure PowerShell. Again this must be a separate phase so that subsequent phases can make use of the Azure cmdlets. To do this simply add a PowerShell task and run this line of script:\"]]],[10,8],[1,\"h3\",[[0,[],0,\"Tip 6: Use PowerShell DSC for OS Configuration\"]]],[1,\"p\",[[0,[],0,\"OS configuration can easily be specified by describing the state: is IIS installed or not? Which other OS roles are installed? So DSC is the perfect tool for this kind of work. You can use a single DSC script to configure a group of machines (or nodes, in DSC) but since we have the VSTS agent you can simply write your scripts for each machine using \\\"node localhost\\\". DSC script are also (usually) idempotent - so they work no matter what state the environment is in when the script executes. No messy if statements to check various conditions - DSC does it for you.\"]]],[1,\"p\",[[0,[],0,\"When you're doing DSC, you should first check if there is a \\\"native\\\" resource for your action - for example, configuring Windows Features uses the WindowsFeature resource. However, there are some custom actions you may want to perform. There are tons of extensions out there - we used xActiveDirectory to configure an Active Directory Domain Controller settings, for example.\"]]],[1,\"p\",[[0,[],0,\"There are times when you'll want to do some custom work that there simply is no custom module for. In that case, you'll need to use the Script resource. The script resource is composed of three parts: GetScript, TestScript and SetScript. GetScript is optional and should return the current state as an object if specified. TestScript should return a boolean - true for \\\"the state is correct\\\" or false for \\\"the state is not correct\\\". If TestScript returns a false, then the SetScript is invoked. Here's an example Script we wrote to configure SMB on a machine according to Security requirements:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 1: Specify the type of resource (Script) and a unique name\"]],[[0,[],0,\"Line 3: The GetScript returns an hash table with a Result property that describes the current state - in this case, the SMB settings on the machine\"]],[[0,[],0,\"Line 4: The start of the TestScript\"]],[[0,[],0,\"Line 6: Query the SMB settings\"]],[[0,[],0,\"Line 7: Determine if we need to configure anything or not - this is a check on the SMBProtocol states\"]],[[0,[],0,\"Lines 8-10: Write a message if we do need to set state\"]],[[0,[],0,\"Line 11: return the bool: true if the state is correct, false otherwise\"]],[[0,[],0,\"Lines 16-17: correct the state of the machine - in this case, set protocols accordingly\"]]]],[1,\"h3\",[[0,[],0,\"Tip 7: Use Plain PowerShell for Application Install and Config\"]]],[1,\"p\",[[0,[],0,\"Expressing application state and configuration as a DSC script can be challenging. I once wrote some DSC that could \"],[0,[7],1,\"install SQL\"],[0,[],0,\". However, I ended up using a Script resource - and the TestScript just checked to see if a SQL service was running. This check isn't enough to determine if SQL features are installed according to some config.\"]]],[1,\"p\",[[0,[],0,\"Instead of writing long Script resources, I just revert to \\\"plain\\\" PowerShell for app install and configuration. This is especially true for more complicated apps. Just make sure your script are \"],[0,[3],1,\"idempotent\"],[0,[],0,\" - that is that they can run and succeed every time. For example, if you're installing a service, you may want to first check to see if the service exists before running the installer (otherwise the installer may fail since the service already exists). This allows you to re-run scripts again if other scripts fail.\"]]],[1,\"h3\",[[0,[],0,\"Tip 8: Attaching Data Disks in Azure VMs\"]]],[1,\"p\",[[0,[],0,\"If you're creating data disks for your VMs, then you usually specify the size and type of disk in the ARM template. But even if you add a disk, you need to attach it in the OS. To do this, I used the \"],[0,[8],1,\"xStorage DSC extension\"],[0,[],0,\". This requires a disk number. When we started, the data disk was always disk 2. Later, we added \"],[0,[9],1,\"Azure Disk Encryption\"],[0,[],0,\" - but this added another disk and so our disk numbers were off. We ended up needing to add some logic to determine the data disk number and pass that in as a parameter to the DSC configuration:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 3-7: We specify that the script requires a dataDiskId parameter\"]],[[0,[],0,\"Lines 11,12: Import the modules we need\"]],[[0,[],0,\"Lines 23-28: Wait for disk with number $dataDiskId to be available (usually it was immediately anyway)\"]],[[0,[],0,\"Lines 30-36: Mount the disk and assign drive letter F with a label of Data\"]],[[0,[],0,\"Lines 41-43: Get potential data disks\"]],[[0,[],0,\"Lines 46-59: Calculate the data disk number, defaulting to 2\"]],[[0,[],0,\"Lines 62,63: Compile the DSC and the invoke the configuration manager to \\\"make it so\\\"\"]]]],[1,\"h3\",[[0,[],0,\"Tip 9: Configuring DNS on Azure VNets\"]]],[1,\"p\",[[0,[],0,\"In our example, we needed a Domain Controller to be on one of the machines. We were able to configure the domain controller using DSC. However, I couldn't get the other machines to join the domain since they could never find the controller. Eventually I realized the problem was a DNS problem. So we added the DNS role to the domain controller VM. We also added a private static IP address for the domain controller so that we could configure the VNet accordingly. Here's a snippet of the DSC script for this:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Lines 1-6: Configure the DNS feature\"]],[[0,[],0,\"Lines 8-14: Configure the network DNS NIC using the static private IP 10.10.0.4\"]]]],[1,\"p\",[[0,[],0,\"Now we needed to configure the DNS on the Azure VNet to use the domain controller IP address. We used this script:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 2: Get the VNet using the resource group name and VNet name\"]],[[0,[],0,\"Line 3: Check if the DNS setting of the VNet is correct\"]],[[0,[],0,\"Lines 4,5: If it's not, then set it to the internal IP address of the DNS server\"]]]],[1,\"p\",[[0,[],0,\"This script needs to run as an \"],[0,[4],1,\"Azure PowerShell script task\"],[0,[],0,\" so that it's already logged in to an Azure context (the equivalent of running Login-AzureRMAccount -ServicePrincipal). It's sweet that you don't have to provide any credentials in the script!\"]]],[1,\"p\",[[0,[],0,\"Now that we've set the DNS on the VNet, we have to reboot every machine on the VNet (otherwise they won't pick up the change). That brings us to the final tip.\"]]],[1,\"h3\",[[0,[],0,\"Tip 10: Wait on Machines When They Reboot\"]]],[1,\"p\",[[0,[],0,\"You can easily reboot a machine by running this (plain) PowerShell:\"]]],[10,13],[1,\"p\",[[0,[],0,\". This is so simple that you can do it as an inline PowerShell task:\"]]],[10,14],[1,\"p\",[[0,[],0,\"Rebooting the machine is easy: it's waiting for it to start up again that's more challenging. If you have a task right after the reboot task, the deployment fails since the agent goes offline. So you have to build in a wait. The simplest method is to add an agentless phase and add a Delay task:\"]]],[10,15],[1,\"p\",[[0,[],0,\"However, you can be slightly more intelligent if you poll the machine states using some Azure PowerShell:\"]]],[10,16],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"The script requires a resource group name, an optional array of machine names (otherwise it will poll all the VMs in the resource group), a delay (defaulted to 30 seconds) and a timeout (defaulted to 2 minutes)\"]],[[0,[],0,\"The script will delay for a small period (to give the machines time to start rebooting) and then poll them until they're all running or the timeout is reached.\"]]]],[1,\"p\",[[0,[],0,\"This script has to run in an Azure PowerShell task in an \"],[0,[3],1,\"agent\"],[0,[],0,\" phase from either the Hosted agent or a private agent - you can't run it in a Deployment Group phase since those machines are rebooting!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Deployment Groups are very powerful - they allow you to dynamically target multiple machines and execute configuration in a local context. This makes complex environment provisioning and configuration much easier to manage. However, it's always good to know limitations, gotchas and practical tips when designing a complex deployment workflow. Hopefully these tips and tricks make your life a bit easier.\"]]],[1,\"p\",[[0,[],0,\"Happy deploying!\"]]]]}","published_at":1514728550000,"status":"published","published_by":1},{"id":"00d81fe4-a4a9-48c0-897f-637154028e22","title":"Unit Testing Javascript in VS 2012","slug":"unit-testing-javascript-in-vs-2012","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-W90N1TOLu-w/UA0mNfQyJFI/AAAAAAAAAbQ/zSOiUXw6SNA/s1600-h/image%25255B6%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-pts2hE6n4vk/UA0mObZsnHI/AAAAAAAAAbY/HE6NwM75sfE/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"509\\\" height=\\\"99\\\"></a>\"}],[\"code\",{\"code\":\"/// <reference path=\\\"../MvcApplication/Scripts/jQuery-1.6.2.js\\\"><br>/// <reference path=\\\"../MvcApplication/Scripts/StateDropDown.js\\\"><br><br><br>module(\\\"State Dropdown Tests:\\\")<br><br>test(\\\"test States\\\", function () {<br><br>});<br></reference></reference>\",\"language\":\"js; ruler\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-hvsCDS8D2SA/UA0mPvQ-MKI/AAAAAAAAAbg/80TVW4D7GVw/s1600-h/image%25255B10%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-e07IlfMvivc/UA0mSBLIVsI/AAAAAAAAAbo/AaE3F7MLOic/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"450\\\" height=\\\"112\\\"></a>\"}],[\"code\",{\"code\":\"module(\\\"State Dropdown Tests:\\\", {<br>    setup: function () {<br>        $(\\\"#qunit-fixture\\\").append(<br>            '<select id=\\\"countrySelect\\\" name=\\\"country\\\">' +                '<option value=\\\"USA\\\">USA</option>' +            '</select>' +<br>            '<select id=\\\"stateSelect\\\" name=\\\"state\\\">' +            '</select>'<br>        );<br>    }<br>});<br>\",\"language\":\"js; ruler\"}],[\"code\",{\"code\":\"test(\\\"test States\\\", function () {<br>    postCountry = '';<br>    postState = '';<br><br>    var stateSelect = document.getElementById('stateSelect');<br><br>    // the dropdown includes all the states PLUS a 'Select State' option<br>    initCountry('US');<br>    equal(stateSelect.options.length, 62 + 1);<br><br>    initCountry('UK');<br>    equal(stateSelect.options.length, 43 + 1);<br><br>    initCountry('CA');<br>    equal(stateSelect.options.length, 15 + 1);<br><br>    initCountry('ZA');<br>    equal(stateSelect.options.length, 0 + 1);<br>});<br>\",\"language\":\"js; ruler\"}],[\"html\",{\"html\":\"<font style=\\\"font-weight: normal\\\"><a href=\\\"http://blogs.msdn.com/b/visualstudioalm/archive/2012/07/09/javascript-unit-tests-on-team-foundation-service-with-chutzpah.aspx\\\" target=\\\"_blank\\\">Javascript Unit Tests on Team Foundation Service with Chutzpah</a></font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://sdrv.ms/NO71vH\"]],[\"a\",[\"href\",\"http://docs.jquery.com/QUnit\"]],[\"a\",[\"href\",\"http://chutzpah.codeplex.com/\"]],[\"a\",[\"href\",\"http://visualstudiogallery.msdn.microsoft.com/f8741f04-bae4-4900-81c7-7c9bfb9ed1fe\"]],[\"a\",[\"href\",\"http://nuget.org/\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"You’re a responsible developer – you write code, and then you write tests (or, perhaps you even write tests and then write code?). You also love good, solid frameworks that separate concerns and utilize dependency injection and inversion of control and all of that good stuff – you’re using MVC for your web applications.\"]]],[1,\"p\",[[0,[],0,\"But you have a little problem – your pages use Javascript, and that’s difficult to unit test, right? Not any more.\"]]],[1,\"h2\",[[0,[],0,\"The Sample Solution\"]]],[1,\"p\",[[0,[],0,\"For this post I’ll use a sample MVC app – of course, the principles apply to any client side app using Javascript, but this is an easy example to use.\"]]],[1,\"p\",[[0,[],0,\"Create a new MVC project in VS, and then add \"],[0,[0],1,\"this script\"],[0,[],0,\" to the Scripts folder. This script contains some Javascript for populating a country / state dropdown combo. This is the script that we’re going to be testing.\"]]],[1,\"h2\",[[0,[],0,\"Chutzpah and QUnit for MVC via NuGet\"]]],[1,\"p\",[[0,[],0,\"You’re going to need a Javascript testing framework, and then the ability to run your tests. There are a few frameworks out there, but one I enjoy using is \"],[0,[1],1,\"QUnit\"],[0,[],0,\". But that only gives you the infrastructure – you want to be able to run the tests easily too. Fortunately, VS 2012’s Test Explorer is extensible, so you can add your own “test adapters”. And there’s already one by Matthew Manela for Javascript testing with QUnit called Chutzpah (there is a \"],[0,[2],1,\"test runner\"],[0,[],0,\" and a \"],[0,[3],1,\"Test Explorer extension\"],[0,[],0,\" – we’ll just need the extension for now).\"]]],[1,\"p\",[[0,[],0,\"Let’s create a unit test project and use \"],[0,[4],1,\"NuGet\"],[0,[],0,\" to install the QUnit package.\"]]],[3,\"ol\",[[[0,[],0,\"Create a new C# Unit Test project. Delete the UnitTest1.cs file.\"]],[[0,[],0,\"Open the Package Manager Console (NuGet) and make sure the test project is set to the default project.\"]],[[0,[],0,\"Type “Install-Package QUnit-MVC” to install QUnit for MVC (this can be used to test any javascript, not just MVC).\"]],[[0,[],0,\"In the Extension Manager, search for Chutzpah and install the Test Adapter extension. Remember to restart VS once you’ve installed the extension.\"]]]],[10,0],[1,\"h2\",[[0,[],0,\"Writing Tests\"]]],[1,\"p\",[[0,[],0,\"Let’s create a test stub for our unit test. Create a new Javascript file in the test project called “StateDropDownTests.js”. Put in the following code:\"]]],[10,1],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Now open the Test Explorer and click “Run All”. You should see the test come up in the list of tests and it should fail (since nothing was tested).\"]]],[1,\"p\",[[1,[],0,1]]],[10,2],[1,\"p\",[[1,[],0,2]]],[1,\"h2\",[[0,[],0,\"The Real Test Code\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"Since the StateDropdown script we’re testing accesses objects from the DOM, you’re going to need to add some basic elements. Fortunately QUnit has hooks for adding elements into a \"],[0,[5],1,\"test fixture\"],[0,[],0,\". Add the following code to the module to create the DOM element we need for testing:\"]]],[10,3],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"Now add the following code into your test:\"]]],[10,4],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"Run the test – and you should see lots of green! Our test is working.\"]]],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"I was going to write a follow on post about getting the tests to run in TeamBuild, but Mathew Aniyan from the MS ALM team beat me to it – here’s a link to his post about\"]]],[10,5],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Happy Javascript testing!\"]]]]}","published_at":1343071440000,"status":"published","published_by":1},{"id":"7f016481-3746-4b40-8183-5be6c9ac9ce9","title":"Upgrading MSF Agile 5.0 to MSF Agile 6.0: Why does my velocity chart not work?","slug":"upgrading-msf-agile-50-to-msf-agile-60-why-does-my-velocity-chart-not-work","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-k3UA7cyuroc/UDx2RUqE2dI/AAAAAAAAAb0/G-o84orw-2w/s1600-h/image%25255B4%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-tJ7OUGeWXRA/UDx2SWR3sCI/AAAAAAAAAb8/oexeXX0st8E/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"518\\\" height=\\\"278\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">witamdin exportcommonprocessconfig /collection:http://<em>server</em>:8080/tfs/<em>collection</em> /p:<em>Project</em> /f:<em>Project</em>CommonConfig.xml</font>\"}],[\"code\",{\"code\":\"<requirementworkitems category=\\\"Microsoft.RequirementCategory\\\" plural=\\\"Stories\\\">  <states>    <state type=\\\"Proposed\\\" value=\\\"Active\\\">    <state type=\\\"InProgress\\\" value=\\\"Resolved\\\">    <state type=\\\"Complete\\\" value=\\\"Closed\\\">  </state></state></state></states></requirementworkitems>\",\"language\":\"xml; ruler\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"So you’ve just upgraded your TFS 2010 server to TFS 2012. And you’ve been using the MSF Agile 5.0 process template. When you open the Web Access webpage, you get a message saying that some features need to be enabled, and you click the link and it \\\"upgrades” your process template so that the Backlogs and Boards work in Web Access. All looks good.\"]]],[1,\"p\",[[0,[],0,\"Then you’re doing some Product Backlog planning, and you add some User Stories and you notice that your velocity chart doesn’t show any “active” story points. What’s up?\"]]],[1,\"h2\",[[0,[],0,\"The Problem: New States\"]]],[1,\"p\",[[0,[],0,\"The “problem” here is that there are new states for the User Story and Task work items in the MSF Agile 6 template. The velocity chart shows any User Story that is “In Progress” (blue) or “Completed” (green).\"]]],[10,0],[1,\"p\",[[0,[],0,\"In the figure above, Iteration 1 has 4 story points “delivered” and 13 “in progress” while Iteration 2 has 10 story points “in progress”. However, for this chart to work, you need to put your User Story into the “Resolved” state (which makes it go blue) or “Complete” (which makes it go green). And that’s not ideal, since the User Story is still “active” – it’s not yet actually resolved!\"]]],[1,\"p\",[[0,[],0,\"TFS 2012 Process Templates now include two new configuration files: an Agile Process Configuration (for configuring what work items and columns appear on the Backlogs) and a Common Process Configuration for configuring mappings from the boards to work items (among other things like categories). When you click the helpful “enable features” link when you first log into Web Access for the project, TFS creates both the Agile and the Common process config files for you (as well as creating ne work item types like Code Review Request and so on). Let’s export the Common config to see what the “upgrade” process does.\"]]],[1,\"p\",[[0,[],0,\"Open a “Developer Command Prompt” (this gets installed with VS 2012 and has a bunch of TFS and VS programs put into the path) and type the following command:\"]]],[10,1],[1,\"p\",[[0,[],0,\"(where server, collection and project are your sever, your collection and your MSF Agile 5 project)\"]]],[1,\"p\",[[0,[],0,\"If you then open the Common Process Configuration file for your “upgraded” MSF Agile 5.0 Template, you’ll see the following mapping:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You’ll notice that the “Proposed” board state maps to the “Active” work item state, that the board state “InProgress” maps to “Resolved” and “Complete” maps to “Closed”. For the velocity chart, any stories in “Proposed” don’t show, any that are “InProgress” are blue and any that are “Complete” are green. And there lies the problem – the User Story from MSF Agile 5.0 doesn’t have enough states for this to work nicely. It would make more sense to add a “New” state and update the mapping.\"]]],[1,\"h2\",[[0,[],0,\"The Solution: Add States\"]]],[1,\"p\",[[0,[],0,\"What you need to do to “fix” this is to update the User Story and Task work item definitions (essentially adding the “New” state for both work item types) and then update the CommonConfig. Here’s the process for doing this:\"]]],[3,\"ol\",[[[0,[],0,\"Go to Team Explorer in VS and connect to your TFS server. On the Home page, click “Settings” and then “Process Template Manager” and export the MSF Agile 6 template to your hard drive.\"]],[[0,[],0,\"If you did not customize the User Story or Task work item types AT ALL, then skip this step. Otherwise, use witadmin exportwitd to export your User Story and Task work items to file. Then “merge” the User Story and Task definition files (so port over any customizations you did to the type definition in MSF Agile 5 to the MSF Agile 6 type definition). Make sure you end up with the New state (at least) for both work item types.\"]],[[0,[],0,\"Use witadmin importwitd to import the new User Story and Task definition files from step 2.\"]],[[0,[],0,\"If you have no state customizations, then simply use witadmin importcommonprocessconfig to upload the MSF Agile 6 common config to your project. If you have other states, make sure you’ve mapped them correctly (for both RequirementWorkItems and TaskWorkItems sections) in  before you import.\"]]]],[1,\"p\",[[0,[],0,\"You’re done! Now you can add User Stories and Tasks (both will go into the “New” state). Then you’ll be able to “commit” to User Stories by transitioning them to Active (when they’ll appear in Blue on your velocity chart). And then you’re good to go!\"]]],[1,\"p\",[[0,[],0,\"Happy Agile Planning!\"]]]]}","published_at":1346172120000,"status":"published","published_by":1},{"id":"ed9074b5-4033-4a64-899a-7c9569f477ca","title":"Updating XAML Release Builds after Upgrading Release Management Legacy from 2013 to 2015","slug":"upgrading-release-management-legacy-from-2013-to-2015","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://msdn.microsoft.com/library/vs/alm/release/overview\"]],[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/get-started/release/rm-for-vs2015-vs\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/library/vs/alm/release/previous-version/trigger-a-release\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/ALM-Rangers/Migrate-assets-from-RM-server-to-VSTS\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"You need to get onto the new \"],[0,[0],1,\"Release Management\"],[0,[],0,\" (the web-based one) in VSTS or TFS 2015 Update 2. The new version is far superior to the old version for numerous reasons – it uses the new Team Build cross-platform agent, has a much simpler UI for designing releases, has better logging etc. etc.\"]]],[1,\"p\",[[0,[],0,\"However, I know that lots of teams are invested in \"],[0,[1],1,\"Release Management “legacy”\"],[0,[],0,\". Over the weekend I helped a customer upgrade their TFS servers from 2013 to 2015.2.1. Part of this included upgrading their Release Management Server from 2013 to 2015. This customer has been using Release Management since it was still InRelease! They have a large investment in their current release tools, so they need it to continue working so that they can migrate over time.\"]]],[1,\"p\",[[0,[],0,\"The team also \"],[0,[2],1,\"trigger releases\"],[0,[],0,\" in Release Management from their XAML builds. Unfortunately, their builds started breaking once we upgraded the Release Management client on the build servers. The build error was something like: “Invalid directory”. (Before upgrading the client, the release step failed saying that the build service needed to be set up as a Service User – which it was. This error is misleading – it’s an indication that you need to upgrade the RM Client on the build machine).\"]]],[1,\"h3\",[[0,[],0,\"Upgrading XAML Build Definitions\"]]],[1,\"p\",[[0,[],0,\"It turns out that the Release Management XAML templates include a step that reads the registry to obtain the location of the Release Management client binaries. This registry key has changed from RM 2013 to 2015, so you have two options:\"]]],[3,\"ol\",[[[0,[],0,\"If you used the older ReleaseGitTemplate.12.xaml or ReleaseTfvcTemplate12.xaml files from RM 2013, then you can replace them with the updated release management templates that ship with Release Management client (find them in \"],[0,[3],1,\"\\\\Program Files (x86)\\\\ Microsoft Visual Studio 14.0\\\\ReleaseManagement\\\\bin\"],[0,[],0,\")\"]],[[0,[],0,\"If you customized your own templates (or customized the RM 2013 templates), you need to update your release template\"]]]],[1,\"p\",[[0,[],0,\"Fortunately updating existing templates to work with the new RM client is fairly trivial. Here are the steps:\"]]],[3,\"ol\",[[[0,[],0,\"Check out your existing XAML template\"]],[[0,[],0,\"Open it in Notepad (or using the XML editor in VS)\"]],[[0,[],0,\"Find the task with DisplayName “Get the Release Management install directory”. One of the arguments is a registry key – it will be something like \"],[0,[3],1,\"HKEY_LOCAL_MACHINE\\\\Software\\\\Microsoft\\\\ReleaseManagement\\\\12.0\\\\Client\\\\.\"],[0,[],0,\" Replace this key with this value: \"],[0,[3],1,\"HKEY_LOCAL_MACHINE\\\\Software\\\\WOW6432Node\\\\Microsoft\\\\ReleaseManagement\\\\14.0\\\\Client\\\\\"],[0,[],0,\"\"]],[[0,[],0,\"The task just below is for finding the x64 directory – you can do the same replacement in this task.\"]],[[0,[],0,\"Commit your changes and checkin\"]],[[0,[],0,\"Build and release\"]],[[0,[],0,\"Party\"]]]],[1,\"p\",[[0,[],0,\"Thanks to Jesse Arens for this great find!\"]]],[1,\"p\",[[0,[],0,\"On a side note – the ALM Rangers have a project that will help you port your “legacy” RM workflows to the new web-based releases. You can find it \"],[0,[4],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Happy releasing! (Just move off XAML builds and Release Management legacy as soon as possible – for your own sanity!)\"]]]]}","published_at":1463519598000,"status":"published","published_by":1},{"id":"50ac53e5-8ca7-4c3d-a28a-a3789a1f3a15","title":"Using Chrome to Solve Identity Hell","slug":"using-chrome-to-solve-identity-hell","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c5684a4c-045c-4878-a9cf-d4ca02e208e3.png\\\" target=\\\"_blank\\\"><img width=\\\"315\\\" height=\\\"240\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/41ad0268-687f-4d26-ab58-115081d1a475.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b046296c-4e92-4ee2-af41-9c5438d179e1.png\\\" target=\\\"_blank\\\"><img width=\\\"308\\\" height=\\\"273\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9ff3f10d-52c4-436e-8fe6-192622845d0c.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bfec3555-7347-480e-86b9-4e19d31a77dc.png\\\" target=\\\"_blank\\\"><img width=\\\"321\\\" height=\\\"254\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/315ab52b-f5f4-458c-bcc7-2f2ea07769e2.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7d1b015a-7023-4d2e-aeae-13987165781f.png\\\" target=\\\"_blank\\\"><img width=\\\"344\\\" height=\\\"51\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/735cf97a-8765-4dd3-8668-8c71c286a4dc.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">--profile-directory=\\\"Profile 1\\\"</font>\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"This week at MVP summit, I showed some of my colleagues a trick that I use to manage identity hell. I have several accounts that I use to access VSTS and the Azure Portal: my own Microsoft Account (MSA), several org accounts and customer org accounts. Sometimes I want to open a release from my 10th Magnitude VSTS account so that I can grab some tasks to put into CustomerX VSTS release. The problem is that if I open the 10M account in a browser, and then open a new browser, I have to sign out of the 10M account and sign in with the CustomerX account and then the windows break… identity hell.\"]]],[1,\"p\",[[0,[],0,\"At first I used to open InPrivate or Incognito windows. That gave me the ability to get to 4 different profiles: IE and IE InPrivate, Chrome and Chrome Incognito. But then my incognito windows don't have cached identities or history or anything that I like to have in my browser. Hacky - very hacky.\"]]],[1,\"h2\",[[0,[],0,\"Solution: Chrome People\"]]],[1,\"p\",[[0,[],0,\"About 2 years ago I stumbled onto Chrome People (or Profiles). This really simple \\\"trick\\\" has been fantastic and I almost never open Incognito anymore. In the upper right of the Chrome chrome (ahem) there is a little text that tells you what your current \\\"person\\\" is:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Click that text to open the People hub:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here you can see that I have 5 People: ColinMSA, 10M, AdminNWC and NWC and another customer profile. To switch profiles, I just click on the name. To add a person, just click \\\"Manage people\\\".\"]]],[10,2],[1,\"p\",[[0,[],0,\"I can easily add a new person from this view - and I can assign an icon to the person.\"]]],[1,\"p\",[[0,[],0,\"When you create a new person, Chrome creates a shortcut to that person's browser on the desktop. I end up clicking on that and adding it to my taskbar:\"]]],[10,3],[1,\"p\",[[0,[],0,\"If I want to open up the Azure Portal or VSTS using my MSA, I click the ColinMSA icon and I'm there. If I need to open my customer VSTS or Portal, I just click that icon. Each window is isolated and my identities don't leak. Very neat, very clean. Under the hood, the shortcuts just add a small arg to the Chrome.exe launcher:\"]]],[10,4],[1,\"p\",[[0,[],0,\". The first profile is Default, the second is Profile 1, the third Profile 2 and so on.\"]]],[1,\"h2\",[[0,[],0,\"Final Thoughts\"]]],[1,\"p\",[[0,[],0,\"You can also do something similar in FireFox, but I like Chrome. This simple trick helps me sort out my identity hell and I can quickly switch to different identity contexts without having to sign in and out all the time. For my MSA I sign into my Google account, but I don't do that for the other browsers. All in all it's a great way to manage multiple identities.\"]]],[1,\"p\",[[0,[],0,\"Happy browsing!\"]]]]}","published_at":1520472839000,"status":"published","published_by":1},{"id":"a4d938a5-7adc-4269-8eec-55d5c2a222e5","title":"Using Data Generation Plans to create repeatable data for code unit tests","slug":"using-data-generation-plans-to-create-repeatable-data-for-code-unit-tests","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"<span style=\\\"color: blue\\\">&lt;</span><span style=\\\"color: #a31515\\\">Project </span><span style=\\\"color: red\\\">DefaultTargets</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">DataGen</span>\\\" <span style=\\\"color: red\\\">xmlns</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\"><a href=\\\"http://schemas.microsoft.com/developer/msbuild/2003\\\">\\\"&gt;http://schemas.microsoft.com/developer/msbuild/2003</a></span><a href=\\\"http://schemas.microsoft.com/developer/msbuild/2003\\\">\\\"<span style=\\\"color: blue\\\">&gt;<br></span></a><br>&lt;<span style=\\\"color: #a31515\\\">Import </span><span style=\\\"color: red\\\">Project</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">$(MSBuildExtensionsPath)\\\\Microsoft\\\\VisualStudio\\\\v10.0\\\\TeamData\\\\Microsoft.Data.Schema.Common.targets</span>\\\" <span style=\\\"color: blue\\\">/&gt;  <br><br>&lt;</span><span style=\\\"color: #a31515\\\">Target </span><span style=\\\"color: red\\\">Name</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">DataGen</span>\\\"<span style=\\\"color: blue\\\">&gt;<br><br>&lt;</span><span style=\\\"color: #a31515\\\">DataGeneratorTask<br><br></span><span style=\\\"color: red\\\">ConnectionString</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">Data Source=dataserver;Initial Catalog=AutomatedTesting;Integrated Security=True;Pooling=False</span>\\\"<br><br><span style=\\\"color: red\\\">SourceFile</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">.\\\\Data Generation Plans\\\\UnitTestPlan.dgen</span>\\\"<br><br><span style=\\\"color: red\\\">PurgeTablesBeforePopulate</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">True</span>\\\"<span style=\\\"color: blue\\\">/&gt;<br><br><!--</span--><span style=\\\"color: #a31515\\\">Target</span><span style=\\\"color: blue\\\">&gt;<br><br><!--</span--><span style=\\\"color: #a31515\\\">Project</span><span style=\\\"color: blue\\\">&gt;</span></span></span>\",\"language\":\"\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TVbCk090_4I/AAAAAAAAAOY/og8wjpkQzuc/s1600-h/screen%5B4%5D.jpg\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"screen\\\" border=\\\"0\\\" alt=\\\"screen\\\" src=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TVbCmk_pTTI/AAAAAAAAAOc/UXQvfo8knHg/screen_thumb%5B6%5D.jpg?imgmax=800\\\" width=\\\"502\\\" height=\\\"351\\\"></a>\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"The Database Professional tools in VS 2010 allow you to create a project that encapsulates the schema of a SQL database. You can also use this project to create unit tests for stored procs or functions – and create test data for these database unit tests.\"]]],[1,\"p\",[[0,[],0,\"However, what if just want repeatable data for normal code unit tests as part of a TeamBuild? I was recently at a client where this scenario came up. I tried to get the data generation plan to run in the [AssemblyInitialize] method of the unit test project, but just got the error “Could not deploy database” or “Could not run data generation plan” – not very helpful.\"]]],[1,\"p\",[[0,[],0,\"So we figured out a workaround – essentially we use MSBuild to run the data generation plan in the solution just before unit testing.\"]]],[1,\"p\",[[0,[],0,\"Here are the high-level steps you need to follow:\"]]],[3,\"ul\",[[[0,[],0,\"Create a DBPro project with a data generation plan\"]],[[0,[],0,\"Create an MSBuild project file that can run the data generation plan\"]],[[0,[],0,\"Customize your build template\"]],[[0,[],0,\"Add 2 arguments – RunDataGeneration and PathToMSBuildProjFile\"]],[[0,[],0,\"Add a couple of build activities to invoke MSBuild to run the data generation plan\"]]]],[1,\"h2\",[[0,[],0,\"The MSBuild Project File\"]]],[1,\"p\",[[0,[],0,\"Once you’ve created the database project (I won’t cover it in this post, but there are plenty of blogs and articles on the web about how to do this), create a data generation plan for your test data. Now create a new xml file in the root of your database project – the one I created is called datagen.proj. Here’s the file contents:\"],[1,[],0,0]]],[10,0],[1,\"p\",[[0,[],0,\"Of course, you’ll need to change the connection string appropriately and set the “PurgeTablesBeforePopulate” to false if you don’t want to blow away the existing data – though since the data generation is creating test data, you should blow away existing data anyway. Set the sourcefile to the dgen file – the path is relative to the root of the database project.\"],[1,[],0,1],[1,[],0,2],[0,[],0,\"You can test the project file by opening a VS command console, navigating to the folder containing the proj file and typing “msbuild datagen.proj”. Make sure this step succeeds before you continue.\"],[1,[],0,3],[1,[],0,4],[1,[],0,5]]],[1,\"h2\",[[0,[],0,\"Customizing the Build Workflow\"]]],[1,\"p\",[[0,[],0,\"I added two arguments to the workflow – a Boolean called “RunDataGeneration” and a string called DataGenMSBuildProjSourcePath. I also added metadata to “prettify” the arguments when builds are created.\"],[1,[],0,6],[1,[],0,7],[0,[],0,\"Now it’s time to customize the build workflow to create the test data before unit testing. I used the DefaultTemplate.xaml and navigated into the “Try, Compile and Test” sequence – then went deeper and found the “If Not TestSpecs = Emtpy” activity. A little further, just before the tasks that actually run the tests, I inserted an “If” activity (setting the condition to “RunDataGeneration = True”). The “else” branch is empty and I show the “Then” branch below:\"],[1,[],0,8],[1,[],0,9]]],[10,1],[1,\"p\",[[1,[],0,10],[1,[],0,11],[0,[],0,\"There are just two activities – a ConvertWorkSpace and an MSBuild activity. The ConvertWorkspace task is set to ServerToLocal and converts the DataGenMSBuildProjSourcePath to a local path (set the workspace parameter to “Workspace”). You’ll need to create a local variable scoped to the sequence called “localProjectPath” for the out parameter.\"],[1,[],0,12],[1,[],0,13],[0,[],0,\"Next, set the MSBuild task’s project to localProjectPath. The only other thing that needs mentioning here is that you MUST set the ToolPlatform of the MSBuild activity to X86 – the data generation seems to only work if called from the x86 msbuild and not from the x64 one.\"],[1,[],0,14],[1,[],0,15],[0,[],0,\"Check in your template and create the build – make sure you set “RunDataGeneration” to true and set the DataGenMSBuildProjSourcePath to the full source control path to the proj file (starting with $/…).\"],[1,[],0,16],[1,[],0,17],[0,[],0,\"Happy testing!\"],[1,[],0,18]]]]}","published_at":1297567500000,"status":"published","published_by":1},{"id":"e17b2be6-4b7a-4f20-9da2-097666e04e8a","title":"Using Indexed Published Symbols from TeamBuilds","slug":"using-indexed-published-symbols-from-teambuilds","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TVDO38WDEsI/AAAAAAAAANg/USzGOo6YjW0/s1600-h/image8.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"337\\\" src=\\\"http://lh5.ggpht.com/_d41Ixos7YsM/TVDO68P984I/AAAAAAAAANk/bWlYznlrpvE/image_thumb4.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom-width: 0px; border-left-width: 0px; border-right-width: 0px; border-top-width: 0px; display: block; float: none; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"466\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TVDO8F4OKCI/AAAAAAAAANo/Yt-KZRQ6s30/s1600-h/image%5B6%5D.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"304\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TVDO-DrempI/AAAAAAAAANs/l0rSBypW9Uo/image_thumb%5B6%5D.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; border-right: 0px; border-top: 0px; display: block; float: none; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"544\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/_d41Ixos7YsM/TVDPA_RWrMI/AAAAAAAAANw/zCxbvkF-Geg/s1600-h/image7.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"331\\\" src=\\\"http://lh3.ggpht.com/_d41Ixos7YsM/TVDPDm1wNGI/AAAAAAAAAN0/F7Y96aDQKTg/image_thumb3.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom-width: 0px; border-left-width: 0px; border-right-width: 0px; border-top-width: 0px; display: block; float: none; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"468\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.amazon.com/gp/product/0735645248/ref=s9_simh_gw_p14_d5_i4?pf_rd_m=ATVPDKIKX0DER&pf_rd_s=center-2&pf_rd_r=19SEA1DDD7X2GG6BYFQM&pf_rd_t=101&pf_rd_p=470938631&pf_rd_i=507846\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"You’ve seen the indexing and symbol publishing options when you set up a Team Build – but have you ever tried to debug and application that has symbols? How do you get VS to use the published symbols? What are these settings for anyway?\"],[1,[],0,0],[0,[],0,\"Source indexing and published symbols make possible to debug an application without shipping symbols. Source indexing produces pdb files during a build – essentially mapping the binary to the source code. During a team build, extra information is wrapped inside the pdbs – like the Team Foundation Server URL where they were built and version control paths to and versions of source code. This allows VS to download source code for debugging – and since there’s security involved (to access the TFS server) – source server support is disabled for debugging by default.\"],[1,[],0,1],[0,[],0,\"Fortunately enabling it is quite easy – especially if you read your Kindle copy of Inside the \"],[0,[0],1,\"MS Build Engine (2nd edition)\"],[0,[],0,\" on the plane!\"],[1,[],0,2],[0,[],0,\"Open VS and go to Tools->Options->Debugging->General settings and tick the “Enable source server support”.\"],[1,[],0,3]]],[10,0],[1,\"p\",[[1,[],0,4],[0,[],0,\"The DefaultTemplate.xaml for default builds supports symbol publishing. When you set up a Team Build, expand the “Basic Section”, set “Index Sources” to True and enter a UNC for the “Path to Publish Symbols” – try to stick to one global symbol store – TeamBuild will organise the symbols within this folder. The only time you’d really want multiple stores is for concurrency – only one build can publish at a time (that’s the way the Default Template is designed), so if you have lots of build going all the time and the indexing is slowing them, then you may want to add another store or two.\"],[1,[],0,5]]],[10,1],[1,\"p\",[[1,[],0,6],[0,[],0,\"The final step is to tell VS where to find the published symbols. To add a symbol store in VS, go to Tools->Options->Debugging->Symbols and add the UNC to the store.\"],[1,[],0,7]]],[10,2],[1,\"p\",[[1,[],0,8],[0,[],0,\"Happy debugging!\"]]]]}","published_at":1297177500000,"status":"published","published_by":1},{"id":"40c6fce7-fe35-409d-a699-a70693953628","title":"Using Linked ARM Templates with VSTS Release Management","slug":"using-linked-arm-templates-with-vsts-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"{\\n  \\\"$schema\\\": \\\"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\\\",\\n  \\\"contentVersion\\\": \\\"1.0.0.0\\\",\\n  \\\"parameters\\\": {\\n    \\\"vnetName\\\": {\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"vnetPrefix\\\": {\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"subnets\\\": {\\n      \\\"type\\\": \\\"object\\\"\\n    }\\n  },\\n  \\\"variables\\\": {\\n  },\\n  \\\"resources\\\": [\\n    {\\n      \\\"name\\\": \\\"[parameters('vnetName')]\\\",\\n      \\\"type\\\": \\\"Microsoft.Network/virtualNetworks\\\",\\n      \\\"location\\\": \\\"[resourceGroup().location]\\\",\\n      \\\"apiVersion\\\": \\\"2016-03-30\\\",\\n      \\\"dependsOn\\\": [],\\n      \\\"tags\\\": {\\n        \\\"displayName\\\": \\\"vnet\\\"\\n      },\\n      \\\"properties\\\": {\\n        \\\"addressSpace\\\": {\\n          \\\"addressPrefixes\\\": [\\n            \\\"[parameters('vnetPrefix')]\\\"\\n          ]\\n        }\\n      }\\n    },\\n    {\\n      \\\"apiVersion\\\": \\\"2015-06-15\\\",\\n      \\\"type\\\": \\\"Microsoft.Network/virtualNetworks/subnets\\\",\\n      \\\"tags\\\": {\\n        \\\"displayName\\\": \\\"Subnets\\\"\\n      },\\n      \\\"copy\\\": {\\n        \\\"name\\\": \\\"iterator\\\",\\n        \\\"count\\\": \\\"[length(parameters('subnets').settings)]\\\"\\n      },\\n      \\\"name\\\": \\\"[concat(parameters('vnetName'), '/', parameters('subnets').settings[copyIndex()].name)]\\\",\\n      \\\"location\\\": \\\"[resourceGroup().location]\\\",\\n      \\\"dependsOn\\\": [\\n        \\\"[parameters('vnetName')]\\\"\\n      ],\\n      \\\"properties\\\": {\\n        \\\"addressPrefix\\\": \\\"[parameters('subnets').settings[copyIndex()].prefix]\\\"\\n      }\\n    }\\n  ],\\n  \\\"outputs\\\": {\\n  }\\n}\\n\",\"language\":\"javascript\"}],[\"code\",{\"code\":\"{\\n  \\\"$schema\\\": \\\"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\\\",\\n  \\\"contentVersion\\\": \\\"1.0.0.0\\\",\\n  \\\"parameters\\\": {\\n    \\\"containerUri\\\": {\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"containerSasToken\\\": {\\n      \\\"type\\\": \\\"string\\\"\\n    }\\n  },\\n  \\\"variables\\\": {},\\n  \\\"resources\\\": [\\n    {\\n      \\\"apiVersion\\\": \\\"2017-05-10\\\",\\n      \\\"name\\\": \\\"linkedTemplate\\\",\\n      \\\"type\\\": \\\"Microsoft.Resources/deployments\\\",\\n      \\\"properties\\\": {\\n        \\\"mode\\\": \\\"incremental\\\",\\n        \\\"templateLink\\\": {\\n          \\\"uri\\\": \\\"[concat(parameters('containerUri'), '/Resources/vNet.json', parameters('containerSasToken'))]\\\",\\n          \\\"contentVersion\\\": \\\"1.0.0.0\\\"\\n        },\\n        \\\"parameters\\\": {\\n          \\\"vnetName\\\": { \\\"value\\\": \\\"testVNet\\\" },\\n          \\\"vnetPrefix\\\": { \\\"value\\\": \\\"10.0.0.0/16\\\" },\\n          \\\"subnets\\\": {\\n            \\\"value\\\": {\\n              \\\"settings\\\": [\\n                {\\n                  \\\"name\\\": \\\"subnet1\\\",\\n                  \\\"prefix\\\": \\\"10.0.0.0/24\\\"\\n                },\\n                {\\n                  \\\"name\\\": \\\"subnet2\\\",\\n                  \\\"prefix\\\": \\\"10.0.1.0/24\\\"\\n                }\\n              ]\\n            }\\n          }\\n        }\\n      }\\n    }\\n  ],\\n  \\\"outputs\\\": {}\\n}\",\"language\":\"javascript\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d859a2fc-824c-4900-8029-6e6460e7874d.png\\\" target=\\\"_blank\\\"><img width=\\\"296\\\" height=\\\"211\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/70c8a31a-fe17-41ed-8319-a1db70eff95a.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/ef6934b9-1161-4256-9e49-e7ecbb5f86b0.png\\\" target=\\\"_blank\\\"><img width=\\\"273\\\" height=\\\"168\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8cab0747-bb3c-4265-a0ea-58193364e490.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/061159e7-1035-4924-b653-82cd7d7280cf.png\\\" target=\\\"_blank\\\"><img width=\\\"276\\\" height=\\\"172\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/136c37f0-a276-4851-8ad5-bfad48ed208c.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/Azure/azure-quickstart-templates\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"If you've ever had to create a complex ARM template, you'll know it can be a royal pain. You've probably been tempted to split out your giant template into smaller templates that you can link to, only to discover that you can only link to a sub-template if the sub-template is accessible via some public URI. Almost all of the examples in the \"],[0,[0],1,\"Template Quickstart repo\"],[0,[],0,\" that have links simply refer to the public Github URI of the linked template. But what if you want to refer to a private repo of templates?\"]]],[1,\"h2\",[[0,[],0,\"Using Blob Containers\"]]],[1,\"p\",[[0,[],0,\"The solution is to use blob containers. You upload the templates to a private container in an Azure Storage Account and then create a SAS token for the container. Then you create the full file URI using the container URI and the SAS token. Sounds simple, right? Fortunately with VSTS Release Management, it actually is easy.\"]]],[1,\"p\",[[0,[],0,\"As an example, let's look at this template that is used to create a VNet and some subnets. First we'll look at the VNet template (the linked template) and then how to refer to it from a parent template.\"]]],[1,\"h3\",[[0,[],0,\"The Child Template\"]]],[10,0],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"There are 3 parameters: the VNet name and prefix (strings) and then an object that contains the subnet settings\"]],[[0,[],0,\"The first resource is the VNet itself - nothing complicated there\"]],[[0,[],0,\"The second resource uses copy to create 0 or more instances. In this case, we're looping over the subnets.settings array and creating a subnet for each element in that array, using copyIndex() as the index as we loop\"]]]],[1,\"p\",[[0,[],0,\"There's really nothing special here - using a copy is slightly more advanced, and the subnets parameter is a complex object. Otherwise, this is plain ol' ARM json.\"]]],[1,\"h3\",[[0,[],0,\"The Parent Template\"]]],[1,\"p\",[[0,[],0,\"The parent template has two things that are different from \\\"normal\\\" templates: it needs two parameters (containerUri and containerSasToken) that let it refer to the linked (child) template and it invokes the template by specifying a \\\"Microsoft.Resources/deployments\\\" resource type. Let's look at an example:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"There are two parameters that pertain to the linked template: the containerUri and the SAS token\"]],[[0,[],0,\"In the resources, there is a \\\"Microsoft.Resources/deployment\\\" resource - this is how we invoke the child template\"]],[[0,[],0,\"In the templateLink, the URI is constructed by concatenating the containerUri, the path to the child template within the container, and the SAS token\"]],[[0,[],0,\"Parameters are passed inline - note that even simple parameters look like JSON objects (see vNetName and vnetPrefix)\"]]]],[1,\"p\",[[0,[],0,\"Initially I tried to make the subnets object an array: but this blew up on the serialization. So I made an object called \\\"settings\\\" that is an array. So the subnets value property is an object called \\\"settings\\\" that is an array. You can look back at the child template to see how I dereference the object to get the values: to get the name of a subnet, I use \\\"parameters('subnet').settings[index].name\\\" (where index is 0 or 1 or whatever). The copy uses the length() method to get the number of elements in the array and then I can use copyIndex() to get the current index within the copy.\"]]],[1,\"p\",[[0,[],0,\"Of course the parent template can contain other resources - I just kept this example really simple to allow us to zoom in on the linking bits.\"]]],[1,\"h3\",[[0,[],0,\"Source Structure\"]]],[1,\"p\",[[0,[],0,\"Here's a look at how I laid out the files in the Azure Resource Group project:\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can see how the vNet.json (the child template) is inside a folder called \\\"Resources\\\". I use that as the relative path when constructing the URI to the child template.\"]]],[1,\"h2\",[[0,[],0,\"The Release Definition\"]]],[1,\"p\",[[0,[],0,\"Now all the hard work is done! To get this into a release, we just create a storage account in Azure (that we can copy the templates to) and we're good to go.\"]]],[1,\"p\",[[0,[],0,\"Now create a new release definition. Add the repo containing the templates as a release artifact. Then in your environment, drop two tasks: Azure File Copy and Azure Resource Group Deployment. We configure the Azure File Copy task to copy all our files to the storage account into a container called templates. We also need to give the task two variable names: one for the containerUri and one for the SAS token:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Once this task has executed, the templates will be available in the (private) container with the same folder structure as we have in Visual Studio.\"]]],[1,\"p\",[[0,[],0,\"On the next task, we can select the parent template as the template to invoke. We can pass in any parameters that are needed - at the very least, we need the containerUri and SAS token, so we pass in the variables from the previous task using $() notation:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now we can run the release and voila - we'll have a vNet with two subnets.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Refactoring templates into linked templates is good practice - it's DRY (don't repeat yourself) and can make maintenance of complicated templates a lot easier. Using VSTS Release Management and a storage container, we can quickly, easily and securely make linked templates available and it all just works ™.\"]]],[1,\"p\",[[0,[],0,\"Happy deploying!\"]]]]}","published_at":1516681773000,"status":"published","published_by":1},{"id":"477d29d3-4106-41e3-8022-456768ddd6b0","title":"Using PowerShell DSC in Release Management: The Hidden Manual","slug":"using-powershell-dsc-in-release-management-the-hidden-manual","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"Configuration CopyDSCResource {\\n    param (\\n        [Parameter(Mandatory=$false)]\\n        [ValidateNotNullOrEmpty()]\\n        [String]\\n        $ModulePath = \\\"$env:ProgramFiles\\\\WindowsPowershell\\\\Modules\\\"\\n    )\\n\\n    Node $AllNodes.NodeName\\n    {\\n        #\\n        # Copy the custom DSC Resource to the target server\\n        #\\n        File DeployWebDeployResource\\n        {\\n            Ensure = \\\"Present\\\"\\n            SourcePath = \\\"$($Node.SourcePath)\\\\$($Node.ModuleName)\\\"\\n            DestinationPath = \\\"$ModulePath\\\\$($Node.ModuleName)\\\"\\n            Recurse = $true\\n            Force = $true\\n            Type = \\\"Directory\\\"\\n        }\\n    }\\n}\\n\\nCopyDSCResource -ConfigurationData $configData -Verbose\\n\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"@{\\n    AllNodes = @(\\n        @{\\n            NodeName = \\\"*\\\"\\n            SourcePath = \\\"\\\\\\\\rmserver\\\\Assets\\\\Resources\\\"\\n            ModuleName = \\\"DSC_ColinsALMCorner.com\\\"\\n         },\\n\\n        @{\\n            NodeName = \\\"fabfiberserver\\\"\\n            Role = \\\"WebServer\\\"\\n         }\\n    );\\n}\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$configData = @{</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4054bc65-0967-415e-8af1-3752290f5216.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5c33849c-66e4-4675-a050-efdc8170b396.png\\\" width=\\\"644\\\" height=\\\"342\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6076c6b4-a6ef-426e-a370-e42f755475da.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d3fc5510-321b-4fb7-8e06-9ce6db7c93f3.png\\\" width=\\\"644\\\" height=\\\"343\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/613957e2-7199-4e86-a0b1-d136a71244be.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6aeee89c-c910-470a-83f4-55b9eb074f32.png\\\" width=\\\"644\\\" height=\\\"341\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/0180bc56-59e7-4b3f-bf44-5e29abf12480.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/501030fa-4737-4961-8f90-7544b801754e.png\\\" width=\\\"644\\\" height=\\\"336\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/00cc9fe9-0b3f-4778-9d78-3b7b75a511a1.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/5bf548b5-98d2-483c-894f-3939fb54150f.png\\\" width=\\\"644\\\" height=\\\"114\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">System.Management.Automation.Remoting.PSRemotingTransportException: Connecting to remote server fabfiberserver.fab.com failed with the following error message : The WinRM client cannot process the request. A computer policy…</font>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d07dc23d-e926-400a-afdb-151d3bb88f94.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2c026a3a-f8fd-449b-8f99-e80eff5c98d7.png\\\" width=\\\"644\\\" height=\\\"384\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">System.Management.Automation.RemoteException: Errors occurred while processing configuration 'SomeConfig'.</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">System.Management.Automation.RemoteException: Unable to find the mof file.</font>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/visualstudioalm/archive/2014/05/22/release-management-for-microsoft-visual-studio-2013-with-update-3-ctp1-is-live.aspx\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Just in case you missed it, \"],[0,[0],1,\"Release Management Update 3 CTP now supports deploying using PowerShell DSC\"],[0,[],0,\". I think this is a great feature and adds to the impressive toolset that Microsoft is putting out into the DevOps area. So I decided to take this feature for a spin!\"]]],[1,\"h2\",[[0,[],0,\"Bleeding Edge\"]]],[1,\"p\",[[0,[],0,\"<rant>I had a boss once who hated being on the bleeding edge – he preferred being at “latest version – 1” of any OS, SQL Server or VS version (with a few notable exceptions). Being bleeding edge can mean risk and churn, but I prefer being there all the same. Anyway, in the case of the Release Management (RM) CTP, it was a little painful – mostly because the documentation is poor. Hopefully this is something the Release Management team will improve on. I know the release is only CTP, but how can the community provide feedback if they can’t even figure out how to use the tool?</rant>\"]]],[1,\"p\",[[0,[],0,\"On top of the Release Management struggles, PowerShell DSC itself isn’t very well documented (yet) since it itself is pretty new technology. This is bleeding BLEEDING edge stuff.\"]]],[1,\"p\",[[0,[],0,\"Anyway, after struggling on my own for a few days I mailed the product group and got “the hidden manual” as a reply (more on this later). At least the team responds fairly quickly when MVPs contact them!\"]]],[1,\"h3\",[[0,[],0,\"Issues\"]]],[1,\"p\",[[0,[],0,\"So here’s a summary of the issues I faced:\"]]],[3,\"ul\",[[[0,[],0,\"The DSC feature only works on domain joined machines. I normally don’t use domains on my experimental VMs, so I had to make one, but most organizations nowadays use domains anyway, so this isn’t such a big issue.\"]],[[0,[],0,\"Following the RM DSC manual, I wanted to enable CredSSP. I ran the Enable-WSManCredSSP command from the manual, but got some credential issues later on.\"]],[[0,[],0,\"The current public documentation on DSC in RM is poor – in fact, without mailing the product group I would never have gotten my Proof-of-Concept release to work at all (fortunately you now have this post to help you!)\"]],[[0,[],0,\"You have to change your DSC scripts to use in Release Management (you can’t have the exact same script run in RM and in a console – the mof compilation is invoked differently, especially with config data)\"]]]],[1,\"h2\",[[0,[],0,\"Proof of Concept – A “Start of Release” Walkthrough\"]]],[1,\"p\",[[0,[],0,\"I want to eventually build up to a set of scripts that will allow me to deploy a complete application (SQL database and ASP.NET website) onto a set of “fresh” servers using only DSC. This will enable me to create some new and unconfigured servers and target them in the Release – the DSC will ensure that SQL gets installed and configured correctly, that IIS, ASP.NET, MVC and any other prerequisites get set up correctly on the IIS server and finally that the database and website are deployed correctly. All without having to install or configure anything manually. That’s the dream. The first step was to create a few DSC scripts and then to get Release Management to execute them as part of the deployment workflow.\"]]],[1,\"p\",[[0,[],0,\"I had to create a custom DSC resource (I may change this later) – but that’s a post for another day. Assume that I have the resource files ready for deployment to a node (a machine). Here’s the script to copy an arbitrary resource to the modules folder of a target node so that subsequent DSC scripts can utilize the custom resource:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The last  of the script “compiles” the DSC script into a mof file that is then used to push this configuration to the target node. I wanted to parameterize the script, so I tried to introduce the RM parameter notation, which is __ pre- and post-fix (such as __ModuleName__). No such luck. I have to hardcode configuration data in the configuration data file.\"]]],[1,\"p\",[[0,[],0,\"To accomplish that I’m using configuration data for executing this script. This is standard DSC practice – however, there’s one trick. For RM, you need to put the configuration data into a variable. Here’s what an “ordinary” config data script looks like:\"]]],[10,1],[1,\"p\",[[0,[],0,\"To get this to work with RM, you need to change the 1st line to this:\"]]],[10,2],[1,\"p\",[[0,[],0,\"This puts the configuration hashtable into a variable called “$configData”. This is the variable that I’m using in the CopyDSCResource DSC script to specify configuration data (see the last line of the previous script).\"]]],[1,\"p\",[[0,[],0,\"Meanwhile, in RM, I’ve set up an environment (using “New Standard Environment”) and added my target server (defaulting to port 5985 for PSRemoting). I’ve configured a Release Path and now I want to configure the Component that is going to execute the script for me.\"]]],[1,\"p\",[[0,[],0,\"I click on “Configure Apps” –> Components and add a new component. I give it a name and specify the package path:\"]]],[10,3],[1,\"p\",[[0,[],0,\"You can access the package path in your scripts using “$applicationPath”.\"]]],[1,\"p\",[[0,[],0,\"Now I click on the “Deployment” tab and configure the tool – I select the “Run PowerShell on Standard Environment” tool (which introduces some parameters) and leave everything as default.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now let’s configure the Release Template. Click on “Configure Apps” –> “Release Templates” and add a new Template. Give it a name and select a Release Path. In the toolbox, right-click on the Components node and add in the DSC script component we just created. Now drag into the designer the server and into the server activity drag the DSC component. We’ll then enter the credentials and the paths to the scripts:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Since I’m accessing a network share, I specify “UseCredSSP” to true. Both ScriptPath and ConfigurationFilePath are relative to the package path (configured in the Source tab of the component). I specify the DSC script for the ScriptPath and the config data file for the ConfigurationFilePath. Finally, I supply a username and password for executing the command. We can now run the deployment!\"]]],[1,\"p\",[[0,[],0,\"Create a new Release and select the newly created template. Specify the build number (either a TFS build or external folder, depending on how you configured your components) and Start it.\"]]],[10,6],[1,\"p\",[[0,[],0,\"Hopefully you get a successful deployment!\"]]],[10,7],[1,\"h2\",[[0,[],0,\"Issues You May Face\"]]],[1,\"p\",[[0,[],0,\"Of course, not everything will run smoothly. Here are some errors I faced and what I did to rectify them.\"]]],[1,\"h3\",[[0,[],0,\"Credential Delegation Failure\"]]],[1,\"p\",[[0,[1],1,\"Symptom:\"],[0,[],0,\" You get the following error message in the deployment log:\"]]],[10,8],[1,\"p\",[[0,[1],1,\"Fix:\"],[0,[],0,\" In the RM DSC manual, they tell you to run an Enable-WSManCredSSP command to allow credential delegation. I have VMs that have checkpoints, so I’ve run this PoC several times, and each time I get stuck I just start again at the “clean” checkpoint. Even though this command always works in PowerShell, I found that sometimes I would get this error. The fix is to edit a group policy on the RM server machine. Type gpedit.msc to open up the console and browse to “Computer Configuration\\\\Administrative Templates\\\\System\\\\Credentials Delegation”. Then click on the “Allow delegating fresh credentials with NTLM-only server authentication”. Enable this rule and then add in your target servers (click the “Show…” button). You can use wildcards if you want to delegate any machine on a domain. Interestingly, the Enable-WSManCredSSP command seems to “edit” the “Allow delegating fresh credentials” setting, not the NTLM-only one. Perhaps there’s a PowerShell command or extra argument that will edit the NTLM-only setting?\"]]],[10,9],[1,\"h3\",[[0,[],0,\"Configuration Data Errors\"]]],[1,\"p\",[[0,[1],1,\"Symptom:\"],[0,[],0,\" You get the following error message in the deployment log:\"]]],[10,10],[1,\"p\",[[0,[1],1,\"Fix:\"],[0,[],0,\" I found that this message occurs for 2 main reasons: first, you forget to put your config data hashtable into a variable (make sure your line 1 is $configData = @{) or you have an error in your hashtable (like a forgotten comma or extra curly brace). If you get this error, then check your configuration data file.\"]]],[1,\"h3\",[[0,[],0,\"Cannot Find Mof File\"]]],[1,\"p\",[[0,[1],1,\"Symptom:\"],[0,[],0,\" You get the following error message in the deployment log:\"]]],[10,11],[1,\"p\",[[0,[1],1,\"Fix:\"],[0,[],0,\" This could mean that you’ve got an “-OutputPath” specified when you invoke your config (the last line of the config script) so that the mof file ends up in some other directory. Or you have the name of your node incorrect. I found that specifying “fabfiberserver.fab.com” caused this error in my scenario – but when I changed the name to “fabfiber” I didn’t get this error. You’ll have to try the machine name or the FQDN to see which one RM is happy with.\"]]],[1,\"h2\",[[0,[],0,\"Challenges\"]]],[1,\"p\",[[0,[],0,\"The ability to run DSC during Releases is a promising tool – but there are some challenges. Here is my list of pros and cons with this feature:\"]]],[1,\"h3\",[[0,[],0,\"Pros of DSC in Release Management\"]]],[3,\"ul\",[[[0,[],0,\"You don’t have to install a deployer agent on the target nodes\"]],[[0,[],0,\"You can use existing DSC PowerShell scripts (with some small RM specific tweaks) in your deployment workflows\"]]]],[1,\"h3\",[[0,[],0,\"Cons of DSC in Release Management\"]]],[3,\"ul\",[[[0,[],0,\"Only works on domain machines at present\"]],[[0,[],0,\"Poor documentation makes figuring out how to structure scripts and assets to RM’s liking a challenge\"]],[[0,[],0,\"You have to change your “normal” DSC script structure to fit the way RM likes to invoke DSC\"]],[[0,[],0,\"You can’t parameterize the scripts (so that you can reuse scripts in different workflows)\"]]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"The ability to run DSC in Release Management workflows is great – not having to install and configure the deployer agent is a bonus and being able to treat “config as code” in a declarative manner is a fantastic feature. However, since DSC is so new (and poorly documented) there’s a steep learning curve. The good news is that if you’ve already invested in DSC, the latest Release Management allows you to leverage that investment during deployments. This is overall a very exciting feature and I look forward to seeing it grow and mature.\"]]],[1,\"p\",[[0,[],0,\"I’ll be posting more in this series as I get further along with my experimentation!\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1404507908000,"status":"published","published_by":1},{"id":"3fd7ac1d-d53d-4198-a662-945c6fe15756","title":"Using Powershell to Replace Config Settings in Lab Management","slug":"using-powershell-to-replace-config-settings-in-lab-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"<span style=\\\"color: blue\\\">&lt;</span><span style=\\\"color: #a31515\\\">configuration</span><span style=\\\"color: blue\\\">&gt;<br>  &lt;</span><span style=\\\"color: #a31515\\\">settings </span><span style=\\\"color: red\\\">LogTraceInfo</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">False</span>\\\" <span style=\\\"color: red\\\">KeepConnectionAlive</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">True</span>\\\"<span style=\\\"color: blue\\\">&gt;<br>    &lt;</span><span style=\\\"color: #a31515\\\">sql </span><span style=\\\"color: red\\\">server</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">dbserver</span>\\\" <span style=\\\"color: red\\\">database</span><span style=\\\"color: blue\\\">=</span>\\\"<span style=\\\"color: blue\\\">Live</span>\\\" <span style=\\\"color: blue\\\">/&gt;<br>  <!--</span--><span style=\\\"color: #a31515\\\">settings</span><span style=\\\"color: blue\\\">&gt;<br><!--</span--><span style=\\\"color: #a31515\\\">configuration</span><span style=\\\"color: blue\\\">&gt;</span></span></span>\",\"language\":\"\"}],[\"code\",{\"code\":\"<span style=\\\"color: green\\\">###########################################################################</span>\",\"language\":\"\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">cmd /c powershell .\\\\ReplaceDatabaseInConfig.ps1 -configPath \\\"AppCustom.config\\\" -database \\\"Live\\\"</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-c0Q-urMxhoE/TfntvqukHII/AAAAAAAAAQs/mB_SyVAIydA/s1600-h/image%25255B3%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-Tc62CFoLl04/TfntwF5AKJI/AAAAAAAAAQw/YpW_GFUSe_I/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"460\\\" height=\\\"90\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.iis.net/download/WebDeploy\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"So you’ve got your virtual environment running and you’re deploying your application using the Lab Default Workflow. Only, you have a config file and you need to update a connection string or something in the config file for automated tests to run properly.\"]]],[1,\"p\",[[0,[],0,\"Now if you’re deploying websites, for goodness sake use \"],[0,[0],1,\"WebDeploy\"],[0,[],0,\"  and the Web.Config transforms within Visual Studio to configure your web.configs as part of the WebDeploy packaging. This is by far the easiest way to deploy web applications into lab environments.\"]]],[1,\"p\",[[0,[],0,\"However, if you’ve got some other configs that need to be twiddled as part of the deployment, then create a PowerShell script to do your replacements.\"]]],[1,\"p\",[[0,[],0,\"Given an XML that looks like this:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Let’s say you want to replace the database property. Here’s the PS to do this:\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"######################################################################################\"],[1,[],0,2],[0,[],0,\"#\"],[1,[],0,3],[0,[],0,\"# Usage: ReplaceDatabaseInConfig.ps1 -configPath \\\"path to config\\\" -database \\\"new database name\\\"\"],[1,[],0,4],[0,[],0,\"#\"],[1,[],0,5],[0,[],0,\"######################################################################################\"],[1,[],0,6],[0,[],0,\"param($configPath, $database)\"],[1,[],0,7],[1,[],0,8],[0,[],0,\"Write-Host \\\"Replacing database name in config file...\\\"\"],[1,[],0,9],[1,[],0,10],[0,[],0,\"# Get the content of the config file and cast it to XML\"],[1,[],0,11],[0,[],0,\"$xml = [xml](get-content $configPath)\"],[1,[],0,12],[0,[],0,\"$root = $xml.get_DocumentElement();\"],[1,[],0,13],[1,[],0,14],[0,[],0,\"# change the db name\"],[1,[],0,15],[0,[],0,\"$root.settings.sql.database = $database\"],[1,[],0,16],[1,[],0,17],[0,[],0,\"#save\"],[1,[],0,18],[0,[],0,\"$xml.Save($configPath)\"],[1,[],0,19],[1,[],0,20],[0,[],0,\"Write-Host \\\"Done!\\\"\"]]],[10,1],[1,\"p\",[[1,[],0,21]]],[1,\"p\",[[0,[],0,\"Check this script into Source Control – make sure that you get it into your drop folder somehow (add it to a solution and mark it’s action to “Copy Always” or something along those lines).\"]]],[1,\"p\",[[1,[],0,22]]],[1,\"p\",[[0,[],0,\"Then it’s a simple matter of executing the script as part of your deployment. If you set the working directory to the folder where your deployment copies the script, then you can invoke it as follows:\"]]],[1,\"p\",[[1,[],0,23]]],[10,2],[1,\"p\",[[1,[],0,24]]],[1,\"p\",[[0,[],0,\"In your build log, you’ll see the following:\"]]],[1,\"p\",[[1,[],0,25]]],[10,3],[1,\"p\",[[1,[],0,26]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1308257340000,"status":"published","published_by":1},{"id":"bb1ce673-db0d-48be-8e84-567458da16bd","title":"Using Release Management to Manage Ad-Hoc Deployments","slug":"using-release-management-to-manage-ad-hoc-deployments","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/08f9fe08-c9f4-4008-8508-d20ed193a22e.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/00d7b0aa-5514-4fc9-8441-91682c57fcad.png\\\" width=\\\"449\\\" height=\\\"297\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/4e5b20a8-69fc-487f-b6bb-a82bb525b9e3.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/fea0da76-88e4-4e94-9e3c-a2a015872a3e.png\\\" width=\\\"448\\\" height=\\\"320\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/9517b602-152c-4ed7-9df0-2520eba8c400.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/7e949357-3b37-40a1-b5e9-22b61cecdc8e.png\\\" width=\\\"452\\\" height=\\\"260\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"https://blogs.msdn.microsoft.com/visualstudioalm/2016/09/26/pricing-for-release-management-in-tfs-15/\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Release Management (RM) is awesome – mostly because it works off the amazing cross platform build engine. Also, now that \"],[0,[0],1,\"pricing is announced\"],[0,[],0,\", we know that it won’t cost an arm and a leg!\"]]],[1,\"p\",[[0,[],0,\"When I work with customers to adopt RM, I see two kinds of deployments: \"],[0,[1],1,\"repeatable\"],[0,[],0,\" and \"],[0,[1],1,\"ad-hoc\"],[0,[],0,\". RM does a great job at repeatable automation – that is, it is great at doing the same thing over and over. But what about deployments that are different in some way every time? Teams love the traceability that RM provides – not just the automation logs, but also the approvals. It would be great if you could track ad-hoc releases using RM.\"]]],[1,\"h2\",[[0,[],0,\"The Problem\"]]],[1,\"p\",[[0,[],0,\"The problem is that RM doesn’t have a great way to handle deployments that are slightly different every time. Let’s take a very typical example: ad-hoc SQL scripts. Imagine that you routinely perform data manipulation on a production database using SQL scripts. How do you audit what script was run and by whom? “We can use RM!” I hear you cry. Yes you can – but there are some challenges.\"]]],[1,\"p\",[[0,[],0,\"Ah-hoc means (in this context) \"],[0,[1],1,\"different every time\"],[0,[],0,\". That means that the script you’re running is bound to change every time you run the release. Also, depending on how dynamic you want to go, even the target servers could change – sometimes you’re executing against server A, sometimes against server B, sometimes both. “Just make the script name or server name a variable that you can change at queue time,” I hear you say. Unfortunately, unlike builds, you can’t specify parameter values at queue time. You \"],[0,[1],1,\"could\"],[0,[],0,\" create a release in draft and then edit the variables for that run of the release, but this isn’t a great experience since you’re bound to forget things – and you’ll have to do this every time you start a release.\"]]],[1,\"h2\",[[0,[],0,\"A Reasonable Solution\"]]],[1,\"p\",[[0,[],0,\"I was at a customer who were trying to convert to RM from a home-grown deployment tool. Besides “repeatable” deployments their tool was handling several hundred ad-hoc deployments a month, so they had to decide whether or not to keep the ad-hoc deployments in the home-grown tool or migrate to RM. So I mailed the Champs List – a mailing list direct to other MVPs and the VSTS Product Group in Microsoft (being an MVP has to have some benefits, right?) – and asked them what \"],[0,[1],1,\"they\"],[0,[],0,\" do for ad-hoc deployments. It turns out that they use ad-hoc deployments to turn feature switches on and off, and they run their ad-hoc deployments with RM – and while I didn’t get a lot of detail, I did get some ideas.\"]]],[1,\"p\",[[0,[],0,\"I see three primary challenges for ad-hoc release definitions:\"]]],[3,\"ol\",[[[0,[],0,\"What to execute\"]]]],[3,\"ul\",[[[0,[],0,\"Where does the Release get the script to execute? You could create a network share and put a script in there called “adhoc.sql” and get the release to execute that script every time it runs. Tracking changes is then a challenge – but we’re developers and already know how to track changes, right? Yes: \"],[0,[1],1,\"source control\"],[0,[],0,\". So source control the script – that way every time the release runs, it gets the latest version of the script and runs that. Now you can track what executed and who changed the script. And you can even perform code-review prior to starting the release – bonus!\"]]]],[3,\"ol\",[[[0,[],0,\"What to execute\"]]]],[3,\"ul\",[[[0,[],0,\"Is there an echo here? Well no – it’s just that if the release is executing the same script every time, there’s a danger that it could well – execute the same script. That means you have to either write your script defensively – that is, in such a manner that it is idempotent (has the same result no matter how many times you run it) or you have to keep a record of whether or not the script has been run before, say using a table in a DB or an Azure table or something. I favor idempotent scripts, since I think it’s a good habit to be in when you’re automating stuff anyway – so for SQL that means doing “if this record exists, skip the following steps” kind of logic or using MERGE INTO etc.\"]]]],[3,\"ol\",[[[0,[],0,\"Where to execute\"]]]],[3,\"ul\",[[[0,[],0,\"Are you executing against the same server every time or do the targets need to be more dynamic? There are a couple of solutions here – you could have a text doc that has a list of servers, and the release definition reads in the file and then loops, executing the script against the target servers one by one. This is dynamic, but dangerous – what if you put in a server that you don’t mean to? Or you could create an environment per server (if you have a small set of servers this is ok) and then set each environment to manual deployment (i.e. no trigger). Then when you’re ready to execute, you create the release, which just sits there until you explicitly tell it which environment (server) to deploy to.\"]]]],[1,\"h2\",[[0,[],0,\"Recommended Steps\"]]],[1,\"p\",[[0,[],0,\"While it’s not trivial to set up an ad-hoc deployment pipeline in RM, I think it’s feasible. Here’s what I’m going to start recommending to my customers:\"]]],[3,\"ol\",[[[0,[],0,\"Create a Git repository with a well-defined script (or root script)\"]],[[0,[],0,\"Create a Release that has a single artifact link – to the Git repo you just set up, on the master branch\"]],[[0,[],0,\"Create an environment per target server. In that environment, you’re conceptually just executing the root script (this could be more complicated depending on what you do for ad-hoc deployments). All the server credentials etc. are configured here so you don’t have to do them every time. You can also configure approvals if they’re required for ad-hoc scripts. Here’s an example where a release definition is targeting (potentially) ServerA, ServerB and/or ServerC. This is only necessary if you have a fixed set of target servers and you don’t always know which server you’re going to target:\"]]]],[10,0],[3,\"ol\",[[[0,[],0,\"Here I’ve got an example of copying a file (the root script, which is in a Git artifact link) to the target server and then executing the script using the WinRM SQL task. These tasks are cloned to each server – of course the server name (and possibly credentials) are different for each environment – but you only have to set this up once.\"]],[[0,[],0,\"Configure each environment to have a manual trigger (under deployment conditions). This allows you to select which server (or environment) you’re deploying to for each instance of the release:\"]]]],[10,1],[3,\"ol\",[[[0,[],0,\"Enable a branch policy on the master branch so that you have to create a Pull Request (PR) to get changes into master. This forces developers to branch the repo, modify the script and then commit and create a PR. At that point you can do code review on the changes before queuing up the release.\"]],[[0,[],0,\"When you’ve completed code review on the PR, you then create a release. Since all the environments are set to manual trigger, you now can go and manually select which environment you want to deploy to:\"]]]],[10,2],[3,\"ol\",[[[0,[],0,\"Here you can see how the status on each environment is “Not Deployed”. You can now use the deploy button to manually select a target. You can of course repeat this if you’re targeting multiple servers for this release.\"]]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"With a little effort, you can set up an ad-hoc release pipeline. This gives you the advantages of automation (since the steps and credentials etc. are already set up) as well as auditability and accountability (since you can track changes to the scripts as well as any approvals). How do you, dear reader, handle ad-hoc deployments? Sound off in the comments!\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1476878936000,"status":"published","published_by":1},{"id":"03f2e264-d1f3-4673-8865-b266e7e70580","title":"Using TFS ALM for Sharepoint Development (The Easy Way – with Lab Management)","slug":"using-tfs-alm-for-sharepoint-development-(the-easy-way--with-lab-management)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-ew0gqU5DhHs/TzT1LAaZVMI/AAAAAAAAAWY/Czz7p9kdSKY/s1600-h/image%25255B3%25255D.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"267\\\" src=\\\"http://lh6.ggpht.com/-EG9Kiid9yR8/TzT1NEsl-cI/AAAAAAAAAWg/Bcw1WzSkl7M/image_thumb%25255B1%25255D.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; border-right: 0px; border-top: 0px; display: block; float: none; margin-left: auto; margin-right: auto; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"343\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-WIvWKQL5Cc4/TzT1Oq2_ffI/AAAAAAAAAWo/H7ouzITP4-o/s1600-h/image%25255B9%25255D.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"157\\\" src=\\\"http://lh3.ggpht.com/-F0z39yVhcF0/TzT1QAwWMnI/AAAAAAAAAWw/qF2HN6XzyUc/image_thumb%25255B3%25255D.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; border-right: 0px; border-top: 0px; display: block; float: none; margin-left: auto; margin-right: auto; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-CNwzt6mgjK8/TzT1RW9RKvI/AAAAAAAAAW4/DCOxfT6DdTs/s1600-h/image%25255B6%25255D.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"244\\\" src=\\\"http://lh3.ggpht.com/-jSs-UwwBDqY/TzT1TIZekCI/AAAAAAAAAXA/nseLctO1QDM/image_thumb%25255B2%25255D.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; border-right: 0px; border-top: 0px; display: block; float: none; margin-left: auto; margin-right: auto; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"228\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-Y2Ld1FDSSMU/TzT1UX2HWOI/AAAAAAAAAXI/mLT0v8LksFg/s1600-h/image%25255B13%25255D.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"178\\\" src=\\\"http://lh4.ggpht.com/-JBRRCsfsYNU/TzT1WTuG6GI/AAAAAAAAAXQ/94WB3zFxwCg/image_thumb%25255B5%25255D.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; border-right: 0px; border-top: 0px; display: block; float: none; margin-left: auto; margin-right: auto; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"313\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-LDp9BCuUWNc/TzT1XpF_2RI/AAAAAAAAAXY/xr4j3yGy8BE/s1600-h/image%25255B17%25255D.png\\\"><img alt=\\\"image\\\" border=\\\"0\\\" height=\\\"274\\\" src=\\\"http://lh3.ggpht.com/-Oy3iLHxD360/TzT1ZPl6rvI/AAAAAAAAAXg/jyrem8Ncc8w/image_thumb%25255B7%25255D.png?imgmax=800\\\" style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; border-right: 0px; border-top: 0px; display: block; float: none; margin-left: auto; margin-right: auto; padding-left: 0px; padding-right: 0px; padding-top: 0px;\\\" title=\\\"image\\\" width=\\\"288\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/sharepointdev/archive/2011/08/25/creating-your-first-tfs-build-process-for-sharepoint-projects.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/sharepointdev/archive/2011/09/22/configuring-versioning-of-assemblies-in-sharepoint-automated-build.aspx\"]],[\"a\",[\"href\",\"http://blogs.msdn.com/b/sharepointdev/archive/2011/11/17/deploying-wsps-as-part-of-an-automated-build.aspx\"]],[\"a\",[\"href\",\"http://archive.msdn.microsoft.com/SPPwrShllTeamBuild\"]],[\"a\",[\"href\",\"https://skydrive.live.com/?cid=64A24E0938D6D062&id=64A24E0938D6D062%21312\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Visual Studio 2010 has some amazing features for Sharepoint development, like project templates, server explorers, feature and package GUIs to name a few. So you’re tasked with creating a WebPart or a Workflow – no problem, fire up VS, create a new project and you’re coding.\"],[1,[],0,0],[0,[],0,\"However, just because you’re up and coding quickly, doesn’t mean you’re being productive (necessarily). What about requirements management? Testing? Source control? And if there’s more than 1 of you coding, what Sharepoint site do you code against? Oh wait, I forgot to mention that you need to install Sharepoint on the same machine that you have VS on to actually get the Sharepoint projects to work.\"],[1,[],0,1]]],[1,\"h2\",[[0,[],0,\"The ChallengesThere are a few challenges that you’ll need to overcome if you want to be a good ALM citizen while doing Sharepoint dev:\"],[1,[],0,2]]],[3,\"ul\",[[[0,[],0,\"Sharepoint and VS need to be on the same machine\"]],[[0,[],0,\"Source code needs to go somewhere other than your hard drive\"]],[[0,[],0,\"Build Automation\"]],[[0,[],0,\"Deployment\"]],[[0,[],0,\"Automated TestingFortunately, without too much work, you can address all of these challenges using TFS, and specifically Lab Management. Of course TFS covers the other ALM concerns too (like requirements management and bug tracking). I won’t focus on these too much in this article; I’ll concentrate more on the “technical challenges” bulleted above.\"],[1,[],0,3]]]],[1,\"h2\",[[0,[],0,\"Setting up a Dev Environment using a VMInstalling Sharepoint on your local machine will allow you to at least get your applications compiling, but it’s not a sustainable solution to the SP/VS-on-the-same-machine problem. I decided to use a VM. Here are the steps I followed to set up the VM for development:\"],[1,[],0,4]]],[3,\"ul\",[[[0,[],0,\"Install OS and join domain\"]],[[0,[],0,\"Install and configure Sharepoint\"]],[[0,[],0,\"Install VS and connect to TFS\"]],[[0,[],0,\"SnapshotNow I have a VM for Sharepoint development. I can easily duplicate this VM for other team members as they need development environments.\"],[1,[],0,5],[0,[],0,\"Note: Because this VM is going to also become a test environment, I was careful to add my solution to Source Control. That way if I restored to a previous snapshot I don’t lose any code!\"],[1,[],0,6]]]],[1,\"h2\",[[0,[],0,\"Build AutomationBuilding the code in TeamBuild is easy – but you’ll have to jump through some extra hoops if you want to create a WSP that can be deployed to your test Sharepoint.\"],[1,[],0,7],[0,[],0,\"Chris O’Brien (from the Sharepoint Dev Team) wrote a series of blogs about Continuous Integration with TFS and Sharepoint (supposedly there are 5 posts, but I only found 3: \"],[0,[0],1,\"one\"],[0,[],0,\", \"],[0,[1],1,\"three\"],[0,[],0,\" and \"],[0,[2],1,\"five\"],[0,[],0,\"). They’re certainly detailed – but it seemed like a lot of work to go through. I think that my Lab Management solution overcomes some of the complexities that Chris had to work through (most of which revolved around remote deployment of WSP’s).\"],[1,[],0,8],[0,[],0,\"To get automated builds working, start off using the DefaultTemplate and get the build to compile your solution. Once you get a passed build, it’s time to create the WSP package. So open up the build template and add\"],[1,[],0,9],[0,[],0,\"/p:IsPackage=true\"],[1,[],0,10],[0,[],0,\"to the MSBuild arguments parameter.\"],[1,[],0,11]]],[10,0],[1,\"p\",[[1,[],0,12],[1,[],0,13],[0,[],0,\"However, your build is now going to fail (unless you have Sharepoint installed on your build server – which you shouldn’t!). The Sharepoint packaging requires some build targets as well as some dll’s that only get installed with Sharepoint.\"],[1,[],0,14],[0,[],0,\"One of the most useful bits of info in Chris O’Brien’s articles is the mention of \"],[0,[3],1,\"this Powershell script\"],[0,[],0,\" that will “harvest” the Sharepoint dll’s and build targets and then “install” them on your build server. I ran the script (1st on the Sharepoint dev VM), copied over the folder it created to the build server and ran the script again to install. Easy as pie. Trigger another build, and you’ll see in the drop folder a shiny new WSP package!\"],[1,[],0,15]]],[1,\"h2\",[[0,[],0,\"DeploymentDeploying your WSP to the Sharepoint site is arguable the most difficult challenge that you’ll face. That’s why Chris O’Brien’s article gets quite complicated. Enter Lab Management.\"],[1,[],0,16],[0,[],0,\"Since we’re working off a VM anyway, let’s bring it into Lab Management. The workflow capability (provided by the TeamBuild agent) will allow us to automate deployment not from a remote perspective, but “locally” as it were (we’re going to execute the deployment from the Sharepoint machine, not remotely from the build server). This is a massive simplification.\"],[1,[],0,17],[0,[],0,\"These are the steps I followed to enable deployment:\"],[1,[],0,18]]],[3,\"ul\",[[[0,[],0,\"Install the TFS Build, Lab and Test agents on the Sharepoint Dev VM\"]],[[0,[],0,\"I configured the Build and Test agents, hooking them up to existing Build and Test controllers in my TFS environment\"]],[[0,[],0,\"Compose a new Virtual Environment in Lab Management\"]],[[0,[],0,\"Bring in the Sharepoint VM and enable Deployment and Testing capabilities\"]]]],[10,1],[1,\"p\",[[1,[],0,19]]],[3,\"ul\",[[[0,[],0,\"Use VS to deploy the package to the Sharepoint site  by debugging the project\"]],[[0,[],0,\"In the Sharepoint Tab of the Sharepoint project properties, make sure you turn off the “Auto-retract after debug” option\"]]]],[10,2],[1,\"p\",[[1,[],0,20]]],[3,\"ul\",[[[0,[],0,\"Create a custom page to add the WebPart in (this is only necessary for visual development, like WebParts)\"]],[[0,[],0,\"Create a Powershell script that can update the WSP (here’s \"],[0,[4],1,\"mine\"],[0,[],0,\" – it’s pretty generic, so should just work)\"]],[[0,[],0,\"Add this to the Sharepoint project and set its “Copy to output directory” property to “Copy always” (this ensures that it ends up in the drop folder)\"]]]],[1,\"h2\",[[0,[],0,\"Coded UI TestingI’m not going to go into too much detail here – I created a test plan and a test suite. I then created a test case and executed the case (with Action Recording enabled). I then added a Test Project to my Sharepoint solution and generated a coded UI test from the Test Case action recording. Then I associated the test method to the Test Case (in the Associated Automation tab of the test case). Voila – one automated test case ready to fire.\"],[1,[],0,21],[0,[],0,\"You’ll need to open MTM and create Test Settings (automated in this case) for your Sharepoint Lab environment.\"],[1,[],0,22]]],[1,\"h2\",[[0,[],0,\"Lab Management WorkflowThe final step is hooking it all up in the Lab Management workflow. Create a new build and change the template to the LabDefaultTemplate. Click the ellipsis to launch the Lab Workflow Wizard.\"],[1,[],0,23]]],[3,\"ul\",[[[0,[],0,\"Environments Tab: Select the Sharepoint environment from the list of environments\"]],[[0,[],0,\"Build Tab: Select the build that you created using the DefaultTemplate and that builds your WSP package\"]],[[0,[],0,\"Note: The coded UI test project must be part of this solution too, so that the dll’s end up in the drop folder\"]],[[0,[],0,\"Deploy Tab: check the “Deploy the build” checkbox\"]],[[0,[],0,\"Add the following 2 scripts (make sure you create the c:\\\\deployment folder on the VM):\"]],[[0,[],0,\"cmd /c xcopy /Y $(BuildLocation)\\\\*.* c:\\\\deployment\"]],[[0,[],0,\"cmd /c powershell c:\\\\deployment\\\\deployWSP.ps1 $(BuildLocation)\"]]]],[10,3],[1,\"p\",[[1,[],0,24]]],[3,\"ul\",[[[0,[],0,\"Test Tab: Select the test plan, suite, configuration and settings\"]]]],[1,\"h2\",[[0,[],0,\"ConclusionUsing Lab Management greatly simplifies the ALM aspects of Sharepoint development – automated build, deployment and testing specifically.\"],[1,[],0,25],[0,[],0,\"Summary of Steps:\"],[1,[],0,26]]],[3,\"ul\",[[[0,[],0,\"Create a VM and install Sharepoint and VS 2010\"]],[[0,[],0,\"Create your SP solution and \"],[0,[5],1,\"check into Source control!\"]],[[0,[],0,\"Install and configure TFS Build, Test and Lab agents\"]],[[0,[],0,\"Compose a new Environment using the VM\"]],[[0,[],0,\"Create automated test settings for the environment\"]],[[0,[],0,\"Deploy the package by debugging the project (turn off auto-retract after debugging)\"]],[[0,[],0,\"Create test cases in a test plan\"]],[[0,[],0,\"Execute them using the Action Recording\"]],[[0,[],0,\"Turn the action recordings into Coded UI Tests\"]],[[0,[],0,\"Associate the test methods to the Test Cases\"]],[[0,[],0,\"Customize the build to produce a WSP package\"]],[[0,[],0,\"This includes “installing” Sharepoint dlls and build targets onto your build server\"]],[[0,[],0,\"Create a powershell script that can update the deployed Package from a WSP file\"]],[[0,[],0,\"Hook it all up using the Lab Management Workflow\"]]]],[10,4],[1,\"p\",[[1,[],0,27],[0,[],0,\"Happy SP dev’ing!\"]]]]}","published_at":1328906760000,"status":"published","published_by":1},{"id":"8dd89270-3cd7-4216-97ce-dc712d3995ca","title":"Using the Fakes Framework to Test TFS API Code (Part 1 of 2)","slug":"using-the-fakes-framework-to-test-tfs-api-code-(part-1-of-2)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"class WorkItemCopyer<br>{<br>    public WorkItemCollection WorkItems { get; private set; }<br>    public QueryHierarchy QueryHierarchy { get; private set; }<br><br>    public WorkItemStore Store { get; private set; }<br>    public TfsTeamProjectCollection TPC { get; private set; }<br>    public string TeamProjectName { get; private set; }<br><br>    public WorkItemCopyer(TfsTeamProjectCollection tpc, string teamProjectName)<br>    {<br>        TPC = tpc;<br>        TeamProjectName = teamProjectName;<br>        Store = TPC.GetService<workitemstore>();<br>        QueryHierarchy = Store.Projects[TeamProjectName].QueryHierarchy;<br>    }<br><br>    public void RunQuery(QueryDefinition queryDef)<br>    {<br>        var dict = new Dictionary<string, string=\\\"\\\">() <br>            {<br>                { \\\"project\\\", TeamProjectName }, <br>                { \\\"me\\\", GetCurrentUserDisplayName() } <br>            };<br><br>        var query = new Query(Store, queryDef.QueryText, dict);<br>        WorkItems = query.RunQuery();<br>    }<br><br>    private string GetCurrentUserDisplayName()<br>    {<br>        var securityService = TPC.GetService<igroupsecurityservice>();<br>        var accountName = string.Format(\\\"{0}\\\\\\\\{1}\\\", Environment.UserDomainName, Environment.UserName);<br>        var memberInfo = securityService.ReadIdentity(SearchFactor.AccountName, accountName, QueryMembership.None);<br>        if (memberInfo != null)<br>        {<br>            return memberInfo.DisplayName;<br>        }<br>        return Environment.UserName;<br>    }<br><br>    public int CopyWorkItems(string targetIterationPath)<br>    {<br>        foreach (WorkItem workItem in WorkItems)<br>        {<br>            var copy = workItem.Copy();<br>            copy.IterationPath = targetIterationPath;<br>            copy.Save();<br>        }<br>        return WorkItems.Count;<br>    }<br><br>    public QueryDefinition FindQuery(string queryName)<br>    {<br>        return FindQueryInFolder(QueryHierarchy, queryName);<br>    }<br><br>    private QueryDefinition FindQueryInFolder(QueryFolder folder, string queryName)<br>    {<br>        foreach (var query in folder.OfType<querydefinition>())<br>        {<br>            if (query.Name == queryName)<br>            {<br>                return query;<br>            }<br>        }<br>        QueryDefinition subQuery = null;<br>        foreach (var subFolder in folder.OfType<queryfolder>())<br>        {<br>            subQuery = FindQueryInFolder(subFolder, queryName);<br>            if (subQuery != null)<br>            {<br>                return subQuery;<br>            }<br>        }<br>        return null;<br>    }<br>}<br><br>class Program<br>{<br>    static void Main(string[] args)<br>    {<br>        var tpcUrl = args[0];<br>        var teamProjectName = args[1];<br>        var queryName = args[2];<br>        var targetIterationPath = args[3];<br><br>        var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(new Uri(tpcUrl));<br>        var copyer = new WorkItemCopyer(tpc, teamProjectName);<br><br>        var query = copyer.FindQuery(queryName);<br>        copyer.RunQuery(query);<br>        var count = copyer.CopyWorkItems(targetIterationPath);<br><br>        Console.WriteLine(\\\"Successfully copied {0} work items\\\", count);<br>        Console.WriteLine(\\\"Press <enter> to quit...\\\");<br>        Console.ReadLine();<br>    }<br>}</enter></queryfolder></querydefinition></igroupsecurityservice></string,></workitemstore>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"[TestMethod]<br>public void TestCopyWithDependencies()<br>{<br>    var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(new Uri(\\\"http://localhost:8080/tfs/defaultcollection\\\"));<br>    var target = new WorkItemCopyer(tpc, \\\"Code11\\\");<br><br>    // test finding the query<br>    var query = target.FindQuery(\\\"Tasks_Release1_Sprint1\\\");<br>    Assert.IsNotNull(query);<br><br>    // test running the query<br>    target.RunQuery(query);<br>    Assert.IsTrue(target.WorkItems.Count &gt; 0);<br><br>    // test copy<br>    var count = target.CopyWorkItems(@\\\"Code11\\\\Release 1\\\\Sprint 2\\\");<br>    Assert.AreEqual(target.WorkItems.Count, count);<br>}\",\"language\":\"csharp; ruler\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-1trIVFT6m6k/T2hn0rjMbrI/AAAAAAAAAXw/YFaKRhncgUg/s1600-h/image2.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-HucNptcHp-I/T2hn2CoGRwI/AAAAAAAAAX4/oMDXUB5rlWs/image_thumb.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"192\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-xuiTHWjdifI/T2hn3qSc-qI/AAAAAAAAAYA/nZONPDWxEb4/s1600-h/image5.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-SS289Mihruo/T2hn5St6rTI/AAAAAAAAAYI/WT_WuO-u7ko/image_thumb1.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"190\\\"></a>\"}],[\"code\",{\"code\":\"[TestMethod]<br>public void TestCopyerInstantiate()<br>{<br>    using (ShimsContext.Create())<br>    {<br>        // set up the fakes<br>        var fakeTPC = new ShimTfsTeamProjectCollection();<br>        <br>        var target = new WorkItemCopyer(fakeTPC, \\\"Code11\\\");<br>        Assert.IsNotNull(target);<br>    }<br>}\",\"language\":\"csharp; ruler\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-5EAFR3qa6QU/T2hn8F4l97I/AAAAAAAAAYQ/B-tg57HeJKI/s1600-h/image9.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-jLModFC2D_s/T2hn9wnpXzI/AAAAAAAAAYY/Y652U7eEqfg/image_thumb3.png?imgmax=800\\\" width=\\\"399\\\" height=\\\"104\\\"></a>\"}],[\"code\",{\"code\":\"var fakeStore = new ShimWorkItemStore();<br>var fakeTPC = new ShimTfsTeamProjectCollection();<br>var fakeBase = new ShimTfsConnection(fakeTPC);<br>fakeBase.GetServiceOf1<workitemstore>(() =&gt; fakeStore);</workitemstore>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"[TestMethod]<br>public void TestCopyerInstantiate()<br>{<br>    using (ShimsContext.Create())<br>    {<br>        // set up the fakes<br>        var fakeStore = new ShimWorkItemStore();<br>        var fakeTPC = new ShimTfsTeamProjectCollection();<br>        ShimTfsConnection.AllInstances.GetServiceOf1<workitemstore>((t) =&gt; fakeStore);<br>        <br>        var target = new WorkItemCopyer(fakeTPC, \\\"Code11\\\");<br>        Assert.IsNotNull(target);<br>    }<br>}</workitemstore>\",\"language\":\"csharp; ruler\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-21NO1X4TrZA/T2hoADyBOFI/AAAAAAAAAYg/Ix6oy94VwIA/s1600-h/image13.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-FD549AZrBs8/T2hoCOPaafI/AAAAAAAAAYo/PP4wDZWstgw/image_thumb5.png?imgmax=800\\\" width=\\\"386\\\" height=\\\"104\\\"></a>\"}],[\"code\",{\"code\":\"[TestMethod]<br>public void TestCopyerInstantiate()<br>{<br>    using (ShimsContext.Create())<br>    {<br>        // set up the fakes<br>        var fakeHierarchy = new ShimQueryHierarchy();<br>        var fakeProject = new ShimProject()<br>        {<br>            NameGet = () =&gt; \\\"TestProject\\\",<br>            QueryHierarchyGet = () =&gt; fakeHierarchy<br>        };<br>        var fakeProjectCollection = new ShimProjectCollection()<br>        {<br>            ItemGetString = (projectName) =&gt; fakeProject<br>        };<br>        var fakeStore = new ShimWorkItemStore()<br>        {<br>            ProjectsGet = () =&gt; fakeProjectCollection<br>        };<br>        var fakeTPC = new ShimTfsTeamProjectCollection();<br>        ShimTfsConnection.AllInstances.GetServiceOf1<workitemstore>((t) =&gt; fakeStore);                <br><br>        // test<br>        var target = new WorkItemCopyer(fakeTPC, \\\"Code11\\\");<br>        Assert.IsNotNull(target);<br>    }<br>}</workitemstore>\",\"language\":\"csharp; ruler\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/03/using-fakes-framework-to-test-tfs-api_20.html\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/hh549175(v=vs.110).aspx\"]],[\"a\",[\"href\",\"http://research.microsoft.com/en-us/projects/moles/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"(Here’s the link to \"],[0,[0],1,\"Part 2\"],[0,[],0,\")\"]]],[1,\"p\",[[0,[],0,\"If you’ve ever written a utility to do TFS “stuff” using the TFS API, you probably tested by hitting F5 and stepping through a bit before letting any large loops do their thing. So what happened to all the goodness is expected of good developers – for example unit testing?\"]]],[1,\"p\",[[0,[],0,\"Well, it turns out it’s insanely difficult to test any code that depends on the TFS APIs. You could craft tests that hit an actual TFS server, but then you’d have to worry about clean up so that your tests could be repeated. And of course you’d *never* do this sort of testing against a Live server (or a Live project), right?\"]]],[1,\"p\",[[0,[],0,\"Besides, unit tests, at least in theory, are supposed to have no dependencies on external systems. So how do you go about unit testing code that uses the TFS API?\"]]],[1,\"h2\",[[0,[],0,\"Scenario: Copy Work Items\"]]],[1,\"p\",[[0,[],0,\"Let’s imagine you’ve written a console app to copy work items (obtained from executing a stored query) to a target iteration. Here’s the code for the WorkItemCopyer class and Program.cs:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Now we get to the interesting part: unit testing. Let’s start off assuming we have a test project that we can run the tests against. A CopyTest would look something like this:\"]]],[10,1],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"But there’s a problem here: if we run these tests and the server is down, they’ll fail. If someone renames the query, the tests will fail. If for some reason the query return 0 work items, the test will at best be inconclusive. So we clearly need to isolate the test from the TFS server. Enter the \"],[0,[1],1,\"Fakes framework\"],[0,[],0,\".\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"h2\",[[0,[],0,\"Fakes\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"The \"],[0,[1],1,\"Fakes Framework\"],[0,[],0,\" came out of the \"],[0,[2],1,\"Moles Framework\"],[0,[],0,\" from the MS RiSE team. It allows you to isolate your test code from almost anything using an interception mechanism.\"]]],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"To use Fakes, you’ll first need to create the Fake assemblies. We’ll right click each TeamFoundation dll reference, select “Create Fakes” and we’ll be ready to go.\"]]],[1,\"p\",[[1,[],0,5]]],[10,2],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"You’ll see the Fakes assemblies in your References now.\"]]],[1,\"p\",[[1,[],0,7]]],[10,3],[1,\"p\",[[1,[],0,8]]],[1,\"h2\",[[0,[],0,\"Faking enough to Instantiate a WorkItemCopyer\"]]],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"The first thing you need to know about fakes is that they only work inside a ShimsContext, which you can wrap into a using. In order to construct the WorkItemCopyer, we’re going to need a TeamProjectCollection object. So let’s see if we can fake it:\"]]],[10,4],[1,\"p\",[[1,[],0,10]]],[1,\"p\",[[0,[],0,\"If you run this code, you’ll get an error in the WorkItemCopyer constructor:\"]]],[10,5],[1,\"p\",[[1,[],0,11]]],[1,\"p\",[[0,[],0,\"The GetService method seems to be confused. We’ll need to fake that call to return a fake WorkItemStore. So we create a fake store (newing up a ShimWorkItemStore). But now how do we fix the GetService call? You’ll notice it’s fake counterpart is not on the ShimTeamProjectCollection class. This is because the method doesn’t exist on the TeamProjectCollection class, but on it’s base class, TfsConnection. Now according to the MSDN Fakes documentation (which talks about faking methods in base classes), I would have expected this code to work:\"]]],[10,6],[1,\"p\",[[1,[],0,12]]],[1,\"p\",[[0,[],0,\"But for some reason, this doesn’t work. So we’ll do the next best thing is to fake the GetService method for all instances of TfsConnection (which will include and TeamProjectCollection object as well). Here’s the code now:\"]]],[10,7],[1,\"p\",[[1,[],0,13]]],[1,\"p\",[[0,[],0,\"Now we get a bit further – we get to the code that initializes the QueryHierarchy property and then we get a ShimNotImplementedException:\"]]],[1,\"p\",[[1,[],0,14]]],[10,8],[1,\"p\",[[1,[],0,15]]],[1,\"p\",[[0,[],0,\"This is because the getter method Projects on our fake store is not faked. So we’ll change the code to return a fake TeamProjectCollection, which in turn needs a fake string indexer method to get a TeamProject which in turn needs a fake QueryHierarchy getter method… to save time, I’ll show you the completed code:\"]]],[10,9],[1,\"p\",[[1,[],0,16]]],[1,\"p\",[[0,[],0,\"So far so good: we can at least instantiate the WorkItemCopyer. In the \"],[0,[0],1,\"Part 2\"],[0,[],0,\" post we’ll fake some Query folders and Query objects as well as some WorkItems so that we can complete a test for copying work items.\"]]],[1,\"p\",[[1,[],0,17]]],[1,\"p\",[[0,[],0,\"Happy faking!\"]]]]}","published_at":1332274800000,"status":"published","published_by":1},{"id":"3cd2ea61-be47-4966-a7b2-ad8c39884231","title":"Using the Fakes Framework to Test TFS API Code (Part 2 of 2)","slug":"using-the-fakes-framework-to-test-tfs-api-code-(part-2-of-2)","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"var myQueries = new QueryFolder(\\\"My Queries\\\");<br>var sharedQueries = new QueryFolder(\\\"Team Queries\\\");<br>sharedQueries.Add(new QueryDefinition(\\\"My Tasks\\\", \\\"SELECT System.Id FROM WorkItems WHERE System.WorkItemType = 'Task'\\\"));<br>sharedQueries.Add(new QueryDefinition(\\\"My Bugs\\\", \\\"SELECT System.Id FROM WorkItems WHERE System.WorkItemType = 'Bug'\\\"));<br>var topQueries = new List<queryitem>() { myQueries, sharedQueries };<br><br>using (ShimsContext.Create())<br>{<br>    // set up the fakes<br>    var fakeHierarchy = new ShimQueryHierarchy();<br>    fakeHierarchy.Bind(topQueries);<br></queryitem>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"var fakeSecurityService = new StubIGroupSecurityService()<br>{<br>    ReadIdentitySearchFactorStringQueryMembership = (searchFactor, criteria, identity) =&gt; ShimsContext.ExecuteWithoutShims<identity>(() =&gt; new Identity() { DisplayName = \\\"Bob\\\" })<br>};<br>ShimTfsConnection.AllInstances.GetServiceOf1<igroupsecurityservice>((t) =&gt; fakeSecurityService);<br></igroupsecurityservice></identity>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"ShimQuery.ConstructorWorkItemStoreStringIDictionary = (q, store, text, dict) =&gt;<br>{<br>    new ShimQuery(q)<br>    {<br>        RunQuery = () =&gt;<br>        {<br>            var workItemType = new ShimWorkItemType()<br>            {<br>                NameGet = () =&gt; \\\"Task\\\"<br>            };<br>            var list = new List<workitem>()<br>            {<br>                new ShimWorkItem()<br>                {<br>                    IdGet = () =&gt; 12,<br>                    TitleGet = () =&gt; \\\"Some Work Item\\\",<br>                    TypeGet = () =&gt; workItemType,<br>                    IterationPathSetString = (path) =&gt; { },<br>                }<br>            };<br>            var workItems = new ShimWorkItemCollection()<br>            {<br>                CountGet = () =&gt; list.Count<br>            };<br>            workItems.Bind(list);<br>            return workItems;<br>        }<br>    };<br>};<br></workitem>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"var copyCount = 0;<br>ShimWorkItem.AllInstances.Copy = (w) =&gt;<br>{<br>    copyCount++;<br>    return new ShimWorkItem(w);<br>};<br>var saveCount = 0;<br>ShimWorkItem.AllInstances.Save = (_) =&gt; saveCount++;<br><br>// test instantiate<br>var target = new WorkItemCopyer(fakeTPC, \\\"Code11\\\");<br>Assert.IsNotNull(target);<br><br>// test find query<br>var query = target.FindQuery(\\\"My Tasks\\\");<br>Assert.IsNotNull(query);<br><br>// test run query<br>target.RunQuery(query);<br>Assert.AreEqual(1, target.WorkItems.Count);<br><br>// test copy work items<br>var count = target.CopyWorkItems(\\\"Test Iteration\\\");<br>Assert.AreEqual(1, count);<br>Assert.AreEqual(1, copyCount);<br>Assert.AreEqual(1, saveCount);<br>\",\"language\":\"csharp; ruler\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/03/using-fakes-framework-to-test-tfs-api.html\"]],[\"a\",[\"href\",\"https://skydrive.live.com/redir.aspx?cid=64a24e0938d6d062&resid=64A24E0938D6D062!335&parid=64A24E0938D6D062!334\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In \"],[0,[0],1,\"Part 1\"],[0,[],0,\", we started faking some TFS objects. We got as far as faking the TeamProjectCollection and WorkItemStore. In this post, we’ll complete the test for copying work items by providing a fake QueryHierarchy and a fake list of WorkItems.\"]]],[1,\"h2\",[[0,[],0,\"Binding IEnumerables\"]]],[1,\"p\",[[0,[],0,\"Since the QueryHierarchy is an IEnumerable, we’ll need to either fake the GetEnumerator() method, or find a way of making the fake QueryHierarchy enumerable! The problem with trying to fake the GetEnumerator() method is that you end up in an infinite loop – if you’re trying to get the enumerator for sub nodes of the hierarchy, you’ll also need to call GetEnumerator() which will call GetEnumerator() and so on and so on… so we’ll see if we can make the fake QueryHiearchy behave like an enumerable object. Which is really quite easy when you know how!\"]]],[1,\"p\",[[0,[],0,\"We create a list of QueryFolders (outside the ShimsContext, since we want these object to be real and not fake) and then call the Bind() method on the fake QueryHierarchy:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Now when we run the code, the QueryHierarchy behaves like an enumeration and a call to find the “My Tasks” query will result in a QueryDefinition being returned!\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"h2\",[[0,[],0,\"Faking an Interface\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"The next thing we’ll need to fake is the RunQuery() method on the Query. The first snag we’ll hit is that the RunQuery() method in the WorkItemCopyer uses the IGroupSecurityService to resolve the current user’s display name for any @me macros in the query. So we’ll first need to fake that. Hang on – how do you fake an interface?? Enter Stubs.\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"So far we’ve only used Shims. For faking interfaces, we’ll need to switch to Stubs. The only method on the IGroupSecurityService interface that we want to fake is the ReadIdentity() method. So we can fake that and return a fake Identity object. Again, we’ll run into the problem of wanting a real object in a fake world (i.e. the ShimsContext) so we’ll use a sneaky way of working around that. Here’s the code for the fake IGroupSecurityService:\"]]],[10,1],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"This will create the Identity object “outside” the current ShimsContext. Of course, we need to tell the TfsConnection GetService call to return the fake security service when asked for an IGroupSecurityService (which is done in the last line above).\"]]],[1,\"p\",[[1,[],0,5]]],[1,\"h2\",[[0,[],0,\"Faking Constructors\"]]],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"The next thing that the WorkItemCopyer code does is instantiate a Query object in order to execute the Work Item Query. We’ll need to fake the constructor in order to return a fake Query. The method we particularly want to fake out on the Query object is the RunQuery() method, which is going to return a WorkItemCollection. We’ll want to create a list of Work Items and bind a fake WorkItemCollection to the list so that we can enumerate the fake results. There is also a call to the IterationPath setter and a couple of getters on the WorkItem itself, so we’ll fake that at the same time. Here’s the code:\"]]],[10,2],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Finally, we’ll create a couple of counters to make sure that our code calls the copy and the save methods of the work items for each work item.\"]]],[10,3],[1,\"p\",[[1,[],0,8]]],[1,\"p\",[[0,[],0,\"Voila! We’ve been able to test our code without actually connecting to a real TFS service. And coverage? Well, it’s up to 97% for the WorkItemCopyer class. Not too bad!\"]]],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"The completed solution is available on my \"],[0,[1],1,\"skydrive\"],[0,[],0,\".\"]]],[1,\"p\",[[1,[],0,10]]],[1,\"p\",[[0,[],0,\"(More) Happy Faking!\"]]]]}","published_at":1332274920000,"status":"published","published_by":1},{"id":"cbf59201-80f9-47b5-934e-7d65c6fafa8d","title":"Using the TFS API to display results of a Hierarchical Work Item Query","slug":"using-the-tfs-api-to-display-results-of-a-hierarchical-work-item-query","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"TF248021: You have specified a query string that is not valid when you use the query method for a flat list of work items. You cannot specify a parameterized query or a query string for linked work items with the query method you specified.\",\"language\":\"\"}],[\"code\",{\"code\":\"class QueryRunner{    public WorkItemStore WorkItemStore { get; private set; }    public string TeamProjectName { get; private set; }    public string CurrentUserDisplayName { get; private set; }    public QueryRunner(string tpcUrl, string teamProjectName)    {        var tpc = TfsTeamProjectCollectionFactory.GetTeamProjectCollection(new Uri(tpcUrl));        WorkItemStore = tpc.GetService<workitemstore>();        TeamProjectName = teamProjectName;    }}</workitemstore>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"private void ResolveCurrentUserDisplayName(){    var securityService = WorkItemStore.TeamProjectCollection.GetService<igroupsecurityservice>();    var accountName = string.Format(\\\"{0}\\\\\\\\{1}\\\", Environment.UserDomainName, Environment.UserName);    var memberInfo = securityService.ReadIdentity(SearchFactor.AccountName, accountName, QueryMembership.None);    if (memberInfo != null)    {        CurrentUserDisplayName = memberInfo.DisplayName;    }    else    {        CurrentUserDisplayName = Environment.UserName;    }}private IDictionary GetParamsDictionary(){    return new Dictionary<string string=\\\"\\\" ,=\\\"\\\">()     {        { \\\"project\\\", TeamProjectName },         { \\\"me\\\", CurrentUserDisplayName }     };}</string></igroupsecurityservice>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"class PropertyChangingBase : INotifyPropertyChanged{    public event PropertyChangedEventHandler PropertyChanged;    protected void OnPropertyChanged(string propertyName)    {        if (PropertyChanged != null)        {            PropertyChanged(this, new PropertyChangedEventArgs(propertyName));        }    }}class WorkItemNode : PropertyChangingBase{    private WorkItem workItem;    public WorkItem WorkItem    {        get { return workItem; }        set        {            workItem = value;            OnPropertyChanged(\\\"WorkItem\\\");        }    }    private string relationshipToParent;    public string RelationshipToParent    {        get { return relationshipToParent; }        set        {            relationshipToParent = value;            OnPropertyChanged(\\\"RelationshipToParent\\\");        }    }    private List<workitemnode> children;    public List<workitemnode> Children    {        get { return children; }        set        {            children = value;            OnPropertyChanged(\\\"Children\\\");        }    }}</workitemnode></workitemnode>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"public List<workitemnode> RunQuery(Guid queryGuid){    // get the query    var queryDef = WorkItemStore.GetQueryDefinition(queryGuid);    var query = new Query(WorkItemStore, queryDef.QueryText, GetParamsDictionary());    // get the link types    var linkTypes = new List<workitemlinktype>(WorkItemStore.WorkItemLinkTypes);    // run the query    var list = new List<workitemnode>();    if (queryDef.QueryType == QueryType.List)    {        foreach (WorkItem wi in query.RunQuery())        {            list.Add(new WorkItemNode() { WorkItem = wi, RelationshipToParent = \\\"\\\" });        }    }    else    {        var workItemLinks = query.RunLinkQuery().ToList();        list = WalkLinks(workItemLinks, linkTypes, null);    }    return list;}</workitemnode></workitemlinktype></workitemnode>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"private List<workitemnode> WalkLinks(List<workitemlinkinfo> workItemLinks, List<workitemlinktype> linkTypes, WorkItemNode current){    var currentId = 0;    if (current != null)    {        currentId = current.WorkItem.Id;    }    var workItems = (from linkInfo in workItemLinks                     where linkInfo.SourceId == currentId                     select new WorkItemNode()                     {                         WorkItem = WorkItemStore.GetWorkItem(linkInfo.TargetId),                         RelationshipToParent = linkInfo.LinkTypeId == 0 ? \\\"Parent\\\" :                             linkTypes.Single(lt =&gt; lt.ForwardEnd.Id == linkInfo.LinkTypeId).ForwardEnd.Name                     }).ToList();    workItems.ForEach(w =&gt; w.Children = WalkLinks(workItemLinks, linkTypes, w));    return workItems;}</workitemlinktype></workitemlinkinfo></workitemnode>\",\"language\":\"csharp; ruler\"}],[\"code\",{\"code\":\"<treeview horizontalalignment=\\\"Stretch\\\" verticalalignment=\\\"Stretch\\\" itemssource=\\\"{Binding}\\\">    <treeview.itemtemplate>        <hierarchicaldatatemplate itemssource=\\\"{Binding Children}\\\">            <stackpanel orientation=\\\"Horizontal\\\">                <textblock text=\\\"{Binding WorkItem.Id, StringFormat=F0}\\\" fontweight=\\\"Bold\\\">                <textblock text=\\\"{Binding WorkItem.Title}\\\" padding=\\\"3, 0, 0, 0\\\">                <textblock text=\\\"{Binding RelationshipToParent, StringFormat=({0})}\\\" padding=\\\"3, 0, 0, 0\\\" fontstyle=\\\"Italic\\\">            </textblock></textblock></textblock></stackpanel>        </hierarchicaldatatemplate>    </treeview.itemtemplate></treeview>\",\"language\":\"xml; ruler\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-NON-EatslBg/TpK81AOND6I/AAAAAAAAATs/uCfNtwhpzgE/s1600-h/image2.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-k1VQK6W6Dfw/TpK82FdMHhI/AAAAAAAAATw/z3N1sZDsNB4/image_thumb.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"170\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-peaYX_PDyUk/TpK827D1pNI/AAAAAAAAAT0/YXwuu9kZObo/s1600-h/image5.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-XZfHQDCLAhI/TpK830-TpTI/AAAAAAAAAT4/9x4iAudJGtU/image_thumb1.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"152\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/bb130306.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.teamfoundation.workitemtracking.client.query.runquery.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.teamfoundation.workitemtracking.client.query.runlinkquery.aspx\"]],[\"a\",[\"href\",\"http://en.wiktionary.org/wiki/Google-fu\"]],[\"a\",[\"href\",\"https://skydrive.live.com/?cid=64a24e0938d6d062&sc=documents&uc=1&id=64A24E0938D6D062%21303#\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"RunQuery won’t work for Hierarchical Queries\"]]],[1,\"p\",[[0,[],0,\"Using the TFS API to display results of a flat query is fairly straightforward – once you have the \"],[0,[0],1,\"WIQL\"],[0,[],0,\" you just execute the \"],[0,[1],1,\"RunQuery()\"],[0,[],0,\" method and voila – a nice WorkItemCollection for you to enumerate over. However, if you try to execute RunQuery() on a tree or one-hop WIQL, you’ll see this error message:\"]]],[10,0],[1,\"p\",[[0,[],0,\"I was working on a proof-of-concept and needed to display the result of a Work Item Query in a WPF form. Once I saw the TF248012 error, I googled binged a bit to see if I could work out how to run a hierarchical query. That brought me to the \"],[0,[2],1,\"RunLinkQuery()\"],[0,[],0,\" method, which returns a WorkItemLinkInfo array. But that’s where my \"],[0,[3],1,\"google-fu\"],[0,[],0,\" ran out of steam – there didn’t seem to be much info about how to proceed once you’ve got this array. So here I’ll show you how I used this array to enumerate the work item hierarchy.\"]]],[1,\"p\",[[0,[],0,\"I’ve uploaded the code to my \"],[0,[4],1,\"skydrive\"],[0,[],0,\" if you want to download it. (Note: If you want to copy code from the snippets below, just double click in the code area and Cntrl-C).\"]]],[1,\"h2\",[[0,[],0,\"Phase 1 – Setting up the QueryRunner Class\"]]],[1,\"p\",[[0,[],0,\"In order to work with hierarchical results, you need to execute the query first! Here’s how I did it:\"]]],[10,1],[1,\"p\",[[0,[],0,\"I defined a class called QueryRunner that has a couple of properties:\"]]],[3,\"ul\",[[[0,[],0,\"WorkItemStore – the WorkItemStore service\"]],[[0,[],0,\"TeamProjectName – the name of the team project that contains the stored query we want to run\"]],[[0,[],0,\"CurrentUserDisplayName – the display name of the current user (more on this later)\"]]]],[1,\"p\",[[0,[],0,\"You’ll need references to Microsoft.TeamFoundation, Microsoft.TeamFoundation.Client and Microsoft.TeamFoundation.WorkItemTracking.Client, all of which can be found in the .NET tab of the Add References dialogue.\"]]],[1,\"p\",[[0,[],0,\"The constructor takes in the Url of the Team Project Collection and the Team Project Name. It then initializes the WorkItemStore.\"]]],[1,\"p\",[[0,[],0,\"Two supporting methods you’ll need are the following:\"]]],[10,2],[1,\"p\",[[0,[],0,\"ResolveCurrentUserDisplayName uses the IGroupSecurityService to resolve the current user’s display name. This is needed if your WIQL contains the “@Me” macro. You can see how it’s used in the GetParamsDictionary method.\"]]],[1,\"h2\",[[0,[],0,\"Phase 2 – Define a Hierarchical Data Structure\"]]],[1,\"p\",[[0,[],0,\"Now we’re almost ready to execute queries – we just need a data structure to hold the results. Here’s what I ended up defining:\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Phase 3 – Run the Query\"]]],[1,\"p\",[[0,[],0,\"Now we’re ready to run the query.\"]]],[10,4],[1,\"p\",[[0,[],0,\"In this method, we take in the Guid of the query we want to run (how to get that Guid is another discussion – you can see the Guids of stored queries by looking at the TeamProject.QueryHierarchy property). Also, this method is using the WIQL from the stored query, but you could just as well pass in raw WIQL too.\"]]],[1,\"p\",[[0,[],0,\"We construct a Query object passing in the WorkItemStore, the WIQL and the parameter dictionary for “@Project” and “@Me” macros. Next we get a list of all the WorkItemLinkTypes in the WorkItemStore. We’ll use these when we enumerate the work item links.\"]]],[1,\"p\",[[0,[],0,\"Finally, we decide on whether or not to run a flat query or a hierarchical query based on the stored query type. If you’re passing in WIQL instead, you’ll have to decide some other way. For flat queries, just construct a list of nodes. For hierarchical queries, get the WorkItemLinkInfo array and walk it using the following (recursive) method:\"]]],[10,5],[1,\"p\",[[0,[],0,\"This method walks the array, starting with links that have a source ID of 0 (these are top level work items in the hierarchy). For each of those work items, create a WorkItemNode and then populate the children using the current node as the “current” for the next level of recursion. When we go to the next level, the name of the WorkItemLinkType to the parent can be found using the LinkTypeId property of the WorkItemLinkInfo and finding the corresponding ForwardEnd Id in the list of WorkItemLinkTypes.\"]]],[1,\"h2\",[[0,[],0,\"Epilogue: Displaying the Result in WPF\"]]],[1,\"p\",[[0,[],0,\"Once you’ve got the tree of WorkItemNodes, displaying them in WPF is really easy. Here’s the XAML for the TreeView:\"]]],[10,6],[1,\"p\",[[0,[],0,\"We specify that the ItemTemplate for the TreeView contains hierarchical data. For each node in the tree, display the Id, Title and RelationshipToParent. Then use the “Children” property to display the next level in the hierarchy. In the code-behind for this XAML, we simply set the DataContext to the QueryRunner.RunQuery() results. Here are some screenshots of the results for a one-hop and a tree query respectively.\"]]],[10,7],[10,8],[1,\"p\",[[0,[],0,\"Happy querying!\"]]]]}","published_at":1318271820000,"status":"published","published_by":1},{"id":"95702ad7-6135-4278-b737-bcfc50a1152e","title":"Using VSTS to Test Python Code (with Code Coverage)","slug":"using-vsts-to-test-python-code-with-code-coverage","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/91750378-2cfa-495b-a5f6-af825264b2ec.png\\\" target=\\\"_blank\\\"><img width=\\\"288\\\" height=\\\"129\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/08219779-1135-43bf-80e2-f9087a65728f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2534aeeb-1d20-43ee-9945-41310a7ec541.png\\\" target=\\\"_blank\\\"><img width=\\\"281\\\" height=\\\"230\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e61c0caa-3020-4d46-8188-27fd66e94780.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2a8e693c-4001-4164-a038-eb3e7da9f3ab.png\\\" target=\\\"_blank\\\"><img width=\\\"302\\\" height=\\\"132\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d05252d7-302d-4f37-863d-439e94dca7e1.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1d354115-a9c1-41af-9630-50057828a7e7.png\\\" target=\\\"_blank\\\"><img width=\\\"318\\\" height=\\\"200\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/362b9fba-dd5c-4304-8998-66ba8504e225.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/665b7439-24b2-46a9-98fb-196994ab1e9a.png\\\" target=\\\"_blank\\\"><img width=\\\"279\\\" height=\\\"266\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/45ea0341-58af-4baf-98ad-fad4542d1d2f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c82f05c7-13d6-4413-9780-baceb95d0bd8.png\\\" target=\\\"_blank\\\"><img width=\\\"339\\\" height=\\\"169\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/2ce2e585-3411-4d7b-886d-a84c622408b7.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4dd40cfd-99ee-42a9-b179-228b9d2fa75d.png\\\" target=\\\"_blank\\\"><img width=\\\"369\\\" height=\\\"215\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7b25d688-d302-4162-a6bb-1983a926c91f.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/42081e0d-1e79-4e14-9272-91ddac06237c.png\\\" target=\\\"_blank\\\"><img width=\\\"335\\\" height=\\\"201\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3572e034-a824-4163-adb9-393a2bec1270.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/e08df916-71bd-424b-a02d-c47a35118e09.png\\\" target=\\\"_blank\\\"><img width=\\\"325\\\" height=\\\"201\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b213d4ba-5798-4615-8250-1c389cb66a13.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/48900b7f-f977-47a2-a5bd-505fd6fc4c32.png\\\" target=\\\"_blank\\\"><img width=\\\"328\\\" height=\\\"270\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4e7f26c6-076d-40aa-bd2f-c1fd51c9c945.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/bbaa4bb5-bd5f-4102-9ac9-7e1eec1306ea.png\\\" target=\\\"_blank\\\"><img width=\\\"324\\\" height=\\\"296\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7c85af86-a3fa-4bbf-abd6-a90720c1518c.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/03d94fec-57e8-4cab-bcb8-411df3902a1b.png\\\" target=\\\"_blank\\\"><img width=\\\"354\\\" height=\\\"246\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/f375f567-c826-4992-8b53-0ff340255d17.png\\\" border=\\\"0\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b89fe1f2-adcc-486a-bff1-a1a2fc167905.png\\\" target=\\\"_blank\\\"><img width=\\\"310\\\" height=\\\"295\\\" title=\\\"image\\\" style=\\\"margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/1ec474e5-75a6-4efc-9bdf-e9559eefa776.png\\\" border=\\\"0\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.com/post/why-you-absolutely-need-to-unit-test\"]],[\"a\",[\"href\",\"https://blog.startifact.com/posts/older/what-is-pythonic.html\"]],[\"a\",[\"href\",\"https://automationpanda.com/2017/03/14/python-testing-101-pytest/\"]],[\"a\",[\"href\",\"https://twitter.com/AutomationPanda\"]],[\"a\",[\"href\",\"https://github.com/AndyLPK247/python-testing-101\"]],[\"a\",[\"href\",\"https://raw.githubusercontent.com/colindembovsky/python-testing-101/master/PythonTesting-CI.json\"]],[\"a\",[\"href\",\"https://www.npmjs.com/package/vsts-coverage-styles\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I recently worked with a customer that had some containers running Python code. The code was written by data scientists and recently a dev had been hired to help the team get some real process in place. As I was helping them with their CI/CD pipeline (which used a Dockerfile to build their image, publish to Azure Container Registry and then spun up the containers in Azure Container Instances), I noted that there were no unit tests. Fortunately the team was receptive to adding tests and I didn't have to have a long discussion proving that they \"],[0,[0],1,\"absolutely need to be unit testing\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Python Testing with PyTest\"]]],[1,\"p\",[[0,[],0,\"I've done a bit of Python programming, but am by no means an expert. However, after a few minutes of searching I realized there are a ton of good Python frameworks out there. One I came across is PyTest (which is cool since it's very \"],[0,[1],1,\"pythonic\"],[0,[],0,\"). This great post (\"],[0,[2],1,\"Python Testing 101: PyTest\"],[0,[],0,\") from \"],[0,[3],1,\"Andy Knight\"],[0,[],0,\" even had a \"],[0,[4],1,\"Github repo\"],[0,[],0,\" with some code and tests. So when I got back to the hotel, I forked the repo and was able to quickly spin up a build definition in VSTS that runs the tests - with code coverage!\"]]],[1,\"h2\",[[0,[],0,\"Continuous Testing with VSTS\"]]],[1,\"p\",[[0,[],0,\"At a high level, here are the steps that your build has to perform:\"]]],[3,\"ul\",[[[0,[],0,\"Clone the repo\"]],[[0,[],0,\"Install PyTest and other packages required\"]],[[0,[],0,\"Run PyTest, instructing it to output the results to (JUnit) XML and produce (Cobertura) coverage reports\"]],[[0,[],0,\"Publish the test results\"]],[[0,[],0,\"(Optional) Fix the styling of the HTML coverage reports\"]],[[0,[],0,\"Publish the coverage reports\"]]]],[1,\"p\",[[0,[],0,\"Why the \\\"fix styles\\\" step? When we create the HTML coverage reports (so that you can see which lines of code are covered and which are not) we publish them to VSTS. However, for security reasons VSTS blocks the css styling when viewing these reports in the Build Summary page. Fortunately if you inline the styles, you get much prettier reports - so we use a node.js package to do this for us.\"]]],[1,\"p\",[[0,[],0,\"Fortunately I've already done this and published the VSTS build definition JSON file in my forked repo. Here's how you can import the code, import the CI definition and run it so you can see this in action yourself.\"]]],[1,\"h3\",[[0,[],0,\"Import the Repo from Github\"]]],[1,\"p\",[[0,[],0,\"The source code is in a Github repo - no problem! We'll import into VSTS and then we can mess with it. We can even fork the Github repo, then import it - that way we can sumbit pull requests on Github for changes we make. In this case, I'll just import the repo directly without forking.\"]]],[1,\"p\",[[0,[],0,\"Log in to VSTS and navigate to the Code hub. Then click on the repo button in the toolbar and click Import repository.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Enter the URL and click Import.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now we have the code in VSTS! Remember, this is just Git, so this is just another remote (that happens to be in VSTS).\"]]],[1,\"h3\",[[0,[],0,\"Import the Build Definition\"]]],[1,\"p\",[[0,[],0,\"Now we can import the build definition. First, navigate to the Github repo and clone it or download the \"],[0,[5],1,\"PythonTesting-CI.json\"],[0,[],0,\" file. Then open VSTS and navigate to a team project and click on Build and Release (in the Blue toolbar at the top) to navigate to the build and release hub. Click on Builds (in the grey secondary toolbar) and click the \\\"+Import\\\" button.\"]]],[10,2],[1,\"p\",[[0,[],0,\"In the import dialog, browse to the json file you downloaded previously and click Import.\"]]],[1,\"p\",[[0,[],0,\"You'll then see the build definition - there are a couple things that need to be fixed, but the steps are all there.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Note how the Agent queue is specifying \\\"Hosted Linux Preview\\\" - yep, this is running on a Linux VM. Now you don't have to do this, since Python will run on Windows, but I like the Linux agent - and it's fast too! Rename the definition if you want to.\"]]],[1,\"p\",[[0,[],0,\"Now we'll fix the \\\"Get sources\\\" section. Click on \\\"Get sources\\\" to tell the build where to get the sources from. Make sure the \\\"This account\\\" tile is selected and then set the repository to \\\"python-testing-101\\\" or whatever you named your repo when you imported. You can optionally set Tag sources and other settings.\"]]],[10,4],[1,\"p\",[[0,[],0,\"One more addition: click on the Triggers tab and enable the CI trigger:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Now you can click Save and queue to queue a new build! While it's running, let's look at the tasks.\"]]],[3,\"ol\",[[[0,[],0,\"Install Packages: this Bash task uses curl to get the pip install script, then install pip and finally install pytest and pytest-cov packages. If you're using a private agent you may not have to install pip, but pip doesn't come out of the box on the Hosted Linux agent so that's why I install it.\"]],[[0,[],0,\"Run Tests: invoke python -m pytest (which will run any tests in the current folder or subfolders), passing --junitxml=testresults.xml and the pycov args to create both an XML and HTML report\"]],[[0,[],0,\"Install fix-styles package: this just runs \\\"npm install\\\" in the root directory to install \"],[0,[6],1,\"this node.js package\"],[0,[],0,\" (you can see this if you look at the package.json file)\"]],[[0,[],0,\"Fix styles: we run the \\\"fix\\\" script from the package.json file, which just invokes the fix-styles.js file to inline the styling into the HTML coverage reports\"]],[[0,[],0,\"Publish Test Results: we publish the XML file, which is just a JUnit test result XML file. Note how under Control Options, this task is set to run \\\"Even if a previous task has failed, unless the build was cancelled\\\". This ensures that the publish step works even when tests fail (otherwise we won't get test results when tests fail).\"]],[[0,[],0,\"Publish Code Coverage: This task published the XML (Cobertura) coverage report as well as the (now style-inlined) HTML reports\"]]]],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Really simple! Let's navigate to the build run (click on the build number that you just queued) and - oh dear, the tests failed!\"]]],[10,6],[1,\"p\",[[0,[],0,\"Seems there is a bug in the code. Take a moment to see how great the test result section is - even though there are failing tests. Then click on Tests to see the failing tests:\"]]],[10,7],[1,\"p\",[[0,[],0,\"All 4 failing tests have \\\"subtract\\\" in them - easy to guess that we have a problem in the subtract method! If we click on a test we can also see the stack trace and the failed assertions from the test failure. Click on the \\\"Bug\\\" button above the test to log a bug with tons of detail!\"]]],[10,8],[1,\"p\",[[0,[],0,\"Just look at that bug: with a single button click we have exception details, stack traces and links to the failing build. Sweet!\"]]],[1,\"p\",[[0,[],0,\"Now let's fix the bug: click on the Code hub and navigate to example-py-pytest/com/automationpanda/example and click on the calc_func.py file. Yep, there's a problem in the subtract method:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Click on the Edit button and change that pesky + to a much better -. Note, this isn't what you'd usually do - you'd normally create a branch from the Bug, pull the repo, fix the bug and push. Then you'd submit a PR. For the sake of this blog, I'm just fixing the code in the code editor.\"]]],[1,\"p\",[[0,[],0,\"Click the Commit button to save the change. In \\\"Work items to link\\\" find the Bug that we created earlier and select it. Then click Commit.\"]]],[10,10],[1,\"p\",[[0,[],0,\"The commit will trigger a new build! Click on Build and you'll see a build is already running. Click on the build number to open the build.\"]]],[1,\"p\",[[0,[],0,\"This time it's a success! Click on the build number to see the report - this time, we see all the tests are passing and we have 100% coverage - nice!\"]]],[10,11],[1,\"p\",[[0,[],0,\"If you click on \\\"Code Coverage*\\\" just below the header, you'll see the (prettified) HTML reports. Normally you won't have 100% coverage and you'll want to see which methods have coverage and which don't - you would do so by browsing your files here and noting which lines are covered or not by the color highlighting:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Also note that we can see that this build is related to the Bug (under Associated work items). It's almost like we're professional developers…\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Just because there is \\\"Visual Studio\\\" in the name, it doesn't mean that VSTS can't do Python - and do it really, really well! You get detailed test logging, continuous integration, code coverage reports and details - and for very little effort. If you're not testing your Python code - just do it ™ with VSTS!\"]]]]}","published_at":1519526711000,"status":"published","published_by":1},{"id":"65a85990-3924-4088-8866-9860cf8045b1","title":"Using WebDeploy in vNext Releases","slug":"using-webdeploy-in-vnext-releases","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6ae1308a-fe3e-4a9f-90ab-373adc8088f9.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b5ea780d-7a2d-4000-bc0a-6a20c0e0e33b.png\\\" width=\\\"199\\\" height=\\\"100\\\"></a>\"}],[\"code\",{\"code\":\"Param(\\n  [string]$srcPath = $env:TF_BUILD_SOURCESDIRECTORY,\\n  [string]$binPath = $env:TF_BUILD_BINARIESDIRECTORY,\\n  [string]$pathToCopy\\n)\\n\\ntry\\n{\\n    $sourcePath = \\\"$srcPath\\\\$pathToCopy\\\"\\n    $targetPath = \\\"$binPath\\\\$pathToCopy\\\"\\n\\n    if (-not(Test-Path($targetPath))) {\\n        mkdir $targetPath\\n    }\\n\\n    xcopy /y /e $sourcePath $targetPath\\n\\n    Write-Host \\\"Done!\\\"\\n}\\ncatch {\\n    Write-Host $_\\n    exit 1\\n}\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/394bb737-3143-4f8e-a417-415513160dcd.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/48ac1d7f-c1eb-4fec-8302-b123c8c50bed.png\\\" width=\\\"644\\\" height=\\\"99\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/63d1eacf-5a3d-449a-a373-ada38cc3789c.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/158e72c7-51d9-472f-b29b-6d29cbbe9fa6.png\\\" width=\\\"644\\\" height=\\\"350\\\"></a>\"}],[\"code\",{\"code\":\"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?&gt;\\n&lt;parameters&gt;\\n  &lt;setParameter name=\\\"IIS Web Application Name\\\" value=\\\"__SiteName__\\\" /&gt;\\n  &lt;setParameter name=\\\"FabrikamFiber-Express-Web.config Connection String\\\" value=\\\"__FabFiberExpressConStr__\\\" /&gt;\\n&lt;/parameters&gt;\\n\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"Configuration Test\\n{\\n    params (\\n        [string]$logLocation\\n    )\\n\\n    Node myNode\\n    {\\n        Log LogLocation\\n        {\\n            Message = \\\"The log location is [$logLocation]\\\"\\n        }\\n\\n        Script DoSomething\\n        {\\n            Get-Script { @{ \\\"DoSomething\\\" = \\\"Yes\\\" } }\\n            Test-Script { $false }\\n            Set-Script\\n            {\\n                Write-Host \\\"Log location is [$logLocation]\\\"\\n                $localParam = \\\"Hello there\\\"\\n                Write-Host \\\"LocalParam is [$localParam]\\\"\\n            }\\n        }\\n    }\\n}\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"#@{\\n$configData = @{\\n    AllNodes = @(\\n        @{\\n            NodeName = \\\"*\\\"\\n            PSDscAllowPlainTextPassword = $true\\n         },\\n\\n        @{\\n            NodeName = \\\"fabfiberserver\\\"\\n            Role = \\\"WebServer\\\"\\n         },\\n\\n        @{\\n            NodeName = \\\"fabfiberdb\\\"\\n            Role = \\\"SqlServer\\\"\\n         }\\n    );\\n}\\n\\n# Note: different 1st line for RM or command line\\n# use $configData = @{ for RM\\n# use @{ for running from command line\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"Configuration FabFibWeb_Db\\n{\\n    param (\\n        [Parameter(Mandatory=$true)]\\n        [ValidateNotNullOrEmpty()]\\n        [String]\\n        $PackagePath\\n    )\\n\\n    Node fabfiberdb #$AllNodes.where{ $_.NodeName -ne \\\"*\\\" -and $_.Role.Contains(\\\"SqlServer\\\") }.NodeName\\n    {\\n        Log DeployAppLog\\n        {\\n            Message = \\\"Starting SqlServer node configuration. PackagePath = $PackagePath\\\"\\n        }\\n\\n        #\\n        # Update the application database\\n        #\\n        File CopyDBSchema\\n        {\\n            Ensure = \\\"Present\\\"\\n            SourcePath = \\\"$PackagePath\\\\FabrikamFiber.Schema.dacpac\\\"\\n            DestinationPath = \\\"c:\\\\temp\\\\dbFiles\\\\FabrikamFiber.Schema.dacpac\\\"\\n            Type = \\\"File\\\"\\n        }\\n\\n        Script DeployDacPac\\n        {\\n            GetScript = { @{ Name = \\\"DeployDacPac\\\" } }\\n            TestScript = { $false }\\n            SetScript =\\n            {\\n                $cmd = \\\"&amp; 'C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\DAC\\\\bin\\\\sqlpackage.exe' /a:Publish /sf:c:\\\\temp\\\\dbFiles\\\\FabrikamFiber.Schema.dacpac /tcs:'server=localhost; initial catalog=FabrikamFiber-Express'\\\"\\n                Invoke-Expression $cmd | Write-Verbose\\n            }\\n            DependsOn = \\\"[File]CopyDBSchema\\\"\\n        }\\n\\n        Script CreateLabUser\\n        {\\n            GetScript = { @{ Name = \\\"CreateLabUser\\\" } }\\n            TestScript = { $false }\\n            SetScript = \\n            {\\n                $sql = @\\\"\\n                    USE [master]\\n                    GO\\n\\n                    IF (NOT EXISTS(SELECT name from master..syslogins WHERE name = 'Lab'))\\n                    BEGIN\\n                        CREATE LOGIN [lab] WITH PASSWORD=N'P2ssw0rd', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=OFF, CHECK_POLICY=OFF\\n    \\n                        BEGIN\\n                            USE [FabrikamFiberExpress]\\n                        END\\n\\n                        CREATE USER [lab] FOR LOGIN [lab]\\n                        ALTER ROLE [db_owner] ADD MEMBER [lab]\\n                    END\\n\\\"@\\n                \\n                $cmdPath = \\\"c:\\\\temp\\\\dbFiles\\\\createLogin.sql\\\"\\n                sc -Path $cmdPath -Value ($sql -replace '\\\\n', \\\"`r`n\\\")\\n                \\n                &amp; \\\"C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\sqlcmd.exe\\\" -S localhost -U sa -P P2ssw0rd -i $cmdPath\\n            }\\n        }\\n    }\\n}\\n\\n# command for RM\\nFabFibWeb_Db -ConfigurationData $configData -PackagePath $applicationPath\\n\\n# test from command line\\n#FabFibWeb -ConfigurationData configData.psd1 -PackagePath \\\"\\\\\\\\rmserver\\\\builddrops\\\\__ReleaseSite\\\\__ReleaseSite_1.0.0.3\\\"\\n#Start-DscConfiguration -Path .\\\\FabFibWeb -Verbose -Wait\\n\",\"language\":\"ps;\"}],[\"code\",{\"code\":\"Configuration FabFibWeb_Site\\n{\\n    param (\\n        [Parameter(Mandatory=$true)]\\n        [ValidateNotNullOrEmpty()]\\n        [String]\\n        $PackagePath\\n    )\\n\\n    Node fabfiberserver #$AllNodes.where{ $_.NodeName -ne \\\"*\\\" -and $_.Role.Contains(\\\"WebServer\\\") }.NodeName\\n    {\\n        Log WebServerLog\\n        {\\n            Message = \\\"Starting WebServer node configuration. PackagePath = $PackagePath\\\"\\n        }\\n\\n        #\\n        # Deploy a website using WebDeploy\\n        #\\n        File CopyWebDeployFiles\\n        {\\n            Ensure = \\\"Present\\\"         \\n            SourcePath = \\\"$PackagePath\\\\_PublishedWebsites\\\\FabrikamFiber.Web_Package\\\"\\n            DestinationPath = \\\"c:\\\\temp\\\\Site\\\"\\n            Recurse = $true\\n            Force = $true\\n            Type = \\\"Directory\\\"\\n        }\\n\\n        Script SetConStringDeployParam\\n        {\\n            GetScript = { @{ Name = \\\"SetDeployParams\\\" } }\\n            TestScript = { $false }\\n            SetScript = {\\n                $paramFilePath = \\\"c:\\\\temp\\\\Site\\\\FabrikamFiber.Web.SetParameters.xml\\\"\\n\\n                $paramsToReplace = @{\\n                    \\\"__FabFiberExpressConStr__\\\" = \\\"data source=fabfiberdb;database=FabrikamFiber-Express;User Id=lab;Password=P2ssw0rd\\\"\\n                    \\\"__SiteName__\\\" = \\\"Default Web Site\\\\FabrikamFiber\\\"\\n                }\\n\\n                $content = gc $paramFilePath\\n                $paramsToReplace.GetEnumerator() | % {\\n                    $content = $content.Replace($_.Key, $_.Value)\\n                }\\n                sc -Path $paramFilePath -Value $content\\n            }\\n            DependsOn = \\\"[File]CopyWebDeployFiles\\\"\\n        }\\n        \\n        Script DeploySite\\n        {\\n            GetScript = { @{ Name = \\\"DeploySite\\\" } }\\n            TestScript = { $false }\\n            SetScript = {\\n                &amp; \\\"c:\\\\temp\\\\Site\\\\FabrikamFiber.Web.deploy.cmd\\\" /Y\\n            }\\n            DependsOn = \\\"[Script]SetConStringDeployParam\\\"\\n        }\\n\\n        #\\n        # Ensure App Insights cloud monitoring for the site is enabled\\n        #\\n        Script AppInsightsCloudMonitoring\\n        {\\n            DependsOn = \\\"[Script]DeploySite\\\"\\n            GetScript = \\n            {\\n                @{\\n                    WebApplication = 'Default Web Site/FabrikamFiber';\\n                }\\n            }\\n            TestScript =\\n            {\\n                $false\\n            }\\n            SetScript =\\n            {\\n                # import module - requires change to PSModulePath for this session\\n                $mod = Get-Module -Name Microsoft.MonitoringAgent.PowerShell\\n                if ($mod -eq $null)\\n                {\\n                    $env:PSModulePath = $env:PSModulePath + \\\";C:\\\\Program Files\\\\Microsoft Monitoring Agent\\\\Agent\\\\PowerShell\\\\\\\"\\n                    Import-Module Microsoft.MonitoringAgent.PowerShell -DisableNameChecking\\n                }\\n        \\n                Write-Verbose \\\"Starting cloud monitoring on FabFiber site\\\"\\n                Start-WebApplicationMonitoring -Cloud -Name 'Default Web Site/FabrikamFiber'\\n            }\\n        }\\n    }\\n}\\n\\n# command for RM\\nFabFibWeb_Site -ConfigurationData $configData -PackagePath $applicationPath\\n\\n# test from command line\\n#FabFibWeb -ConfigurationData configData.psd1 -PackagePath \\\"\\\\\\\\rmserver\\\\builddrops\\\\__ReleaseSite\\\\__ReleaseSite_1.0.0.3\\\"\\n#Start-DscConfiguration -Path .\\\\FabFibWeb -Verbose -Wait\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/19e05d8e-b16c-41ac-ac41-e4dc6d5b2c4f.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6bddda2e-8136-4ba1-8a15-03989bb30af9.png\\\" width=\\\"644\\\" height=\\\"221\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/db1b526e-b3f7-4bdc-8fa7-e195118e41cf.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/3422cd7a-b93d-4e8a-a0cf-cd28bb087d81.png\\\" width=\\\"644\\\" height=\\\"160\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/560818af-d355-4e20-a6bf-b34bb0848b5a.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/06d1a702-41cd-479d-9eec-300ecfb0ecbd.png\\\" width=\\\"581\\\" height=\\\"484\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d9dd1e64-6503-4033-ae69-8822fd4f9c45.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/31dbb82e-70e4-4738-a035-d9585801d034.png\\\" width=\\\"644\\\" height=\\\"279\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/7011f11c-27f2-4808-865f-06388d205e29.png\\\" target=\\\"_blank\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/4572c2dc-1135-4784-be02-c4be8eeec65c.png\\\" width=\\\"644\\\" height=\\\"279\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://blogs.msdn.com/b/visualstudioalm/archive/2014/07/07/how-to-setup-environments-for-agent-less-deployments-in-release-management-release-management-2013-with-update-3-rc.aspx\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/using-powershell-dsc-in-release-management-the-hidden-manual\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/more-dsc-release-management-goodness-readying-a-webserver-for-deployment\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/install-and-configure-sql-server-using-powershell-dsc\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/new-vnext-config-variable-options-in-rm-update-4-rc\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/automated-buildswhy-theyre-absolutely-essential-(part-1)\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/webdeploy-and-release-management--the-proper-way\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/matching-binary-version-to-build-number-version-in-tfs-2013-builds\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/data/tools.aspx\"]],[\"a\",[\"href\",\"http://technet.microsoft.com/en-us/library/cc180827.aspx\"]],[\"em\"],[\"a\",[\"href\",\"https://social.technet.microsoft.com/Forums/en-US/2eb97d67-f1fb-4857-8840-de9c4cb9cae0/dsc-configuration-data-for-script-resources?forum=winserverpowershell\"]],[\"a\",[\"href\",\"http://www.hanselman.com/blog/WebDeploymentMadeAwesomeIfYoureUsingXCopyYoureDoingItWrong.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"A few months ago Release Management (RM) Update 3 preview was released. One of the big features in that release was the ability to \"],[0,[0],1,\"deploy without agents using PowerShell DSC\"],[0,[],0,\". Once I saw this feature, I started a journey to see how far I could take deployments using this amazing technology. I had to learn how DSC worked, and from there I had to figure out how to use DSC with RM! The ride was a bit rocky at first, but I feel comfortable with what I am able to do using RM with PowerShell DSC.\"]]],[1,\"h2\",[[0,[],0,\"Readying Environments for Deployment\"]]],[1,\"p\",[[0,[],0,\"In my mind there were two distinct steps that I wanted to be able to manage using RM/DSC:\"]]],[3,\"ul\",[[[0,[],0,\"Configure an environment (set of machines) to make them ready to run my application\"]],[[0,[],0,\"Deploy my application to these servers\"]]]],[1,\"p\",[[0,[],0,\"The RM/DSC posts I’ve blogged so far deal with readying the environment:\"]]],[3,\"ul\",[[[0,[1],1,\"Using PowerShell DSC in Release Management: The Hidden Manual\"],[0,[],0,\"\"]],[[0,[2],1,\"More DSC Release Management Goodness: Readying a Webserver for Deployment\"],[0,[],0,\"\"]],[[0,[3],1,\"Install and Configure SQL Server using PowerShell DSC\"],[0,[],0,\"\"]],[[0,[4],1,\"New vNext Config Variable Options in RM Update 4 RC\"]]]],[1,\"p\",[[0,[],0,\"So we’re now at a point where we can ensure that the machines that we want to deploy our application to are ready for our application – in the case of a SQL server SQL is installed and configured correctly. In the case of a webserver, IIS is installed and configured, additional runtimes are present (like MVC) and Webdeploy is installed and ports opened so that I can deploy using Webdeploy. So how then do I deploy my application?\"]]],[1,\"h2\",[[0,[],0,\"Good Packages\"]]],[1,\"p\",[[0,[],0,\"Good deployment always beings with good packages. To get a good package, you’ll need an \"],[0,[5],1,\"automated build\"],[0,[],0,\" that ties into source control (and hopefully work items) and performs automated unit testing with coverage. This gives you some metrics as to the quality of your builds. The next critical piece that you’ll need is to make sure that you can manage multiple configurations – after all, you’ll be wanting to deploy the same package to Production that you deployed and testing in UAT, so the package shouldn’t have configuration hard-coded in. In my agent-based \"],[0,[6],1,\"Webdeploy/RM post\"],[0,[],0,\", I show how you can create a team build that puts placeholders into the SetParameters.xml file, so that you can put in environment-specific values when you deploy. The package I created for that deployment process can be used for deployment via DSC as well – just showing that if you create a good package during build, you have more release options available to you.\"]]],[1,\"p\",[[0,[],0,\"Besides the package, you’ll want to source control your DSC scripts. This way you can track changes that you make to your scripts over time. Also, having the scripts “travel” with your binaries means you only have to look in one location to find both deployment packages (or binaries) and the scripts you need to deploy them. Here’s how I organized my website and scripts in TF Version Control:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The actual solution (with my websites, libraries and database schema project) is in the FabrikamFiber.CallCenter folder. I have some 3rd party libraries that are checked into the lib folder. The build folder has some utilities for running the build (like the xunit test adapter). And you can also see the DscScripts folder where I keep the scripts for deploying this application.\"]]],[1,\"p\",[[0,[],0,\"By default on a team build, only compiled output is placed into the drop folder – you don’t typically get any source code. I haven’t included the scripts in my solution or projects, so I used a post-build script to copy the scripts from the source folder to the bin folder during the build – the build then copies everything in the bin folder to the drop folder. You could use this technique if you wanted to share scripts with multiple solutions – in that case you’d have the scripts in a higher level folder in SC. Here’s the script:\"]]],[10,1],[3,\"ul\",[[[0,[],0,\"Lines 2-3: you can use the $env parameters that get set when team build executes a custom script. Here I am using the sources and binaries directory settings.\"]],[[0,[],0,\"Line 4: the subfolder to copy from the $srcPath to the $binPath.\"]],[[0,[],0,\"Line 12-14: ensure that the target path exists.\"]],[[0,[],0,\"Line 16: xcopy the files to the target folder.\"]]]],[1,\"p\",[[0,[],0,\"Calling the script with $pathToCopy set to DscScripts will result in my DSC scripts being copied to the drop folder along with my build binaries. Using the TFVC 2013 default template, here’s what my advanced build parameters look like:\"]]],[10,2],[3,\"ul\",[[[0,[],0,\"The MSBuild arguments build a Webdeploy package for me. The profile (specified when you right-click the project and select “Publish”) also inserts RM placeholders into environment specific settings (like connection strings, for example). I don’t hard-code the values since this same package can be deployed to multiple environments. Later we’ll see how the actual values replace the tokens at deploy time.\"]],[[0,[],0,\"The post-build script is the script above, and I pass “-pathToCopy DscScripts” to the script in order to copy the scripts to the bin (and ultimately the drop) folder.\"]],[[0,[],0,\"I also use a pre-build script to version my assemblies so that I can match the \"],[0,[7],1,\"binary file versions with the build\"],[0,[],0,\".\"]]]],[1,\"p\",[[0,[],0,\"Here’s what my build output folders look like:\"]]],[10,3],[1,\"p\",[[0,[],0,\"There are 3 “bits” that I really care about here:\"]]],[3,\"ul\",[[[0,[],0,\"The DscScripts folder has all the scripts I need to deploy this application.\"]],[[0,[],0,\"The FabrikamFiber.Schema.dacpac is the binary of my database schema project.\"]],[[0,[],0,\"The _PublishedWebsites folder contains 2 folders: the “xcopyable” site (which I ignore) and the FabrikamFiber.Web_package folder which is shown on the right in the figure above, containing the cmd file to execute WebDeploy, the SetParameters.xml file for configuration and the zip file containing the compiled site.\"]]]],[1,\"p\",[[0,[],0,\"Here’s what my SetParameters file looks like:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Note the “__” (double underscore) pre- and post-fix, making SiteName and FabFiberExpressConStr parameters that I can use in both agent-based and agent-less deployments.\"]]],[1,\"p\",[[0,[],0,\"Now that all the binaries and scripts are together, we can look at how to do the deployment.\"]]],[1,\"h2\",[[0,[],0,\"Deploying a DacPac\"]]],[1,\"p\",[[0,[],0,\"To deploy the database component of my application, I want to use the DacPac (the compiled output of my \"],[0,[8],1,\"SSDT\"],[0,[],0,\" project). The DacPac is a “compiled model” of how I want the database to look. To deploy a DacPac, you invoke sqlpackage.exe (installed with SQL Server Tools when you install and configure SQL Server). SqlPackage then reverse engineers the target database (the database you’re deploying the model to) into another model, does a compare and produces a diff script. You can also make SqlPackage run the script (which will make the target database look exactly like the DacPac model you compiled your project into).\"]]],[1,\"p\",[[0,[],0,\"To do this inside a DSC script, I implement a “Script” resource. The Script resource has 3 parts: a Get-Script, a Set-Script and a Test-Script. The Get-Script is executed when you run DSC in interrogative mode – it won’t change the state of the target node at all. The Test-Script is used to determine if any action must be taken – if it return $true, then no action is taken (the target is already in the desired state). If the Test-Script returns $false, then the target node is not in the desired state and the Set-Script is invoked. The Set-Script is executed in order to bring the target node into the desired state.\"]]],[1,\"h3\",[[0,[],0,\"A Note on Script Resource Parameters\"]]],[1,\"p\",[[0,[],0,\"A caveat here though: the Script resource can be a bit confusing in terms of parameters. The DSC script actually has 2 “phases” – first, the PowerShell script is “compiled” into a \"],[0,[9],1,\"mof file\"],[0,[],0,\". This file is then pushed to the target server and executed during the “deploy” phase. The parameters that you use in the configuration script are available on the RM server at “compile” time, while parameters in the Script resources are only available on the target node during “deploy” time. That means that you can’t pass a parameter from the config file “into” the Script resource – all parameters in the Script resource need to be hard-coded or calculated on the target node at execution time.\"]]],[1,\"p\",[[0,[],0,\"For example, let’s look at this example script:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Here the intent is to have a parameter called $logLocation that we pass into the config script. When you see this script, it seems to make perfect sense – however, while the log will show the message “The log location is [c:\\\\temp]”, for example (line 11), when the Set-Script of the Script resource runs on the target node, you’ll see the message “Log location is []” (Line 20). Why? Because the $logLocation parameter does not exist when this script is run \"],[0,[10],1,\"at deploy time on the target node\"],[0,[],0,\". The parameter is available to the Log resource (or other resources like File) but won’t be to the Script resource. You will be able to create other parameters “at deploy time” (like $localParam on Line 21). This is frustrating, but kind of understandable. The Script resource script blocks are not evaluated for parameters. I found a \"],[0,[11],1,\"string manipulation hack\"],[0,[],0,\" that allows you to fudge config parameters into the script blocks, but decided against using it.\"]]],[1,\"h3\",[[0,[],0,\"ConfigData\"]]],[1,\"p\",[[0,[],0,\"Before we look at the DSC script used to deploy the database, I need to show you my configData script:\"]]],[10,6],[3,\"ul\",[[[0,[],0,\"Line 1: When running from the command line, you just specify a hash-table. DSC requires this hash-table to be put into a variable. I have both in the script (though I default to the format RM requires) just so that I can test the script outside of RM.\"]],[[0,[],0,\"Line 3: AllNodes is a hash-table of all the nodes I want to affect with my configuration scripts.\"]],[[0,[],0,\"Lines 5/6 – common properties for all nodes (the name is “*” so DSC applies these properties to all nodes).\"]],[[0,[],0,\"Line 10/11 and 15/16: I specify the nodes I have as well as a Role property. This is so that I can deploy the same configuration to multiple servers that have the same role (like a web farm for example).\"]],[[0,[],0,\"You can specify other parameters, each with another value for each server.\"]]]],[1,\"p\",[[0,[],0,\"Here’s the DSC script I use to deploy a DacPac to a target server:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Let’s take a look at what is going on:\"]]],[3,\"ul\",[[[0,[],0,\"Line 7: I need a parameter to tell me where the DacPac is – this will be my build drops folder.\"]],[[0,[],0,\"Line 10: I specify the node I want to bring into the desired state. I wanted to apply this config to all nodes that have the role “SqlServer” and this worked from the command line – for some reason I couldn’t get it to work with RM, so I hardcode the node-name here. I think this is particular to my environment, since this should work.\"]],[[0,[],0,\"Lines 12-15: Log a message.\"]],[[0,[],0,\"Lines 20-26: Use the File resource to copy the DacPac from a subfolder in the $PackagePath to a known folder on the local machine. I did this because I couldn’t pass the drop-folder path in to the Script resource – so I copied using the File Resource to a known location and can just “hard code” that location in my Script resources.\"]],[[0,[],0,\"Line 28: This is the start of the script Resource for invoking sqlpackage.exe.\"]],[[0,[],0,\"Line 30: Just return the name of the resource.\"]],[[0,[],0,\"Line 31: Always return false – meaning that the Set-Script will always be run. You could have some check here if you didn’t want the script to execute for some specific condition.\"]],[[0,[],0,\"Lines 32-36: This is the script that actually does the work – I create the command and then Invoke it, piping output to the verbose log for logging. I use “/a:Publish” to tell SqlPackage to execute the incremental changes on the database, using the DacPac as the source file (/sf) and targeting the database specified in the target connection string (/tcs).\"]],[[0,[],0,\"Line 37: Invoking the DacPac is dependent on the DacPac being present, so I express the dependency.\"]],[[0,[],0,\"The final resource in this script is also a Script resource – the Get- and Test-Scripts are self-explanatory. The Set-Script takes the SQL string I have in the script, writes it to a file (using sc – Set-Content) and then executes the file using sqlcmd.exe. This is specific to my environment, but shows that you can execute arbitrary SQL against a server fairly easily using the Script resource.\"]],[[0,[],0,\"Line 73: When using DSC with RM, you need to compile the configuration (do this by invoking the Configuration) into mof files. Don’t call Start-DscConfiguration (which pushes the mof files to the target nodes for running the configuration) since RM will do this step. You can see how I use $applicationPath – this is the path that you specify when you create the vNext component (relative to a drop folder) – we’ll see later how to set this up. RM sets this parameter when before it calls the script. Also, you need to specify the parameter that contains the configuration hash-table. In my case this is $configData, which you’ll see at the top of the configData script above. RM “executes” this script so the parameter is in memory by the time the DSC script is executed.\"]]]],[1,\"p\",[[0,[],0,\"When working with DSC, you have to think about idempotency. In other words, the script must produce the same result every time you run it – no matter what the starting state is. Since deploying a DacPac to a database is already idempotent, I don’t have too much to worry about in this case, so that’s why the Test-Script for the DeployDacPac Script resource always returns false.\"]]],[1,\"h2\",[[0,[],0,\"Deploying a Website using WebDeploy\"]]],[1,\"p\",[[0,[],0,\"You could be publishing your website out of Visual Studio. But don’t – seriously, \"],[0,[5],1,\"don’t EVER do this\"],[0,[],0,\". So you’re smart: you’ve got an automated build to compile your website. Well done! Now you could be deploying this site using xcopy. Don’t – primarily because managing configuration is hard to do using this method, and you usually end up deploying all sorts of files that you don’t actually require (like web.debug.config etc.). You should be using \"],[0,[12],1,\"WebDeploy\"],[0,[],0,\"!\"]]],[1,\"p\",[[0,[],0,\"I’ve got a post about \"],[0,[6],1,\"how to use WebDeploy with agent-based templates\"],[0,[],0,\". What follows is how to deploy sites using WebDeploy in vNext templates (using PowerShell DSC). In a \"],[0,[2],1,\"previous post\"],[0,[],0,\" I show how you can use DSC to ready a webserver for your application. Now we can look at what we need to do to actually deploy a site using WebDeploy. Here’s the script I use:\"]]],[10,8],[1,\"p\",[[0,[],0,\"You’ll see some similarities to the database DSC script – getting nodes by role (“WebServer” this time instead of “SqlServer”), Log resources to log messages and the “compilation” command which passes in the $configData and $applicationPath.\"]]],[3,\"ul\",[[[0,[],0,\"Lines 20-28: I copy the entire FabrikamFiber.Web_package folder (containing the cmd, SetParameters and zip file) to a temp folder on the node.\"]],[[0,[],0,\"Line 30: I use a Script Resource to do config replacement.\"]],[[0,[],0,\"Lines 32-33: Always execute the Set-Script, and return the name of the resource when interrogating the target system.\"]],[[0,[],0,\"Lines 34-47: The “guts” of this script – replacing the tokens in the SetParameters file with real values and then invoking WebDeploy.\"]],[[0,[],0,\"Line 35: Set a parameter to the known local location of the SetParameters file.\"]],[[0,[],0,\"Lines 37-40: Create a hash-table of key/value pairs that will be replaced in the SetParameters file. I have 2: the site name and the database connection string. You can see the familiar __ pre- and post-fix for the placeholders names – I can use this same package in agent-based deployments if I want to.\"]],[[0,[],0,\"Line 42: read in the contents of the SetParameters file.\"]],[[0,[],0,\"Lines 43-45: Replace the token placeholders with the actual values from the hash-table.\"]],[[0,[],0,\"Line 46: overwrite the SetParameters file – it now has actual values instead of just placeholder values.\"]],[[0,[],0,\"Lines 51-59: I use another Script resource to execute the cmd file (invoking WebDeploy).\"]],[[0,[],0,\"Lines 64-90: This is optional – I include it here as a reference of how to ensure that the site is being monitored using Application Insights once it’s deployed.\"]]]],[1,\"h2\",[[0,[],0,\"The Release\"]]],[1,\"p\",[[0,[],0,\"In order to run vNext (a.k.a. agent-less a.k.a DSC) deployments, you need to import your target nodes. Since vNext servers are agent-less, you don’t need to install anything on the target node. You just need to make sure you can run remote PowerShell commands against the node and have the username/password for doing so. When adding a new server, just type in the name of the machine and specify the remote port, which is 5985 by default. This adds the server into RM as a “Standard” server. These servers always show their status as “Ready”, but this can be misleading since there is no agent. You can then compose your servers into “Standard Environments”. Next you’ll want to create a vNext Release Path (which specifies the environments you’re deploying to as well as who is responsible for approvals).\"]]],[10,9],[10,10],[1,\"p\",[[0,[],0,\"You can specify other \"],[0,[4],1,\"configuration variables and defaults in RM Update 4 RC\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"vNext Components\"]]],[1,\"p\",[[0,[],0,\"In order to use the binaries and scripts we’ve created, we need to specify a vNext component in RM. Here’s how I specify the component:\"]]],[10,11],[1,\"p\",[[0,[],0,\"All this is really doing is setting the value of the $packagePath (which I set to the root of the drop folder here). Also note how I only need a single component even though I have several scripts to invoke (as we’ll see next).\"]]],[1,\"h3\",[[0,[],0,\"The vNext Template\"]]],[1,\"p\",[[0,[],0,\"I create a new vNext template. I select a vNext release path. I right-click the “Components” node in the toolbox and add in the vNext component I just created. Since I am deploying to (at least) 2 machines, I drag a “Parallel” activity onto the design surface. On the left of the parallel, I want scripts for my SQL servers. On the right, I want scripts for my webservers. Since I’ve already installed SQL on my SQL server, I am not going to use that script – I’ll just deploy my database model. On the webserver, I want to run the prerequisites script (to make sure IIS, Webdeploy, MVC runtime and the MMA agent are all installed and correctly configured. Then I want to deploy my website using Webdeploy. So I drag on 3 “Deploy using PS/DSC” activities. I select the appropriate server and component from the “Server” and “Component” drop-downs respectively. I set the username/password for the identity that RM will use to remote onto the target nodes. Then I set the path to the scripts (relative to the root of the drop folder, which is the “Path to Package” in the component I sepcified (which becomes $applicationPath inside the DSC script). I also set the path to the PsConifgurationPath to my configData.psd1 script. Finally I set UseCredSSP and UseHTTPS both to false and SkipCaCheck to true (you can vary these according to your environment).\"]]],[10,12],[1,\"p\",[[0,[],0,\"Now I can trigger the release (either through the build or manually). Here’s what a successful run looks like and a snippet of one of the logs:\"]]],[10,13],[1,\"h2\",[[0,[],0,\"To Agent or Not To Agent?\"]]],[1,\"p\",[[0,[],0,\"Looking at the features and improvements to Release Management Update 3 and Update 4, it seems that the TFS product team are not really investing in agent-based deployments and templates any more. If you’re using agent-based deployments, it’s a good idea to start investing in DSC (or at the very least just plain ol’ PowerShell) so that you can use agent-less (vNext) deployments. As soon as I saw DSC capabilities in Update 3, I guessed this was the direction the product team would pursue, and Update 4 seems to confirm that guess. While there is a bit of a learning curve, this technology is very powerful and will ultimately lead to better deployments – which means better quality for your business and customers.\"]]],[1,\"p\",[[0,[],0,\"Happy deploying!\"]]]]}","published_at":1414424601000,"status":"published","published_by":1},{"id":"332495e1-19e0-454f-b59e-59d205183221","title":"VS 2012 RC – Fakes Bugs Fixed","slug":"vs-2012-rc--fakes-bugs-fixed","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<font size=\\\"3\\\" face=\\\"Courier New\\\"> <p></p><pre>Failed to configure settings for runsettings plugin ‘Fakes’ as it threw following exception:<br>‘Object reference not set to an instance of an object’<br>Please contact the plugin author.<br></pre></font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-1MVUGWXy5D0/T-GlfdEgggI/AAAAAAAAAYw/S_ebiSuuo2U/s1600-h/image%25255B3%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-XX9_uZrD4ZA/T-GlgTnpT-I/AAAAAAAAAY4/dFyQv0M3kuY/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"466\\\" height=\\\"141\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2012/04/more-on-fakes-beta-has-issues.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In a \"],[0,[0],1,\"previous post\"],[0,[],0,\" about the MS Fakes framework, I made mention of some bugs that appeared in the Beta. I finally had some time to test out the same code in the RC, and I am pleased to tell you that the bugs have been fixed (well, the ones I found anyway!).\"]]],[1,\"h2\",[[0,[],0,\"Upgrading Gotcha\"]]],[1,\"p\",[[0,[],0,\"The only gotcha I came across appears to be a snag when you open in the RC a test project that was created in the Beta. I initially simply opened up the code that I had from the Beta, updated references to TeamFoundation dlls from version 10 to version 11, regenerated the fakes and tried to run the tests. FAIL – the tests were failing with “ShimNotSupportedExceptions”. After trying various things, I noticed some text in the Output window from the “Tests” section (the Show output from dropdown):\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[10,1],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"For some reason, the Fakes framework wasn’t generating fakes for the v 11 assemblies correctly. I eventually created a new Test project in VS 2012 RC, copied across the code and re-added references and fakes, and voila, everything is working great!\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"I’m not sure if this is a bug or not, so if you’re getting this error, my suggestion is to just create a new Test project in the RC and copy the code across.\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"Now back to (fake) testing…\"]]]]}","published_at":1340220420000,"status":"published","published_by":1},{"id":"09bc52c9-4c36-49ac-8f7b-40759f10d6fe","title":"VSTS, One Team Project and Inverse Conway Maneuver","slug":"vsts-one-team-project-and-inverse-conway-maneuver","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://www.youtube.com/watch?v=Cu0AU8vw3xw\"]],[\"a\",[\"href\",\"http://www.melconway.com/Home/Conways_Law.html\"]],[\"a\",[\"href\",\"https://www.thoughtworks.com/radar/techniques/inverse-conway-maneuver\"]],[\"em\"],[\"a\",[\"href\",\"http://donovanbrown.com/post/what-is-devops\"]],[\"a\",[\"href\",\"https://twitter.com/DonovanBrown\"]],[\"strong\"],[\"a\",[\"href\",\"http://agilemanifesto.org/principles.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"There are a lot of ALM MVPs that advocate the \\\"One Team Project to Rule Them All\\\" when it comes to Visual Studio Team Services (VSTS) and Team Foundation Server (TFS). I've been recommending it for a long time to any customer I work with. My recommendation was based mostly on experience - I've experienced far too much pain when organizations have multiple Team Projects, or even worse, multiple Team Project Collections.\"]]],[1,\"p\",[[0,[],0,\"While on a flight to New Jersey I watched a fantastic talk by Allan Kelley titled \"],[0,[0],1,\"Continuous Delivery and Conway's Law\"],[0,[],0,\". I've heard about \"],[0,[1],1,\"Conway's Law\"],[0,[],0,\" before and know that it is applied to systems design. A corollary to Conway's Law, referred to as \"],[0,[2],1,\"Inverse Conway Maneuver\"],[0,[],0,\", is to structure your organization intentionally to promote a desired system architecture. This has a lot of appeal to me with regards to DevOps - since DevOps is not a tool or a product, but a culture: a way of thinking.\"]]],[1,\"p\",[[0,[],0,\"With these thoughts in mind, as I was watching Kelley's talk I had an epiphany: you can perform an Inverse Conway Maneuver by the way you structure your VSTS account or TFS install!\"]]],[1,\"h2\",[[0,[],0,\"What is Conway's Law?\"]]],[1,\"p\",[[0,[],0,\"In April 1968, Mel Conway published a paper called \\\"How Do Committees Invent?\\\" The central thesis of this paper is this: \\\"Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.\\\" In other words, the design of your  organizational and team structures will impose itself on your system designs. At least, if they are out of sync, you will experience friction. The Inverse Conway Maneuver recognizes Conway's Law and makes it intentional: use organizational and team structure to \"],[0,[3],1,\"promote desired systems design\"],[0,[],0,\". For example, distributed teams tend to develop more modular products, while centralized teams tend to develop monoliths.\"]]],[1,\"p\",[[0,[],0,\"Historical side-note: Conway's paper was rejected by Harvard Business Review in 1967 since Conway had \\\"failed to prove his thesis\\\". Ironically, a team of MIT and Harvard Business School researchers published a paper in 2015 which found \\\"strong evidence\\\" to support the hypothesis.\"]]],[1,\"p\",[[0,[],0,\"How does this apply to VSTS and TFS? I'll explain, but it's awkward having to type \\\"VSTS and TFS\\\". For the remainder of this blog I'll just write VSTS - but the same principles apply to TFS: the VSTS account is like a TFS Team Project Collection. If we equate VSTS account to Team Project Collection, then the rest of the hierarchy (Team Project, Team etc.) is exactly equivalent. In short, when I say VSTS account I also mean TFS Team Project Collection.\"]]],[1,\"h2\",[[0,[],0,\"One Objective: Deliver Value to End Users\"]]],[1,\"p\",[[0,[],0,\"In days gone by, IT was a service center to Business. Today, most organizations are IT companies - irrespective of the industry they operate in. Successful businesses are those that embrace the idea that IT is a business enabler and differentiator, not just a cost center. There should be very little (if any) division between \\\"business\\\" and \\\"IT\\\" - there is one team with one goal: deliver value to customers. Interestingly the definition of DevOps, \"],[0,[4],1,\"according\"],[0,[],0,\" to \"],[0,[5],1,\"Donovan Brown\"],[0,[],0,\" (Principal DevOps Manager at Microsoft), is \\\"the union of people, process and products to enable continuous \"],[0,[6],1,\"delivery\"],[0,[],0,\" of \"],[0,[6],1,\"value\"],[0,[],0,\" to our \"],[0,[6],1,\"end\"],[0,[],0,\" \"],[0,[6],1,\"users\"],[0,[],0,\"\\\" (emphases mine).\"]]],[1,\"p\",[[0,[],0,\"One Objective means everyone is aligned to the overall goal of the business. If you look at two of the \"],[0,[7],1,\"Principles behind the Agile Manifesto\"],[0,[],0,\":\"]]],[3,\"ul\",[[[0,[],0,\"Business people and developers must work together daily throughout the project.\"]],[[0,[],0,\"The best architectures, requirements, and designs emerge from self-organizing teams.\"]]]],[1,\"p\",[[0,[],0,\"you'll see a common theme: aligning everyone to the One Objective. The point I'm making is that there needs to be a \\\"one team\\\" culture that permeates the organization. DevOps is \"],[0,[3],1,\"cultural\"],[0,[],0,\" before it's about tools and products. But putting the thinking into practice is no easy task. Fortunately, having the correct VSTS structures supports an Inverse Conway Maneuver.\"]]],[1,\"h2\",[[0,[],0,\"One Team Project\"]]],[1,\"p\",[[0,[],0,\"So how do you use VSTS for an Inverse Conway Maneuver? \"],[0,[3,6],2,\"You have a single VSTS account with a single Team Project\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Having all the work in a single Team Project allows you to view work at a \\\"portfolio\\\" (or organizational) level - that is, across the entire organization. This is (currently) impossible to do with multiple VSTS accounts and very difficult with multiple Team Project Collections. Even viewing portfolio level information with  multiple Team Projects can be difficult. Work item queries are scoped to Team Projects by default; widgets, dashboards, builds, releases, package feeds, test plans - these all live at Team Project level. If you have multiple Team Projects you've probably experienced a fragmented view of work across the organization. Interestingly, that's probably not only from a VSTS point of view, but this structure (by Conway's Law) is probably responsible for silos within the organization.\"]]],[1,\"p\",[[0,[],0,\"Hence the recommendation for a single \\\"Team Project to Rule Them All.\\\" Not only will this allow anyone to see work at a portfolio level, but this allows teams to share source repositories, build definitions, release definitions, reports and package feeds. It's a technical structure that encourages the One Objective.\"],[1,[],0,0]]],[1,\"h2\",[[0,[],0,\"Teams\"]]],[1,\"p\",[[0,[],0,\"I can hear you already: \\\"But I have 500 developers/analysts/testers/DBAs/Ops managers (let's say engineers, shall we?) - how can I possibly organize them under a single team project?\\\" That's where Teams come in. Teams allow organizations to organize work into manageable sets. When you're an engineer and you want to deliver value, you probably only need a general idea of the One Objective, rather than having to know the minutia of every bit of work across the entire organization. Having your team's work in a separate ring-fenced area allows you to focus on what you need day-to-day. You can go up to the portfolio level when you need a wider context - but you probably don't need that every day. Leadership will more likely spend most of their time looking at work at the portfolio level rather than all the way down to the minutia of the team-level work.\"]]],[1,\"p\",[[0,[],0,\"So how should you organize your teams? Again, Conway's Law is going to have enormous impact here. Do you have a 3-tier application? Then you might be tempted to create a DBA Team, a Service Team and a UI Team. Perhaps create a Mobile team and a Data Analytics Team too. Surely that's reasonable, right?\"]]],[1,\"p\",[[0,[],0,\"The answer, to quote Consultese (the dialect of the consultant) is: \"],[0,[3],1,\"It Depends\"],[0,[],0,\". Perhaps that is the way to go since that is how your application is architected. But that could be boxing you in: horizontally composed teams violate the Agile principle of cross-functional teams. A better approach is to have your teams composed around functional area or module. Where possible, they should be loosely coupled. Again by Conway's Law this will start reflecting in your app architecture - and you'll start seeing your applications become loosely coupled services. Have you ever wondered why micro-services are so popular today? Could it be the Agile movement started to break huge monolithic organizations into small, loosely-coupled, cross-functional and self-organizing teams, and now we're starting to see that reflected in our architecture? Inverse Conway Maneuvers at work.\"]]],[1,\"p\",[[0,[],0,\"In short, create Teams around functional areas and use Area Paths to denote ownership of that work. If an Epic/Feature/Story belongs to a team, put it in the Area Path for that team and it appears on their backlogs. Another tip is that your Area Paths should be durable (long-lived) while your work items should not: work items should have a definite start and end date. Don't make an Epic for \\\"Security\\\" since that's not likely to end at a specific date. Rather, have an Area Path for Security and place work items in that area path.\"]]],[1,\"h2\",[[0,[],0,\"Organizational Dimensions in VSTS\"]]],[1,\"p\",[[0,[],0,\"There are four axes that most organizations use to organize work: functional area, iteration, release and team. Unfortunately, VSTS only really gives us two: Area and Iteration. While Release Management in VSTS is brilliant, there isn't yet a first-class citizen for the concept of a Release. And while you can create a custom Team Field in TFS and slice teams on that field, you can't do so in VSTS, so you have to munge Team and Area Path together. In my experience it's best not to fight these limits: use Area Path to denote Team, use iterations to time-box, and if you really need a Release concept, add a custom field.\"]]],[1,\"h2\",[[0,[],0,\"Areas and Work Item States\"]]],[1,\"p\",[[0,[],0,\"Organizations will still need inter-team communication, but this should be happening far less frequently that intra-team communication. That's why we optimize for intra-team communication. It's also why co-locating a team wherever possible is so important. If you do this, then by Conway's Law you are more likely to end up with modules that are stable, resilient, independent and optimized.\"]]],[1,\"p\",[[0,[],0,\"We've already established that vertical Teams are tied to Area Paths. Each Team \\\"owns\\\" a root area path, typically with the same name as the Team. This is the area path for the team's backlog. The team can then create sub-areas if they need do (leave this up to the team - they're self-organizing after all). Kanban boards can be customized at the team level, so each team can decide on whatever columns and swim-lanes they want in order to optimize their day-to-day work. Again, leave this up to the team rather than dictating from the organizational level.\"]]],[1,\"p\",[[0,[],0,\"Work Item states can't be customized at Team level - only at the Team Project level. If you only have a single Team Project, that means every team inherits the same work item states. This is actually a good thing: a good design paradigm is to have standard communication protocols, and to have services have good contracts or interfaces, without dictating what the internals of the service should look like. This is reflected by the common \\\"language\\\" of work item state, but let's teams decide how to manage work internally via customized Kanban boards. Let Conway's Law work for you!\"]]],[1,\"h2\",[[0,[],0,\"Iterations\"]]],[1,\"p\",[[0,[],0,\"While teams should have independent backlogs and areas, they should synchronize on \"],[0,[3],1,\"cadence\"],[0,[],0,\". That is, it's best to share iterations. This means that teams are independent during a sprint, but co-ordinate at the end of the sprint. This enforces the loose coupling: teams are going to have dependencies and you still want teams to communicate - you just want to streamline that communication. Sharing iterations and synchronizing on that heartbeat is good for the Teams as well as the software they're delivering.\"]]],[1,\"h2\",[[0,[],0,\"Enterprise Alignment vs Team Autonomy\"]]],[1,\"p\",[[0,[],0,\"The VSTS team have a single Team Project Collection for their work. They speak about Enterprise Alignment vs Team Autonomy. I heard a great illustration the other day: the Enterprise is like a tanker - it takes a while to turn. Agile Teams are like canoes - they can turn easily. However, try to get 400 canoes pointed in the same direction! As you work to self-organizing teams, keep them on the One Objective so that they're pointed in the same direction. Again, that's why I like the One Team Project concept: the Team Project is the One Direction, while Teams still get autonomy in their Team Areas for daily work.\"]]],[1,\"h2\",[[0,[],0,\"Organizing Source Code, Builds, Releases, Test Plans and Feeds\"]]],[1,\"p\",[[0,[],0,\"If you have a single Team Project, then you'll have a challenge: all repositories, builds, releases, test plans and package feeds are in a single place. Builds have the concept of Build Folders, so you can organize builds by folders if you need to. However, repos, releases, test plans and feeds don't have folders. That means you'll need a good naming strategy and make use of Favorites to manage the noise. In my opinion this is a small price to pay for the benefits of One Team Project.\"]]],[1,\"h2\",[[0,[],0,\"Security\"]]],[1,\"p\",[[0,[],0,\"Often I come across organizations that want to set up restrictions on who can see what. In general: don't do this! Why do you care if Team A can see Team B's backlog? In fact it should be encouraged! Find out what other teams are working on so that you can better manage dependencies and eliminate double work. Same principle with Source Code: why do you care if Team C and see Team D's repos?\"]]],[1,\"p\",[[0,[],0,\"There are of course exceptions: if you have external contractors, you may want to restrict visibility for them. In VSTS, deny overrides allow, so in general, leave permissions as \\\"Not Set\\\" and then explicitly Deny groups when you need to. The Deny should be the exception rather than the rule - if not, you're probably doing something wrong.\"]]],[1,\"p\",[[0,[],0,\"Of course you want to make sure you Branch Policies (with Pull Requests) in your source code and approval gates in your Releases. This ensures that the teams are aware of code changes and code deployments. Don't source control secrets - store them in Release Management or Azure Key Vault. And manage by exception: every action in VSTS is logged in the Activity Log, so you can always work out who did what after the fact. Trust your teams!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Don't fight Conway's Law: make it work for you! Slim down to a single VSTS Account with a single Team Project and move all your Teams into that single Team Project. Give Teams the ability to customize their sub-areas, backlogs, boards and so on: this gives a good balance of Enterprise Alignment and Team Autonomy.\"]]],[1,\"p\",[[0,[],0,\"Here is a brief summary of how you should structure your VSTS account:\"]]],[3,\"ul\",[[[0,[],0,\"A single VSTS Account (or TFS Team Project Collection)\"]],[[0,[],0,\"A single Team Project\"]],[[0,[],0,\"Multiple Teams, all owning their root Area Path\"]],[[0,[],0,\"Shared Iteration Paths\"]],[[0,[],0,\"Use naming conventions/favorites for Repos, Releases, Test Plans and Feeds\"]],[[0,[],0,\"Use folders for organizing Build Definitions\"]],[[0,[],0,\"Enforce Branch Policies in your Repos and use Approval Gates in Release Management\"]],[[0,[],0,\"Have simple permissions with minimal DENY (prefer NOT SET and ALLOW)\"]]]],[1,\"p\",[[0,[],0,\"Happy delivering!\"]]]]}","published_at":1525379195000,"status":"published","published_by":1},{"id":"cd3e099c-da22-42cb-8bec-9d6143822f0e","title":"WebDeploy and Release Management – The Proper Way","slug":"webdeploy-and-release-management--the-proper-way","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"<connectionstrings>\\n  <add name=\\\"FabrikamFiber-Express\\\" connectionstring=\\\"somestring\\\" providername=\\\"System.Data.SqlClient\\\">\\n</add></connectionstrings>\\n<appsettings>\\n  <add key=\\\"webpages:Version\\\" value=\\\"2.0.0.0\\\">\\n  <add key=\\\"PreserveLoginUrl\\\" value=\\\"true\\\">\\n  <add key=\\\"ClientValidationEnabled\\\" value=\\\"true\\\">\\n  <add key=\\\"UnobtrusiveJavaScriptEnabled\\\" value=\\\"true\\\">\\n  <add key=\\\"DemoEnv\\\" value=\\\"Development\\\">\\n</add></add></add></add></add></appsettings>\\n\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"<!--?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\" ?-->\\n<parameters>\\n  <parameter name=\\\"DemoEnv\\\" description=\\\"Please enter the name of the Environment\\\" defaultvalue=\\\"__EnvironmentName__\\\" tags=\\\"\\\">\\n    <parameterentry kind=\\\"XmlFile\\\" scope=\\\"\\\\\\\\web.config$\\\" match=\\\"/configuration/appSettings/add[@key='DemoEnv']/@value\\\">\\n  </parameterentry></parameter>\\n</parameters>\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-BKy-yRAbHXk/UpibT9ZeS_I/AAAAAAAABHQ/-fIW3W4iOOI/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-L2-25D-E9Zs/UpibUzdojhI/AAAAAAAABHY/djCwst29Ao4/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"316\\\" height=\\\"253\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-dr2kUo3r4qk/UpibVv_h09I/AAAAAAAABHg/0-XrR3y2Y6c/s1600-h/image%25255B7%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-ky2u8_fgRjg/UpibWc3G-eI/AAAAAAAABHo/9fKAZ66AJ6I/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"344\\\" height=\\\"275\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-bLD-gkoVE7w/UpibXVfLJdI/AAAAAAAABHw/H7y4q5xfpr8/s1600-h/image%25255B11%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-n6Dy-Hh0AP0/UpibYFHx2sI/AAAAAAAABH4/5LgEgrjYwNU/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"374\\\" height=\\\"300\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-7m6izo5bi-Q/UpibYY9SZUI/AAAAAAAABIA/zxRvh9PaPKQ/s1600-h/image%25255B15%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-4X-gvNm-JSU/UpibZD86GgI/AAAAAAAABII/twPi65DvaUQ/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"326\\\" height=\\\"106\\\"></a>\"}],[\"code\",{\"code\":\"<!--?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?-->\\n<parameters>\\n  <setparameter name=\\\"IIS Web Application Name\\\" value=\\\"__SiteName__\\\">\\n  <setparameter name=\\\"DemoEnv\\\" value=\\\"__EnvironmentName__\\\">\\n  <setparameter name=\\\"FabrikamFiber-Express-Web.config Connection String\\\" value=\\\"__FabrikamFiber-Express-Connection__\\\">\\n  <setparameter name=\\\"FabrikamFiber.DAL.Data.FabrikamFiberWebContext-Web.config Connection String\\\" value=\\\"FabrikamFiber.DAL.Data.FabrikamFiberWebContext_ConnectionString\\\">\\n</setparameter></setparameter></setparameter></setparameter></parameters>\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<font size=\\\"3\\\" face=\\\"Courier New\\\">/p:DeployOnBuild=true;PublishProfile=Release</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-uKhtFlTnzyo/UpibZ4qFJxI/AAAAAAAABIQ/y2ZsO6HSoQM/s1600-h/image%25255B19%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-X8uDXRV0rCU/UpibaqIWiiI/AAAAAAAABIY/Uxpr3yprHQ4/image_thumb%25255B9%25255D.png?imgmax=800\\\" width=\\\"388\\\" height=\\\"126\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-gHn2eC9W3fg/UpibbeduqMI/AAAAAAAABIg/zItSLhdGzqQ/s1600-h/image%25255B23%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-kUwemv74Z64/UpibcBVq8oI/AAAAAAAABIo/U21i-1S2HTU/image_thumb%25255B11%25255D.png?imgmax=800\\\" width=\\\"344\\\" height=\\\"153\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh5.ggpht.com/-clawD7s-38g/UpibcsqSGNI/AAAAAAAABIw/OZr35qPHyEg/s1600-h/image%25255B27%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-27wu5FbvcGg/UpibdWVY3OI/AAAAAAAABI4/B4GX9Y_xoh8/image_thumb%25255B13%25255D.png?imgmax=800\\\" width=\\\"351\\\" height=\\\"252\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-DUYVtKdFZlE/UpzbinXEAbI/AAAAAAAABK8/NPaqH1fm-lQ/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-ZtXq92R57no/UpzblJm-d8I/AAAAAAAABLE/qWOijdmN0sQ/image_thumb.png?imgmax=800\\\" width=\\\"179\\\" height=\\\"244\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-cuRJ-5r4AMQ/UpibeaE8hdI/AAAAAAAABJA/ODMZaTPHUN8/s1600-h/image%25255B31%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/--wVySNRIExI/UpibfIyZfVI/AAAAAAAABJI/kGVuQ0pxJxs/image_thumb%25255B15%25255D.png?imgmax=800\\\" width=\\\"346\\\" height=\\\"254\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-OvMisXbj-Ig/UpibfiIJmOI/AAAAAAAABJQ/sYX_w3m0kJ0/s1600-h/image%25255B35%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-5JOJUUe-Mzg/Upibgen3uCI/AAAAAAAABJY/Ceo8wP7RWqs/image_thumb%25255B17%25255D.png?imgmax=800\\\" width=\\\"345\\\" height=\\\"140\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-9HZdDKHIzNc/UpibhDM1GsI/AAAAAAAABJg/866sYFe6DEk/s1600-h/image%25255B48%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-Z3Bb_yMFpFo/UpibhtVqlMI/AAAAAAAABJo/juQ1WIBOCrs/image_thumb%25255B26%25255D.png?imgmax=800\\\" width=\\\"404\\\" height=\\\"109\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-UD7iPt1-3H0/UpibiAJaFvI/AAAAAAAABJs/TzD6LX0QVOs/s1600-h/image%25255B65%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-RnR2KZA1OeA/Upibi_mtMeI/AAAAAAAABJ4/SAtDRZ6pM7E/image_thumb%25255B39%25255D.png?imgmax=800\\\" width=\\\"345\\\" height=\\\"167\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-0T4pIX50Wak/UpibjUMIWVI/AAAAAAAABJ8/F5FZ2DGVBVA/s1600-h/image%25255B68%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-rBhwrP4AaGQ/Upibj5XAWUI/AAAAAAAABKI/c7xsUGtqZZ0/image_thumb%25255B42%25255D.png?imgmax=800\\\" width=\\\"368\\\" height=\\\"159\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-itWALleab10/UpiblWgZXqI/AAAAAAAABKQ/okA7XG__Uuk/s1600-h/image%25255B72%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-YhinJ2iGZwI/UpibmqkG-eI/AAAAAAAABKY/AzlHE2rYj9I/image_thumb%25255B44%25255D.png?imgmax=800\\\" width=\\\"392\\\" height=\\\"304\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.imaginet.com/events\"]],[\"a\",[\"href\",\"http://download.microsoft.com/download/B/C/8/BC8558E1-192E-4286-B3B0-320A8B7CE49D/Embracing%20Continuous%20Delivery%20with%20Release%20Management%20for%20Visual%20Studio%202013.docx\"]],[\"em\"],[\"a\",[\"href\",\"http://support.inreleasesoftware.com/entries/21448302-Tokenization-of-configuration-files\"]],[\"a\",[\"href\",\"http://support.inreleasesoftware.com/entries/21487316-InRelease-with-Web-Deploy\"]],[\"a\",[\"href\",\"http://support.inreleasesoftware.com/attachments/token/kfvabql0jv4omje/?name=irmsdeploy.exe\"]],[\"strong\"],[\"u\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve just completed recording a Release Management webcast (more on Imaginet’s Visual Studio webcasts \"],[0,[0],1,\"here\"],[0,[],0,\"). While doing the webcast, I wanted to show how you can use tokens which Release Management can substitute during the Release Workflow. Brian Keller suggests a .token file (basically an exact copy of your web.config file except that you use tokens instead of values) in his \"],[0,[1],1,\"Release Management hands on lab\"],[0,[],0,\", but I hate having to keep 2 copies of the same file around.\"]]],[1,\"p\",[[0,[],0,\"Of course, being a huge fan of Web Deploy, I could use config transforms. The problem with mixing config transforms and Release Management is that you’d have to have a configuration per environment in your solution, and you’d end up having to create \"],[0,[2],1,\"n\"],[0,[],0,\" number of web deploy packages where \"],[0,[2],1,\"n\"],[0,[],0,\" is the number of Release Management stages you have. So if you had Dev, QA and Prod stages, you’d have to add at least one configuration to your solution so that you’d have Debug (for Dev), QA (for the QA environment) and Release (for Prod). Technically you wouldn’t be deploying the same package to each environment, even though they could be built at the same time from the same source files.\"]]],[1,\"p\",[[0,[],0,\"I bingled a bit and found two posts that looked useful. The \"],[0,[3],1,\"first was about tokenization\"],[0,[],0,\", and had the downside is that you’re still doing an xcopy deployment rather than a web deploy package deployment, and you have to do some rather nasty gymnastics with the build parameters in order get it to work. The \"],[0,[4],1,\"second\"],[0,[],0,\" was a lot cleaner, except for the fact that you have to know the folder on the server where the website ends up after the web deploy command, since the token replacement is done \"],[0,[2],1,\"after\"],[0,[],0,\" the invocation of the web deploy Tool.\"]]],[1,\"p\",[[0,[],0,\"I was convinced there was a cleaner solution, and I managed to come up with one. Basically, we use Web Deploy Parameters to tokenize the web.config file, and then do the token replacement before invoking Web Deploy.\"]]],[1,\"h2\",[[0,[],0,\"Parameters.xml\"]]],[1,\"p\",[[0,[],0,\"Web Deploy lets you define parameters in a file called Parameters.xml. If there isn’t one in your project (alongside the web.config) then it creates a default one during publishing, so normally you don’t see it at all.\"]]],[1,\"p\",[[0,[],0,\"Let’s imagine that you have the following web.config snippet:\"]]],[10,0],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"There’s a connection string and an appSetting key that we want to tokenize. Right click the project and add a new xml file called “Parameters.xml”. Right click the file, select Properties and set the “Build Action” to None to make sure this file doesn’t end up deployed to your website. Now we add the following xml:\"]]],[10,1],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"We create a parameter with a name, description and default value. The format of the default value is important – it needs to be pre- and post-fixed with double underscore “__” since this is the token format for Release Management. We then specify a “kind” of XmlFile, set web.config as the scope and specify an XPath to find the parameter in the web.config file.\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"(You’ll notice that we don’t need to specify any parameter for the connection string, since that will be tokenized in the publish profile)\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"h2\",[[0,[],0,\"Publish Profile\"]]],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"Right-click your web project and select “Publish” to create (or edit) a publish profile. Expand the dropdown and select  to create a new profile. I named mine “Release”. Click Next.\"]]],[1,\"p\",[[1,[],0,5]]],[10,2],[1,\"p\",[[0,[],0,\"On the Connection page, select “Web Deploy Package” as the publish method and enter a name for the package location. Typically this is (name of your project).zip. For Site name, enter “__SiteName__” to create a Release Management token for your site name. Click Next.\"],[1,[],0,6]]],[10,3],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"On the Settings page, select your configuration (I selected Release, which applies and config transforms in the Web.Release.config file in my solution, such as removing the Debug attribute from the  element). For each connection string you have, instead of entering in a real connection, again enter a Release Management token – I entered “__FabrikamFiber-Express-Connection__”.\"]]],[1,\"p\",[[1,[],0,8]]],[10,4],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"Click Close and save the profile. The profile appears under the Properties\\\\PublishProfiles folder of your web project.\"]]],[1,\"p\",[[1,[],0,10]]],[10,5],[1,\"p\",[[0,[],0,\"Now check your solution into source control.\"],[1,[],0,11]]],[1,\"p\",[[0,[],0,\"If you do actually publish, you’ll see a SetParameters.xml file alongside the web deploy zip file. The contents of the file should be something like this:\"]]],[10,6],[1,\"p\",[[1,[],0,12]]],[1,\"p\",[[0,[],0,\"You can see that there are 3 Release Management tokens – SiteName, EnvironmentName and FabrikamFiber-Express-Connection. These are the tokens we’ll replace when creating a release component for this website.\"]]],[1,\"p\",[[1,[],0,13]]],[1,\"h2\",[[0,[],0,\"Building the Package\"]]],[1,\"p\",[[1,[],0,14]]],[1,\"p\",[[0,[],0,\"You can now create a build – make sure you use the ReleaseDefaultTemplate11.1.xaml (from the Release Management bin folder). Specify any arguments you want to as usual, but make sure you have this in the MSBuild arguments:\"]]],[1,\"p\",[[1,[],0,15]]],[10,7],[1,\"p\",[[1,[],0,16]]],[1,\"p\",[[0,[],0,\"That will instruct web deploy to create the package when we build using the settings of the Release profile. I recommend that you set “Release to Build” to false just to make sure your build is producing the correct artifacts.\"]]],[1,\"p\",[[1,[],0,17]]],[10,8],[1,\"p\",[[1,[],0,18]]],[1,\"p\",[[0,[],0,\"After running the build, you should see the following in the _PublishedWebSites folder of your drop:\"]]],[1,\"p\",[[1,[],0,19]]],[10,9],[1,\"p\",[[1,[],0,20]]],[1,\"p\",[[0,[],0,\"If you open the SetParameters.xml file (the one highlighted above) then you should see your Release Management tokens.\"]]],[1,\"p\",[[1,[],0,21]]],[1,\"h2\",[[0,[],0,\"Create a Web Deploy Tool in Release Management\"]]],[1,\"p\",[[1,[],0,22]]],[1,\"p\",[[0,[],0,\"Download the InRelease MS Deploy wrapper from \"],[0,[5],1,\"here\"],[0,[],0,\". This is a simple exe that wraps the call to Web deploy so that any errors are reported in a way that Release Management understands. Let’s then go to Release Management->Inventory->Tools and click on New to create a new Tool:\"]]],[1,\"p\",[[1,[],0,23]]],[1,\"p\",[[0,[],0,\"Enter the following parameters:\"]]],[1,\"p\",[[1,[],0,24]]],[10,10],[1,\"p\",[[1,[],0,25]]],[1,\"p\",[[0,[],0,\"You can see that I made a new parameter called “WebAppName” to make this a generic tool.\"]]],[1,\"p\",[[1,[],0,26]]],[1,\"p\",[[0,[],0,\"At the bottom under Resources, click the “Add” button and import the irmsdeploy.exe that you downloaded.\"]]],[1,\"p\",[[1,[],0,27]]],[1,\"p\",[[0,[6,7],2,\"Update: 2013-12-02\"],[0,[],0,\" When running through this “demo” again on my VM checkpoint, the WebDeploy custom tool kept failing with an “Unable to find file” exception. After trying this several times and tearing my hair out in chunks, I thought I would make sure this wasn’t a security issue – turns out, it was exactly that. Once you’ve downloaded the irmsdeploy.exe file, make sure you right-click it, select properties and “Unblock” it (see the below screenshot).\"]]],[1,\"p\",[[1,[],0,28]]],[10,11],[1,\"p\",[[1,[],0,29]]],[1,\"h2\",[[0,[],0,\"Create a Component\"]]],[1,\"p\",[[1,[],0,30]]],[1,\"p\",[[0,[],0,\"Once you’ve set up your Release Path, you’ll be able to define the release workflow for each stage. You’re going to need to create a component for your website in order to deploy it. Navigate to “Configure Apps->Components” and click New. Enter a name (and optional description) and then enter the following on the Source tab:\"]]],[1,\"p\",[[1,[],0,31]]],[10,12],[1,\"p\",[[0,[],0,\"The “Path to Package” is the path that contains the web deploy package.\"],[1,[],0,32]]],[1,\"p\",[[0,[],0,\"Now click on the “Deployment” tab:.\"]]],[1,\"p\",[[1,[],0,33]]],[10,13],[1,\"p\",[[1,[],0,34]]],[1,\"p\",[[0,[],0,\"Select WebDeploy from the Tool dropdown – that will automatically create the WebAppName parameter for this component.\"]]],[1,\"p\",[[1,[],0,35]]],[1,\"p\",[[0,[],0,\"Finally, move to the “Configuration Variables” tab:\"]]],[1,\"p\",[[1,[],0,36]]],[10,14],[1,\"p\",[[1,[],0,37]]],[1,\"p\",[[1,[],0,38]]],[1,\"p\",[[0,[],0,\"Change the “Variable Replacement Mode” to “Before Installation” and set the “File Extension Filter” to “*.SetParameters.xml”. Now add all of your parameters (these are the tokens that are in your SetParameters.xml file after the build). You can type descriptions too.\"]]],[1,\"p\",[[1,[],0,39]]],[1,\"h2\",[[0,[],0,\"Specifying Values in a Release Template\"]]],[1,\"p\",[[1,[],0,40]]],[1,\"p\",[[0,[],0,\"We’re finally ready to use the component inside a Release Template. Create a new template for a release path, and inside a Server activity drop your component. When you expand it, you’ll see that you can specify values for the parameters. Here are screenshots of my Dev and QA components:\"]]],[1,\"p\",[[1,[],0,41]]],[10,15],[10,16],[1,\"p\",[[1,[],0,42]]],[1,\"p\",[[0,[],0,\"The best part is, not only can you use these parameters in Release Management, but if you import this web deploy package directly in IIS, you get prompted to supply values for the parameters too:\"]]],[1,\"p\",[[1,[],0,43]]],[10,17],[1,\"p\",[[1,[],0,44]]],[1,\"p\",[[0,[],0,\"Happy Releasing!\"]]]]}","published_at":1385769000000,"status":"published","published_by":1},{"id":"4db636fd-d165-4d7d-beb8-96e9c8e9d745","title":"WebDeploy, Configs and Web Release Management","slug":"webdeploy-configs-and-web-release-management","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/2a26c2ff-f9a0-4dc4-9619-4111a7e8e9ab.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/54e7111d-5892-44e8-b0f8-f2eb7f277d3b.png\\\" width=\\\"377\\\" height=\\\"114\\\"></a>\"}],[\"code\",{\"code\":\"&lt;!--?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\" ?--&gt;\\n&lt;parameters&gt;\\n  &lt;parameter name=\\\"CoolKey\\\" description=\\\"The CoolKey setting\\\" defaultvalue=\\\"__CoolKey__\\\" tags=\\\"\\\"&gt;\\n    &lt;parameterentry kind=\\\"XmlFile\\\" scope=\\\"\\\\\\\\web.config$\\\" match=\\\"/configuration/appSettings/add[@key='CoolKey']/@value\\\"&gt;\\n    &lt;/parameterentry&gt;\\n  &lt;/parameter&gt;\\n&lt;/parameters&gt;\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/47811e0e-47d3-4131-a14c-4da0c5fd828f.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/0ca3df4c-51ba-486a-ae98-772b7034b779.png\\\" width=\\\"423\\\" height=\\\"325\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/2a5f8f9c-c7cf-4264-b22b-351cb07046bb.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4d9e81c0-4cd6-4685-b27d-080a7fa2b106.png\\\" width=\\\"393\\\" height=\\\"127\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/a249909e-a879-4e40-9ef2-a2a3cff04ad2.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/6f8812b9-142f-434b-a6f9-4b060630517d.png\\\" width=\\\"395\\\" height=\\\"95\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/664fa560-0f9f-4034-b959-b97b5b5d88e3.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/66fb8d87-a768-4adf-8ff8-6ab42fcfcf50.png\\\" width=\\\"407\\\" height=\\\"78\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/ddd96c1f-4c66-4cfe-b666-202738ffe9b9.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/4c86556e-4b23-4852-9aa3-9c0bf78486a8.png\\\" width=\\\"302\\\" height=\\\"177\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/499b4107-1af6-4272-bbc1-a8d5fb0c4494.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/f90dcc60-02cc-429e-9016-9b401a7eb6e1.png\\\" width=\\\"345\\\" height=\\\"130\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/374f429e-bcb4-49e4-9a94-4760f4703ae3.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/1ec03479-9722-4704-a3bd-a028d1663996.png\\\" width=\\\"346\\\" height=\\\"154\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/cfddee21-bdd1-4a0e-bbde-6bd7e61560b9.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/55f87666-1e7e-4228-91e7-260b515a0597.png\\\" width=\\\"348\\\" height=\\\"84\\\"></a>\"}],[\"code\",{\"code\":\"&lt;?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?&gt;\\n&lt;parameters&gt;\\n  &lt;setParameter name=\\\"IIS Web Application Name\\\" value=\\\"__SiteName__\\\" /&gt;\\n  &lt;setParameter name=\\\"CoolKey\\\" value=\\\"__CoolKey__\\\" /&gt;\\n  &lt;setParameter name=\\\"EntityDB-Web.config Connection String\\\" value=\\\"__EntityDB__\\\" /&gt;\\n&lt;/parameters&gt;\\n\",\"language\":\"xml;\"}],[\"code\",{\"code\":\"param(\\n    [string]$setParamsFilePath\\n)\\nWrite-Verbose -Verbose \\\"Entering script Replace-SetParameters.ps1\\\"\\nWrite-Verbose -Verbose (\\\"Path to SetParametersFile: {0}\\\" -f $setParamsFilePath)\\n\\n# get the environment variables\\n$vars = gci -path env:*\\n\\n# read in the setParameters file\\n$contents = gc -Path $setParamsFilePath\\n\\n# perform a regex replacement\\n$newContents = \\\"\\\";\\n$contents | % {\\n    $line = $_\\n    if ($_ -match \\\"__(\\\\w+)__\\\") {\\n        $setting = gci -path env:* | ? { $_.Name -eq $Matches[1]  }\\n        if ($setting) {\\n            Write-Verbose -Verbose (\\\"Replacing key {0} with value from environment\\\" -f $setting.Name)\\n            $line = $_ -replace \\\"__(\\\\w+)__\\\", $setting.Value\\n        }\\n    }\\n    $newContents += $line + [Environment]::NewLine\\n}\\n\\nWrite-Verbose -Verbose \\\"Overwriting SetParameters file with new values\\\"\\nsc $setParamsFilePath -Value $newContents\\n\\nWrite-Verbose -Verbose \\\"Exiting script Replace-SetParameters.ps1\\\"\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/e75f4709-35f4-44d0-aef2-b2dfbbe64479.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/218622b6-48aa-453c-92d3-eb5d9884aacf.png\\\" width=\\\"301\\\" height=\\\"196\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/7a9168c1-a851-4693-a267-6c6d420c42d8.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/509668ce-0c8d-4c0d-807c-e509c7f02c8b.png\\\" width=\\\"296\\\" height=\\\"161\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/61cc8620-e0f0-49e4-9e52-95bc65246bc6.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/45f0a4f9-0cd8-40af-bc7b-b26dd6fdb872.png\\\" width=\\\"288\\\" height=\\\"289\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/58caff03-e790-4eec-96e1-8f9a4fa15a70.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/5b4b9268-fc4e-408a-99bb-5d0899151fd4.png\\\" width=\\\"325\\\" height=\\\"326\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/c53f51fe-d3eb-4e92-9fd8-81875695b77c.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/67406726-55b5-4513-803d-7f9c372cfbf7.png\\\" width=\\\"367\\\" height=\\\"217\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/3515cdc0-e574-4893-87d0-5313b84ef2ab.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/854cc6b2-2d55-492f-bd3a-1ab7afd8de10.png\\\" width=\\\"531\\\" height=\\\"142\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/ffd5137d-6ee4-4330-a9cd-1fc1401bffdc.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/ed956818-8992-49a9-b73e-d4ca69dd7dd0.png\\\" width=\\\"473\\\" height=\\\"182\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/ceef56c7-7946-4390-94b2-a3fd237b6d7f.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/886e0140-1a69-41cb-9004-8c3df029bc7e.png\\\" width=\\\"535\\\" height=\\\"163\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/86ece78f-6886-40a8-951a-934cac54fc2d.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/62df6094-71eb-4416-b260-e4afe6b260f9.png\\\" width=\\\"587\\\" height=\\\"177\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">$(Build.BuildNumber)-$(rev:r)</font>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.visualstudio.com/en-us/get-started/release/release-management-vs\"]],[\"a\",[\"href\",\"http://colinsalmcorner.com/post/webdeploy-and-release-management--the-proper-way\"]],[\"a\",[\"href\",\"https://msdn.microsoft.com/en-us/library/azure/dn722468.aspx\"]],[\"a\",[\"href\",\"https://octopus.com/\"]],[\"a\",[\"href\",\"https://github.com/colindembovsky/cols-agent-tasks/tree/master/Tasks/VersionAssemblies\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"It’s finally here – the new \"],[0,[0],1,\"web-based Release Management\"],[0,[],0,\" (WebRM). At least, it’s here in preview on VSTS (formerly VSO) and should hopefully come to TFS 2015 in update 2.\"]]],[1,\"p\",[[0,[],0,\"I’ve blogged frequently about Release Management, the “old” WPF tool that Microsoft purchased from InCycle (it used to be called InRelease). The tool was good in some ways, and horrible in others – but it always felt like a bit of a stop-gap while Microsoft implemented something truly great – which is what WebRM is!\"]]],[1,\"p\",[[0,[],0,\"One of the most common deployment scenarios is deploying web apps – to IIS or to Azure. I blogged about using the old tool along with WebDeploy \"],[0,[1],1,\"here\"],[0,[],0,\". This post is a follow-on – how to use WebDeploy and WebRM correctly.\"]]],[1,\"p\",[[0,[],0,\"First I want to outline a problem with the out-of-the-box Tasks for deploying web apps. Then I’ll talk about how to tokenize the build package ready for multi-environment deployments, and finally I’ll show you how to create a Release Definition.\"]]],[1,\"h2\",[[0,[],0,\"Azure Web App Deployment Task Limitations\"]]],[1,\"p\",[[0,[],0,\"If you create a new Release Definition, there is an “Azure Web App Deployment” Task. Why not just use that to deploy web apps? There are a couple of issues with this Task:\"]]],[3,\"ol\",[[[0,[],0,\"You can’t use it to deploy to IIS\"]],[[0,[],0,\"You can’t manage different configurations for different environments (with the exception of connection strings)\"]]]],[1,\"p\",[[0,[],0,\"The Task is great in that it uses a predefined Azure Service Endpoint, which abstracts credentials away from the deployment. However, the underlying implementation invokes an Azure PowerShell cmdlet \"],[0,[2],1,\"Publish-AzureWebsiteProject\"],[0,[],0,\". This cmdlet works – as long as you don’t intend to change any configuration except the connection strings. Have different app settings in different environments? You’re hosed. Here’s the Task UI in VSTS:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The good:\"]]],[3,\"ul\",[[[0,[],0,\"You select the Azure subscription from the drop-down – no messing with passwords\"]],[[0,[],0,\"You can enter a deployment slot\"]]]],[1,\"p\",[[0,[],0,\"The bad:\"]]],[3,\"ul\",[[[0,[],0,\"You have to select the zip file for the packaged site – no place for handling configs\"]],[[0,[],0,\"Additional arguments – almost impossible to figure out what to put here. You can use this to set connection strings if you’re brave enough to figure it out\"]]]],[1,\"p\",[[0,[],0,\"The ugly:\"]]],[3,\"ul\",[[[0,[],0,\"Web App Name is a combo-box, but it’s never populated, so you have to type the name yourself (why is it a combo-box then?)\"]]]],[1,\"p\",[[0,[],0,\"In short, this demo’s nicely, but you’re not really going to use it for any serious deployments – unless you’ve set the app settings on the slots in the Azure Portal itself. Perhaps this will work for you – but if you change a setting value (or add a new setting) you’re going to have to manually update the slot using the Portal. Not a great automation story.\"]]],[1,\"h2\",[[0,[],0,\"Config Management\"]]],[1,\"p\",[[0,[],0,\"So besides not being able to use the Task for IIS deployments, your biggest challenge is config management. Which is ironic, since building a WebDeploy package actually handles the config well – it places config into a SetParameters.xml file. Unfortunately the Task (because it is calling Publish-AzureWebsiteProject under the hood) only looks for the zip file – it ignores the SetParameters file.\"]]],[1,\"p\",[[0,[],0,\"So I got to thinking – and I stole an idea from \"],[0,[3],1,\"Octopus Deploy\"],[0,[],0,\": what if the deployment would just automagically replace any config setting value with any correspondingly named variable defined in the Release Definition for the target Environment? That would mean you didn’t have to edit long lists of arguments at all. Want a new value? Just add it to the Environment variables and the deployment takes care of it for you.\"]]],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"The solution turned out to be fairly simple:\"]]],[1,\"p\",[[0,[],0,\"For the VS Solution:\"]]],[3,\"ol\",[[[0,[],0,\"Add a parameters.xml file to your Website project for any non-connecting string settings you want to manage, using tokens for values\"]],[[0,[],0,\"Create a publish profile that inserts tokens for the website name and any db connection strings\"]]]],[1,\"p\",[[0,[],0,\"For the Build:\"]]],[3,\"ol\",[[[0,[],0,\"Configure a Team Build to produce the WebDeploy package (and cmd and SetParameters files) using the publish profile\"]],[[0,[],0,\"Configure the Build to upload the zip and supporting files as the output\"]]]],[1,\"p\",[[0,[],0,\"For the Release:\"]]],[3,\"ol\",[[[0,[],0,\"Write a script to do the parameter value substitution (replacing tokens with actual values defined in the target Environment) into the SetParameters file\"]],[[0,[],0,\"Invoke the cmd to deploy the Website\"]]]],[1,\"p\",[[0,[],0,\"Of course, the “parameter substituting script” has to be checked into the source repo and also included as a build output in order for you to use it in the Release.\"]]],[1,\"h3\",[[0,[],0,\"Creating a Tokenized WebDeploy Package in a Team Build\"]]],[1,\"p\",[[0,[],0,\"Good releases start with good packages. Since the same package is going to be deployed to multiple environments, you cannot “hardcode” any config settings into the package. So you have to create the package in such a way that it has tokens for any config values that the Release pipeline will replace with Environment specific values at deployment time. In my previous \"],[0,[1],1,\"WebDeploy and Release Management post\"],[0,[],0,\", I explain how to add the parameters.xml file and how to create a publish profile to do exactly that. That technique stays exactly the same as far as the VS solution goes.\"]]],[1,\"p\",[[0,[],0,\"Here’s my sample parameters.xml file for this post:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Note how I’m sticking with the double-underscore pre- and post-fix as the token, so the value (token) for CoolKey is __CoolKey__.\"]]],[1,\"p\",[[0,[],0,\"Once you’ve got a parameters.xml file and a publish profile committed into your source repo (Git or TFVC – either one works fine), you’re almost ready to create a Team Build (vNext Build). You will need the script that “hydrates” the parameters from the Environment variables. I’ll cover the contents of that script shortly – let’s assume for now that you have a script called “Replace-SetParameters.ps1” checked into your source repo along with your website. Here’s the structure I use:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Create a new Build Definition – select Visual Studio Build as the template to start from. You can then configure whatever you like in the build, but you have to do 3 things:\"]]],[3,\"ol\",[[[0,[],0,\"Configure the MSBuild arguments as follows in the “Visual Studio Build” Task:\"]],[[0,[],0,\"/p:DeployOnBuild=true /p:PublishProfile=Release /p:PackageLocation=\\\"$(build.StagingDirectory)\\\"\"]],[[0,[],0,\"The name of the PublishProfile is the same name as the pubxml file in your solution\"]],[[0,[],0,\"The package location is set to the build staging directory\"]]]],[10,3],[3,\"ol\",[[[0,[],0,\"Configure the “Copy and Publish Build Artifacts” Task to copy the staging directory to a server drop:\"]]]],[10,4],[3,\"ol\",[[[0,[],0,\"Add a new “Publish Build Artifact” Task to copy the “Replace-SetParameters.ps1” script to a server drop called “scripts”:\"]]]],[10,5],[1,\"p\",[[0,[],0,\"I like to version my assemblies so that my binary versions match my build number. I use a \"],[0,[4],1,\"custom build Task\"],[0,[],0,\" to do just that. I also run unit tests as part of the build. Here’s my entire build definition:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Once the build has completed, the Artifacts look like this:\"]]],[10,7],[10,8],[10,9],[1,\"p\",[[0,[],0,\"Here’s what the SetParameters file looks like if you open it up:\"]]],[10,10],[1,\"p\",[[0,[],0,\"The tokens for SiteName and EntityDB both come from my publish profile – the token for CoolKey comes from my parameters.xml file.\"]]],[1,\"p\",[[0,[],0,\"Now we have a package that’s ready for Release!\"]]],[1,\"h3\",[[0,[],0,\"Filling in Token Values\"]]],[1,\"p\",[[0,[],0,\"You can see how the SetParameters file contains tokens. We will eventually define values for each token for each Environment in the Release Definition. Let’s assume that’s been done already – then how does the release pipeline perform the substitution? Enter PowerShell!\"]]],[1,\"p\",[[0,[],0,\"When you execute PowerShell in a Release, any Environment variables you define in the Release Definition are created as environment variables that the script can access. So I wrote a simple script to read in the SetParameters file, use Regex to find any tokens and replace the tokens with the environment variable value. Of course I then overwrite the file. Here’s the script:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Notes:\"]]],[3,\"ul\",[[[0,[],0,\"Line 2: The only parameter required is the path to the SetParameters file\"]],[[0,[],0,\"Line 8: Read in all the environment variables – these are populated according to the Release Definition\"]],[[0,[],0,\"Line 11: Read in the SetParameters file\"]],[[0,[],0,\"Line 15: Loop through each line in the file\"]],[[0,[],0,\"Line 17: If the line contains a token, then:\"]],[[0,[],0,\"Line 18-22: Find the corresponding environment variable, and if there is one, replace the token with the value\"]],[[0,[],0,\"Line 27: Overwrite the SetParameters file\"]]]],[1,\"p\",[[0,[],0,\"Caveats: note, this can be a little bit dangerous since the environment variables that are in scope include more than just the ones you define in the Release Definition. For example, the environment includes a “UserName” variable, which is set to the build agent user name. So if you need to define a username variable, make sure you name it “WebsiteUserName” or something else that’s going to be unique.\"]]],[1,\"h2\",[[0,[],0,\"Creating the Release Definition\"]]],[1,\"p\",[[0,[],0,\"We now have all the pieces in place to create a Release Definition. Each Environment is going to execute (at least) 2 tasks:\"]]],[3,\"ul\",[[[0,[],0,\"PowerShell – to call the Replace-SetParameters.ps1 script\"]],[[0,[],0,\"Batch Script – to invoke the cmd file to publish the website\"]]]],[1,\"p\",[[0,[],0,\"The PowerShell task is always going to be exactly the same – however, the Batch Script arguments are going to change slightly depending on if you’re deploying to IIS or to Azure.\"]]],[1,\"p\",[[0,[],0,\"I wanted to make sure this technique worked for IIS as well as for Azure (both deployment slots and “real” sites). So in this example, I’m deploying to 3 environments: Dev, Staging and Production. I’m using IIS for dev, to a staging deployment slot in Azure for Staging and the “real” Azure site for Production.\"]]],[1,\"p\",[[0,[],0,\"Here are the steps to configure the Release Definition:\"]]],[3,\"ol\",[[[0,[],0,\"Go to the Release hub in VSTS and create a new Release Definition. Select “Empty” to start with an empty template.\"]],[[0,[],0,\"Enter a name for the Release Definition and change “Default Environment” to Dev\"]]]],[10,12],[3,\"ol\",[[[0,[],0,\"Click “Link to a Build Definition” and select the build you created earlier:\"]]]],[10,13],[3,\"ol\",[[[0,[],0,\"Click “+ Add Tasks” and add a PowerShell Task:\"]],[[0,[],0,\"For the “Script filename”, browse to the location of the Replace-SetParameters.ps1 file:\"]]]],[10,14],[3,\"ol\",[[[0,[],0,\"For the “Arguments”, enter the following:\"]],[[0,[],0,\"-setParamsFilePath $(System.DefaultWorkingDirectory)\\\\CoolWebApp\\\\drop\\\\CoolWebApp.SetParameters.xml\"]],[[0,[],0,\"Of course you’ll have to fix the path to set it to the correct SetParameters file – $(System.DefaultWorkingDirectory) is the root of the Release downloads. Then there is a folder with the name of the Build (e.g. CoolWebApp), then the artifact name (e.g. drop), then the path within the artifact source.\"]],[[0,[],0,\"Click “+ Add Tasks” and add a Batch Script Task:\"]],[[0,[],0,\"For the “Script filename”, browse to the location of the WebDeploy cmd file:\"]]]],[10,15],[3,\"ol\",[[[0,[],0,\"Enter the correct arguments (discussed below).\"]],[[0,[],0,\"Configure variables for the Dev environment by clicking the ellipses button on the Environment tile and selecting “Configure variables”\"]],[[0,[],0,\"Here you add any variable values you require for your web app – these are the values that you tokenized in the build:\"]]]],[10,16],[3,\"ol\",[[[0,[],0,\"Azure sites require a username and password – I’ll cover those shortly.\"]]]],[1,\"p\",[[0,[],0,\"The Definition should now look something like this:\"]]],[10,17],[1,\"h3\",[[0,[],0,\"Cmd Arguments and Variables\"]]],[1,\"p\",[[0,[],0,\"For IIS, you don’t need username and password for the deployments. This means you’ll need to configure the build agent to run as an identity that has permissions to invoke WebDeploy. The SiteName variable is going to be the name of the website in IIS plus the name of your virtual application – something like “Default Web Site/cool-webapp”. Also, you’ll need to configure the Agent on the Dev environment to be an on-premise agent (so select an on-premise queue) since the hosted agent won’t be able to deploy to your internal IIS servers.\"]]],[1,\"p\",[[0,[],0,\"For Azure, you’ll need the website username and password (which you can get by downloading the Publish profile for the site from the Azure Portal). They’ll need to be added as variables in the environment, along with another variable called “WebDeploySiteName” (which is required only if you’re using deployment slots). The SiteName is going to be the name of the site in Azure. Of course you’re going to “lock” the password field to make it a secret. You can use the Hosted agent for Environments that deploy to Azure.\"]]],[1,\"p\",[[0,[],0,\"Here are the 2 batch commands – the first is for local deployment to IIS, the 2nd for deployment to Azure:\"]]],[3,\"ul\",[[[0,[],0,\"/Y /M:http://$(WebDeploySiteName)/MsDeployAgentService\"]],[[0,[],0,\"/Y /M:https://$(WebDeploySiteName).scm.azurewebsites.net:443/msdeploy.axd /u:$(AzureUserName) /p:$(AzurePassword) /a:Basic\"]]]],[1,\"p\",[[0,[],0,\"For IIS deployments, you can set WebDeploySiteName to be the name or IP of the target on-premises server. Note that you’ll have to have WebDeploy remote agent running on the machine, with the appropriate permissions for the build agent identity to perform the deployment.\"]]],[1,\"p\",[[0,[],0,\"For Azure, the WebDeploySiteName is of the form “siteName[-slot]”. So if you have a site called “MyWebApp”, and you just want to deploy to the site, then WebDeploySiteName will be “MyWebApp”. If you want to deploy to a slot (e.g. Staging), then WebDeploySiteName must be set to “MyWebApp-staging”. You’ll also need to set the SiteName to the name of the site in Azure (“MyWebApp” for the site, “MyWebApp__slot” for a slot – e.g. “MyWebApp__staging”). Finally, you’ll need “AzureUserName” and “AzurePassword” to be set (according to the publish settings for the site).\"]]],[1,\"h4\",[[0,[],0,\"Cloning Staging and Production Environments\"]]],[1,\"p\",[[0,[],0,\"Once you’re happy with the Dev Environment, clone it to Staging and update the commands and variables. Then repeat for Production. You’ll now have 3 Environments in the Definition:\"]]],[10,18],[1,\"p\",[[0,[],0,\"Also, if you click on “Configuration”, you can see all the Environment variables by clicking “Release variables” and selecting “Environment Variables”:\"]]],[10,19],[1,\"p\",[[0,[],0,\"That will open a grid so you can see all the variables side-by-side:\"]]],[10,20],[1,\"p\",[[0,[],0,\"Now you can ensure that you’ve set each Environment’s variables correctly. Remember to set approvals on each environment as appropriate!\"]]],[1,\"h3\",[[0,[],0,\"2 More Tips\"]]],[1,\"p\",[[0,[],0,\"If you want to trigger the Release every time the linked Build produces a new package, then click on Triggers and enable “Continuous Deployment”.\"]]],[1,\"p\",[[0,[],0,\"You can get the Release number to reflect the Build package version. Click on General and change the Release Name format to:\"]]],[10,21],[1,\"p\",[[0,[],0,\"Now when you release 1.0.0.8, say, your release will be “1.0.0.8-1”. If you trigger a new release with the same package, it will be numbered “1.0.0.8-2” and so on.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"WebRM is a fantastic evolution of Release Management. It’s much easier to configure Release Definitions, to track logs to see what’s going on and to configure deployment Tasks – thanks to the fact that the Release agent is the same as the Build agent. As far as WebDeploy goes, I like this technique of managing configuration – I may write a custom Build Task that bundles the PowerShell and Batch Script into a single task – that will require less argument “fudging” and bundle the PowerShell script so you don’t have to have it in your source repo. However, the process is not too difficult to master even without a custom Task, and that’s pleasing indeed!\"]]],[1,\"p\",[[0,[],0,\"Happy releasing!\"]]]]}","published_at":1449083918000,"status":"published","published_by":1},{"id":"37daa003-330d-42dd-8666-fc51151edeea","title":"WebDeploy Gets Even More Awesome – Profile Specific Transforms","slug":"webdeploy-gets-even-more-awesome--profile-specific-transforms","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-YGiHKqsIuZ8/U1dtkN4GlOI/AAAAAAAABV0/b3mj46J2nIU/s1600-h/image%25255B2%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-lJ00JDqtfSc/U1dtlA39wWI/AAAAAAAABV8/cTDXA9jNxVM/image_thumb.png?imgmax=800\\\" width=\\\"189\\\" height=\\\"70\\\"></a>\"}],[\"code\",{\"code\":\"<configuration xmlns:xdt=\\\"http://schemas.microsoft.com/XML-Document-Transform\\\">\\n  <system.web>\\n    <compilation xdt:transform=\\\"RemoveAttributes(debug)\\\">\\n  </compilation></system.web>\\n</configuration>\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-FRvfdnnlHAU/U1dtmdB-kTI/AAAAAAAABWE/f8OzAWgYOP4/s1600-h/image%25255B6%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-8nl4r4oTsmA/U1dtn3KHksI/AAAAAAAABWM/r2Awo15BsdI/image_thumb%25255B2%25255D.png?imgmax=800\\\" width=\\\"360\\\" height=\\\"214\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-Ed1Le7fHzUI/U1dtokAE42I/AAAAAAAABWU/z01rc2Sh_4Q/s1600-h/image%25255B10%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-CNFnS-2x-tM/U1dtpdKZjwI/AAAAAAAABWc/VmX_sjROfCU/image_thumb%25255B4%25255D.png?imgmax=800\\\" width=\\\"303\\\" height=\\\"242\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-TMI7lv6iMsE/U1dtqMIv9fI/AAAAAAAABWk/hV7eh3j_V5g/s1600-h/image%25255B16%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-bQ7HqTRb49w/U1dtq7Yb5TI/AAAAAAAABWs/380jDzBBqs0/image_thumb%25255B6%25255D.png?imgmax=800\\\" width=\\\"222\\\" height=\\\"147\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-73XD-8jNbg8/U1dtrpPahzI/AAAAAAAABW0/UlSYjbBx8Gs/s1600-h/image%25255B19%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-4a-PCyFWa5M/U1dtsp7CUWI/AAAAAAAABW8/btc9n2ZAEuA/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"244\\\" height=\\\"163\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-zHhXxUy9JCo/U1dttULnqJI/AAAAAAAABXE/_UZ1iOC2OBo/s1600-h/image%25255B22%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-9zZ2EkJIfMU/U1dtt7iVk8I/AAAAAAAABXM/hPFKWV-21ak/image_thumb%25255B8%25255D.png?imgmax=800\\\" width=\\\"207\\\" height=\\\"105\\\"></a>\"}],[\"code\",{\"code\":\"<appsettings>\\n  <add key=\\\"Profile\\\" value=\\\"UAT-debug\\\" xdt:transform=\\\"Insert\\\">\\n</add></appsettings>\\n\",\"language\":\"xml;\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-EF7G_5Bhsek/U1dtuykRrZI/AAAAAAAABXU/XeErJsRZPCs/s1600-h/image%25255B26%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-uKlJh1sJ63A/U1dtvvQRWSI/AAAAAAAABXc/K16S2zGhATg/image_thumb%25255B10%25255D.png?imgmax=800\\\" width=\\\"352\\\" height=\\\"94\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-iVpv87hfRIk/U1dtxAZ8FJI/AAAAAAAABXk/4105KU36sOA/s1600-h/image%25255B30%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-iYnIq-p7JFI/U1dty8jNAJI/AAAAAAAABXs/_y_vP11HLBo/image_thumb%25255B12%25255D.png?imgmax=800\\\" width=\\\"326\\\" height=\\\"193\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.hanselman.com/blog/WebDeploymentMadeAwesomeIfYoureUsingXCopyYoureDoingItWrong.aspx\"]],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/dd465326(v=vs.110).aspx\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/11/webdeploy-and-release-management.html\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I love WebDeploy – I have ever since I read Scott Hanselman’s post \"],[0,[0],1,\"“Web Deploy Made Awesome: If You’re Using XCopy, You’re Doing It Wrong”\"],[0,[],0,\". Whenever I’m helping teams that build web applications improve their ALM processes, invariable I end up moving them onto Web Deploy. Not only is it an easier and cleaner way to deploy, but you get the bonus of being able to manage configuration files (Web.config) in your project.\"]]],[1,\"h2\",[[0,[],0,\"Config as Code\"]]],[1,\"p\",[[0,[],0,\"Maturing in ALM means that you need to head towards continuous deployment. You need to be able to deploy your application with a single click of a button. However, one challenge with this is managing configurations. How do you manage Web.config files in dev, UAT and Production environments? Do you find yourself copying config files out the way before you deploy? That’s BAD. You should be managing your configs as if they were code. They need to be source controlled.\"]]],[1,\"p\",[[0,[],0,\"Fair enough, I hear you say. So you’ll just copy all your config files from all your servers into a folder and manage them from there. Better – but still lots of pain. A far better approach is to use config transforms.\"]]],[1,\"p\",[[0,[],0,\"Config transforms came into web applications in VS 2010. With the latest release of VS 2013, not only are they also available for web sites (NOTE: please don’t use websites – always use web applications!) but you can new preview your transforms from VS and there’s now support for profile-specific transforms.\"]]],[1,\"p\",[[0,[],0,\"Let’s have a look at a config transform in a newly created MVC web application. You can see that the Web.config file expands to show 2 other files – one per project configuration:\"]]],[10,0],[1,\"p\",[[0,[],0,\"If you add project configurations, then you can right-click the Web.config and select “Add config Transforms” and VS will create a new transform file for you.\"]]],[1,\"p\",[[0,[],0,\"Let’s have a look at the Web.Release.config (I’ve removed the comments that are auto-generated):\"]]],[10,1],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"You can see that in the  tag there’s an xdt:Transform to remove the debug attribute. When applied, the transform uses the structure of the transform file to find the correct element and perform the necessary transforms – whether it’s an insert, a remove or some other transform. You can read about the syntax for transforms \"],[0,[1],1,\"here\"],[0,[],0,\". Note that when you debug out of VS, your application will use the Web.config file irrespective of if you’re running in Debug or Release – the transforms only happen when you publish or package your web app.\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"This is all existing “2012” stuff. What’s new in the latest release?\"]]],[1,\"p\",[[1,[],0,2]]],[1,\"h2\",[[0,[],0,\"Preview and Profile Specific Transforms\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"You can now preview your transform by right-clicking on the transform file (Web.Release.config, for example) and selecting “Preview Transform”. This immediately shows a diff – the original Web.config on the left and the transformed config on the right:\"]]],[1,\"p\",[[1,[],0,4]]],[10,2],[1,\"p\",[[1,[],0,5]]],[1,\"p\",[[0,[],0,\"Now you can debug your transforms!\"]]],[1,\"p\",[[1,[],0,6]]],[1,\"p\",[[0,[],0,\"So let’s imagine you have a UAT environment. You want to publish your site, so you create a new project configuration called UAT. You then copy the release transforms and put in your UAT connection strings and so on. After doing some testing in UAT, you realize that you actually need to debug, so you’ll have to update your Web.UAT.config file again, or create a UAT-DEBUG project configuration. Soon you’ll get lots of project configurations, and that’s just a mess.\"]]],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"However, you can now create a profile-specific transform on top of the project configuration transforms! Let’s create a publish profile. I’ll right-click the web project and select “Publish”. I’ll create a new profile called “UAT-Debug” and give it whatever settings I need to. On the “Configuration” page, I’ll change the configuration from Release to Debug.\"]]],[1,\"p\",[[1,[],0,8]]],[10,3],[1,\"p\",[[0,[],0,\"Then I’ll click on “Close” to save the profile without publishing. That will create a pubxml file for me. I’ll repeat the process and create a publish profile called “UAT-Release”. Looking under the Properties of my project I’ll see the pubxml files:\"],[1,[],0,9]]],[10,4],[1,\"p\",[[1,[],0,10]]],[1,\"p\",[[0,[],0,\"Now I can right-click a pubxml and select “Add Config Transform” – which creates a new transform for me that’s tied to this publish profile.\"]]],[1,\"p\",[[1,[],0,11]]],[10,5],[1,\"p\",[[0,[],0,\"I’ll create one for each profile. You’ll see that I now have 4 transforms (but still only 2 project configurations – hooray!)\"],[1,[],0,12]]],[10,6],[1,\"p\",[[1,[],0,13]]],[1,\"p\",[[0,[],0,\"Let’s insert an attribute in both of the new config files:\"]]],[10,7],[1,\"p\",[[1,[],0,14]]],[1,\"p\",[[0,[],0,\"I’ve also removed the debug attribute from the compilation tag on the UAT-Release config and removed that transform from the debug one:\"]]],[1,\"p\",[[1,[],0,15]]],[10,8],[1,\"p\",[[1,[],0,16]]],[1,\"p\",[[0,[],0,\"Now when I right-click Web.UAT-Release.config and select “Preview Transform” I can see the diff. Note that the right-hand file tells me that it has actually applied 2 transforms: first the Release transform (since the profile specifies Release for the project configuration) and secondly the Web.UAT-Release.config transforms themselves.\"]]],[1,\"p\",[[1,[],0,17]]],[10,9],[1,\"p\",[[0,[],0,\"Voila! I can now maintain 2 project configurations (Debug and Release) and have a publish profile and transform config per project config and environment. Awesome!\"],[1,[],0,18]]],[1,\"p\",[[0,[],0,\"If you’re using Release Management, don’t forget to check out my post about \"],[0,[2],1,\"how to use config transforms for parameterizing releases\"],[0,[],0,\"!\"]]],[1,\"p\",[[1,[],0,19]]],[1,\"p\",[[0,[],0,\"Happy transforming!\"]]]]}","published_at":1398271020000,"status":"published","published_by":1},{"id":"96322811-9103-4f65-8f46-71493e82be79","title":"Why You Absolutely Need to Unit Test","slug":"why-you-absolutely-need-to-unit-test","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-yQl5GxEmUC4/UefXy1Vb9yI/AAAAAAAAA-I/9B7Kov4SEqU/s1600-h/image%25255B3%25255D.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; display: block; padding-right: 0px; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-Vkm4wDuUYPM/UefXzn1D3hI/AAAAAAAAA-Q/MV_eRfIz-b8/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"413\\\" height=\\\"350\\\"></a>\"}]],\"markups\":[[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/automated-buildswhy-theyre-absolutely.html\"]],[\"a\",[\"href\",\"http://www.colinsalmcorner.com/2013/06/automated-buildswhy-theyre-absolutely_18.html\"]],[\"em\"],[\"a\",[\"href\",\"http://www.ru.ac.za/computerscience/people/staff/alfredoterzoli/\"]],[\"a\",[\"href\",\"http://nakedalm.com/you-are-doing-it-wrong-if-you-are-not-using-test-first/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I’ve written about why builds are absolutely essential in modern application development (\"],[0,[0],1,\"Part 1\"],[0,[],0,\") and what why Team Build is a great build engine (\"],[0,[1],1,\"Part 2\"],[0,[],0,\"). However, if you don’t include unit tests in your builds, it’s like brushing your teeth without toothpaste – there’s a lot of movement, but it’s not the most effective way to do things. In this post, I want to put forward a few thoughts about why you absolutely need to be unit testing.\"]]],[1,\"p\",[[0,[],0,\"Here’s the highlight list:\"]]],[3,\"ol\",[[[0,[],0,\"Coding with unit testing in mind forces you to think about the design and architecture of your code – making it better code\"]],[[0,[],0,\"Unit testing provides immediate feedback – every change in code is tested (at least partly) as you’re coding\"]],[[0,[],0,\"A small investment now leads to a massive saving later – bugs in production cost way more than bugs in development\"]],[[0,[],0,\"Unit testing provides metrics for the quality of your code\"]],[[0,[],0,\"Unit testing builds inherent quality into your releases\"]]]],[1,\"p\",[[0,[],0,\"Let’s consider each of these statements.\"]]],[1,\"h2\",[[0,[],0,\"Think-Driven Development\"]]],[1,\"p\",[[0,[],0,\"You’ve probably heard of terms like “test-driven development (TDD)” or “behaviour-driven development (BDD)”. I’d love to coin another term – \"],[0,[2],1,\"think-driven development\"],[0,[],0,\". I come from a development background – and since I love coding (as most developers do), I suffer from “never-mind-the-details-I-just-want-to-start-coding” syndrome. Most developers I meet do. Hopefully you never lose this passion – but there’s a lot to be said for taking a breath and thinking first.\"]]],[1,\"p\",[[0,[],0,\"If you just jump into the code, and you don’t have tests, you’ll probably hack something out that will work initially for a couple of scenarios (usually with the attendant caveat, “it works on my machine!?!”). However, this isn’t sustainable since it leads to what one of my \"],[0,[3],1,\"university lecturer\"],[0,[],0,\"’s used to call “spaghetti code” (which was especially funny since he’s Italian). You write some code and get it deployed. Something breaks. You band-aid it and re-deploy. Now you’ve broken something else. So you add some sticky-tape (metaphorically, of course). Eventually you’re down to chewing gum and tin-foil, and things are going from bad to worse.\"]]],[1,\"p\",[[0,[],0,\"If you start out writing tests (or at least start writing your code with testing in mind), you’re more likely to come up with better code. If you’ve every tried to write unit tests for legacy code, you’ll understand what I mean. If you don’t code with tests in mind, not surprisingly, your code ends up untestable (or at least really hard to unit test). Writing testable code forces you to think of good decomposition, good interfaces, good separation of concerns, inversion of control and dependency injection and a whole slew of other principles we all learn about but somehow forget to use in our daily grind.\"]]],[1,\"p\",[[0,[],0,\"So start with your tests. This forces you to use all of the good stuff (they’re not called \"],[0,[2],1,\"best practices\"],[0,[],0,\" for nothing). The little bit of thinking required up-front is going to save you a whole lot of pain down the line (not to mention decrease the amount of chewing gum you find in your source repository).\"]]],[1,\"h2\",[[0,[],0,\"Immediate Feedback\"]]],[1,\"p\",[[0,[],0,\"Let me ask you a question – how long is the feedback loop between the time you write some code and when you get feedback about the validity of that code? Or phrased another way, think about the last bug you fixed. What’s the length of time between the writing of the code and the report of the bug? One day? One week? One year?\"]]],[1,\"p\",[[0,[],0,\"The fact is that the sooner after writing code that you find a bug, the cheaper it is to fix. Let’s consider two ends of the spectrum: coding-time and in production.\"]]],[1,\"p\",[[0,[],0,\"When a bug is found in production, it can take a while to find it. Then it’s reported to a service desk. They investigate. They then escalate to 2nd line support. They investigate. They then escalate to the developers, who have to investigate and repro. In other words, a long time.\"]]],[1,\"p\",[[0,[],0,\"What about when you’re coding? You write some code and run your unit tests. The test fails. You investigate and fix. In other words, a really short time.\"]]],[1,\"p\",[[0,[],0,\"The quicker you get feedback (and the more often you get feedback) the more efficient and effective you’re going to be. On a large scale, this is broadly the reason for Agile methodologies – the rapid iteration increases the frequency of the feedback loop. It works at a project management level, and it works at a coding level too.\"]]],[1,\"p\",[[0,[],0,\"Here’s how that looks on a graph:\"]]],[10,0],[1,\"p\",[[0,[],0,\"The earlier you find your bugs, the less time you’ll spend finding them and the cheaper it is to fix them. If you don’t have unit tests, you’re already moving your first feedback opportunity to the 3rd zone (manual testing). If you don’t do manual testing, you’re pushing it further out again into UAT. If you don’t do that – then the 1st feedback you’re going to get is from production – which is the most time-intensive and the most costly.\"]]],[1,\"p\",[[0,[],0,\"Getting immediate feedback while you’re coding is great when you’re doing “greenfield” projects (projects that have never been deployed before. It really starts to shine when you come back after 6 (or 12 or 18) months to add features – you still have your suite of tests to ensure that you’re not breaking anything with your new code.\"]]],[1,\"h2\",[[0,[],0,\"Spend a little now – save a lot later on\"]]],[1,\"p\",[[0,[],0,\"Almost without fail the teams that don’t unit test claim that “they don’t have time to unit test”. I argue that in the vast majority of such cases, it’s exactly because you don’t invest in unit tests that you don’t have time to code properly. Every time you deploy untested code, you increase your technical debt. The sad fact is that technical debt tends to grow exponentially.\"]]],[1,\"p\",[[0,[],0,\"Look again at the graph above. Where do you think you’ll spend more time finding and fixing bugs? Obviously the further “to the right” you are, the more time you need to fix a bug. So invest a little “now” while you’re coding, so that you don’t have to spend a lot of time later on dealing with production issues!\"]]],[1,\"h2\",[[0,[],0,\"Quality Metrics\"]]],[1,\"p\",[[0,[],0,\"How do you measure the quality of your code? Bugs in Production per time-period? Mean Time to Resolve (MTTR)? Most of these measurements are helpful to some degree, but because they’re “production time” statistics, the feedback is too late in the cycle.\"]]],[1,\"p\",[[0,[],0,\"The teams I work with that do have metrics invariably have unit test metrics – pass / fail rates and code coverage statistics. The teams that don’t have metrics almost always don’t have unit tests. Unit tests provide you with an objective measure of the quality of your code. How can you improve if you don’t know where you currently are?\"]]],[1,\"p\",[[0,[],0,\"What coverage should you be aiming for? This is debatable, but I always recommend that you rather concentrate on your trends – make sure each deployment is better than the last. That way the absolute number doesn’t really matter. But the only way you can do any sort of trend analysis is if you’re actually collecting the metrics. That’s why I love unit tests (with code coverage) in TFS Team Builds, since the metrics get collated into the warehouse and it’s really easy to do trend reports. Showing business a steadily improving graph of quality is one of the best things you’ll ever do as a developer!\"]]],[1,\"h2\",[[0,[],0,\"Inherent Quality\"]]],[1,\"p\",[[0,[],0,\"All of these practices lead to \"],[0,[2],1,\"inherent\"],[0,[],0,\" quality – a quality that’s built into what you’re doing day-to-day, rather than something you tack on at the end of an iteration (or quickly before release). If you build inherent quality into your processes and practices, you can sleep soundly the night of your big deployment. If you don’t, you’ll have to keep the phone close-by for all those inevitable calls of, “Help, the system is broken…”\"]]],[1,\"h2\",[[0,[],0,\"Legacy Code\"]]],[1,\"p\",[[0,[],0,\"In my previous company we had large amounts of “legacy” code – code that was in production, that we constantly supported, that had no tests. When I started pushing for unit tests and coverage, we inevitably got to the awkward moment where someone blurted, “There’s just too much code to start testing now!”. I argued that 1 test was better than 0 tests. And 2 tests are better than 1 test, and so on.\"]]],[1,\"p\",[[0,[],0,\"We then made it a policy that whatever code you worked on (be that new features or bug-fixes) needed to be unit tested. This started us on the path to building up our comprehensive test suite. We also put a policy in place that your code coverage had to be higher for this deployment than the previous deployment in order for your deployment to be approved. At first our coverage was 1.5 or 2%. After only 6 months, we were somewhere in the 40%’s for coverage. And the number of production issues we were working on was decreased dramatically.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Just like you can’t afford not to do automated builds, you really can’t afford not to do unit testing. The investment now will provide massive benefits all along your application lifecycle – not only for you and your team, but for your business and stakeholders too.\"]]],[1,\"p\",[[0,[],0,\"For more reading, Martin Hinshelwood wrote a \"],[0,[4],1,\"great post\"],[0,[],0,\" about test-first that I highly recommend.\"]]],[1,\"p\",[[0,[],0,\"Happy testing!\"]]]]}","published_at":1374180960000,"status":"published","published_by":1},{"id":"6e9734a0-0b2e-4127-8f0d-c5339ea01d47","title":"Why You Should Switch to Build VNext","slug":"why-you-should-switch-to-build-vnext","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"Param(\\n  [string]$pathToSearch = $env:BUILD_SOURCESDIRECTORY,\\n  [string]$buildNumber = $env:BUILD_BUILDNUMBER,\\n  [string]$searchFilter = \\\"VersionInfo.\\\",\\n  [regex]$pattern = \\\"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\"\\n)\\n \\nif ($buildNumber -match $pattern -ne $true) {\\n    . . .\\n}\\n\",\"language\":\"ps;\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/c44d3dfa-16c9-4752-b24c-05238e17949e.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/8cc692f7-904d-459c-8091-b19b18bde139.png\\\" width=\\\"358\\\" height=\\\"182\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/9e6ee14f-69ca-461a-9072-3d2adb4b363e.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/6b058cfb-ede4-4b1a-a91d-b99b0943517d.png\\\" width=\\\"289\\\" height=\\\"182\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d398df7e-511d-470d-8ca7-9afcc17026ec.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; border-bottom: 0px; float: none; margin-left: auto; border-left: 0px; display: block; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/b7c8345d-5e98-4863-89e8-fb9ddb921303.png\\\" width=\\\"304\\\" height=\\\"222\\\"></a>\"}],[\"html\",{\"html\":\"<a href=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/d8b0cc35-039e-498d-ab21-95bf5e7a8b88.png\\\"><img title=\\\"image\\\" style=\\\"border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; float: none; margin-left: auto; display: block; border-top-width: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"https://colinsalmcorner.azureedge.net/ghostcontent/images/files/80812162-c566-43a4-b073-7fec889fce6c.png\\\" width=\\\"388\\\" height=\\\"146\\\"></a>\"}],[\"code\",{\"code\":\"Param(\\n  [string]$srcDir = $env:BUILD_SOURCESDIRECTORY\\n)\\n\\ntry {\\n    if (Test-Path(\\\".\\\\empty\\\")) {\\n        del .\\\\empty -Recurse -Force\\n    }\\n    mkdir empty\\n\\n    robocopy .\\\\empty \\\"$srcDir\\\\src\\\\Nwc.Web\\\\node_modules\\\" /MIR &gt; robo.log\\n    del .\\\\empty -Recurse -Force\\n    del robo.log -Force\\n\\n    Write-Host \\\"Successfully deleted node_modules folder\\\"\\n    exit 0\\n} catch {\\n    Write-Error $_\\n    exit 1\\n}\\n\",\"language\":\"ps;\"}]],\"markups\":[[\"a\",[\"href\",\"http://vsalmdocs.azurewebsites.net/library/vs/alm/build/overview\"]],[\"strong\"],[\"a\",[\"href\",\"https://github.com/Microsoft/vso-agent-tasks/\"]],[\"a\",[\"href\",\"https://github.com/Microsoft/vso-agent\"]],[\"a\",[\"href\",\"http://vsalmdocs.azurewebsites.net/library/vs/alm/build/scripts/variables\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Now that \"],[0,[0],1,\"VNext builds\"],[0,[],0,\" are in Preview, you should be moving your build definitions over from the “old” XAML definitions to the new VNext definitions. Besides the fact that I suspect at some point that XAML builds will be deprecated, the VNext builds are just much better, in almost every respect.\"]]],[1,\"h2\",[[0,[],0,\"Why Switch?\"]]],[1,\"p\",[[0,[],0,\"There are several great reasons to switch to (or start using) the new VNext builds. Here’s a (non-exhaustive) list of some of my favorites:\"]]],[3,\"ol\",[[[0,[1],1,\"Build is now an orchestrator, not another build engine.\"],[0,[],0,\"This is important – VNext build is significantly different in architecture from the old XAML engine. Build VNext is basically just an orchestrator. That means you can orchestrate whatever build engine (or mechanism) you already have – no need to lose current investments in engines like Ant, CMake, Gradle, Gulp, Grunt, Maven, MSBuild, Visual Studio, Xamarin, XCode or any other existing engine. “Extra” stuff – like integrating with work items, publishing drops and test results and other “plumbing” is handled by Build.VNext.\"]],[[0,[1],1,\"Edit build definitions in the Web\"],[0,[],0,\". You no longer have to download, edit or – goodness – learn a new DSL. You can stitch together fairly complex builds right in Web Access.\"]],[[0,[1],1,\"Improved Build Reports\"],[0,[],0,\". The build reports are much improved – especially Test Results, which are now visible on the Web (with nary a Visual Studio in sight).\"]],[[0,[1],1,\"Improved logging\"],[0,[],0,\". Logging in VNext builds is significantly better – the logs are presented in a console window, and not hidden somewhere obscure.\"]],[[0,[1],1,\"Improved Triggers\"],[0,[],0,\". The triggers have been improved – you can have multiple triggers for the same build, including CI triggers (where a checkin/commit triggers the build) and scheduled triggers.\"]],[[0,[1],1,\"Improved Retention Policies\"],[0,[],0,\". Besides being able to specify multiple rules, you can now also use “days” to keep builds, rather than “number” of builds. This is great when a build is being troublesome and produces a number of builds – if you were using “number of builds” you’d start getting drop-offs that you don’t really want.\"]],[[0,[1],1,\"Composability\"],[0,[],0,\". Composing builds from the Tasks is as easy as drag and drop. Setting properties is a snap, and properties such as “Always Run” make the workflow easy to master.\"]],[[0,[1],1,\"Simple Customization\"],[0,[],0,\". Have scripts that you want to invoke? No problem – drag on a “PowerShell” or “Bat” Task. Got a one-liner that needs to execute? No problem – use the “Command Line” task and you’re done. No mess, no fuss.\"]],[[0,[1],1,\"Deep Customization\"],[0,[],0,\". If the Tasks aren’t for you, or there isn’t a Task to do what you need, then you can easily create your own.\"]],[[0,[1],1,\"Open Source Toolbox\"],[0,[],0,\". Don’t like the way an out-of-the-box Task works? Simply download its source code from the \"],[0,[2],1,\"vso-agent-tasks Github repo\"],[0,[],0,\", and fix it! Of course you can share your awesome Tasks once you’ve created them so that the community benefits from your genius (or madness, depending on who you ask!)\"]],[[0,[1],1,\"Cross Platform\"],[0,[],0,\". The \"],[0,[3],1,\"cross-platform agent\"],[0,[],0,\" will run on Mac or Linux. There’s obviously a windows agent too. That means you can build on whatever platform you need to.\"]],[[0,[1],1,\"Git Policies\"],[0,[],0,\". Want to make sure that a build passes before accepting merges into a branch? No problem – set up a VNext build, and then add a Policy to your branch that forces the build to run (and pass) before merges are accepted into the branch (via Pull Requests). Very cool.\"]],[[0,[1],1,\"Auditing\"],[0,[],0,\". Build definitions are now stored as JSON objects. Every change to the build (including changing properties) is kept in a history. Not only can you see who changed what when, but you can do side-by-side comparisons of the changes. You can even enter comments as you change the build to help browse history.\"]],[[0,[1],1,\"Templating\"],[0,[],0,\". Have a brilliant build definition that you want to use as a template for other builds? No problem – just save your build definition as a template. When you create a new build next time, you can start from the template.\"]],[[0,[1],1,\"Deployment\"],[0,[],0,\". You can now easily deploy your assets. This is fantastic for Continuous Delivery – not only can you launch a build when someone checks in (or commits) but you can now also include deployment (to your test rigs, obviously!). Most of the deployment love is for Azure – but since you can create your own Tasks, you can create any deployment-type Task you want.\"]],[[0,[1],1,\"Auto-updating agents\"],[0,[],0,\". Agents will auto-update themselves – no need to update every agent in your infrastructure.\"]],[[0,[1],1,\"Build Pools and Queues\"],[0,[],0,\". No more limitations on “1 TPC per Build Controller and 1 Controller per build machine”. Agents are xcopyable, and live in a folder. That means you can have as many agents (available to as many pools, queues and TPCs as you want) on any machine. The security and administration of the pools, queues and agents is also better in build vNext.\"]],[[0,[1],1,\"Capabilities and Demands\"],[0,[],0,\". Agents will report their “capabilities” to TFS (or VSO). When you create builds, the sum of the capabilities required for each Task is the list of “demands” that the build requires. When a build is queued, TFS/VSO will find an agent that has capabilities that match the demands. A ton of capabilities are auto-discovered, but you can also add your own. For example, I added “gulp = 0.1.3” to my build agent so that any build with a “Gulp” task would know it could run on my agent. This is a far better mechanism of matching agents to builds than the old “Agent Tags”.\"]]]],[1,\"p\",[[0,[],0,\"Hopefully you can see that there are several benefits to switching. Just do it! It’s worth noting that there are also Hosted VNext agents, so you can run your VNext builds on the “Hosted” queue too. Be aware though that the image for the agent is “stock”, so it may not work for every build. For example, we’re using TypeScript 1.5 beta, and the hosted agent build only has TypeScript extensions 1.4, so our builds don’t work on the Hosted agents.\"]]],[1,\"h2\",[[0,[],0,\"Environment Variables Name Change\"]]],[1,\"p\",[[0,[],0,\"When you use a PowerShell script Task, the script is invoked in a context that includes a number of \"],[0,[4],1,\"predefined environment variables\"],[0,[],0,\". Need access to the build number? No problem – just look up $env.BUILD_BUILDNUMBER. It’s way easier to use the environment variables that to remember how to pass parameters to the scripts. Note – the prefix “TF_” has been dropped – so if you have PowerShell scripts that you were invoking as pre- or post-build or test scripts in older XAML builds, you’ll have to update the names.\"]]],[1,\"p\",[[0,[],0,\"Just a quick tip: if you directly access $env.BUILD_BUILDNUMBER in your script, then you have to set the variable yourself before testing the script in a PowerShell console. I prefer to use the value as a default for a parameter – that way you can easily invoke the script outside of Team Build to test it. Here’s an example:\"]]],[10,0],[1,\"p\",[[0,[],0,\"See how I default the $pathToSearch and $buildNumber parameters using the $env variable? Invoking the script myself when testing is then easy – just supply values for the variables explicitly.\"]]],[1,\"h2\",[[0,[],0,\"Node Packages – Path too Long\"]]],[1,\"p\",[[0,[],0,\"I have become a fan of node package manager – npm. Doing some web development recently, I have used it a log. The one thing I have against it (admittedly this is peculiar only to npm on Windows), is that the node_modules path can get very deep – way longer than good ol’ 260 character limit.\"]]],[1,\"p\",[[0,[],0,\"This means that any Task that does a wild-char search on folders is going to error when there’s a node_modules folder in the workspace. So you have to explicitly specify the paths to files like sln or test (for the VSBuild and VSTest Tasks) respectively – you can’t use the “**/*.sln” path wildcard (**) because it will try to search in the node_modules folder, and will error out when the path gets too long. No big deal – I just specify the path using the repo browser dialog. I was also forced to check “Continue on Error” on the VSBuild Task – the build actually succeeds (after writing a “Path too long” error in the log), but because the Task outputs the “Path too long” error to stderr, the Task fails.\"]]],[10,1],[1,\"p\",[[0,[],0,\"EDIT: If you are using npm and run into this problem, you can uncheck “Restore NuGet Packages” (the VSBuild Task internally does a wild-card search for package.config, and this is what is throwing the path too long error as it searches the node_modules folder). You’ll then need to add a “Nuget installer” Task before the VSBuild task and explicitly specify the path to your sln file.\"]]],[10,2],[10,3],[1,\"h2\",[[0,[],0,\"Migrating from XAML Builds\"]]],[1,\"p\",[[0,[],0,\"Migrating may be too generous – you have to re-create your builds. Fortunately, trying to move our build from XAML to VNext didn’t take all that long, even with the relatively large customizations we had – but I was faced with Task failures due to the path limit, and so I had to change the defaults and explicitly specify paths wherever there was a “**/” folder. Also, the npm Task itself has a bug that will soon be fixed – for now I’m getting around that by invoking “npm install” as part of a “Command Line” Task (don’t forget to set the working directory):\"]]],[10,4],[1,\"h3\",[[0,[],0,\"No PreGet Tasks\"]]],[1,\"p\",[[0,[],0,\"At first I had npm install before my “UpdateVersion” script – however, the UpdateVersion script searches for files with a matching pattern using Get-ChildItem. Unfortunately, this errors out with “path too long” when it goes into the node_modules directory. No problem, I thought to myself – I’ll just run UpdateVersion before npm install. That worked – but the build still failed on the VSBuild Task. So I set “Continue on Error” on the VSBuild Task – and I got a passing build!\"]]],[1,\"p\",[[0,[],0,\"I then queued a new build – and the build failed. The build agent couldn’t even get the sources because – well, “Path too long”. Our XAML build actually had a “pre-Pull” script hook so that we could delete the node_modules folder (using RoboCopy which can handle too long paths). However, VNext builds cannot execute Tasks before getting sources. Fortunately Chris Patterson, the build PM, suggested that I run the delete at the end of the build.\"]]],[1,\"p\",[[0,[],0,\"Initially I thought this was a good idea – but then I thought, “What if the build genuinely fails – like failed tests? Then the ‘delete’ task won’t be run, and I won’t be able to build again until I manually delete the agent’s working folder”. However, when I looked at the Tasks, I saw that there is a “Run Always” checkbox on the Task! So I dropped a PowerShell Task at the end of my build that invokes the “CleanNodeDirs.ps1” script, and check “Always Run” so that even if something else in the build fails, the CleanNodeDirs script always runs. Sweet!\"]]],[1,\"h3\",[[0,[],0,\"CleanNodeDirs.ps1\"]]],[1,\"p\",[[0,[],0,\"To clean the node_modules directory, I initially tried “rm –rf node_modules”. But it fails – guess why? “Path too long”. After searching around a bit, I came across a way to use RoboCopy to delete folders. Here’s the script:\"]]],[10,5],[1,\"h2\",[[0,[],0,\"Build.VNext Missing Tasks\"]]],[1,\"p\",[[0,[],0,\"There are a couple of critical Tasks that are still missing:\"]]],[3,\"ol\",[[[0,[],0,\"No “Associate Work Items” Task\"]],[[0,[],0,\"No “Create Work Item on Build Failure” Task\"]],[[0,[],0,\"No “Label sources” Tasks\"]]]],[1,\"p\",[[0,[],0,\"These will no doubt be coming soon. It’s worth working on converting your builds over anyway – when the Tasks ship, you can just drop them into your minty-fresh builds!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"You really need to be switching over to BuildVNext – even though it’s still in preview, it’s still pretty powerful. The authoring experience is vastly improved, and the Task library is going to grow rapidly – especially since it’s open source. I’m looking forward to what the community is going to come up with.\"]]],[1,\"p\",[[0,[],0,\"Happy building!\"]]]]}","published_at":1432327119000,"status":"published","published_by":1},{"id":"5da82e54-0ab7-4d15-9656-ed4d8acb755c","title":"Workspaces: Updating the owner","slug":"workspaces-updating-the-owner","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-lUJI6YV8boc/TxU81Kcm_5I/AAAAAAAAAU4/YSmqTMWiexY/s1600-h/image3.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-3aTW1F9wLTo/TxU83qyh8sI/AAAAAAAAAVA/_t6o80zFlS8/image_thumb1.png?imgmax=800\\\" width=\\\"334\\\" height=\\\"168\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tf workspaces</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-eJy28msIT_w/TxU85nC-v7I/AAAAAAAAAVI/veXItMsiIrY/s1600-h/image%25255B4%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh6.ggpht.com/-nx-bTh1nkjc/TxU87lEusaI/AAAAAAAAAVQ/hQGM92PRhIs/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"465\\\" height=\\\"96\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tf workspaces /collection:tpcUrl /owner:*</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh6.ggpht.com/-xDCuCF8GPFs/TxU89DyI8bI/AAAAAAAAAVY/Jekvqx2H_fI/s1600-h/image%25255B8%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh4.ggpht.com/-kctwalPO8mg/TxU8_OxuP_I/AAAAAAAAAVg/2_qAvJrWagc/image_thumb%25255B3%25255D.png?imgmax=800\\\" width=\\\"486\\\" height=\\\"96\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tf workspace /name:workspacename;workspaceowner /collection:tpcUrl</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-B5nJv8SHY40/TxU9B6eTWXI/AAAAAAAAAVo/8lKa6aCnehk/s1600-h/image%25255B12%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-eV3OTwvKX34/TxU9EEbpawI/AAAAAAAAAVw/8P-18lr6jtU/image_thumb%25255B5%25255D.png?imgmax=800\\\" width=\\\"369\\\" height=\\\"173\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tf status /collection:tpcUrl /workspace:workspacename;workspaceowner</font>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tf workspace /name:workspacename;workspaceowner /collection:tpcUrl</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-rWQ-NNl6yE4/TxU9FwnnuWI/AAAAAAAAAV4/MIHVq7_A_Ps/s1600-h/image%25255B16%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-InKi0a4e6ac/TxU9ILxZDDI/AAAAAAAAAWA/-d84ixOfjTw/image_thumb%25255B7%25255D.png?imgmax=800\\\" width=\\\"346\\\" height=\\\"290\\\"></a>\"}],[\"html\",{\"html\":\"<font face=\\\"Courier New\\\">tf shelve /recursive name;owner *.* /move</font>\"}],[\"html\",{\"html\":\"<a href=\\\"http://lh3.ggpht.com/-fAllOFTm59s/TxU9J7qW-lI/AAAAAAAAAWI/3D7-zg01PII/s1600-h/image%25255B20%25255D.png\\\"><img style=\\\"background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; margin-left: auto; border-top: 0px; margin-right: auto; border-right: 0px; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh3.ggpht.com/-k1O-d3zcd9M/TxU9MP4p0zI/AAAAAAAAAWQ/2E30rPD3WLw/image_thumb%25255B9%25255D.png?imgmax=800\\\" width=\\\"402\\\" height=\\\"339\\\"></a>\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Recently we changed our internal domain. It was a little bit of a pain, since I had to migrate my setting and preferences. One “snag” I hit was reconnecting to TFS once I had changed my primary login from the old domain to the new domain.\"]]],[1,\"p\",[[0,[],0,\"If you change your username (or domain), open Visual Studio and go to the Source Control Explorer, you’ll see that (of course) you don’t have any mappings in your default workspace. If you try to create a mapping to an existing folder on your hard-drive, you’ll get an error stating that the folder is mapped in another workspace:\"]]],[10,0],[1,\"p\",[[0,[],0,\"So you drop into the Visual Studio command prompt and use the tf.exe command to list the workspaces:\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"However (and this may be a bug, I’m not sure), this only brings back workspaces for the current user (on the new domain). So if you want to see all the workspaces (for any user) you’ll have to enter this command:\"]]],[10,3],[1,\"p\",[[0,[],0,\"(Of course tpcUrl is the url of your team project collection).\"]]],[10,4],[1,\"p\",[[0,[],0,\"Now that you can see all the workspaces, you’ll want to update the owner of the old workspace.\"]]],[10,5],[1,\"p\",[[0,[],0,\"This launches the dialogue for the workspace properties. So I tried to just update the name in the owner field, but there were pending changes in the workspace, so I got an error message:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Back to the console. To see the pending changes in the old workspace change dir to the root of your workspace mapping and run the following command:\"]]],[10,7],[1,\"p\",[[0,[],0,\"This lists all the pending changes. Now from the list of changes, I could see some that I wanted to keep and others that I didn’t – but I didn’t want to think about sorting the changes out now. So I decided to follow the previous error log’s advice and shelve the pending changes.\"]]],[1,\"p\",[[0,[],0,\"To “select” the old workspace from the command prompt, run the tf workspace command:\"]]],[10,8],[1,\"p\",[[0,[],0,\"When the dialogue opens, change the “Permissions” to Public (if it’s not set already) to make sure that the new user can shelve in the old user’s workspace and press the OK button.\"]]],[10,9],[1,\"p\",[[0,[],0,\"Now you can run the shelve command:\"]]],[10,10],[1,\"p\",[[0,[],0,\"(Here name is the name of the shelveset and owner is the domain\\\\username of the new user)\"]]],[1,\"p\",[[0,[],0,\"The shelve dialogue opens, so you can review the shelveset. Press Shelve to complete the operation.\"]]],[1,\"p\",[[0,[],0,\"Now run the workspace command again, and change the owner:\"]]],[10,11],[1,\"p\",[[0,[],0,\"(I had to rename the new workspace since it had the same name as the old one before this worked).\"]]],[1,\"p\",[[0,[],0,\"Finally, to get the pending changes back, you need to unshelve. Open the Pending Changes window (View->Other Windows->Pending Changes in VS) and press the Unshelve button. Find the shelveset that you saved the pending changes in and select it. Then hit Unshelve and your pending changes are back.\"]]],[1,\"p\",[[0,[],0,\"Now your workspace is updated to the new user, and you can get back to work!\"]]]]}","published_at":1326827940000,"status":"published","published_by":1},{"id":"c06a21f4-38e2-4e8b-a5e0-cf6ac4cbd9d6","title":"WpfCell – The Key to Coded UI Automation of DataGrids","slug":"wpfcell--the-key-to-coded-ui-automation-of-datagrids","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://lh4.ggpht.com/-NDzCm1n52lM/TrOnvyA17wI/AAAAAAAAAUo/hYcDlxTMcqI/s1600-h/image%25255B3%25255D.png\\\"><img style=\\\"background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: block; float: none; border-top-width: 0px; border-bottom-width: 0px; margin-left: auto; border-left-width: 0px; margin-right: auto; padding-top: 0px\\\" title=\\\"image\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://lh5.ggpht.com/-Ja3SyJ25esY/TrOnxFuRPqI/AAAAAAAAAUw/uyUTQK2EA-U/image_thumb%25255B1%25255D.png?imgmax=800\\\" width=\\\"392\\\" height=\\\"280\\\"></a>\"}],[\"code\",{\"code\":\"internal void ValidateCell(int row, int column, string value)<br>{<br>    var cell = FindCell(row, column);<br>    Assert.AreEqual(value, cell.Value);<br>}<br><br>internal void UpdateCell(int row, int column, string value)<br>{<br>    var cell = FindCell(row, column);<br>    cell.Value = value;<br>    Keyboard.SendKeys(\\\"\\\\t\\\");<br>}<br><br>private WpfCell FindCell(int row, int column)<br>{<br>    var cell = new WpfCell(DemoWindow.UIItem.Table);<br>    cell.SearchProperties.Add(WpfCell.PropertyNames.RowIndex, row.ToString());<br>    cell.SearchProperties.Add(WpfCell.PropertyNames.ColumnIndex, column.ToString());<br>    return cell;<br>}<br>\",\"language\":\"xml; ruler\"}],[\"code\",{\"code\":\"UIMap.ValidateCell(0, 1, \\\"135\\\");<br>UIMap.UpdateCell(0, 1, \\\"140\\\");<br>UIMap.ValidateCell(0, 1, \\\"140\\\");<br>\",\"language\":\"xml; ruler\"}]],\"markups\":[[\"a\",[\"href\",\"http://colinsalmcorner.blogspot.com/2011/11/genericautomationpeer-helping-coded-ui.html\"]],[\"strong\"],[\"a\",[\"href\",\"http://msdn.microsoft.com/en-us/library/microsoft.visualstudio.testtools.uitesting.wpfcontrols.aspx\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"As a follow on from my \"],[0,[0],1,\"post\"],[0,[],0,\" yesterday about some of the intricacies of using Coded UI testing on WPF applications, I wanted to give some details about using the WpfCell class to automate validation and updates into DataGrids.\"]]],[1,\"p\",[[0,[1],1,\"The Problem:\"],[0,[],0,\" When you bind data to a datagrid, the test recorder often complains that the rows (and / or) cells don’t have any “good identification property”. This prevents both validating cell contents as well as recording actions (such as updates) onto the cell.\"]]],[1,\"p\",[[0,[],0,\"The reason for this is that the cells don’t have any good automation properties – they all look the same as far as the UIMap is concerned. If you have a DataGrid with a cell that has the text “135” in it, and you find the control using the coded UI Test builder, you’ll see the Name of the cell is “135”. But what if the data isn’t always 135? What if the cell value that displays is dependent on the current record? The framework won’t be able to find that cell and the test will fail.\"]]],[10,0],[1,\"h2\",[[0,[],0,\"The Solution: WpfCell\"]]],[1,\"p\",[[0,[],0,\"Using WpfCell (from the \"],[0,[2],1,\"Microsoft.VisualStudio.TestTools.UITesting.WpfControls\"],[0,[],0,\" namespace) lets us access cells logically (instead of visually). Add the following code to the UIMap.cs file in your coded UI test project:\"]]],[10,1],[1,\"p\",[[1,[],0,0]]],[1,\"p\",[[0,[],0,\"You’ll have to change the Table that is used in the FindCell method – you’ll find the table class generated into the UIMap if you look at the controls (you may have to record a click in the table or something to get this control into the map). The only bit that was interesting here was the Keyboard.SendKeys(“\\\\t”) which presses tab – this actually confirms the update of the value into the cell (otherwise the cell is still “dirty” and the update won’t actually have happened).\"]]],[1,\"p\",[[1,[],0,1]]],[1,\"p\",[[0,[],0,\"Now from within your coded UI Test, you can add code like this:\"]]],[10,2],[1,\"p\",[[1,[],0,2]]],[1,\"p\",[[0,[],0,\"Happy (DataGrid) testing!\"]]]]}","published_at":1320429120000,"status":"published","published_by":1},{"id":"a6dca067-ea25-4eaa-9e1a-20c44b162a84","title":"You Suck: Or, How to Process Criticism","slug":"you-suck-or-how-to-process-criticism","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<a href=\\\"http://www.colinsalmcorner.com/posts/files/9ae10199-c15a-460c-860a-d617e8f4239f.png\\\"><img title=\\\"image\\\" style=\\\"border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; float: none; padding-top: 0px; padding-left: 0px; margin-left: auto; border-left: 0px; display: block; padding-right: 0px; margin-right: auto\\\" border=\\\"0\\\" alt=\\\"image\\\" src=\\\"http://www.colinsalmcorner.com/posts/files/6cc8f12b-a071-44f9-be4d-b0ec63785175.png\\\" width=\\\"311\\\" height=\\\"416\\\"></a>\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://www.flickr.com/photos/innoxiuss/308920352\"]],[\"a\",[\"href\",\"https://www.flickr.com/photos/innoxiuss/\"]],[\"a\",[\"href\",\"https://creativecommons.org/licenses/by/2.0/\"]]],\"sections\":[[10,0],[1,\"p\",[[0,[],0,\"Recently I received some criticism from a customer. Sometimes I find it difficult to process criticism – I justify or argue or dismiss. Some of that is my personality – I like to be right! Part of that is the fact that I strive for excellence, so when I’m told I missed the mark, it can feel like I’m being told I’m a failure. You, dear reader, probably strive for perfection – but let’s face facts: we’re not perfect. If you’re like me and you have a difficult time receiving criticism, then this post is for you – hopefully you can learn something from how I process.\"]]],[1,\"h2\",[[0,[],0,\"I’m Not Perfect\"]]],[1,\"p\",[[0,[],0,\"This one’s tough. My natural instinct when receiving criticism is to justify. For example, the criticism might be, “You didn’t finish when you said you would.” My inclination is to retort: “Well, you weren’t clear enough on what you wanted,” or something like that. However, the most critical key to successfully processing criticism is to \"],[0,[0],1,\"remain teachable\"],[0,[],0,\" – and that means acknowledging that I missed the mark. I have to tell myself not to argue, not to justify. I have to take a step back and see the situation from the other perspective.\"]]],[1,\"h2\",[[0,[],0,\"I Have Blind Spots\"]]],[1,\"p\",[[0,[],0,\"That leads to the second critical principle – I have blind spots. No matter how much I stare in the mirror to gel my hair to perfection, I still can’t see what’s going on with that stubborn crown on the back of my head! Even if I’m prone to introspection and self-improvement, I’m going to miss stuff. About me. If I reject criticism outright, I’ll never get a chance to see into those blind spots. I have to let criticism be a catalyst to stepping back and honestly assessing what I said or did from someone else’s perspective. I can only improve if there’s something to work on – so I have to let criticism surface things that I can work on.\"]]],[1,\"h2\",[[0,[],0,\"I Am Not Defined By a Moment\"]]],[1,\"p\",[[0,[],0,\"This is a big one for me – I can tend to take criticism hard, so it becomes overwhelming. I have to realize that even if I blow it, that moment (or engagement) doesn’t define me. I’m more than this one moment. I may have gotten this thing wrong, but I get a lot of things right too! Remembering previous moments where I got things right helps me process moments when I get things wrong.\"]]],[1,\"h2\",[[0,[],0,\"I Can’t Always Win\"]]],[1,\"p\",[[0,[],0,\"Sometimes, no matter how hard I try, I can’t win. Someone is going to be disappointed in something I did or said. Most of the time I don’t set out to disappoint, but life happens. Expectations aren’t clear, or are just different, or communication fails. Things beyond my control happen. I have admit that I lost a round – as long as I get up and keep on going!\"]]],[1,\"h2\",[[0,[],0,\"Learning is a Team Sport\"]]],[1,\"p\",[[0,[],0,\"Sometimes criticism is deserved. Sometimes it isn’t. And sometimes it’s hard to tell the difference. I make sure I surround myself with people that know and love me – that way, when I’m criticized I have a team I can go to. I like to make my team diverse – my colleagues of course, but also my friends and family. Even if the criticism is work-related, sometimes having a “personal” perspective can help process a “professional” issue. I also make sure I get someone who’s more experienced than me who can mentor me through a situation.\"]]],[1,\"p\",[[0,[],0,\"Often criticism has some meat and some bones. Take the meat, spit out the bones. My team helps me to sort the meat from the bones. They help me to keep things in perspective.\"]]],[1,\"h2\",[[0,[],0,\"Make it Right\"]]],[1,\"p\",[[0,[],0,\"Finally, if it’s appropriate to do so, make it right. Sometimes I can take some criticism and just improve, learn and get better. Sometimes I may need to make things right. My team helps me figure out “action items” – things I can do to improve, but also things that I can do to make it right. This doesn’t always apply, but I like to look for things to do or say that will make things right. Although doing this without justifying myself is challenging for me!\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Unless you’re particularly reclusive, you’ll get criticized at some point. Learning how to embrace and deal with criticism is an important skill to learn. If you use it as a chance to learn and improve, and surround yourself with people who can coach and encourage you, you can process criticism positively and become better!\"]]],[1,\"p\",[[0,[],0,\"Happy learning!\"]]],[1,\"p\",[[0,[],0,\"* \"],[0,[1],1,\"Image\"],[0,[],0,\" by \"],[0,[2],1,\"innoxiuss\"],[0,[],0,\" used under \"],[0,[3],1,\"Creative Commons\"]]]]}","published_at":1477998685000,"status":"published","published_by":1}],"posts_tags":[{"tag_id":14,"post_id":"7a05b039-c6bc-41d5-9867-7e1deed48020"},{"tag_id":13,"post_id":"766de1fc-9caa-4086-aa6f-440f6b95c40f"},{"tag_id":4,"post_id":"0782b46c-8d2c-44cf-bcf1-d8b127aefd6d"},{"tag_id":2,"post_id":"0782b46c-8d2c-44cf-bcf1-d8b127aefd6d"},{"tag_id":5,"post_id":"0782b46c-8d2c-44cf-bcf1-d8b127aefd6d"},{"tag_id":10,"post_id":"3d89a6d3-fdd0-47fa-8851-87e58d94b4e8"},{"tag_id":11,"post_id":"b8336a26-79cb-40b1-87f5-6f1ad62bf89f"},{"tag_id":5,"post_id":"b8336a26-79cb-40b1-87f5-6f1ad62bf89f"},{"tag_id":11,"post_id":"afd683da-1062-474e-87a6-aa871dc3a1b6"},{"tag_id":8,"post_id":"6906f0b6-6bba-40be-bf4f-7b4532d52870"},{"tag_id":7,"post_id":"2dd3eb55-32b7-482e-8f05-0091807c8040"},{"tag_id":7,"post_id":"59a5b680-7651-440d-8286-901593717cc1"},{"tag_id":10,"post_id":"59a5b680-7651-440d-8286-901593717cc1"},{"tag_id":7,"post_id":"af9e1bcd-cac4-40f0-84e7-b9013e2882cf"},{"tag_id":2,"post_id":"af9e1bcd-cac4-40f0-84e7-b9013e2882cf"},{"tag_id":7,"post_id":"bc966292-6e4f-4da0-9bca-3803edadbd19"},{"tag_id":1,"post_id":"4588bc07-bdbf-4a27-bbe8-3a81d3ae4400"},{"tag_id":10,"post_id":"4588bc07-bdbf-4a27-bbe8-3a81d3ae4400"},{"tag_id":1,"post_id":"f1d113dc-e056-4376-878f-18ce3306043a"},{"tag_id":1,"post_id":"6fcbdace-8044-4d76-a52b-6125f4c0d015"},{"tag_id":1,"post_id":"230f6f5d-1161-4cdb-9844-47a4014af9e4"},{"tag_id":13,"post_id":"877ccf49-570e-4250-8cc4-8be24114dfc2"},{"tag_id":1,"post_id":"12e92756-f244-471c-ada5-81b6f17889bc"},{"tag_id":1,"post_id":"7849d529-0d25-465f-a56f-e1c681830d8b"},{"tag_id":9,"post_id":"fd91132f-63dd-4673-8776-0d6938e9a758"},{"tag_id":7,"post_id":"03668746-5f30-4079-93cf-00ba2ecc9847"},{"tag_id":3,"post_id":"03668746-5f30-4079-93cf-00ba2ecc9847"},{"tag_id":12,"post_id":"3f78dc36-09f6-4f09-982e-03d07180b196"},{"tag_id":10,"post_id":"3f78dc36-09f6-4f09-982e-03d07180b196"},{"tag_id":5,"post_id":"2baae65d-d243-4a40-b6ae-aa9a7b12e2ef"},{"tag_id":7,"post_id":"2baae65d-d243-4a40-b6ae-aa9a7b12e2ef"},{"tag_id":6,"post_id":"2baae65d-d243-4a40-b6ae-aa9a7b12e2ef"},{"tag_id":1,"post_id":"2baae65d-d243-4a40-b6ae-aa9a7b12e2ef"},{"tag_id":1,"post_id":"1051dd8a-6bc8-423e-98d9-6cc6993d476d"},{"tag_id":1,"post_id":"9a28514f-b873-4d2c-8f7d-b5b3b6ec7b54"},{"tag_id":1,"post_id":"72f2a5ba-171f-4501-ac5f-6ee0152fba8b"},{"tag_id":1,"post_id":"74311567-f3e9-4920-b4ef-d344b4465f58"},{"tag_id":7,"post_id":"74311567-f3e9-4920-b4ef-d344b4465f58"},{"tag_id":1,"post_id":"c56c1592-d9f0-4215-b213-d912bdf42d29"},{"tag_id":1,"post_id":"2b2a6a00-ab03-400e-a4e5-e95b7bc63d95"},{"tag_id":12,"post_id":"de940a83-6934-46bd-bafc-ed24c3b7bb08"},{"tag_id":1,"post_id":"fbaa5899-b4a8-4de4-aad3-accd3245840f"},{"tag_id":1,"post_id":"0f8e3e1a-5ab6-4101-85a9-0ad520d99372"},{"tag_id":13,"post_id":"1df2bacc-0591-4168-9724-c9d9154075b9"},{"tag_id":13,"post_id":"615ed0fa-3132-44d7-b2b0-c06e9dd3867f"},{"tag_id":13,"post_id":"1bbcc27b-a919-4085-b8fb-13972c6d09a0"},{"tag_id":5,"post_id":"fa6f24cc-c3eb-4880-bf7f-77e8fc93b968"},{"tag_id":1,"post_id":"fa6f24cc-c3eb-4880-bf7f-77e8fc93b968"},{"tag_id":7,"post_id":"2b4355d9-bfb3-4f92-bc2b-323249145854"},{"tag_id":9,"post_id":"2b4355d9-bfb3-4f92-bc2b-323249145854"},{"tag_id":4,"post_id":"3685326d-516d-4b11-bfc5-b886ef9aa339"},{"tag_id":4,"post_id":"af19a9ac-9c7a-4c8c-9bc0-08d34c2546ba"},{"tag_id":4,"post_id":"cbd1728d-c185-4adb-b46f-7efa5280f928"},{"tag_id":4,"post_id":"bd6e560e-ba9f-4ce5-a4ab-6b747c711372"},{"tag_id":4,"post_id":"0e91ed7c-8db2-4203-b7bf-b1a899fa5274"},{"tag_id":5,"post_id":"0e91ed7c-8db2-4203-b7bf-b1a899fa5274"},{"tag_id":5,"post_id":"381e0e17-64e3-42e4-a4b6-b259216df0a6"},{"tag_id":9,"post_id":"381e0e17-64e3-42e4-a4b6-b259216df0a6"},{"tag_id":1,"post_id":"381e0e17-64e3-42e4-a4b6-b259216df0a6"},{"tag_id":5,"post_id":"edab574f-cfe4-4f95-8afe-0db906a015d8"},{"tag_id":1,"post_id":"edab574f-cfe4-4f95-8afe-0db906a015d8"},{"tag_id":13,"post_id":"37e6514b-1351-4e4a-b9fd-efa03efe5097"},{"tag_id":2,"post_id":"c50beb53-e089-4ab2-9bde-d0eb2a91c82e"},{"tag_id":10,"post_id":"2c998041-3f46-4481-8c07-11def62db972"},{"tag_id":1,"post_id":"d0d94ae6-97c3-48db-b341-bd173dc92270"},{"tag_id":1,"post_id":"1fa924fa-8e75-4153-83cd-de3667eae737"},{"tag_id":1,"post_id":"750ebb13-c391-4e6a-921a-e8fa2856b923"},{"tag_id":1,"post_id":"370cc512-abdc-4eda-8df2-945babb088fd"},{"tag_id":1,"post_id":"413fe24a-0557-490f-8841-923326a54f4a"},{"tag_id":14,"post_id":"413fe24a-0557-490f-8841-923326a54f4a"},{"tag_id":5,"post_id":"4938af2d-98f1-4eaf-a02b-ec520fde6e0e"},{"tag_id":5,"post_id":"bdc0a35a-694f-413e-b679-800d6f84a242"},{"tag_id":5,"post_id":"b02f6455-0718-454f-887f-4a96b18f64ef"},{"tag_id":10,"post_id":"b02f6455-0718-454f-887f-4a96b18f64ef"},{"tag_id":4,"post_id":"4419f29f-297a-49ea-9fa7-d5e0087503b2"},{"tag_id":7,"post_id":"4419f29f-297a-49ea-9fa7-d5e0087503b2"},{"tag_id":4,"post_id":"d126598f-09bb-4b0e-9972-f9c52074f5a2"},{"tag_id":5,"post_id":"d126598f-09bb-4b0e-9972-f9c52074f5a2"},{"tag_id":5,"post_id":"1baf155e-0d4d-46cd-af32-a567ee2db446"},{"tag_id":9,"post_id":"1baf155e-0d4d-46cd-af32-a567ee2db446"},{"tag_id":7,"post_id":"f24edcc9-4b00-47a6-b097-5afbfb7e0f63"},{"tag_id":5,"post_id":"b15481b1-c8e3-4b34-8642-1dbe15fd5f7b"},{"tag_id":4,"post_id":"b15481b1-c8e3-4b34-8642-1dbe15fd5f7b"},{"tag_id":6,"post_id":"87511efc-04f5-428b-a7aa-87d82b8f6a83"},{"tag_id":13,"post_id":"bc9e66d6-7521-4bd6-ba5b-a023b207414a"},{"tag_id":7,"post_id":"bc9e66d6-7521-4bd6-ba5b-a023b207414a"},{"tag_id":8,"post_id":"fda7db7d-7f62-4153-855a-87027f3c7cfd"},{"tag_id":10,"post_id":"fda7db7d-7f62-4153-855a-87027f3c7cfd"},{"tag_id":1,"post_id":"c00f0ff0-2629-4a06-8e88-b2763448ab04"},{"tag_id":6,"post_id":"3e2e295b-080f-41a9-ad93-6c4116932d08"},{"tag_id":1,"post_id":"3e2e295b-080f-41a9-ad93-6c4116932d08"},{"tag_id":3,"post_id":"51f7f99f-78a2-4844-9aa9-340f86b3483d"},{"tag_id":1,"post_id":"ad4f8327-ba16-436f-8873-51ca2f31d09e"},{"tag_id":10,"post_id":"e1fbddf0-ba96-4486-bcd5-1a9a6534a4e6"},{"tag_id":2,"post_id":"021cadfa-cd44-495e-a030-473ad226f00f"},{"tag_id":1,"post_id":"021cadfa-cd44-495e-a030-473ad226f00f"},{"tag_id":1,"post_id":"109d3c9c-bcae-42b6-b03e-cd47bffecde3"},{"tag_id":14,"post_id":"109d3c9c-bcae-42b6-b03e-cd47bffecde3"},{"tag_id":1,"post_id":"2ed5e073-5c57-4d78-a1da-13de1e03f60d"},{"tag_id":10,"post_id":"9617735a-1580-4696-9609-4dd423f72e4d"},{"tag_id":6,"post_id":"6e265334-6093-4854-9cfb-4a0940406960"},{"tag_id":6,"post_id":"2fc67b76-82b9-4f5b-a296-4e3673c4fb45"},{"tag_id":1,"post_id":"01cae2ee-780a-45d6-8929-e111983abdca"},{"tag_id":10,"post_id":"0d165ae8-5cfa-4e67-9c34-52a962925b4a"},{"tag_id":2,"post_id":"81c2acfc-4bb9-41d6-8d0d-ea487121b3b6"},{"tag_id":10,"post_id":"39fd604e-0498-4f7b-985c-8edd63f07dde"},{"tag_id":7,"post_id":"31a9b33f-0671-4edf-85a9-65fa82638079"},{"tag_id":1,"post_id":"7dad79a2-0571-4f29-84f2-2225f0350985"},{"tag_id":2,"post_id":"7dad79a2-0571-4f29-84f2-2225f0350985"},{"tag_id":14,"post_id":"7dad79a2-0571-4f29-84f2-2225f0350985"},{"tag_id":13,"post_id":"36081bf4-c602-47ae-beea-9dd2533c659e"},{"tag_id":13,"post_id":"ddead0a8-9793-4305-aaa9-2f1ee2abb58c"},{"tag_id":6,"post_id":"227c45a4-594d-4647-9a02-f470d43b25f7"},{"tag_id":10,"post_id":"f6ff63db-be8b-4b3b-818e-88c69c2cfadc"},{"tag_id":5,"post_id":"114625bd-a2e9-471a-990e-b8c0544724b3"},{"tag_id":10,"post_id":"85753968-4f09-4e02-bcd3-501fcdab3278"},{"tag_id":7,"post_id":"6cfecdf7-7faf-4532-aeff-28b425326e56"},{"tag_id":7,"post_id":"18d024a9-c3ba-492d-b014-5acee1ff7215"},{"tag_id":10,"post_id":"f3e022ef-88c7-4982-ae2d-1358ded549a9"},{"tag_id":13,"post_id":"b819b2d7-b26f-40a4-b0ab-4516909e8548"},{"tag_id":7,"post_id":"48fe61fa-9328-4d5c-8f32-e2156fce53e2"},{"tag_id":14,"post_id":"f59859e8-da3e-4194-981d-69468f3b0d4f"},{"tag_id":14,"post_id":"0262e1e1-27e5-49e5-bd69-d1213bffdb7a"},{"tag_id":14,"post_id":"09c777fe-b4bb-4073-b800-56c903764699"},{"tag_id":5,"post_id":"4de3daec-0f0f-4b17-9ddb-44635915665e"},{"tag_id":8,"post_id":"dad66e80-4957-4928-9691-f0ba19618869"},{"tag_id":4,"post_id":"a214994f-998d-4b17-9eda-32ad40b62847"},{"tag_id":6,"post_id":"53a1a627-983e-461c-8211-4dc12ba21745"},{"tag_id":1,"post_id":"53a1a627-983e-461c-8211-4dc12ba21745"},{"tag_id":8,"post_id":"9d796ced-3e31-42d7-a2a5-1d5914f1979a"},{"tag_id":1,"post_id":"c903c414-40a9-432f-b4d3-edfb544d425c"},{"tag_id":2,"post_id":"d831b783-6130-41d8-a92d-856ea78ac89c"},{"tag_id":3,"post_id":"2bc46956-c317-44a2-a112-b39db7d286a9"},{"tag_id":10,"post_id":"fc147461-3e0f-44f6-bf5b-3eff0aa837f0"},{"tag_id":10,"post_id":"f6c6c4cf-b801-4504-81cf-bdf381e616ca"},{"tag_id":5,"post_id":"62b9be8e-8df7-49fb-9f60-d644c2be8e9c"},{"tag_id":2,"post_id":"3e48503a-f2cf-4afe-8cf9-c561539bd393"},{"tag_id":13,"post_id":"e8d39da4-2299-4384-af17-945f8666474c"},{"tag_id":13,"post_id":"98953978-7539-41d5-9f44-d8057976473a"},{"tag_id":12,"post_id":"fbc62e33-951c-4322-bccd-0b1d7835b1b7"},{"tag_id":4,"post_id":"43d7f0ea-9afc-4ea7-9c43-62d574fb533f"},{"tag_id":1,"post_id":"43d7f0ea-9afc-4ea7-9c43-62d574fb533f"},{"tag_id":10,"post_id":"73fc1d40-d5ef-4321-a09c-70e503fb0b94"},{"tag_id":7,"post_id":"b789df7e-8071-4fac-8587-57ce03b9811a"},{"tag_id":6,"post_id":"b789df7e-8071-4fac-8587-57ce03b9811a"},{"tag_id":6,"post_id":"cbf153fd-bdaa-4831-b9ce-ad13fd99536a"},{"tag_id":9,"post_id":"ffdc18cc-651e-415c-ba1a-f286c1460ef1"},{"tag_id":2,"post_id":"a26bb194-f177-46f5-83da-facf5894e8e2"},{"tag_id":5,"post_id":"a26bb194-f177-46f5-83da-facf5894e8e2"},{"tag_id":7,"post_id":"a8dea587-aec4-47ca-9315-be1c5e333c5d"},{"tag_id":7,"post_id":"f1f1aa7d-fb02-4b54-bece-9aab22d7b797"},{"tag_id":13,"post_id":"ccb6479b-d5ee-4c54-8fcb-6c001c609397"},{"tag_id":13,"post_id":"c8cebfd3-7a89-4dd9-a8fe-e057705abcb7"},{"tag_id":10,"post_id":"3d6ba9d8-3575-4066-b1a4-9b2dd4bc0506"},{"tag_id":7,"post_id":"e1cebc5f-43db-46cc-be63-789f3b64aff7"},{"tag_id":14,"post_id":"f2f2a399-17f3-4866-8b86-6859183a6586"},{"tag_id":6,"post_id":"2f651e25-6374-4511-979b-c23a9fb6406b"},{"tag_id":6,"post_id":"eaf9eda4-c107-41c6-a3dc-3225b35ecd12"},{"tag_id":6,"post_id":"09155b6b-ceeb-4416-baaa-04299c6970bd"},{"tag_id":1,"post_id":"09155b6b-ceeb-4416-baaa-04299c6970bd"},{"tag_id":1,"post_id":"7875fd49-8c7c-4557-a611-4e663979078d"},{"tag_id":1,"post_id":"85ea1e67-05c2-4a2a-be40-e21b26b9ae6a"},{"tag_id":14,"post_id":"76a570a7-36fe-42e5-8805-57f783afbe02"},{"tag_id":14,"post_id":"c4d7be22-4772-4cfc-b912-c41a9d9dcaba"},{"tag_id":4,"post_id":"034cf4fd-cdd5-4a23-8759-3b17e3807a7a"},{"tag_id":6,"post_id":"034cf4fd-cdd5-4a23-8759-3b17e3807a7a"},{"tag_id":4,"post_id":"73e616a4-0a4a-47a1-a97c-80ecba1f7319"},{"tag_id":1,"post_id":"73e616a4-0a4a-47a1-a97c-80ecba1f7319"},{"tag_id":2,"post_id":"7fba557e-7fb4-4693-a79b-6d154f6b3924"},{"tag_id":6,"post_id":"7fba557e-7fb4-4693-a79b-6d154f6b3924"},{"tag_id":6,"post_id":"2e56728e-68e8-47ea-aa70-2a97595dc16e"},{"tag_id":3,"post_id":"2e56728e-68e8-47ea-aa70-2a97595dc16e"},{"tag_id":5,"post_id":"d80f4a55-6924-4893-addf-d204e37dcb05"},{"tag_id":10,"post_id":"d0849c5c-4646-4faf-b9d8-cb6d50eddad4"},{"tag_id":13,"post_id":"d64076ea-5dd2-4256-83d1-2193124f9e60"},{"tag_id":6,"post_id":"a18b9c80-5f9c-410f-b264-a0535d2cd7fb"},{"tag_id":2,"post_id":"31d67685-9c54-4430-8ba0-b8f8e167510f"},{"tag_id":2,"post_id":"63319a5b-a0d2-42a5-a06b-ade5a1081cc7"},{"tag_id":6,"post_id":"f1538d9f-d573-4cda-9846-e29c4329f8b0"},{"tag_id":5,"post_id":"f1538d9f-d573-4cda-9846-e29c4329f8b0"},{"tag_id":2,"post_id":"fc1a4cee-4641-4220-9fc8-ae0169f6279c"},{"tag_id":8,"post_id":"d63df9b7-5cc6-4091-b42a-3a8c0c548517"},{"tag_id":10,"post_id":"2634783f-2b60-4d70-9ca5-db396d520576"},{"tag_id":10,"post_id":"68d775d8-bf4f-4873-b533-ce18cf1ca599"},{"tag_id":1,"post_id":"9637e458-6c2d-412b-b915-92a95a12cc37"},{"tag_id":10,"post_id":"9def5255-6e02-4201-b8d5-c29436b3d12d"},{"tag_id":10,"post_id":"18858a91-6592-461e-ae0d-3866c194a008"},{"tag_id":10,"post_id":"7a86a013-7ebc-4486-a36f-72ef87e858d0"},{"tag_id":10,"post_id":"4b0d3491-425a-4610-b009-91c2bd27cc89"},{"tag_id":6,"post_id":"5711ae92-a06d-4001-abe5-bc90f2aa37b0"},{"tag_id":6,"post_id":"35e3b882-cfa5-4de3-a9e1-368c30c8876d"},{"tag_id":5,"post_id":"35e3b882-cfa5-4de3-a9e1-368c30c8876d"},{"tag_id":7,"post_id":"00d81fe4-a4a9-48c0-897f-637154028e22"},{"tag_id":10,"post_id":"7f016481-3746-4b40-8183-5be6c9ac9ce9"},{"tag_id":6,"post_id":"ed9074b5-4033-4a64-899a-7c9569f477ca"},{"tag_id":7,"post_id":"50ac53e5-8ca7-4c3d-a28a-a3789a1f3a15"},{"tag_id":1,"post_id":"a4d938a5-7adc-4269-8eec-55d5c2a222e5"},{"tag_id":1,"post_id":"e17b2be6-4b7a-4f20-9da2-097666e04e8a"},{"tag_id":9,"post_id":"40c6fce7-fe35-409d-a699-a70693953628"},{"tag_id":6,"post_id":"40c6fce7-fe35-409d-a699-a70693953628"},{"tag_id":5,"post_id":"477d29d3-4106-41e3-8022-456768ddd6b0"},{"tag_id":14,"post_id":"3fd7ac1d-d53d-4198-a662-945c6fe15756"},{"tag_id":6,"post_id":"bb1ce673-db0d-48be-8e84-567458da16bd"},{"tag_id":10,"post_id":"03f2e264-d1f3-4673-8865-b266e7e70580"},{"tag_id":12,"post_id":"8dd89270-3cd7-4216-97ce-dc712d3995ca"},{"tag_id":2,"post_id":"8dd89270-3cd7-4216-97ce-dc712d3995ca"},{"tag_id":12,"post_id":"3cd2ea61-be47-4966-a7b2-ad8c39884231"},{"tag_id":2,"post_id":"3cd2ea61-be47-4966-a7b2-ad8c39884231"},{"tag_id":12,"post_id":"cbf59201-80f9-47b5-934e-7d65c6fafa8d"},{"tag_id":2,"post_id":"95702ad7-6135-4278-b737-bcfc50a1152e"},{"tag_id":6,"post_id":"65a85990-3924-4088-8866-9860cf8045b1"},{"tag_id":7,"post_id":"332495e1-19e0-454f-b59e-59d205183221"},{"tag_id":5,"post_id":"09bc52c9-4c36-49ac-8f7b-40759f10d6fe"},{"tag_id":6,"post_id":"cd3e099c-da22-42cb-8bec-9d6143822f0e"},{"tag_id":6,"post_id":"4db636fd-d165-4d7d-beb8-96e9c8e9d745"},{"tag_id":1,"post_id":"4db636fd-d165-4d7d-beb8-96e9c8e9d745"},{"tag_id":6,"post_id":"37daa003-330d-42dd-8666-fc51151edeea"},{"tag_id":1,"post_id":"96322811-9103-4f65-8f46-71493e82be79"},{"tag_id":2,"post_id":"96322811-9103-4f65-8f46-71493e82be79"},{"tag_id":7,"post_id":"96322811-9103-4f65-8f46-71493e82be79"},{"tag_id":1,"post_id":"6e9734a0-0b2e-4127-8f0d-c5339ea01d47"},{"tag_id":3,"post_id":"5da82e54-0ab7-4d15-9656-ed4d8acb755c"},{"tag_id":2,"post_id":"c06a21f4-38e2-4e8b-a5e0-cf6ac4cbd9d6"},{"tag_id":10,"post_id":"a6dca067-ea25-4eaa-9e1a-20c44b162a84"}],"tags":[{"id":1,"name":"Build","slug":"build","description":"Posts about build"},{"id":2,"name":"Testing","slug":"testing","description":"Posts about Testing"},{"id":3,"name":"Source Control","slug":"sourcecontrol","description":"Posts about Source Control"},{"id":4,"name":"Docker","slug":"docker","description":"Posts about Docker"},{"id":5,"name":"DevOps","slug":"devops","description":"Posts about DevOps"},{"id":6,"name":"Release Management","slug":"releasemanagement","description":"Posts about Release Manaagement"},{"id":7,"name":"Development","slug":"development","description":"Posts about Development"},{"id":8,"name":"TFS Config","slug":"tfsconfig","description":"Posts about TFS Config"},{"id":9,"name":"Cloud","slug":"cloud","description":"Posts about Cloud"},{"id":10,"name":"ALM","slug":"alm","description":"Posts about ALM"},{"id":11,"name":"AppInsights","slug":"appinsights","description":"Posts about AppInsights"},{"id":12,"name":"TFS API","slug":"tfsapi","description":"Posts about TFS API"},{"id":13,"name":"News","slug":"news","description":"Posts about News"},{"id":14,"name":"Lab Management","slug":"labmanagement","description":"Posts about Lab Management"},{"id":15,"name":"General","slug":"general","description":"Posts about General topics"}]}}]}